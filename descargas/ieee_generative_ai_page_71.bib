@ARTICLE{11002570,
  author={Yin, Qing and Wang, Zhihua and Bai, Liang and Song, Yunya and Xu, Dongling and Yang, Xian},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Towards Trustworthy Dialogue Systems with Advanced Out-of-Scope Intent Detection Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={The growing demand for trustworthy dialogue systems emphasizes the need for consistently accurate responses to user inputs. The first step in developing a trustworthy dialogue system is detecting user inputs with the out-of-scope (OOS) intent. Advanced research on OOS intent detection enhances effectiveness by using data augmentation to generate numerous artificial OOS samples from a limited set of true OOS data, modelling its distribution for training. However, data augmentation presents challenges, including higher costs, increased time, and a greater risk of overfitting. Additionally, current studies treat the OOS intent as a homogeneous category equivalent to known intents within a classification framework, overlooking the inherent diversity of OOS intents. To tackle these challenges, we introduce a novel method called Anchor-Integrated Dynamic Out-of-scope Intent Learning (AIDOIL), which integrates the selected anchor to represent the OOS intent adapting to diverse inputs dynamically. The intent representations transform the global classification problem into a matching task that determines if a user input aligns with each intent. This eliminates the necessity to augment OOS data and accommodate the diversity of OOS intents through dynamic representation learning. We conducted extensive experiments on three public dialogue datasets, demonstrating that AIDOIL achieves an average 7.21% improvement in OOS detection accuracy, while maintaining an acceptable increase in training time.},
  keywords={Training;Data augmentation;Artificial intelligence;Meteorology;Oral communication;Data models;Virtual assistants;Testing;Safety;Representation learning;Trustworthy Dialogue Systems;Out-of-scope Intent Detection;Anchor-Integrated Dynamic OOS Intent Learning},
  doi={10.1109/TAI.2025.3567212},
  ISSN={2691-4581},
  month={},}@INPROCEEDINGS{11160519,
  author={Niu, Kangjia},
  booktitle={2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Autonomous Driving Planning Algorithm Based on Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={01-05},
  abstract={The current mainstream imitation learning framework learns driving strategies through expert data. Mainstream planners usually regress unimodal trajectories from ego vehicle queries. However, this framework mode of regressing unimodal trajectories does not take into account the uncertainty and multimodal nature of driving behavior. In addition, existing planners solely utilize data from the preceding few seconds of the current frame to forecast future trajectories, neglecting the interrelationships between consecutive predictions, and the generated trajectories exhibit temporal inconsistency. Robotics research has empirically validated diffusion-based frameworks as robust generative paradigms for decision-making tasks, particularly in scenarios requiring multi-modal behavior synthesis and probabilistic reasoning capabilities. It directly samples multiple reasonable action spaces from Gaussian distributions through an iterative denoising process, generates multimodal trajectories, meets the diversity of driving behaviors, and provides an effective method for motion planning in autonomous vehicle control frameworks. However, specifically in the field of autonomous driving, human drivers follow established driving patterns and dynamically adjust them according to real-time traffic conditions, and have high requirements for real-time performance, which is obviously difficult to meet with the original diffusion strategy. Therefore, we introduce a large number of fixed anchor trajectories in the diffusion strategy, construct a diffusion process by adding Gaussian noise to the anchor trajectories, and embed the previous driving pattern into the diffusion strategy. In order to enable the model to learn to denoise from the anchored Gaussian distribution to the desired driving strategy, the diffusion process is further truncated during training, only a small amount of Gaussian noise is added to the anchor point, and the denoising process initializes from noisy anchor trajectories rather than the standard Gaussian distribution. By adopting the proposed truncated diffusion strategy, the denoising steps required for reasoning are greatly reduced, and real-time, high-quality multimodal planning is achieved.},
  keywords={Training;Gaussian noise;Imitation learning;Noise reduction;Diffusion processes;Gaussian distribution;Real-time systems;Trajectory;Planning;Autonomous vehicles;autonomous driving;imitation learning;diffusion model},
  doi={10.1109/AIEA66061.2025.11160519},
  ISSN={},
  month={Aug},}@ARTICLE{10330096,
  author={Liu, Yinqiu and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Cui, Shuguang and Shen, Xuemin and Zhang, Ping},
  journal={IEEE Network}, 
  title={Optimizing Mobile-Edge AI-Generated Everything (AIGX) Services by Prompt Engineering: Fundamental, Framework, and Case Study}, 
  year={2024},
  volume={38},
  number={5},
  pages={220-228},
  abstract={As the next-generation paradigm for content creation, AI-Generated Content (AIGC), i.e., generating content automatically by Generative AI (GAI) based on user prompts, has gained great attention and success recently. With the ever-increasing power of GAI, especially the emergence of Pretrained Foundation Models (PFMs) that contain billions of parameters and prompt engineering methods (i.e., finding the best prompts for the given task), the application range of AIGC is rapidly expanding, covering various forms of information for human, systems, and networks, such as network designs, channel coding, and optimization solutions. In this article, we present the concept of mobile-edge AI-Generated Everything (AIGX). Specifically, we first review the building blocks of AIGX, the evolution from AIGC to AIGX, as well as practical AIGX applications. Then, we present a unified mobile-edge AIGX framework, which employs edge devices to provide PFM-empowered AIGX services and optimizes such services via prompt engineering. More importantly, we demonstrate that suboptimal prompts lead to poor generation quality, which adversely affects user satisfaction, edge network performance, and resource utilization. Accordingly, we conduct a case study, showcasing how to train an effective prompt optimizer using ChatGPT and investigating how much improvement is possible with prompt engineering in terms of user experience, quality of generation, and network performance.},
  keywords={Optimization;Visualization;Videos;Bandwidth;Generative adversarial networks;Artificial intelligence;Prompt engineering;Edge computing;AI-generated Everything (AIGX);Prompt Engineering;Edge Networks;Optimization;Efficiency},
  doi={10.1109/MNET.2023.3335255},
  ISSN={1558-156X},
  month={Sep.},}@ARTICLE{10002426,
  author={Xu, Yonghao and He, Fengxiang and Du, Bo and Tao, Dacheng and Zhang, Liangpei},
  journal={IEEE Transactions on Multimedia}, 
  title={Self-Ensembling GAN for Cross-Domain Semantic Segmentation}, 
  year={2023},
  volume={25},
  number={},
  pages={7837-7850},
  abstract={Deep neural networks (DNNs) have greatly contributed to the performance gains in semantic segmentation. Nevertheless, training DNNs generally requires large amounts of pixel-level labeled data, which is expensive and time-consuming to collect in practice. To mitigate the annotation burden, this paper proposes a self-ensembling generative adversarial network (SE-GAN) exploiting cross-domain data for semantic segmentation. In SE-GAN, a teacher network and a student network constitute a self-ensembling model for generating semantic segmentation maps, which together with a discriminator, forms a GAN. Despite its simplicity, we find SE-GAN can significantly boost the performance of adversarial training and enhance the stability of the model, the latter of which is a common barrier shared by most adversarial training-based methods. We theoretically analyze SE-GAN and provide an $\mathcal {O}(1/\sqrt{N})$ generalization bound ($N$ is the training sample size), which suggests controlling the discriminator's hypothesis complexity to enhance the generalizability. Accordingly, we choose a simple network as the discriminator. Extensive and systematic experiments in two standard settings demonstrate that the proposed method significantly outperforms current state-of-the-art approaches.},
  keywords={Training;Semantic segmentation;Generators;Adaptation models;Visualization;Generative adversarial networks;Semantics;Deep learning;domain adaptation;semantic segmentation;adversarial learning},
  doi={10.1109/TMM.2022.3229976},
  ISSN={1941-0077},
  month={},}@ARTICLE{10632877,
  author={Apolo, Yvonne and Michael, Katina},
  journal={IEEE Transactions on Technology and Society}, 
  title={Beyond A Reasonable Doubt? Audiovisual Evidence, AI Manipulation, Deepfakes, and the Law}, 
  year={2024},
  volume={5},
  number={2},
  pages={156-168},
  abstract={THE CAPTURE is a mystery thriller series, that completed its second season on Peacock and BBC One. The British television drama revolves around the alteration of direct audiovisual evidence on the command of a special unit that believes there is enough circumstantial evidence to either convict or acquit an individual of a felony. Based on the plot of the television series, this paper explores the potential for a variety of AI-enabled applications to be used in the course of criminal proceedings. The implications of evidence tampering are considered through AI manipulation toward the realization that deepfake evidence may well be admitted in court dependent on the human decision-maker. Will the future demand the interpolation of visual evidence for high profile criminal cases, and what does the existence of Generative AI and deepfakes mean for the forensic analysis of audiovisual evidence? After contemplating the socio-technical plausibility of the central premise of THE CAPTURE, this paper then turns to its legal implications. Drawing on examples from U.S. and Australian legal frameworks, the paper considers the consequences of AI-corrected, augmented or generated audiovisual evidence on three facets of natural justice: (1) the presumption of innocence; (2) the fair trial; and (3) lawyers’ ethical duties of competence and to the administration of justice. The key takeaways of the paper are that: (1) deepfake evidence will continue to proliferate; (2) that the law will need to address both the substantive and procedural impacts of such evidence, and (3) that the legal profession must continue to educate its lawyers and practitioners, and associated stakeholders, of the nature, uses and risks posed by deepfake audiovisual artefacts to maintain public trust in the legal system.},
  keywords={Deepfakes;Ethics;Visualization;Interpolation;TV;Media;Audio-visual systems;Generative AI;Criminal law;Forensics;Artificial intelligence;Audiovisual recording technology;GenAI;deepfakes;evidence;law enforcement;criminal investigations;forensics;auditing tools;law;lawyers;courts;fair trial},
  doi={10.1109/TTS.2024.3427816},
  ISSN={2637-6415},
  month={June},}@ARTICLE{10363360,
  author={Cao, Yansong and Wang, Yutong and Wang, Jiangong and Tian, Yonglin and Wang, Xiao and Wang, Fei-Yue},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Parameter Identification and Refinement for Parallel PCB Inspection in Cyber–Physical–Social Systems}, 
  year={2024},
  volume={11},
  number={3},
  pages={3978-3987},
  abstract={Replacing manual inspection, automated optical inspection (AOI) equipment is widely used in printed circuit board (PCB) factories for automatic PCB defect segmentation. However, parameter refinement of AOI devices has gradually become an efficiency bottleneck in AOI usage, posing a highly challenging task. Since a large number of AOI parameters and different types of inspected objects make timely proper parameter refinement for clear images quite difficult. Considering this, we propose the concept of parallel PCB inspection in cyber–physical–social systems (CPSSs). Based on artificial systems, computational experiments, and parallel execution (ACP) theory with automatic parameter identification and refinement, we perform descriptive intelligence to build an artificial imaging system, obtain knowledge about the mapping relationships of parameter settings and imaging results, and realize automatic parameter identification given image input; conduct predictive intelligence to obtain image quality assessment results and maximize quality score for refinement strategies; and carry out prescriptive intelligence to guide parameter refinement for better imaging. This system could guide engineers proactively with constructive suggestions on parameter refinement when imaging failures occur, greatly reducing the training cost of engineers while improving work efficiency and work quality. To validate that our parallel PCB inspection could perform automatic AOI results evaluation without human participation, we evaluate it on distortion-free and different distortion images and confirm image quality score is positively associated with segmentation accuracy.},
  keywords={Imaging;Inspection;Image segmentation;Image quality;Hardware;Training;Software;Automated optical inspection (AOI);parallel printed circuit board (PCB) inspection;parameter identification;parameter refinement},
  doi={10.1109/TCSS.2023.3330762},
  ISSN={2329-924X},
  month={June},}@ARTICLE{9856683,
  author={Doan, Bao Gia and Xue, Minhui and Ma, Shiqing and Abbasnejad, Ehsan and C. Ranasinghe, Damith},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems}, 
  year={2022},
  volume={17},
  number={},
  pages={3816-3830},
  abstract={Deep neural networks (DNNs), regardless of their impressive performance, are vulnerable to attacks from adversarial inputs and, more recently, Trojans to misguide or hijack the decision of the model. We expose the existence of an intriguing class of spatially bounded, physically realizable, adversarial examples— Universal NaTuralistic adversarial paTches—we call TnTs, by exploring the super set of the spatially bounded adversarial example space and the natural input space within generative adversarial networks. Now, an adversary can arm themselves with a patch that is naturalistic, less malicious-looking, physically realizable, highly effective—achieving high attack success rates, and universal. A TnT is universal because any input image captured with a TnT in the scene will: i) misguide a network (untargeted attack); or ii) force the network to make a malicious decision (targeted attack). Interestingly, now, an adversarial patch attacker has the potential to exert a greater level of control—the ability to choose a location independent, natural-looking patch as a trigger in contrast to being constrained to noisy perturbations—an ability is thus far shown to be only possible with Trojan attack methods needing to interfere with the model building processes to embed a backdoor at the risk discovery; but, still realize a patch deployable in the physical world. Through extensive experiments on the large-scale visual classification task, ImageNet with evaluations across its entire validation set of 50,000 images, we demonstrate the realistic threat from TnTs and the robustness of the attack. We show a generalization of the attack to create patches achieving higher attack success rates than existing state-of-the-art methods. Our results show the generalizability of the attack to different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural networks such as WideResnet50, Inception-V3 and VGG-16.},
  keywords={Trojan horses;Generative adversarial networks;Task analysis;Perturbation methods;Training;Neural networks;Deep learning;Artificial intelligence;machine learning;adversarial machine learning neural networks;artificial neural networks;convolutional neural networks},
  doi={10.1109/TIFS.2022.3198857},
  ISSN={1556-6021},
  month={},}@ARTICLE{9684694,
  author={Rodriguez-Pardo, Carlos and Garces, Elena},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={SeamlessGAN: Self-Supervised Synthesis of Tileable Texture Maps}, 
  year={2023},
  volume={29},
  number={6},
  pages={2914-2925},
  abstract={Real-time graphics applications require high-quality textured materials to convey realism in virtual environments. Generating these textures is challenging as they need to be visually realistic, seamlessly tileable, and have a small impact on the memory consumption of the application. For this reason, they are often created manually by skilled artists. In this work, we present SeamlessGAN, a method capable of automatically generating tileable texture maps from a single input exemplar. In contrast to most existing methods, focused solely on solving the synthesis problem, our work tackles both problems, synthesis and tileability, simultaneously. Our key idea is to realize that tiling a latent space within a generative network trained using adversarial expansion techniques produces outputs with continuity at the seam intersection that can then be turned into tileable images by cropping the central area. Since not every value of the latent space is valid to produce high-quality outputs, we leverage the discriminator as a perceptual error metric capable of identifying artifact-free textures during a sampling process. Further, in contrast to previous work on deep texture synthesis, our model is designed and optimized to work with multi-layered texture representations, enabling textures composed of multiple maps such as albedo, normals, etc. We extensively test our design choices for the network architecture, loss function, and sampling parameters. We show qualitatively and quantitatively that our approach outperforms previous methods and works for textures of different types.},
  keywords={Measurement;Crops;Training;Semantics;Generative adversarial networks;Estimation;Virtual environments;Artificial intelligence;artificial neural network;machine vision;image texture;graphics;computational photography},
  doi={10.1109/TVCG.2022.3143615},
  ISSN={1941-0506},
  month={June},}@ARTICLE{9627923,
  author={Yang, Xi and Wang, Xiaoqi and Wang, Nannan and Gao, Xinbo},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={SRDN: A Unified Super-Resolution and Motion Deblurring Network for Space Image Restoration}, 
  year={2022},
  volume={60},
  number={},
  pages={1-11},
  abstract={Space target super-resolution (SR) is a domain-specific single image SR problem aiming to help distinguish the satellite and spacecrafts from numerous space debris. Compared to the other object SR problem, images for space target are always in low quality with varies of degradation condition, as a result of long distance and motion blur, which significantly reduces the manual classification reliability, especially for these small targets, e.g., satellite payloads. To address this challenge, we present an end-to-end SR and deblurring network (SRDN). Concretely, focusing on the low-resolution (LR) space target images with blind motion blur, we integrate the SR and deblur function together, improving the image quality by a unified generative adversarial network (GAN)-based framework. We implement a deblur module by using contrastive learning to extract degradation feature and add symmetrical downsampling and upsampling modules to the SR network in order to restore texture information, while shortcut connections are redesigned to maintain the global similarity. Extensive experiments on the public satellite dataset, BUAA-SID-share1.5, demonstrate that our network outperforms the state-of-the-art SR and deblur methods.},
  keywords={Feature extraction;Target recognition;Degradation;Image resolution;Superresolution;Generative adversarial networks;Image restoration;Artificial intelligence;artificial neural networks;high-resolution (HR) imaging;image denoising},
  doi={10.1109/TGRS.2021.3131264},
  ISSN={1558-0644},
  month={},}@ARTICLE{10612836,
  author={Kaleem, Zeeshan and Orakzai, Farooq Alam and Ishaq, Waqar and Latif, Kamran and Zhao, Jun and Jamalipour, Abbas},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Emerging Trends in UAVs: From Placement, Semantic Communications to Generative AI for Mission-Critical Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Unmanned Aerial Vehicles (UAVs) have gained popularity across academia and various industries due to their ability to operate in challenging and complex environments. They are particularly valuable in emergency response scenarios where conventional communication infrastructure is unavailable. However, deploying UAVs in such situations poses challenges, requiring intelligent decision-making to meet mission-critical network (MCN) constraints such as latency, quality-of-service, and reliability. To overcome those challenges, this paper aims to provide a comprehensive overview ranging from knowledge driven (Semantic) approaches to generative artificial intelligence (GAI) by integrating UAVs into MCNs. Initially, existing technologies used in UAV-assisted communication are reviewed. While previous literature often emphasizes UAVs’ general applications, it tends to overlook their specific role in MCN scenarios and lacks depth in discussing emerging technologies. Therefore, state-of-the-art data-driven and intelligent future approaches are presented as key enablers for future UAV-assisted MCNs. Representative technologies include semantic communication, Native AI, GAI, joint sensing and communication (JSAC), open-radio access networks (O-RAN), and digital twins. For instance, these technologies prove beneficial when traditional networks fail, allowing UAVs equipped with advanced intelligent systems to establish resilient links, collect real-time data, transmit meaningful information with minimal delay, and optimize resource allocation. Finally, research challenges are outlined, and potential research directions are proposed to encourage further investigation.},
  keywords={Sensors;Mission critical systems;Autonomous aerial vehicles;Artificial intelligence;Semantics;Open RAN;Internet of Things;Unmanned Aerial Vehicles (UAVs);Federated learning;Generative AI;Native AI;O-RAN;Sensing},
  doi={10.1109/TCE.2024.3434971},
  ISSN={1558-4127},
  month={},}@INPROCEEDINGS{10099110,
  author={Cheng, Yihang and Zhang, Lan and Li, Anran},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
  title={GFL: Federated Learning on Non-IID Data via Privacy-Preserving Synthetic Data}, 
  year={2023},
  volume={},
  number={},
  pages={61-70},
  abstract={Federated learning (FL) enables large amounts of participants to construct a global learning model, while storing training data privately at local client devices. A fundamental issue in FL systems is the susceptibility to the highly skewed distributed data. A series of methods have been proposed to mitigate the Non-IID problem by limiting the distances between local models and the global model, but they cannot address the root cause of skewed data distribution eventually. Some methods share extra samples from the server to clients, which requires comprehensive data collection by the server and may raise potential privacy risks. In this work, we propose an efficient and adaptive framework, named Generative Federated Learning (GFL), to solve the skewed data problem in FL systems in a privacy-friendly way. We introduce Generative Adversarial Networks (GAN) into FL to generate synthetic data, which can be used by the server to balance data distributions. To keep the distribution and membership of clients' data private, the synthetic samples are generated with random distributions and protected by a differential privacy mechanism. The results show that GFL significantly outperforms existing approaches in terms of achieving more accurate global models (e.g., 17% - 50% higher accuracy) as well as building global models with faster convergence speed without increasing much computation or communication costs.},
  keywords={Pervasive computing;Privacy;Adaptation models;Costs;Federated learning;Computational modeling;Generative adversarial networks;Federated Learning;Non-IID;Membership In-ference Attack},
  doi={10.1109/PERCOM56429.2023.10099110},
  ISSN={2474-249X},
  month={March},}@INPROCEEDINGS{9956115,
  author={Miranda, Miro and Drees, Lukas and Roscher, Ribana},
  booktitle={2022 26th International Conference on Pattern Recognition (ICPR)}, 
  title={Controlled Multi-modal Image Generation for Plant Growth Modeling}, 
  year={2022},
  volume={},
  number={},
  pages={5118-5124},
  abstract={Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.},
  keywords={Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis},
  doi={10.1109/ICPR56361.2022.9956115},
  ISSN={2831-7475},
  month={Aug},}@INPROCEEDINGS{10186698,
  author={Eskandar, George and Farag, Youssef and Yenamandra, Tarun and Cremers, Daniel and Guirguis, Karim and Yang, Bin},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Urban-StyleGAN: Learning to Generate and Manipulate Images of Urban Scenes}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={A promise of Generative Adversarial Networks (GANs) is to provide cheap photorealistic data for training and validating AI models in autonomous driving. Despite their huge success, their performance on complex images featuring multiple objects is understudied. While some frameworks produce high-quality street scenes with little to no control over the image content, others offer more control at the expense of high-quality generation. A common limitation of both approaches is the use of global latent codes for the whole image, which hinders the learning of independent object distributions. Motivated by SemanticStyleGAN (SSG), a recent work on latent space disentanglement in human face generation, we propose a novel framework, Urban-StyleGAN, for urban scene generation and manipulation. We find that a straightforward application of SSG leads to poor results because urban scenes are more complex than human faces. To provide a more compact yet disentangled latent representation, we develop a class grouping strategy wherein individual classes are grouped into super-classes. Moreover, we employ an unsupervised latent exploration algorithm in the $\mathcal{S}$-space of the generator and show that it is more efficient than the conventional ${\mathcal{W}^ + }$-space in controlling the image content. Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.},
  keywords={Training;Image quality;Codes;Semantic segmentation;Process control;Controllability;Generative adversarial networks;GANs;Image Manipulation and Editing;Semantic Prior;Disentanglement;Latent Space Exploration},
  doi={10.1109/IV55152.2023.10186698},
  ISSN={2642-7214},
  month={June},}@ARTICLE{10838598,
  author={Fan, Yixin and Wu, Jun},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={GAI-AntiCopy: Infrequent Transformation Aided Accuracy-Consistent Copyright Protection for Generative AI Instructions in NGN}, 
  year={2025},
  volume={11},
  number={2},
  pages={1013-1023},
  abstract={Generative artificial intelligence (GAI) brings an unprecedented revolution to the next-generation networks (NGN) from resource allocation to network traffic monitoring. With its powerful creative content generation capabilities, GAI significantly enhances the interaction and quality of customized services in NGN. Currently, benefiting from the thriving GAI services, it is possible to build personalized GAIs through designing GAI instructions without the need for training models from scratch. Meanwhile, infringements like pirating are emerging, necessitating effective copyright protection schemes. However, current schemes suffer from an unacceptable decrease in task processing accuracy when applied to GAIs, and the success rate of watermarking is extremely low on GAI instructions. Therefore, we propose an infrequent transformation aided accuracy-consistent copyright protection scheme for GAI instructions. We first build a comprehensive GAI instruction copyright protection system for NGN, designing a complete watermarking and verification mechanism. Additionally, we integrate copyright watermark messages with the syntactic features of GAI instructions to select the embedding positions. Watermarks are embedded through emphasis and passivization, which are infrequent transformations that minimize semantic distortion. Finally, we conduct experiments on real GAI instructions datasets and compare our scheme with existing works to demonstrate that ours effectively realizes accuracy-consistent copyright protection for GAI instructions in NGN.},
  keywords={Watermarking;Copyright protection;Next generation networking;Semantics;Artificial intelligence;Steganography;Training;Protection;Linguistics;Accuracy;Copyright protection;generative AI;next-generation network;sentence transformation},
  doi={10.1109/TCCN.2025.3528893},
  ISSN={2332-7731},
  month={April},}@ARTICLE{9189772,
  author={Guarnera, Luca and Giudice, Oliver and Battiato, Sebastiano},
  journal={IEEE Access}, 
  title={Fighting Deepfake by Exposing the Convolutional Traces on Images}, 
  year={2020},
  volume={8},
  number={},
  pages={165085-165098},
  abstract={Advances in Artificial Intelligence and Image Processing are changing the way people interacts with digital images and video. Widespread mobile apps like FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to produce extreme transformations on human face photos such gender swap, aging, etc. The results are utterly realistic and extremely easy to be exploited even for non-experienced users. This kind of media object took the name of Deepfake and raised a new challenge in the multimedia forensics field: the Deepfake detection challenge. Indeed, discriminating a Deepfake from a real image could be a difficult task even for human eyes but recent works are trying to apply the same technology used for generating images for discriminating them with preliminary good results but with many limitations: employed Convolutional Neural Networks are not so robust, demonstrate to be specific to the context and tend to extract semantics from images. In this paper, a new approach aimed to extract a Deepfake fingerprint from images is proposed. The method is based on the Expectation-Maximization algorithm trained to detect and extract a fingerprint that represents the Convolutional Traces (CT) left by GANs during image generation. The CT demonstrates to have high discriminative power achieving better results than state-of-the-art in the Deepfake detection task also proving to be robust to different attacks. Achieving an overall classification accuracy of over 98%, considering Deepfakes from 10 different GAN architectures not only involved in images of faces, the CT demonstrates to be reliable and without any dependence on image semantic. Finally, tests carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the fake detection task, demonstrated the effectiveness of the proposed technique on a real-case scenario.},
  keywords={Information integrity;Videos;Generative adversarial networks;Faces;Task analysis;Computer architecture;Gallium nitride;Deepfake detection;generative adversarial networks;multimedia forensics;image forensics},
  doi={10.1109/ACCESS.2020.3023037},
  ISSN={2169-3536},
  month={},}@ARTICLE{4204165,
  author={Han, Feng and Zhu, Song-Chun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Two-Level Generative Model for Cloth Representation and Shape from Shading}, 
  year={2007},
  volume={29},
  number={7},
  pages={1230-1243},
  abstract={In this paper, we present a two-level generative model for representing the images and surface depth maps of drapery and clothes. The upper level consists of a number of folds which will generate the high contrast (ridge) areas with a dictionary of shading primitives (for 2D images) and fold primitives (for 3D depth maps). These primitives are represented in parametric forms and are learned in a supervised learning phase using 3D surfaces of clothes acquired through photometric stereo. The lower level consists of the remaining flat areas which fill between the folds with a smoothness prior (Markov random field). We show that the classical ill-posed problem-shape from shading (SFS) can be much improved by this two-level model for its reduced dimensionality and incorporation of middle-level visual knowledge, i.e., the dictionary of primitives. Given an input image, we first infer the folds and compute a sketch graph using a sketch pursuit algorithm as in the primal sketch (Guo et al., 2003). The 3D folds are estimated by parameter fitting using the fold dictionary and they form the "skeleton" of the drapery/cloth surfaces. Then, the lower level is computed by conventional SFS method using the fold areas as boundary conditions. The two levels interact at the final stage by optimizing a joint Bayesian posterior probability on the depth map. We show a number of experiments which demonstrate more robust results in comparison with state-of-the-art work. In a broader scope, our representation can be viewed as a two-level inhomogeneous MRF model which is applicable to general shape-from-X problems. Our study is an attempt to revisit Marr's idea (Marr and Freeman, 1982) of computing the 2frac12D sketch from primal sketch. In a companion paper (Barbu and Zhu, 2005), we study shape from stereo based on a similar two-level generative sketch representation.},
  keywords={Shape;Dictionaries;Surface fitting;Supervised learning;Photometry;Markov random fields;Pursuit algorithms;Parameter estimation;Skeleton;Boundary conditions;Shape from shading;generate model;shading primitive;sketch graph.},
  doi={10.1109/TPAMI.2007.1040},
  ISSN={1939-3539},
  month={July},}@ARTICLE{10436645,
  author={Shajalal, Md. and Bohlouli, Milad and Das, Hari Prasanna and Boden, Alexander and Stevens, Gunnar},
  journal={IEEE Access}, 
  title={Improved Thermal Comfort Model Leveraging Conditional Tabular GAN Focusing on Feature Selection}, 
  year={2024},
  volume={12},
  number={},
  pages={30039-30053},
  abstract={The indoor thermal comfort in both homes and workplaces significantly influences the health and productivity of inhabitants. The heating system, controlled by Artificial Intelligence (AI), can automatically calibrate the indoor thermal condition by analyzing various physiological and environmental variables. To ensure a comfortable indoor environment, smart home systems can adjust parameters related to thermal comfort based on accurate predictions of inhabitants’ preferences. Modeling personal thermal comfort preferences poses two significant challenges: the inadequacy of data and its high dimensionality. An adequate amount of data is a prerequisite for training efficient machine learning (ML) models. Additionally, high-dimensional data tends to contain multiple irrelevant and noisy features, which might hinder ML models’ performance. To address these challenges, we propose a framework for predicting personal thermal comfort preferences, combining the conditional tabular generative adversarial network (CTGAN) with multiple feature selection techniques. We first address the data inadequacy challenge by applying CTGAN to generate synthetic data samples, incorporating challenges associated with multimodal distributions and categorical features. Then, multiple feature selection techniques are employed to identify the best possible sets of features. Experimental results based on a wide range of settings on a standard dataset demonstrated state-of-the-art performance in predicting personal thermal comfort preferences. The results also indicated that ML models trained on synthetic data achieved significantly better performance than models trained on real data. Overall, our method, combining CTGAN and feature selection techniques, outperformed existing known related work in thermal comfort prediction in terms of multiple evaluation metrics, including area under the curve (AUC), Cohen’s Kappa, and accuracy. Additionally, we presented a global, model-agnostic explanation of the thermal preference prediction system, providing an avenue for thermal comfort experiment designers to consciously select the data to be collected.},
  keywords={Predictive models;Feature extraction;Data models;Synthetic data;Buildings;Thermal analysis;Task analysis;Generative adversarial networks;Machine learning;Human factors;Personal thermal comfort;generative adversarial network;feature selection;machine learning;data inadequacy},
  doi={10.1109/ACCESS.2024.3366453},
  ISSN={2169-3536},
  month={},}@ARTICLE{10596088,
  author={Rychert, Alan and Ganuza, María Luján and Selzer, Matias Nicolás},
  journal={IEEE Computer Graphics and Applications}, 
  title={Integrating GPT as an Assistant for Low-Cost Virtual Reality Escape-Room Games}, 
  year={2024},
  volume={44},
  number={4},
  pages={14-25},
  abstract={This work explores the integration of generative pretrained transformer (GPT), an AI language model developed by OpenAI, as an assistant in low-cost virtual escape games. The study focuses on the synergy between virtual reality (VR) and GPT, aiming to evaluate its performance in helping solve logical challenges within a specific context in the virtual environment while acting as a personalized assistant through voice interaction. The findings from user evaluations revealed both positive perceptions and limitations of GPT in addressing highly complex challenges, indicating its potential as a valuable tool for providing assistance and guidance in problem-solving situations. The study also identified areas for future improvement, including adjusting the difficulty of puzzles and enhancing GPT’s contextual understanding. Overall, the research sheds light on the opportunities and challenges of integrating AI language models such as GPT in virtual gaming environments, offering insights for further advancements in this field.},
  keywords={Artificial intelligence;Games;Education;Solid modeling;Problem-solving;Virtual reality;Entertainment industry;Extended reality;Virtual environments;Symbols;Multisensor systems;Machine learning;Computer languages;Algorithm design and analysis;Chatbots;Transformers},
  doi={10.1109/MCG.2024.3426314},
  ISSN={1558-1756},
  month={July},}@ARTICLE{10879291,
  author={Hachana, Rafik and Rasheed, Bader},
  journal={IEEE Access}, 
  title={Probe-Assisted Fine-Grained Control for Non-Differentiable Features in Symbolic Music Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={28059-28070},
  abstract={As symbolic music generation evolves, research interest is shifting toward more controlled and steerable generative processes to support creative decisions. Previous methods focus on global conditioning or fine-grained control through input sequences but often limit flexibility for real-time interventions and require modifications to the model’s architecture. We introduce a novel symbolic music generation framework by combining a Transformer encoder-decoder with probe models, which enable us to interpret the encoder hidden state using pre-defined non-differentiable musical features, and subsequently manipulate the hidden state to achieve a set of desired attributes in the generated music. This method allows fine-grained control over specific musical features without altering the underlying model architecture. Probes can be trained jointly with the generative model or applied post-training, enabling adaptable control without retraining the model. Our experiments demonstrate that this intervention effectively influences the model output without hindering the music quality. This approach enhances both the flexibility and interpretability of symbolic music generation, enabling better real-world applicability for music generation models.},
  keywords={Music;Probes;Transformers;Training;Adaptation models;Predictive models;Aerospace electronics;Process control;Multiple signal classification;Generators;Machine learning;music generation;symbolic music;generative AI;probes;conditional generative models},
  doi={10.1109/ACCESS.2025.3540543},
  ISSN={2169-3536},
  month={},}@ARTICLE{11029979,
  author={Wang, Haiyan and Wang, Bingjie and Huang, Wenbo and Liu, Yibin and Du, Yu and Hung, Guang-Uei and Hu, Zhanli and Mok, Greta S. P.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Deep-learning-based Partial Volume Correction in 99mTc-TRODAT-1 SPECT for Parkinson's Disease: A Preliminary Study on Clinical Translation}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={99mTc-TRODAT-1 SPECT is effective for the early detection of Parkinson's disease (PD). However, SPECT images suffer from severe partial volume effect, which impairs tissue boundary clarity and subsequent quantification accuracy. This work proposes an anatomical prior- and segmentation-free deep learning (DL)-based partial volume correction (PVC) method using an attentionbased conditional generative adversarial network (Att-cGAN) for 99mTc-TRODAT-1 SPECT. A population of 454 digital brain phantoms modelling anatomical and 99mTc-TRODAT activity variations in different PD categories are used to generate realistic SPECT projections using the SIMIND Monte Carlo code, and then reconstructed using ordered subset expectation maximization algorithm. The dataset is split into 320, 44 and 90 used for training, validation, and testing. Att-cGAN, cGAN and U-Net are implemented based on simulated data, then directly tested on 100 retrospectively collected clinical 99mTc-TRODAT data, with same acquisition and reconstruction parameters as in simulations. Non-DL PVC methods of Van-Cittert and iterative Yang are implemented for comparison. Physical and clinical metrics, as well as a no-gold standard technique (NGST) are applied to evaluate different PVC methods in the absence of clinical ground truth. Att-cGAN yields superior PVC performance in simulations as compared to other methods in physical and clinical evaluations. NGST assessment is generally consistent with the clinical metric evaluation. For the clinical study, Att-cGAN also obtains better NGST result than others striatal compartments can be discriminated on DLbased processed images. DL-PVC method is feasible for clinical PD SPECT using highly realistic simulated data.},
  keywords={Single photon emission computed tomography;Image reconstruction;Data models;Training;Imaging phantoms;Image segmentation;Phantoms;Diseases;Artificial intelligence;Image resolution;Parkinson's disease;SPECT;partial volume correction;deep learning},
  doi={10.1109/JBHI.2025.3578526},
  ISSN={2168-2208},
  month={},}@INPROCEEDINGS{10216583,
  author={Desticourt, Etienne and Letort, Véronique and D'Alché-Buc, Florence},
  booktitle={2022 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Interpretable Generative Modeling Using a Hierarchical Topological VAE}, 
  year={2022},
  volume={},
  number={},
  pages={1415-1421},
  abstract={Generating realistic datasets with fine-grained control over their properties can help overcome challenges linked to the scarcity of data in many domains, such as medical applications. To that end, we extend Variational Autoencoders by using a hierarchical and topological prior consisting of a sequence of Self-Organizing Maps (SOM), which are stacked in the latent space and learned without supervision, jointly with the parameters of the variational autoencoder. We induce a hierarchy between the codes of the SOM sequence, each SOM corresponding to a different hierarchical level and learning increasingly finer-grained representations of the data. Our model combines the power of deep learning with the interpretability of hierarchical and topological clustering and produces competitive results when evaluated on three well-known computer vision benchmarks and a custom medical dataset.},
  keywords={Self-organizing feature maps;Deep learning;Computer vision;Scientific computing;Computational modeling;Impedance matching;Neural networks;Deep-Learning;Generative Modeling;Hierarchical models;Variational Autoencoders},
  doi={10.1109/CSCI58124.2022.00253},
  ISSN={2769-5654},
  month={Dec},}@INPROCEEDINGS{11101097,
  author={Xu, Xiaoxia and Liu, Yuanwei and Mu, Xidong and Nallanathan, Arumugam},
  booktitle={2024 IEEE Globecom Workshops (GC Wkshps)}, 
  title={End-to-End Latency Reduction for Mobile Edge Generation (MEG)}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={A novel low-latency mobile edge generation (MEG) framework is proposed to generate high-resolution images on mobile devices. Exploiting a compressed latent diffusion model (LDM) distributed across the edge server (ES) and the user equipment (UE), only low-dimension features need to be transmitted for creating artificial intelligence generative content (AIGC). The proposed framework adaptively compresses feature denoising steps and merges similar feature neurons, thereby significantly reducing both computation and transmission overheads, respectively. A joint denoising step and feature compression ratio optimization problem is formulated, which minimizes the end-to- end generation latency while maximizing the image generation quality. To tailor the distributed sub-models, a backbone LDM architecture is first trained by offline distillation, which supports various options of feature denoising and compressions over noisy wireless channels. Then, a policy learning algorithm is developed to dynamically predict denoising steps and feature compression ratios in online channel environment. Numerical results demonstrate that: 1) The proposed framework effectively mitigates image distortions, while reducing over 40% latency compared to conventional generation schemes. 2) The developed algorithm strikes a flexible trade-off between image quality and end-to-end generation latency.},
  keywords={Image coding;Image synthesis;Image edge detection;Heuristic algorithms;Noise reduction;Neurons;Prediction algorithms;Distortion;Low latency communication;Optimization;Artificial intelligence generated content (AIGC);dynamic compression;edge artificial intelligence (AI);generative AI;mobile edge generation (MEG);reinforcement learning (RL)},
  doi={10.1109/GCWkshp64532.2024.11101097},
  ISSN={2166-0077},
  month={Dec},}@INPROCEEDINGS{11026496,
  author={Mohana Sundari, V and Mabel Rose, R.A. and Sassirekha, S.M. and Nalini, M. and Girija, P. and R, Siva Subramanian},
  booktitle={2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)}, 
  title={Deep Learning: Techniques, Taxonomy, Applications, and Future Directions}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep learning has become one of the key AI techniques of the modern world and has changed many fields by its capability to learn from data, which is often very complex. The presented survey paper aims at describing fundamental techniques and taxonomy of deep learning, various applications, and potential perspectives for further research. The paper starts with presenting the concept of deep learning and its origins as well as emphasizing the important position of DL in the modern development of AI. It then goes further into deep learning workflow and frameworks and presents feedforward neural networks, CNN, RNN, generative adversarial networks, attention mechanisms, and transformers. An overview of deep learning frameworks is discussed, including supervised, unsupervised, reinforcement, transfer, semi-supervised, & self-supervised learning. The survey further continues to provide a detailed description on how DL is being widely applied in CV, NLP, speech recognition, disease diagnosis & stock price forecasting. The issues related to challenges and limitations include data dependency, interpretability, overfitting, high computational burden, and ethical concerns are presented along with ongoing work and directions for future investigations. This includes explainability of AI, incorporation of symbolic AI methods, improving the resilience and security of models, breakthroughs in hardware, as well as expansion into future domains such as quantum computing and robotics. Finally, this survey paper summarizes existing literature and predicts future research direction, emphasizing on how deep learning will revolutionalise technology and how it may foster development in various fields.},
  keywords={Deep learning;Surveys;Ethics;Quantum computing;Computational modeling;Taxonomy;Transformers;Hardware;Artificial intelligence;Robots;CNN;RNN;Deep Learning;AI;Machine Learning},
  doi={10.1109/ICSTSDG61998.2024.11026496},
  ISSN={},
  month={Nov},}@ARTICLE{10980001,
  author={Wang, Zige and Wang, Yashuai and Liu, Tianyu and Zhang, Peng and Xie, Lei and Guo, Yangming},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Audio-Driven Talking Face Generation With Segmented Static Facial References for Customized Health Device Interactions}, 
  year={2025},
  volume={71},
  number={2},
  pages={5404-5413},
  abstract={In a variety of human-machine interaction (HMI) applications, the high-level techniques based on audio-driven talking face generation are often challenged by the issues of temporal misalignment and low-quality outputs. Recent solutions have sought to improve synchronization by maximizing the similarity between audio-visual pairs. However, the temporal disturbances introduced during the inference phase continue to limit the enhancement of generative performance. Inspired by the intrinsic connection between the segmented static facial image and the stable appearance representation, in this study, two strategies, Manual Temporal Segmentation (MTS) and Static Facial Reference (SFR), are proposed to improve performance during the inference stage. The corresponding functionality consists of: MTS involves segmenting the input video into several clips, effectively reducing the complexity of the inference process, and SFR utilizes static facial references to mitigate the temporal noise generated by dynamic sequences, thereby enhancing the quality of the generated outputs. Substantial experiments on the LRS2 and VoxCeleb2 datasets have demonstrated that the proposed strategies are able to significantly enhance inference performance with the LSE-C and LSE-D metrics, without altering the network architecture or training strategy. For effectiveness validation in realistic scenario applications, a deployment has also been conducted on the healthcare devices with the proposed solution.},
  keywords={Lips;Faces;Synchronization;Training;Deep learning;Artificial intelligence;Consumer electronics;Real-time systems;Computational modeling;Three-dimensional displays;Talking face generation;lip synthesis;video generation;inference performance;AI health care},
  doi={10.1109/TCE.2025.3565518},
  ISSN={1558-4127},
  month={May},}@INPROCEEDINGS{9578900,
  author={He, Sen and Liao, Wentong and Yang, Michael Ying and Yang, Yongxin and Song, Yi-Zhe and Rosenhahn, Bodo and Xiang, Tao},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Context-Aware Layout to Image Generation with Enhanced Object Appearance}, 
  year={2021},
  volume={},
  number={},
  pages={15044-15053},
  abstract={A layout to image (L2I) generation model aims to generate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in generative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object’s appearance is typically distorted lacking the key defining characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encoding in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work. First, a context-aware feature transformation module is introduced in the generator to ensure that the generated feature encoding of either object or stuff is aware of other coexisting objects/stuff in the scene. Second, instead of feeding location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appearance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks. Code available at: https://github.com/wtliao/layout2img.},
  keywords={Visualization;Image synthesis;Computational modeling;Layout;Benchmark testing;Inspection;Generators},
  doi={10.1109/CVPR46437.2021.01480},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10327767,
  author={Liu, Liqin and Chen, Bowen and Chen, Hao and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Diverse Hyperspectral Remote Sensing Image Synthesis With Diffusion Models}, 
  year={2023},
  volume={61},
  number={},
  pages={1-16},
  abstract={Hyperspectral image (HSI) synthesis overcomes the limitations of imaging sensors and enables low-cost acquisition of HSIs with high spatial resolution. Using RGB as a conditional input for hyperspectral generation is promising and valuable, as it can leverage abundant existing multispectral/RGB images without the intervention of hyperspectral sensors. However, most existing generation methods follow one-to-one mapping frameworks and ignore generation diversity. In addition, the current evaluation metrics of hyperspectral generation are based on the similarity with the reference image, which cannot reflect the diversity of the generated spectra. In this article, we propose a novel method for diverse hyperspectral remote sensing image generation based on the diffusion model. The diffusion model uses a denoising model to gradually remove noise from the normal distribution and generates the hyperspectral data step-by-step with the conditional RGB image as input. To address the high-dimensional noise prediction problem caused by a large number of bands in the HSI, we introduce a conditional vector quantized generative adversarial network (VQGAN) that maps the high-dimensional hyperspectral data into a low-dimensional latent space and conduct the diffusion process in the latent space. The latent-diffusion process makes the diffusion process faster and more stable. The conditional VQGAN decodes HSIs from the latent code generated by diffusion, with the conditional RGB image as the input, which restricts the diversity to a specific object distribution. We also designed two new metrics to evaluate the generation spectral diversity (SD). Experiments on the IEEE grss_dfc_2018 dataset demonstrate that our method can synthesize highly diverse hyperspectral data. In addition, the rationality of the proposed metrics is also verified.},
  keywords={Hyperspectral imaging;Measurement;Image synthesis;Data models;Noise reduction;Spatial resolution;Codes;Diffusion model;diverse spectral synthesis;hyperspectral image (HSI) synthesis;remote sensing},
  doi={10.1109/TGRS.2023.3335975},
  ISSN={1558-0644},
  month={},}@ARTICLE{9559870,
  author={Bai, Weiming and Zhang, Zhipeng and Li, Bing and Wang, Pei and Li, Yangxi and Zhang, Congxuan and Hu, Weiming},
  journal={IEEE Transactions on Image Processing}, 
  title={Robust Texture-Aware Computer-Generated Image Forensic: Benchmark and Algorithm}, 
  year={2021},
  volume={30},
  number={},
  pages={8439-8453},
  abstract={With advances in rendering techniques and generative adversarial networks, computer-generated (CG) images tend to be indistinguishable from photographic (PG) images. Revisiting previous works towards CG image forensic, we observed that existing datasets are constructed years ago and limited in both quantity and diversity. Besides, current algorithms only consider the global visual features for forensic, ignoring finer differences between CG and PG images. To mitigate these problems, we first contribute a Large-Scale CG images Benchmark (LSCGB), and then propose a simple yet strong baseline model to address the forensic task. On the one hand, the introduced benchmark has three superior properties, 1) large-scale: the benchmark contains 71168 CG and 71168 PG images with the corresponding expert-annotated labels. It is orders of magnitude bigger than previous datasets. 2) high diversity: we collect CG images from 4 different scenes generated by various rendering techniques. The PG images are varied in terms of image content, camera models, and photographer styles. 3) small bias: we carefully filter the collected images to ensure that the distributions of color, brightness, tone and saturation between CG and PG images are close. Furthermore, inspired by an empirical study on texture difference between CG and PG images, an effective texture-aware network is proposed to improve forensic accuracy. Concretely, we first strengthen texture information of multilevel features extracted from a backbone. Then, the relations among feature channels are explored by learning its gram matrix. Each feature channel represents a specific texture pattern. The gram matrix is thus able to embed the finer texture differences. Experimental results demonstrate that this baseline surpasses the existing methods. The benchmark is publically available at https://github.com/wmbai/LSCGB.},
  keywords={Benchmark testing;Feature extraction;Three-dimensional displays;Image forensics;Task analysis;Motion pictures;Games;Computer-generated image benchmark;texture aware;computer-generated images forensic},
  doi={10.1109/TIP.2021.3114989},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{9577952,
  author={Das, Ayan and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao and Song, Yi-Zhe},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Cloud2Curve: Generation and Vectorization of Parametric Sketches}, 
  year={2021},
  volume={},
  number={},
  pages={7084-7093},
  abstract={Analysis of human sketches in deep learning has advanced immensely through the use of waypoint-sequences rather than raster-graphic representations. We further aim to model sketches as a sequence of low-dimensional parametric curves. To this end, we propose an inverse graphics framework capable of approximating a raster or waypoint based stroke encoded as a point-cloud with a variable-degree Bézier curve. Building on this module, we present Cloud2Curve, a generative model for scalable high-resolution vector sketches that can be trained end-to-end using point-cloud data alone. As a consequence, our model is also capable of deterministic vectorization which can map novel raster or waypoint based sketches to their corresponding high-resolution scalable Bézier equivalent. We evaluate the generation and vectorization capabilities of our model on Quick, Draw! and K-MNIST datasets.},
  keywords={Training;Graphics;Deep learning;Computer vision;Computational modeling;Fitting;Computer architecture},
  doi={10.1109/CVPR46437.2021.00701},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10036942,
  author={Liu, Jiannan and Han, Jing and Fu, Kang and Jia, Jun and Zhu, Dandan and Zhai, Guangtao},
  journal={IEEE Internet of Things Journal}, 
  title={Application of QR Code Watermarking and Encryption in the Protection of Data Privacy of Intelligent Mouth-Opening Trainer}, 
  year={2023},
  volume={10},
  number={12},
  pages={10510-10518},
  abstract={Quick response (QR) codes are widely used in offline to online channels to transfer information from promotional materials to mobile devices. Self-service medical equipment can record the data of each test, so the use of QR codes can realize the data exchange between patients and doctors, medical institutions, and self-service medical equipment, and create a medical information platform for health files. However, since anyone can easily read the information in the QR code, it is not conducive to the protection of patient privacy. Therefore, we propose a QR code encryption and decryption model based on robust digital watermarking. We implement digital watermarking through the generative adversarial networks and increase the robustness of the watermark by adding noise to the model. At the same time, we encrypt and decrypt the QR code information through advanced encryption standards. Experimental results show that the proposed method can well protect the privacy of patients without affecting the data acquisition by patients and doctors.},
  keywords={Watermarking;Pipelines;Decoding;QR codes;Distortion;Internet of Things;Transform coding;Adversarial training;data privacy;information hiding;intelligent medical care;quick response (QR) code},
  doi={10.1109/JIOT.2023.3242319},
  ISSN={2327-4662},
  month={June},}@ARTICLE{9662671,
  author={Dai, Pingyang and Chen, Peixian and Wu, Qiong and Hong, Xiaopeng and Ye, Qixiang and Tian, Qi and Lin, Chia-Wen and Ji, Rongrong},
  journal={IEEE Transactions on Image Processing}, 
  title={Disentangling Task-Oriented Representations for Unsupervised Domain Adaptation}, 
  year={2022},
  volume={31},
  number={},
  pages={1012-1026},
  abstract={Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.},
  keywords={Task analysis;Adaptation models;Image color analysis;Vehicle dynamics;Linear programming;Image retrieval;Semantics;Unsupervised domain adaptation;deep learning;image retrieval;person re-identification},
  doi={10.1109/TIP.2021.3136615},
  ISSN={1941-0042},
  month={},}@ARTICLE{9747948,
  author={Zhang, Yunlong and Lin, Xin and Zhuang, Yihong and Sun, Liyan and Huang, Yue and Ding, Xinghao and Wang, Guisheng and Yang, Lin and Yu, Yizhou},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Harmonizing Pathological and Normal Pixels for Pseudo-Healthy Synthesis}, 
  year={2022},
  volume={41},
  number={9},
  pages={2457-2468},
  abstract={Synthesizing a subject-specific pathology-free image from a pathological image is valuable for algorithm development and clinical practice. In recent years, several approaches based on the Generative Adversarial Network (GAN) have achieved promising results in pseudo-healthy synthesis. However, the discriminator (i.e., a classifier) in the GAN cannot accurately identify lesions and further hampers from generating admirable pseudo-healthy images. To address this problem, we present a new type of discriminator, the segmentor, to accurately locate the lesions and improve the visual quality of pseudo-healthy images. Then, we apply the generated images into medical image enhancement and utilize the enhanced results to cope with the low contrast problem existing in medical image segmentation. Furthermore, a reliable metric is proposed by utilizing two attributes of label noise to measure the health of synthetic images. Comprehensive experiments on the T2 modality of BraTS demonstrate that the proposed method substantially outperforms the state-of-the-art methods. The method achieves better performance than the existing methods with only 30% of the training data. The effectiveness of the proposed method is also demonstrated on the LiTS and the T1 modality of BraTS. The code and the pre-trained model of this study are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.},
  keywords={Pathology;Image segmentation;Training;Lesions;Biomedical imaging;Generators;Measurement;Medical image synthesis;low-contrast medical image segmentation;adversarial training;image enhancement;label noise},
  doi={10.1109/TMI.2022.3164095},
  ISSN={1558-254X},
  month={Sep.},}@ARTICLE{10051654,
  author={Kong, Zhe and Zhang, Wentian and Liu, Feng and Luo, Wenhan and Liu, Haozhe and Shen, Linlin and Ramachandra, Raghavendra},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Taming Self-Supervised Learning for Presentation Attack Detection: De-Folding and De-Mixing}, 
  year={2024},
  volume={35},
  number={8},
  pages={10639-10650},
  abstract={Biometric systems are vulnerable to presentation attacks (PAs) performed using various PA instruments (PAIs). Even though there are numerous PA detection (PAD) techniques based on both deep learning and hand-crafted features, the generalization of PAD for unknown PAI is still a challenging problem. In this work, we empirically prove that the initialization of the PAD model is a crucial factor for generalization, which is rarely discussed in the community. Based on such observation, we proposed a self-supervised learning-based method, denoted as DF-DM. Specifically, DF-DM is based on a global–local view coupled with de-folding and de-mixing to derive the task-specific representation for PAD. During de-folding, the proposed technique will learn region-specific features to represent samples in a local pattern by explicitly minimizing the generative loss. While de-mixing drives detectors to obtain the instance-specific features with global information for more comprehensive representation by minimizing the interpolation-based consistency. Extensive experimental results show that the proposed method can achieve significant improvements in terms of both face and fingerprint PAD in more complicated and hybrid datasets when compared with the state-of-the-art methods. When training in CASIA-FASD and Idiap Replay-Attack, the proposed method can achieve an 18.60% equal error rate (EER) in OULU-NPU and MSU-MFSD, exceeding the baseline performance by 9.54%. The source code of the proposed technique is available at https://github.com/kongzhecn/dfdm.},
  keywords={Fingerprint recognition;Face detection;Feature extraction;Detectors;Face recognition;Training;Self-supervised learning;Presentation attack detection (PAD);self-supervised learning},
  doi={10.1109/TNNLS.2023.3243229},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{9928310,
  author={Li, Guanglin and Li, Bin and Tan, Shunquan and Qiu, Guoping},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Learning Deep Co-Occurrence Features}, 
  year={2023},
  volume={33},
  number={4},
  pages={1610-1623},
  abstract={We exploit the computational capability of deep convolutional neural network (CNN) architecture and the natural interpretability of the co-occurrence matrix (CM) to learn deep co-occurrence features (DCOFs). The DCOFs represent the statistics of the co-occurrences of pixels thus overcoming the black box nature of traditional deep representation learning while at the same time solving the inherent computational difficulty of CM. We propose a parametric co-occurrence matrix (PCM) model to approximate the CM with multivariate Gaussian functions, and have developed three approaches to decomposing the PCM model into linear and nonlinear operations such that the model can be easily implemented using standard CNN operations and to learn the DCOFs of arbitrary shapes. The CNN implementation of the PCM model, termed PCMCNN, can be used as a standard plugin module of a deep learning system and adaptively learns the DCOFs for downstream applications. We demonstrate the broad applicability of the DCOFs and their effectiveness in fine-grained image classification tasks such as texture classification and GAN (generative adversarial network) image detection. The introduction of the PCMCNN module makes it much more compact and efficient than conventional implementations of deep learning models, achieving comparable classification performances to state of the art methods on a variety of benchmarking datasets with models that are more than 30 folds smaller and 11 times less complex. The small model size for learning the DCOFs makes the new method particularly effective for few shot classification of large number of texture categories where the small number of training samples can easily cause traditional deep learning models to overfit. This work shows the potential benefits of combining the principles of traditional handcrafted features and deep representation learning to take advantage of both for advancing state of the art.},
  keywords={Standards;Convolutional neural networks;Feature extraction;Computational modeling;Histograms;Pulse modulation;Phase change materials;Dimensionality reduction;co-occurrence matrix;convolutional neural network;image classification},
  doi={10.1109/TCSVT.2022.3216905},
  ISSN={1558-2205},
  month={April},}@INPROCEEDINGS{10582038,
  author={Barros, Jilliam M. Díaz and Wang, Chen-Yu and Malik, Jameel and Arafa, Abdalla and Stricker, Didier},
  booktitle={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={SynthSL: Expressive Humans for Sign Language Image Synthesis}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Around 5% of the world's population live with disabling hearing loss. Despite recent advancements to improve accessibility to the Deaf community, research on sign language is still limited. In this work, we introduce a large-scale synthetic dataset on sign language, SynthSL, targeted to sign language production, recognition and translation. Using state-of-the-start methods for human body modelling, SynthSL aims to augment current datasets by providing additional ground truth data such as depth and normal maps, rendered models, segmentation masks and 2D/3D body joints. We additionally explore a generative architecture for the synthesis of sign images and propose a new generator based on Swin Transformers, conditioned on given body poses and appearance. We believe that an increase on the publicly available data on sign language would boost research and close the performance gap with related topics on human body synthesis. Our code, models and dataset are available at https://github.com/jilliam/SynthSL.},
  keywords={Sign language;Image segmentation;Image synthesis;Pipelines;Rendering (computer graphics);Transformers;Data models},
  doi={10.1109/FG59268.2024.10582038},
  ISSN={2770-8330},
  month={May},}@INPROCEEDINGS{11022124,
  author={Chang, Mengdi and Yu, Qinghua and Ren, Junkai and Zhou, Zhiqian and Zheng, Zhiqiang and Lu, Huimin},
  booktitle={2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Pedestrian Trajectory Prediction Based on the View-Constrained Spatio-Temporal Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1294-1302},
  abstract={Pedestrian trajectory prediction is essential for mobile robots to navigate in crowds. Most current trajectory prediction algorithms are generally designed for datasets and are susceptible to interference from redundant information. To achieve more accurate and efficient trajectory prediction, we propose a pedestrian trajectory prediction algorithm based on the view-constrained spatio-temporal graph, namely V-Social-STGCNN. The algorithm considers the pedestrians’ view restrictions when calculating the weighted adjacency matrix, more accurately describing the interaction relationship between pedestrians. When obtaining the final predicted trajectories, Quasi-Monte Carlo (QMC) sampling is utilized to ensure more uniform sampling, better reflecting the uncertainty of pedestrians’ future movements. Compared with benchmark algorithms, V-Social-STGCNN reduces the average displacement error (ADE) and final displacement error (FDE) by more than 5% and 8% on the ETH and UCY datasets, respectively. In addition, we build an online pedestrian trajectory prediction system based on V-Social-STGCNN on Fetch robot, verifying the algorithm’s feasibility in real-world scenarios.},
  keywords={Pedestrians;Uncertainty;Accuracy;Navigation;Interference;Benchmark testing;Prediction algorithms;Trajectory;Spatiotemporal phenomena;Mobile robots;pedestrian trajectory prediction;spatio-temporal graph;graph convolutional network},
  doi={10.1109/ACAIT63902.2024.11022124},
  ISSN={},
  month={Nov},}@ARTICLE{11032152,
  author={Li, Na and Wang, Haining and Zhao, Huijie and Ou, Wen},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Cross-Modal Visible-to-Infrared Image Translation in Remote Sensing Guided by Thermal Features}, 
  year={2025},
  volume={63},
  number={},
  pages={1-16},
  abstract={Weakly supervised visible (VI)-to-infrared (IR) image translation in remote sensing scenes is a challenging task that attempts to generate corresponding IR images from VI images with cross-model manner, thus tackling the acute scarcity of IR data in certain crucial task scenes. Existing methods frequently concentrate solely on style transfer, resulting in stylistic similarity but physical inconsistency, which limits the authenticity of cross-modal translation. IR thermal features stemming from brightness and darkness contrasts among distinct regions due to temperature disparities, are associated with visual representations in foreground objects in VI images. Therefore, this article proposes ThermalMask, a framework for IR image generation guided by thermal features, aiming to make the generated images more closely with IR features in remote sensing. Specifically, a saliency mask generation network (SMGN) and a semantic attention generation network (SAGN) are designed in the generator. The SMGN is used to generate background and foreground masks to preserve the background and most prominent features of the input remote sensing image. Meanwhile, the SAGN generates attention maps through pyramid nonlocal attention mechanism, guiding the network to adaptively focus on salient regions within both VI and IR feature maps. In addition, to refine the generated IR features, a pixel-spectrum multispatial constraint (PSC) targeting high-frequency components is designed, enabling the generative network to focus more on expressing IR features. Extensive experimental results have shown that our method has good performance in VI-to-IR image translation within remote sensing scenes, surpassing existing state-of-the-art image translation methods. Moreover, it holds promise for application in downstream tasks where IR data is insufficient.},
  keywords={Translation;Remote sensing;Image synthesis;Training;Semantics;Optical sensors;Optical imaging;Generators;Feature extraction;Diffusion models;Cross-modal;image translation;thermal features;visible (VI)-to-infrared (IR)},
  doi={10.1109/TGRS.2025.3579267},
  ISSN={1558-0644},
  month={},}@ARTICLE{11072230,
  author={Gao, Jinfeng and Li, Gangqiang and Yao, Ruxian and Liu, Qiang and Zhang, Junming},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Progressive Conditional Diffusion Model for Multistage Spectral Restoration of Remote Sensing Image}, 
  year={2025},
  volume={18},
  number={},
  pages={18330-18344},
  abstract={Due to the high cost and relatively low image quality of hyperspectral sensors, spectral super-resolution seeks to explore the mapping mechanisms between multispectral and hyperspectral images (HSIs), with the goal of reconstructing high-quality HSIs. In recent years, deep learning algorithms have achieved significant success in spectral super-resolution. However, most of these methods compute loss only at the final stage of the network, neglecting the intermediate generative processes, which leads to considerable spectral distortion in the reconstructed images. To address these issues, we propose a progressive conditional diffusion model (PCDM) for multistage spectral restoration. PCDM constructs a channel synthesis module that generates a ground truth set through band synthesis, and designs an image reconstruction module (IRM) to ensure that the synthesized image in the next stage can effectively reconstruct the synthesized features from the previous stage. Multiple conditional diffusion models are then constructed based on the dataset. For each conditional diffusion model, the network parameters of the corresponding IRM are shared with the multispectral image for spectral up-sampling. The spectral-up-sampled multispectral features, combined with the output from the previous diffusion model, serve as a conditional matrix, which is input into the next diffusion model to obtain the final result. Experimental results on both synthetic and real datasets demonstrate that PCDM can effectively reconstruct HSIs, showing robustness and outperforming state-of-the-art methods.},
  keywords={Image reconstruction;Diffusion models;Superresolution;Feature extraction;Mathematical models;Hyperspectral imaging;Transformers;Spatial resolution;Image restoration;Training;Hyperspectral image (HSI);multispectral image (MSI);multistage diffusion;progressive spectral restoration},
  doi={10.1109/JSTARS.2025.3586437},
  ISSN={2151-1535},
  month={},}@ARTICLE{11072916,
  author={Chen, Xingjian and Fu, Zhongzheng and Zhang, Peng and Chen, Xinxing and Huang, Jian},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Intracortical Brain-Machine Interfaces with High-Performance Neural Decoding through Efficient Transfer Meta-learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Implantable brain-machine interfaces (iBMIs) have emerged as a groundbreaking neural technology for restoring motor function and enabling direct neural communication pathways. Despite their therapeutic potential in neurological rehabilitation, the critical challenge of neural decoder calibration persists, particularly in the context of transfer learning. Traditional calibration approaches assume the availability of extensive neural recordings, which is often impractical in clinical settings due to patient fatigue and neural signal variability. Furthermore, the inherent constraints of implanted neural processors–including limited computational capacity and power consumption requirements–demand streamlined processing algorithms. To address these clinical and technical challenges, we developed DMM-WcycleGAN (Dimensionality Reduction Model-Agnostic Meta-Learning based Wasserstein Cycle Generative Adversarial Networks), a novel neural decoding framework that integrates meta-learning principles with optimal transfer learning strategies. This innovative approach enables efficient decoder calibration using minimal neural data while implementing dimensionality reduction techniques to optimize computational efficiency in implanted devices. In vivo experiments with non-human primates demonstrated DMM-WcycleGAN's superior performance in mitigating neural signal distribution shifts between historical and current recordings, achieving a 3% enhancement in neural decoding accuracy using only ten calibration trials while reducing the calibration duration by over 70%, thus significantly improving the clinical viability of iBMI systems.},
  keywords={Decoding;Calibration;Metalearning;Biomedical engineering;Recording;Training;Transfer learning;Grasping;Adaptation models;Microelectrodes;Intracortical brain-machine interface;Meta-learning;Transfer learning;CycleGAN},
  doi={10.1109/TBME.2025.3586870},
  ISSN={1558-2531},
  month={},}@ARTICLE{10946240,
  author={Liu, Yun and Wang, Xinran and Hu, Enping and Wang, Anzhi and Shiri, Babak and Lin, Weisi},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={VNDHR: Variational Single Nighttime Image Dehazing for Enhancing Visibility in Intelligent Transportation Systems via Hybrid Regularization}, 
  year={2025},
  volume={26},
  number={7},
  pages={10189-10203},
  abstract={The visibility of images plays a crucial role in Intelligent Transportation Systems (ITS). However, images captured under hazy environments can degrade visual quality, significantly reducing the working performance of ITS. Although existing dehazing methods have achieved remarkable performance for daytime hazy images, they struggle to overcome the unique degradations under nighttime haze conditions such as glows, weak illumination, hidden noise, and color distortions. To simultaneously address these degradations, we propose VNDHR, a novel Variational Nighttime Dehazing framework using Hybrid Regularization focusing on enhancing the perceptual visibility of nighttime hazy scenarios. Specifically, a new physical model that accounts for multiple degradations under nighttime haze conditions is first constructed. Then, a novel hybrid variational model comprising an  $\ell _{p}$  norm, a weighted  $\ell _{2}$  norm, and a total variation regularization is developed to obtain a structure-aware illumination and a noise-free reflectance, simultaneously. To remove the nonhomogeneous haze in the illumination, we employ the dark channel prior to estimate parameters in each grid patch. Furthermore, a simple but effective nonlinear stretching function is designed to enhance the texture in the decomposed reflectance component. Finally, the dehazed illumination and the stretched reflectance are combined to generate a haze-free result. Experiments performed on synthetic and real-world nighttime hazy images prove that our VNDHR framework achieves state-of-the-art dehazing performance, providing results with clear details and less noise. Besides, our VNDHR can also handle various types of degraded images well, such as low-light images, daytime hazy images, sandstorm images, and underwater images.},
  keywords={Atmospheric modeling;Lighting;Degradation;Reflectivity;Image color analysis;Scattering;Image restoration;Electronic mail;Visualization;Colored noise;Nighttime image dehazing;hybrid regularization;ℓₚ norm;stretching},
  doi={10.1109/TITS.2025.3550267},
  ISSN={1558-0016},
  month={July},}@INPROCEEDINGS{10565228,
  author={Lu, Chih-Hsin and Lin, Chia-Chia and Tang, Tzu-Chun and Lin, Chung-Yi and Chang, Jay and Tsai, Chung-Hao and Hsia, Harry and Twu, J. C. and Liu, C. S. and Wu, Gene and Yee, Kuo-Chung and Yu, Douglas C.H.},
  booktitle={2024 IEEE 74th Electronic Components and Technology Conference (ECTC)}, 
  title={High Bandwidth and Energy Efficient Electrical-Optical System Integration Using COUPE Technology}, 
  year={2024},
  volume={},
  number={},
  pages={893-897},
  abstract={Various optical engine architectures are proposed to overcome insufficient bandwidth and high-power consumption of electrical links for generative AI applications. This paper depicts a compact universal photonic engine (COUPE) with system-on-integrated-chip (SoIC) bond for a high-bandwidth and energy efficient optical link. SoIC bond is a bump-less interconnect for 2.5D/3D system integration. The scaling of SoIC bond not only eliminates the parasitic of electrical IC (EIC) and photonic IC (PIC) but also increases the density of micro-ring modulators and photodetectors. The overall system performances of XPU-to-XPU with optical interconnect using COUPEs are studied. Regarding the transmitter design in COUPE, SoIC-based optical engine (OE) can offer 23x more bandwidth density than the solder-bump based OE. On the receiver side, SoIC bond with low capacitance improves the sensitivity of transimpedance amplifiers (TIA), reducing laser power consumption by over 40%. Moreover, the enhanced interposer is proposed to improve the energy efficiency and bandwidth of EIC-to-XPU and XPU-MEM. The integration of electrical-optical systems using COUPE and enhanced interposer enables a super GPU platform for future generative AI applications.},
  keywords={Optical interconnections;Generative AI;Power lasers;Bandwidth;System integration;Energy efficiency;Optical transmitters;Silicon Photonics;3D Integration;Optical Engine;Optical Transmitter and Receiver;Interposer},
  doi={10.1109/ECTC51529.2024.00144},
  ISSN={2377-5726},
  month={May},}@INPROCEEDINGS{10487392,
  author={Shams, Afsaneh and Becker, Drew and Becker, Kyle and Amirian, Soheyla and Rasheed, Khaled},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)}, 
  title={Evolving Efficient CNN Based Model for Image Classification}, 
  year={2023},
  volume={},
  number={},
  pages={228-235},
  abstract={Evolutionary algorithms, rooted in Darwin's theorem, can be considered as a base for implementing deep/machine learning models. This approach can noticeably increase the accuracy in most cases as shown in this paper. This experiment aims to evaluate the performance of two evolutionary algorithms, an evolutionary neural network (ENN) and an evolutionary CNN-based algorithm with mutation and crossover (ECNNB), on the Fashion-MNIST, MNIST, and EMNIST_Digits datasets. The performance of the ENN algorithm is examined for 10, 50, and 100 generations, with 50 generations being used due to computational limitations. The results show that the accuracy of the model improves as the number of generations increases. However, the ECNNB model consistently outperforms the ENN model on all three datasets, with an average accuracy of 92.58% on Fashion-MNIST, 99.32% on MNIST, and 99.50% on EMNIST_Digits, compared to 88.54 %, 98.05 %, and 98.95 %, respectively, for the ENN model. The performance of both models is compared with other state-of-the-art models in the literature. These results highlight the significance of well-designed models in achieving high accuracy in machine learning tasks.},
  keywords={Computational modeling;Neural networks;Evolutionary computation;Machine learning;Task analysis;Image classification;Neural Networks;Convolutional Neural Network;Neural Evolutionary;Fashion_MNIST;MNIST;EMNIST_Digits},
  doi={10.1109/CSCE60160.2023.00041},
  ISSN={},
  month={July},}@ARTICLE{10327761,
  author={Chen, Yifan and Zhao, Yang and Li, Xuelong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Adaptive Gait Feature Learning Using Mixed Gait Sequence}, 
  year={2025},
  volume={36},
  number={1},
  pages={1545-1554},
  abstract={Gait recognition has become a mainstream technology for identification, as it can recognize the identity of subjects from a distance without any cooperation. However, when subjects wear coats (CL) or backpacks (BG), their gait silhouette will be occluded, which will lose some gait information and bring great difficulties to the identification. Another important challenge in gait recognition is that the gait silhouette of the same subject captured by different camera angles varies greatly, which will cause the same subject to be misidentified as different individuals under different camera angles. In this article, we try to overcome these problems from three aspects: data augmentation, feature extraction, and feature refinement. Correspondingly, we propose gait sequence mixing (GSM), multigranularity feature extraction (MFE), and feature distance alignment (FDA). GSM is a method that belongs to data enhancement, which uses the gait sequences in NM to assist in learning the gait sequences in BG or CL, thus reducing the influence of lost gait information in abnormal gait sequences (BG or CL). MFE explores and fuses different granularity features of gait sequences from different scales, and it can learn as much useful information as possible from incomplete gait silhouettes. FDA refines the extracted gait features with the help of the distribution of gait features in real world and makes them more discriminative, thus reducing the influence of various camera angles. Extensive experiments demonstrate that our method has better results than some state-of-the-art methods on CASIA-B and mini-OUMVLP. We also embed the GSM module and FDA module into some state-of-the-art methods, and the recognition accuracy of these methods is greatly improved.},
  keywords={Feature extraction;Legged locomotion;Gait recognition;Cameras;GSM;Data models;Data mining;Feature distance alignment (FDA);gait recognition;multigranularity feature;spatiotemporal gait feature},
  doi={10.1109/TNNLS.2023.3331050},
  ISSN={2162-2388},
  month={Jan},}@INPROCEEDINGS{10425383,
  author={Shanckin, Shyam and Mayank and Singh, Anshuman and Patwadi, Rahul},
  booktitle={2023 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)}, 
  title={Unveiling Latent Spaces with Variational Autoencoders}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Variational Autoencoders learn the latent representation space of the input domain by learning to compress and decompress from a stochastic latent representation space. This allows the information contained in the sample to be compressed into a lower dimension. Therefore, when used on classifiable datasets, it can retain the classification factors when transformed into the latent vector. By learning the conditional distribution within the latent space of each label set within the domain, it is possible to perform categorical generation of artificial samples. In this work, we study the MNIST dataset and demonstrate a novel approach to conditional generation and that even a simple statistical analysis instead of using complex generators can result in a high-accuracy generative model.},
  keywords={Training;Statistical analysis;Neural networks;Stochastic processes;Learning (artificial intelligence);Generators;Logistics;Variational Autoencoder (VAE);Generative Models;Latent Space Modeling;Conditional Generation},
  doi={10.1109/SOLI60636.2023.10425383},
  ISSN={2768-1890},
  month={Dec},}@INPROCEEDINGS{9089924,
  author={Graben, Peter beim and Römer, Ronald and Meyer, Werner and Huber, Markus and Wolff, Matthias},
  booktitle={2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)}, 
  title={Reinforcement Learning of Minimalist Numeral Grammars}, 
  year={2019},
  volume={},
  number={},
  pages={67-72},
  abstract={Speech-controlled user interfaces facilitate the operation of devices and household functions to laymen. State-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic slots to interpret the user's intent. In order to develop proper cognitive information and communication technologies, simple slot-filling should be replaced by utterance meaning transducers (UMT) that are based on semantic parsers and a mental lexicon, comprising syntactic, phonetic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. We outline a reinforcement learning algorithm for the acquisition of the syntactic morphology and arithmetic semantics of English numerals, based on minimalist grammar (MG), a recent computational implementation of generative linguistics. Number words are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as arithmetic terms. Since MG encodes universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.},
  keywords={Semantics;Grammar;Syntactics;Linguistics;Learning (artificial intelligence);Calculus;Morphology},
  doi={10.1109/CogInfoCom47531.2019.9089924},
  ISSN={2380-7350},
  month={Oct},}@INPROCEEDINGS{11058501,
  author={Patias, Ioannis},
  booktitle={2025 8th International Conference on Information and Computer Technologies (ICICT)}, 
  title={The Evolution of Pricing Models in the AI Age: A Comparative Analysis of Time-Based and Results-Based Approaches}, 
  year={2025},
  volume={},
  number={},
  pages={417-422},
  abstract={The traditional paradigm of pricing services, particularly in reliant on intellectual capital and service delivery industries like software development, has been predominantly time-based. However, the oncoming of advanced technologies, such as Large Language Models (LLMs) and generative AI, has disrupted this conventional approach. This paper delves into a comparative analysis of time-based and results-based pricing models, examining their strengths, weaknesses, and implications in the contemporary digital landscape. Time-based pricing, while offering simplicity and predictability, can often lead to inefficiencies and a lack of alignment with client outcomes. Results-based pricing, on the other hand, incentivizes performance and fosters a shared risk-reward relationship between service providers and clients. However, it requires robust measurement frameworks and can be more complex to implement. The emergence of LLMs and generative AI has further complicated the pricing landscape. These technologies can significantly enhance productivity and efficiency, potentially reducing the time required to deliver services. Yet, they also raise questions about the valuation of intellectual property, the allocation of costs, and the definition of “results.” This paper explores the potential benefits and challenges of adopting results-based pricing models in the age of AI. By examining two case studies we aim to provide insights into the factors that influence pricing decisions and the strategies that organizations can employ to optimize their pricing strategies. Ultimately, this research seeks to contribute to a deeper understanding of the evolving dynamics of service pricing in the digital age.},
  keywords={Industries;Analytical models;Adaptation models;Technological innovation;Costs;Generative AI;Pricing;Organizations;Time measurement;Complexity theory;time-based;fixed-price-based;results-based;LLMs},
  doi={10.1109/ICICT64582.2025.00071},
  ISSN={2769-4542},
  month={March},}@INPROCEEDINGS{10905514,
  author={Huang, Wenli and Deng, Ye and Xin, Xiaomeng and Zhao, Zhihong and He, Jinbao and Wang, Jinjun},
  booktitle={IECON 2024 - 50th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={Semi-independent Convolution for Image Inpainting}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In typical image inpainting tasks, the locations and shapes of damaged or masked areas are often random and irregular. Vanilla convolutions, commonly employed in learning-based inpainting models, treat all spatial features as valid and share parameters across different regions. This approach can struggle with irregular damage patterns, leading to inpainted results that may suffer from color discrepancies and blurriness. In this paper, we introduce a novel operator known as Semi-Independent Convolution (SIConv) to tackle this challenge. The proposed SIConv, on top of the regular convolution with shared weights, also introduces dynamic terms that assign their own independent weights to each part of the image, and the overall computation is formulated as a shared convolution parameter with an additional term to describe the local structure. Qualitative and quantitative experiments demonstrate that our method outperforms the state-of-the-art, yielding clearer, more coherent, and visually convincing inpainting results.},
  keywords={Industrial electronics;Shape;Image color analysis;Maintenance engineering;Kernel;Standards;Painting;Image inpainting;convolution;semi-independent},
  doi={10.1109/IECON55916.2024.10905514},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10837773,
  author={Kotha, Aditya and Sankepally, Sainath Reddy and Vasanth, Murukuri S V S V and Kosaraju, Nishoak and Lovanshi, Mayank and Ingle, Rajesh},
  booktitle={2024 4th Asian Conference on Innovation in Technology (ASIANCON)}, 
  title={ConvAE-512: Bandwidth-Efficient Image Compression in Edge with Auto-Encoders}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The exponential growth in data creation and transmission, particularly high-resolution images, poses a significant challenge to network bandwidth, necessitating advanced compression techniques. This paper introduces ConvAE-512, a novel convolutional neural network-based encoder-decoder architecture optimized for edge computing environments. Our approach significantly outperforms existing lossy compression algorithms in image fidelity while efficiently reducing data transmission size and network utilization. ConvAE-512 achieves a compression rate of 6x, outperforming traditional Direct Compression with a higher Structural Similarity Index (SSIM) of 0.9150 and a Peak Signal-to-Noise Ratio (PSNR) of 27.15 dB, underscoring its efficiency in preserving image quality despite compression. These results establish ConvAE-512 as an advanced alternative for efficient image compression, adeptly balancing quality retention and compression ratio.},
  keywords={Image quality;Technological innovation;Image coding;PSNR;Image edge detection;Propagation losses;Indexes;Data communication;Convolutional neural networks;Edge computing;Image Compression;Edge Computing;Neural Networks;Auto-encoders},
  doi={10.1109/ASIANCON62057.2024.10837773},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9396023,
  author={Raina, Sakshi and Gupta, Abhishek},
  booktitle={2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS)}, 
  title={A Study on Various Techniques for Plant Leaf Disease Detection Using Leaf Image}, 
  year={2021},
  volume={},
  number={},
  pages={900-905},
  abstract={Agriculture plays an important role in the economy of any country and there are a lot of varieties of crops for farmers. The problem or issues occurs when the crops are infected by some disease and the farmers do not know about that disease of plants at the right time. And when the disease is detected, the farmers do not know which disease it is. Therefore, the examination of automatic leaf disease detection in agriculture is a fundamental subject of research as it could display advantages in the observation of vast fields of yields and thus identify manifestations of disease as they occur on plant leaves. The study of plant disease means the study of different patterns visible with the eyes above the plants' leaves. By looking at the different color and texture features of the same plant, now it can be analyzed that which portion of the plant is healthy and which part of the plant has a disease. The process of knowing the disease of the plant occurs in the laboratory. This process takes a lot of time and it is very expensive. For that, the researcher used different types of techniques so that disease will be detected on time and expenses should be reduced. So, this research work attempts to describe the approach suggested by the study articles. Different scholars view the images in terms of Artificial Intelligence, Machine learning and demonstrate their achievements and problems that still exist. To draw some assumptions, our study of the various approaches suggested is also given. Image Acquisition, Image Preprocessing, Image segmentation, Feature Extraction and Statistical Analysis, Classification based on classifier are the key steps for the identification of diseases. This paper provides, along with the available datasets, a survey of the available approaches to solving the problem discussed.},
  keywords={Image segmentation;Image recognition;Statistical analysis;Plants (biology);Machine learning;Agriculture;Diseases;Agriculture;Machine Learning;Artificial Neural Network;Image Segmentation;Feature Extraction;Automatic disease Recognition},
  doi={10.1109/ICAIS50930.2021.9396023},
  ISSN={},
  month={March},}@ARTICLE{10064042,
  author={Xu, Jianan and Bi, Wanqing and Yan, Lier and Du, Hongwei and Qiu, Bensheng},
  journal={IEEE Access}, 
  title={An Efficient Lightweight Generative Adversarial Network for Compressed Sensing Magnetic Resonance Imaging Reconstruction}, 
  year={2023},
  volume={11},
  number={},
  pages={24604-24614},
  abstract={Compressed-sensing-based magnetic resonance imaging (CS-MRI) methods can significantly shorten scanning time while ensuring reconstructed image quality. Recently, deep learning methods, particularly generative adversarial networks (GAN), have been introduced into CS-MRI. However, these GAN-based methods suffer from their heavy learning parameters and ignore long-range dependency, which degrades the reconstructed image quality. Thus, the objective of this study is to design an efficient lightweight GAN to achieve more accurate MRI reconstruction. The proposed framework, named SepGAN, utilises depthwise separable convolution as the basic component to reduce the number of learning parameters. Two modules, the dilated depthwise separable convolution dense block and a squeeze-and-excitation lightweight self-attention module were proposed to extract the long-range dependency and improve the representational ability. The focal frequency loss was also involved in assisting the model to focus on high-frequency information. To evaluate the performance of the three proposed methods and SepGAN, two brain datasets were used in our experiment. From the results of our comparison analysis, SepGAN possesses the minimum number of parameters and multiply–accumulate operations (i.e., 7.32 M and 13.62G) and outperforms other methods in a variety of evaluation metrics, especially in Frechet inception distance, proving that reconstructed images of our method have better visual effects. For unseen pathological data, SepGAN can also perform effective reconstruction with explicit tumour textures and boundaries. The experimental results demonstrate that SepGAN can reconstruct high quality images with fewer parameters and exhibit remarkable generalisation ability.},
  keywords={Image reconstruction;Magnetic resonance imaging;Convolutional neural networks;Generative adversarial networks;Training;Radio frequency;Deep learning;GAN;magnetic resonance imaging;deep learning;lightweight network},
  doi={10.1109/ACCESS.2023.3254136},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11027829,
  author={Guan, Dawei and Li, Wei},
  booktitle={2024 17th International Symposium on Computational Intelligence and Design (ISCID)}, 
  title={Image Drag Editing in Diffusion Models with Enhanced Dense-UNet}, 
  year={2024},
  volume={},
  number={},
  pages={5-8},
  abstract={Accurate and controllable image editing is a challenging task that has garnered significant attention in recent years. Pixel-level interactive editing is a hot research direction in this field. The point-based editing framework represented by DragGAN [1] proposes a standard paradigm for interactive editing. More recently, the combination of interactive editing with drag operations and the generative ability of diffusion models has led to even better solutions [2]. However, the generation effect of this scheme still suffers from issue of low robustness of the method. In this work, we proposed DenseDrag to optimize the diffusion based latent space editing process by introducing an additional technique: Dense-UNet latent space feature network. This method increases the semantic and information transmission ability of the latent space feature network, and improves the robustness of the method. We experimentally demonstrate the high quality of DenseDrag.},
  keywords={Accuracy;Semantics;Production;Information processing;Diffusion models;Robustness;Standards;Computational intelligence;diffusion model;drag editing;Densenet;UNet},
  doi={10.1109/ISCID63852.2024.00010},
  ISSN={2473-3547},
  month={Dec},}@ARTICLE{10328884,
  author={Chen, Cuiqun and Ye, Mang and Qi, Meibin and Du, Bo},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SketchTrans: Disentangled Prototype Learning With Transformer for Sketch-Photo Recognition}, 
  year={2024},
  volume={46},
  number={5},
  pages={2950-2964},
  abstract={Matching hand-drawn sketches with photos (a.k.a sketch-photo recognition or re-identification) faces the information asymmetry challenge due to the abstract nature of the sketch modality. Existing works tend to learn shared embedding spaces with CNN models by discarding the appearance cues for photo images or introducing GAN for sketch-photo synthesis. The former unavoidably loses discriminability, while the latter contains ineffaceable generation noise. In this paper, we start the first attempt to design an information-aligned sketch transformer (SketchTrans$_+$+) via cross-modal disentangled prototype learning, while the transformer has shown great promise for discriminative visual modelling. Specifically, we design an asymmetric disentanglement scheme with a dynamic updatable auxiliary sketch (A-sketch) to align the modality representations without sacrificing information. The asymmetric disentanglement decomposes the photo representations into sketch-relevant and sketch-irrelevant cues, transferring sketch-irrelevant knowledge into the sketch modality to compensate for the missing information. Moreover, considering the feature discrepancy between the two modalities, we present a modality-aware prototype contrastive learning method that mines representative modality-sharing information using the modality-aware prototypes rather than the original feature representations. Extensive experiments on category- and instance-level sketch-based datasets validate the superiority of our proposed method under various metrics.},
  keywords={Prototypes;Transformers;Image retrieval;Learning systems;Task analysis;Semantics;Optimization;Asymmetric disentanglement;dynamic synthesis;prototype learning;sketch-photo;recognition},
  doi={10.1109/TPAMI.2023.3337005},
  ISSN={1939-3539},
  month={May},}@ARTICLE{10164104,
  author={Shin, Ah-Hyung and Kim, Seong Tae and Park, Gyeong-Moon},
  journal={IEEE Access}, 
  title={Time Series Anomaly Detection Using Transformer-Based GAN With Two-Step Masking}, 
  year={2023},
  volume={11},
  number={},
  pages={74035-74047},
  abstract={Time series anomaly detection is a task that determines whether an unseen signal is normal or abnormal, and it is a crucial function in various real-world applications. Typical approach is to learn normal data representation using generative models, like Generative Adversarial Network (GAN), to discriminate between normal and abnormal signals. Recently, a few studies actively adopt Transformer to model time series data, but there is no pure Transformer-based GAN framework for time series anomaly detection. As a pioneer work, we propose a new pure Transformer-based GAN framework, called AnoFormer, and its effective training strategy for better representation learning. Specifically, we improve the detection ability of our model by introducing two-step masking strategies. The first step is Random masking: we design a random mask pool to hide parts of the signal randomly. This allows our model to learn the representation of normal data. The second step is Exclusive and Entropy-based Re-masking: we propose a novel refinement step to provide feedback to accurately model the exclusive and uncertain parts in the first step. We empirically demonstrate the effectiveness of re-masking step that generates more normal-like signals robustly. Extensive experiments on various datasets show that AnoFormer significantly outperforms the state-of-the-art methods in time series anomaly detection.},
  keywords={Transformers;Time series analysis;Anomaly detection;Generative adversarial networks;Data models;Generators;Task analysis;Anomaly detection;masking;self-attention;signal reconstruction;transformer;time series analysis},
  doi={10.1109/ACCESS.2023.3289921},
  ISSN={2169-3536},
  month={},}@ARTICLE{10373158,
  author={Wang, Haotian and Yang, Meng and Zheng, Nanning},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={G2-MonoDepth: A General Framework of Generalized Depth Inference From Monocular RGB+X Data}, 
  year={2024},
  volume={46},
  number={5},
  pages={3753-3771},
  abstract={Monocular depth inference is a fundamental problem for scene perception of robots. Specific robots may be equipped with a camera plus an optional depth sensor of any type and located in various scenes of different scales, whereas recent advances derived multiple individual sub-tasks. It leads to additional burdens to fine-tune models for specific robots and thereby high-cost customization in large-scale industrialization. This article investigates a unified task of monocular depth inference, which infers high-quality depth maps from all kinds of input raw data from various robots in unseen scenes. A basic benchmark G2-MonoDepth is developed for this task, which comprises four components: (a) a unified data representation RGB+X to accommodate RGB plus raw depth with diverse scene scale/semantics, depth sparsity ([0%, 100%]) and errors (holes/noises/blurs), (b) a novel unified loss to adapt to diverse depth sparsity/errors of input raw data and diverse scales of output scenes, (c) an improved network to well propagate diverse scene scales from input to output, and (d) a data augmentation pipeline to simulate all types of real artifacts in raw depth maps for training. G2-MonoDepth is applied in three sub-tasks including depth estimation, depth completion with different sparsity, and depth enhancement in unseen scenes, and it always outperforms SOTA baselines on both real-world data and synthetic data.},
  keywords={Task analysis;Data models;Estimation;Training;Semantics;Pipelines;Service robots;Robot;unified model;generalization;depth estimation;depth completion;depth enhancement},
  doi={10.1109/TPAMI.2023.3346466},
  ISSN={1939-3539},
  month={May},}@ARTICLE{10487929,
  author={Iurada, Leonardo and Bucci, Silvia and Hospedales, Timothy M. and Tommasi, Tatiana},
  journal={IEEE Access}, 
  title={Fairness Meets Cross-Domain Learning: A Benchmark of Models and Metrics}, 
  year={2024},
  volume={12},
  number={},
  pages={47854-47867},
  abstract={Deep learning-based recognition systems are deployed at scale for real-world applications that inevitably involve our social life. Although of great support when making complex decisions, they might capture spurious data correlations and leverage sensitive attributes (e.g., age, gender, ethnicity). How to factor out this information while maintaining high performance is a problem with several open questions, many of which are shared with those of the domain adaptation and generalization literature which aims at avoiding visual domain biases. In this work, we propose an in-depth study of the relationship between cross-domain learning (CD) and model fairness, by experimentally evaluating 14 CD approaches together with 3 state-of-the-art fairness algorithms on 5 datasets of faces and medical images spanning several demographic groups. We consider attribute classification and landmark detection tasks: the latter is introduced here for the first time in the fairness literature, showing how keypoint localization may be affected by sensitive attribute biases. To assess the analyzed methods, we adopt widely used evaluation metrics while also presenting their limits with a detailed review. Moreover, we propose a new Harmonic Fairness (HF) score that can ease unfairness mitigation model comparisons. Overall, our work shows how CD approaches can outperform state-of-the-art fairness algorithms and defines a framework with dataset and metrics as well as a code suite to pave the way for a more systematic analysis of fairness problems in computer vision (Code available at: https://github.com/iurada/fairness_crossdomain).},
  keywords={Measurement;Task analysis;Hafnium;Biological system modeling;Benchmark testing;Visualization;Trust management;Face recognition;Detection algorithms;Domain adaptation;domain generalization;fair and trustworthy artificial intelligence;face recognition;landmark detection},
  doi={10.1109/ACCESS.2024.3383841},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10923492,
  author={P, Anbumani and R, Vasantharaja and P, Gokul M and S, Roopesh V and D, Hareesh S},
  booktitle={2024 International Conference on IoT, Communication and Automation Technology (ICICAT)}, 
  title={Improving LLM and Generative Model Efficiency using Predictive Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={69-73},
  abstract={They are highly nontrivial generative models and LLMs that could unlock or open a whole new world of opportunities for data processing and generation. However, it can be inaccurate, slow in calculating and approximating data, and not suitable with some type of problems. Below are some of the questions that have been associated with the current generation of LLMs: To solve these problems, it is vital to enhance LLM’s that are less computational and integrate generative models. In this paper, I have suggested using PA techniques to enhance the performance of both generative models and LLMs; however, limitations and the creep usage of PA techniques should not be overlooked. The project of Natalia Amaglo is regarding combining of PA with generative modelling to develop artificial intelligence interfaces simple enough when a person interacts with various business applications.},
  keywords={Analytical models;Computational modeling;Large language models;Machine learning;Predictive models;Data processing;Data models;Object recognition;Predictive analytics;Next generation networking;Large Language Model (LLM);Data Processing;Generative Models;Predictive Analysis;Machine learning},
  doi={10.1109/ICICAT62666.2024.10923492},
  ISSN={},
  month={Nov},}@ARTICLE{9247443,
  author={Xiang, Zhiyang and Guo, Yibo},
  journal={IEEE Transactions on Games}, 
  title={Controlling Melody Structures in Automatic Game Soundtrack Compositions With Adversarial Learning Guided Gaussian Mixture Models}, 
  year={2021},
  volume={13},
  number={2},
  pages={193-204},
  abstract={The vastness of gaming plots and variety of environments in computer games require a large amount of labors in soundtrack compositions. Since human composers are expensive, artificial intelligence composing techniques have been proposed in several open-source projects. Current technologies have good performances at improvisations in short melody compositions, but face great challenges in industrial level automatic compositions of highly structured tracks for games. In this article, the overall structure specifying transitions and repetitions of melodies is given by human, and detailed contents like notes and rhythms are completed with a Gaussian mixture model (GMM) and generative adversarial nets (GAN). Different from recurrent neural networks, which are the mainstream automated melody generators, the GMM can be controlled to form structures because its latent space is often similar to the data space. A layered framework is devised where the basic layer composes melodies and high-level layers organize melodies according to long-term structures. In each layer, a Gaussian mixture generative model with constraints is constructed to compose candidate tracks, whereas another GMM network is trained in competition with the generator, such that optimal tracks from the generator are identified. Experiments show that the proposed framework has a high rate of composing acceptable soundtracks. Entropy curves calculated show that the composed tracks are more similar to game soundtracks than existing methods. In a user study, 11 out of 16 human criticizers favor the proposed compositions over the original GAN and recurrent neural networks.},
  keywords={Games;Generators;Gallium nitride;Artificial intelligence;Generative adversarial networks;Recurrent neural networks;Task analysis;Automatic composition;content creation;Gaussian mixture model (GMM);generative adversarial nets (GAN)},
  doi={10.1109/TG.2020.3035593},
  ISSN={2475-1510},
  month={June},}@INPROCEEDINGS{10421629,
  author={Preeti and Bansal, Sandhya},
  booktitle={2023 Second International Conference on Informatics (ICI)}, 
  title={Artifact Based Deepfake Detection Methods}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={One of the biggest innovations of Artificial Intelligence (AI) is the ability to generate manipulated or synthesized media (images, videos). When these generated media is created in to look like real and original people then it is called a Deepfake. Deepfakes have gained attention due to their potential to create convincing and misleading content that can be difficult to distinguish from authentic media. While they have garnered positive value in entertainment sector but have seen as a biggest threat regarding its ethical and societal implications particularly in the context of misinformation, identity theft, privacy invasion, and defamation. Rigorous research has been done since its beginning to prevent and detect media forgery and many detection techniques have been developed and discussed in literature. There is no clear winner in identification of DeepFakes however the artefact-based approaches seem to be much superior in the literature. In order to find the lean and identify best detection technique, an evaluation of existing techniques is needed which is the main purpose of our paper. This compares the video forgery detection techniques in terms of convergence, accuracy and equal error rate (EER).},
  keywords={Deepfakes;Technological innovation;Privacy;Ethics;Forgery;Artificial intelligence;Informatics;Artificial Intelligence;Machine Learning;Generative AI;DeepFakes;Artifacts},
  doi={10.1109/ICI60088.2023.10421629},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10555683,
  author={Bonny, Justin W. and Wynne, Kevin T.},
  booktitle={2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS)}, 
  title={Increasing Human-Likeness and Acceptance of Conversational Autonomy through Experience}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={An increasing number of online platforms offer access to conversational artificial intelligence (AI), such as ChatGPT, allowing individuals to engage and talk with AI. Conversing using natural language may foster perceptions of AI as having more human-like qualities. Within the context of human-autonomy teaming (HAT), this may contribute to AI being viewed more as a human-like teammate than an instrumental tool. The present research investigated factors that influenced the perceptions of conversational AI as a human-like teammate. Using the autonomous agent teammate-likeness (AAT) framework, participants were presented with simulated conversations between an AI and user within a smartphone app across multiple contexts. Participants were more likely to rate the AI as a human-like teammate with increased exposure to conversations, when they had a less negative attitude towards robots, and with greater general propensity to trust. In addition, perceptions of human-likeness were associated with (stronger) perceptions of the AI as trustworthy, useful, and acceptable. This suggests that fostering human-likeness via experience with natural language conversations can contribute to AI being perceived more like a human teammate and improve (HAT) outcomes.},
  keywords={Instruments;Human-machine systems;Natural languages;Oral communication;Chatbots;Autonomous agents;Artificial intelligence;human-autonomy teaming;artificial intelligence;generative AI;trust;ChatGPT},
  doi={10.1109/ICHMS59971.2024.10555683},
  ISSN={},
  month={May},}@INPROCEEDINGS{9643330,
  author={Niculescu, Mihai Alexandru and Ruseti, Stefan and Dascalu, Mihai},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={RoGPT2: Romanian GPT2 for Text Generation}, 
  year={2021},
  volume={},
  number={},
  pages={1154-1161},
  abstract={Text generation is one of the most important and challenging tasks in NLP, where models have shown a significant performance increase in recent years. However, most generative models are available only for English, whereas low-resource languages like Romanian have no available alternatives. As such, we introduce RoGPT2, a Romanian version of the GPT2 model, trained on the largest corpus available for the Romanian language. Three versions of the model were trained, namely base (124M parameters), medium (354M parameters), and large (774M parameters). Six tasks from the LiRo benchmark were selected to test the performance and limitations of our encoder versus BERT-Base models for Romanian (RoBERT, BERT-ro-base, and RoDiBERT). RoGPT2 manages to achieve similar or even better performance, except for the task of zero-shot learning cross-lingual question answering. RoGPT2 also obtains state-of-the-art results for grammar error correction (RoGEC) using the RONACC corpus, thus arguing for the model’s capability to generate grammatically correct text (F0.5 = 69.01). In addition, we introduce two use cases in which we showcase the different versions and explore the extent to which RoGPT2 is able to continue Romanian news articles. After fine-tuning, the model generated rather long text which accounts for the context of the news.},
  keywords={Conferences;Benchmark testing;Knowledge discovery;Grammar;Error correction;Task analysis;Artificial intelligence;GPT2;Generative Model for Romanian;LiRo benchmark;Romanian Grammatical Error Corrections;News generation},
  doi={10.1109/ICTAI52525.2021.00183},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{9415242,
  author={Sasada, Taisho and Kawai, Masataka and Taenaka, Yuzo and Fall, Doudou and Kadobayashi, Youki},
  booktitle={2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Differentially-Private Text Generation via Text Preprocessing to Reduce Utility Loss}, 
  year={2021},
  volume={},
  number={},
  pages={042-047},
  abstract={To provide user-generated texts to third parties, various anonymization used to process the texts. Since this anonymization assume the knowledge possessed by the adversary, sensitive information may be leaked depending on the adversary's knowledge even after this anonymization. Moreover, setting the strongest assumptions about the adversary's knowledge leads to the degradation of the utility as the data by removing any quasi-identifiers. Therefore, instead of providing original data, a method to generate differentially-private synthetic data has been proposed. Differential privacy is more flexible than anonymization technologies because it does not require the assumption of the adversary's knowledge. However, if a large noise is added to the gradient in text generative model to satisfy differential privacy, the utility of the synthetic text is degraded. Since differential privacy can be satisfied with a small noise in data containing duplicates, it is possible to reduce utility loss as text by creating duplicates before adding noise. In this study, we reduce the amount of noise added by creating duplicates through generalization, thereby minimizing text utility loss. By constructing a differentially-private text generation model, we can provide synthetic text and promote text utilization while protecting privacy information in the text.},
  keywords={Degradation;Differential privacy;Privacy;Gaussian noise;Bit error rate;Data models;Artificial intelligence;Privacy-Preserving Data Mining;Data Privacy;Generative Model;Differential Privacy},
  doi={10.1109/ICAIIC51459.2021.9415242},
  ISSN={},
  month={April},}@INPROCEEDINGS{10062688,
  author={Abdullah, Malak and Madain, Alia and Jararweh, Yaser},
  booktitle={2022 Ninth International Conference on Social Networks Analysis, Management and Security (SNAMS)}, 
  title={ChatGPT: Fundamentals, Applications and Social Impacts}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Recent progress in large language models has pushed the boundaries of natural language processing, setting new standards for performance. It is remarkable how artificial intelligence can mimic human behavior and writing style in such a convincing way. As a result, it is hard to tell if a human or a machine wrote something. Deep learning and natural language processing have recently advanced large language models. These newer models can learn from large amounts of data to better capture the nuances of language, making them more accurate and robust than ever before. Additionally, these models can now be applied to tasks such as summarizing text, translating between languages, and even generating original content. ChatGPT is a natural language processing (NLP) model developed in 2022 by OpenAI for open-ended conversations. It is based on GPT-3.5, the third-generation language processing model from OpenAI. ChatGPT can power conversational AI applications like virtual assistants and chatbots. In this paper, we describe the current version of ChatGPT and discuss the model's potential and possible social impact. Disclaimer: This paper was not written by ChatGPT: it was written by the listed authors.},
  keywords={Vocabulary;Automation;Customer services;Virtual assistants;Education;Oral communication;Writing;ChatGPT;Generative Pre-trained Transformer;Language Models;Social Impact},
  doi={10.1109/SNAMS58071.2022.10062688},
  ISSN={2831-7343},
  month={Nov},}@INPROCEEDINGS{10413684,
  author={Naffziger, S.},
  booktitle={2023 International Electron Devices Meeting (IEDM)}, 
  title={Innovations For Energy Efficient Generative AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={While the explosion in the capabilities of generative artificial intelligence (AI) and the associated benefits has received a tremendous amount of attention, both in the technology world and popular press, so has the unprecedented amount of compute and energy required to train and serve these extremely complex models. Many of these concerns are well founded given the hundreds of megawatthours required to train large language models (LLMs). However, our contention is that history and recent developments indicate that our industry’s ability to innovate is up to the task of delivering the benefits of LLMs while reining in energy use. This paper will review some of that history and the approaches that are likely to provide the required efficiency gains.},
  keywords={Training;Technological innovation;Solid modeling;Generative AI;Computational modeling;Memory management;Market research},
  doi={10.1109/IEDM45741.2023.10413684},
  ISSN={2156-017X},
  month={Dec},}@INPROCEEDINGS{10631508,
  author={Lin, Chien-Hung and Hsu, Jeng-Yun and Yu, Cheng-Ying and Hsu, Chia-Wei and Tsai, Yi-Min and Wu, Kuo-Sheng and Huang, Chung-Lun and Hsieh, Meng-Han and Lin, Tsung-Yao},
  booktitle={2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)}, 
  title={A Quad-Core AI Processing Unit for Generative AI in 4nm 5G Smartphone SoC}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={This work presents the quad-core AI processing unit (APU) for efficient execution of generative artificial intelligence (GenAI) applications on smartphones. GenAI applications featured with billions of computations and memory accesses are out of tune with smartphone platforms. The APU applies mixed-precision and hybrid-precision operations, inter-core direct data link, zero-overhead layer fusion, and data broadcasting to reduce 50% of DRAM footprint and 90% of DRAM BW. The 4nm 5G smartphone powered by the APU takes less than 1 second to generate an image with optimized stable diffusion (SD) and outputs more than 20 tokens per second with 7B speculative large language model (LLM).},
  keywords={Generative AI;5G mobile communication;Large language models;Random access memory;Very large scale integration;Broadcasting;Smart phones},
  doi={10.1109/VLSITechnologyandCir46783.2024.10631508},
  ISSN={2158-9682},
  month={June},}@INPROCEEDINGS{10892995,
  author={Barlowe, Scott and Aoulou, Daniel and Ponce-Castillo, Alex},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={WIP: Generative AI as an Instructional Resource in a Computer Science Ethics Course}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This work-in-progress research paper reports our initial attempt at integrating generative artificial intelligence (Gen AI) into our two credit hour ethics course required for computer science majors. Course content includes ethical frameworks, presentations, current event and stakeholder analysis, formal debates, job seeking, and codes of conduct. Given the wide applicability of computer ethics and the crowded schedule in the course, our inquiry seeks to find ways of utilizing Gen AI to streamline content delivery, to provide opportunities for independent student exploration, and to aid students during preparation for class activities. In this paper, we first describe a novel assignment integrating Gen AI given to students enrolled in the Spring 2024 offering of our computer science ethics course. The findings from a survey addressing student use of Gen AI before and during the assignment and the analysis of assignment artifacts submitted by students are then reported. Finally, we present additional student data and results from our separate experimentation, both of which focus on the use of Gen AI for debate preparation. Our efforts reveal that Gen AI can be a useful instructional tool for a computer science ethics course but should be integrated carefully.},
  keywords={Computer science;Surveys;Ethics;Schedules;Codes;Generative AI;Stakeholders;Springs;ethics;professional skills;computer science},
  doi={10.1109/FIE61694.2024.10892995},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10496651,
  author={Nezhad, Mansoureh Motahari and Kangavari, Mohammadreza},
  booktitle={2024 10th International Conference on Artificial Intelligence and Robotics (QICAR)}, 
  title={Personalized Persuasive Text Generation}, 
  year={2024},
  volume={},
  number={},
  pages={261-267},
  abstract={Persuasion has been defined as a purposeful attempt to change attitudes or behaviors in many organizations that are trying to influence the beliefs, opinions, decisions, or behaviors of their audiences. In various areas, persuasive messages have demonstrated their effectiveness. In order to be persuasive, these messages use psychological strategies. Using strategies can be effective, but their efficacy varies for each individual. For maximum effect, these messages can be tailored to each person based on their personal information and related strategies. During the past few years, natural language text generation methods have improved for automated text generation. However, according to the literature review, no method exists to intelligently generate text for personalized persuasion. In addition, most persuasive technologies are developed with the aim of changing user behavior to achieve user persuasion over time, while changing individual attitudes with just one text has not been sufficiently addressed. By using the CATGAN model and considering the personality of the audience, we present a Personalized Persuasive Text Generation System that achieves state-of-the-art results in a personalized persuasion text generation.},
  keywords={Bibliographies;Natural languages;Psychology;Organizations;Generative adversarial networks;Data models;Artificial intelligence;Text Generation;Persuasion;Personalization},
  doi={10.1109/QICAR61538.2024.10496651},
  ISSN={},
  month={Feb},}@ARTICLE{11178141,
  author={Ebert, Christof},
  journal={Computer}, 
  title={The Generative AI Dividend: Cost Reductions in Software and IT}, 
  year={2025},
  volume={58},
  number={10},
  pages={152-156},
  abstract={Traditional cost-cutting in software and IT often leads to a downward spiral of low-quality, rework, and loss of innovation. Leading companies use generative artificial intelligence to cut costs, but not for innovation. We explore how to reduce costs with GenAI.},
  keywords={Technological innovation;Costs;Spirals;Generative AI;Companies;Software},
  doi={10.1109/MC.2025.3593102},
  ISSN={1558-0814},
  month={Oct},}@INPROCEEDINGS{10409847,
  author={Kobasyashi, Atsuya and Yamaguchi, Saneyasu},
  booktitle={2023 Eleventh International Symposium on Computing and Networking Workshops (CANDARW)}, 
  title={Extraction of Subjective Information from Generative AI Models}, 
  year={2023},
  volume={},
  number={},
  pages={313-317},
  abstract={Pre-trained language models based on deep learning have achieved significant improvements in natural language processing. Many studies have been published to investigate the knowledge in language models. In this paper, we investigate the extraction of subjective information from generative artificial intelligence models, show that the extraction of subjective information from language models is possible, and discuss superior extraction methods.},
  keywords={Deep learning;Generative AI;Conferences;Computational modeling;Focusing;Natural language processing;Data mining;large language model;subjective information;Natural Language Processing;NLP},
  doi={10.1109/CANDARW60564.2023.00059},
  ISSN={2832-1324},
  month={Nov},}@INBOOK{10359422,
  author={Banafa, Ahmed},
  booktitle={Transformative AI: Responsible, Transparent, and Trustworthy AI Systems}, 
  title={7 Generative AI: Types, Skills, Opportunities, and Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={41-48},
  abstract={Transformational Artificial Intelligence provides a comprehensive overview of the latest trends, challenges, applications, and opportunities in the field of Artificial Intelligence. The book covers the state of the art in AI research, including machine learning, natural language processing, computer vision, and robotics, and explores how these technologies are transforming various industries and domains, such as healthcare, finance, education, and entertainment. The book also addresses the challenges that come with the widespread adoption of AI, including ethical concerns, bias, and the impact on jobs and society. It provides insights into how to mitigate these challenges and how to design AI systems that are responsible, transparent, and trustworthy. The book offers a forward-looking perspective on the future of AI, exploring the emerging trends and applications that are likely to shape the next decade of AI innovation. It also provides practical guidance for businesses and individuals on how to leverage the power of AI to create new products, services, and opportunities. Overall, the book is an essential read for anyone who wants to stay ahead of the curve in the rapidly evolving field of Artificial Intelligence and understand the impact that this transformative technology will have on our lives in the coming years.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770040181},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10359422},}@INPROCEEDINGS{10698147,
  author={Lee, Hae Won and Park, Ga Eun and Kim, Bo Min and Bae, Seong Geon},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Improvement Research of Zero-DCE-Based Algorithm for Low-Light Style Transfer in Generative Cartoon GANs}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Among the rapidly developed generative artificial intelligence, CartoonGAN is attracting a lot of attention as it can overcome the limitations of Cartoon production occurring in fields such as Cartoon and animation. CartoonGAN has a problem of not properly preserving the subject shape and edge of a low-light image. To this end, research should be conducted to recover the shape of the subject and improve the structural features of the image through the adjustment of brightness and noise. Therefore, in this study, CartoonGAN, a Cartoon style image generation model using deep learning, proposes a method of applying Zero-DCE to the pre-processing process to overcome the limitations arising from low-light image processing. Through this process, the brightness and color of a low-light image are improved without a reference image. The performance of the proposed algorithm was evaluated through image quality evaluation indicators such as a peak signal-to-noise ratio (PSNR) and a structural similarity index (SSIM). The experimental results showed that the proposed algorithm is effective in improving the performance of CartoonGAN. In addition, it was confirmed that the proposed algorithm effectively improved the brightness of the image through the measurement of the brightness improvement, and a performance evaluation for the recovery of subject information was also performed by introducing a new evaluation method. These results show that CartoonGAN can be improved and effectively applied to low-light image processing. Therefore, this study contributes to the development of image generation technology based on deep learning and presents an important direction to provide high performance in real-world application fields.},
  keywords={Deep learning;Performance evaluation;Image quality;PSNR;Image synthesis;Shape;Generative AI;Brightness;Data models;Synthetic data;GaN;CartoonGAN;Zero-DCE;Low-light Image;Computer Vision},
  doi={10.1109/ICECET61485.2024.10698147},
  ISSN={},
  month={July},}@INPROCEEDINGS{11064451,
  author={Huang, Xin and Zhang, Ziqi and Qi, Yijiashun and Li, Yankaiqi and Li, Xuan and Li, Xinshi},
  booktitle={2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={Reinforcement Learning-Based Q-Learning Approach for Optimizing Data Mining in Dynamic Environments}, 
  year={2025},
  volume={},
  number={},
  pages={1783-1787},
  abstract={This study proposed a new data mining strategy based on the reinforcement learning Q-learning method to solve complex decision-making optimization problems in dynamic environments. By modeling the data mining task as a Markov decision process, the study designed a reasonable state representation, reward mechanism and strategy update method, and experimentally verified it on a real data set. The results show that this method outperforms traditional machine learning models and deep learning models in key indicators such as cumulative return value and accuracy, showing strong generalization and global optimization capabilities. The study also shows that the Q-learning method can adapt to time series and highly dynamic environments, providing an effective solution for optimizing long-term returns. In the future, combining deep learning technology with more complex practical application scenarios, this method is expected to further enhance the intelligence and real-time performance of data mining, laying a solid foundation for its widespread application in the field of artificial intelligence.},
  keywords={Adaptation models;Q-learning;Computational modeling;Decision making;Time series analysis;Reinforcement learning;Real-time systems;Data models;Data mining;Optimization;Reinforcement learning;Q-learning;data mining;dynamic decision making},
  doi={10.1109/NNICE64954.2025.11064451},
  ISSN={},
  month={Jan},}@ARTICLE{10836820,
  author={Zhu, Ye and Han, Jiaxin and Pan, Bin and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={DiffPR-Net: Few-Shot Remote Sensing Scene Classification Based on Generative Diffusion and Prototype Rectified Model}, 
  year={2025},
  volume={63},
  number={},
  pages={1-13},
  abstract={Few-shot remote sensing scene classification (FSRSSC) aims to identify unseen scene classes from limited labeled samples, facing the challenge of accurately modeling data distribution and preserving image details in complex backgrounds with high intraclass variance and interclass similarity. To address this challenge, we propose a novel diffusion prototype rectified network (DiffPR-Net), which is comprised of three core modules: diffusion augmentation (DA), dual attention fusion module (DAFM), and prototype rectified module (PRM). The DA is constructed to generate high-quality remote sensing images with the objective of augmenting the training dataset. Besides, the DAFM facilitates the model to focus discriminative regions by transmitting highly fused image detail features from higher to lower layers. What is more, the PRM addresses prototype deviation by adaptively assigning temporary labels to unlabeled data based on prediction confidence, thereby correcting the initial prototypes. Experiments indicate that our proposed method is highly promising, achieving competitive or state-of-the-art (SOTA) classification performance while addressing the scarcity of remotely sensed data and enhancing focus on discriminative regions.},
  keywords={Remote sensing;Prototypes;Scene classification;Diffusion models;Feature extraction;Training;Data models;Data augmentation;Sensors;Semantics;Data augmentation;diffusion model;few-shot remote sensing scene classification (FSRSSC);prototype network},
  doi={10.1109/TGRS.2025.3528071},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{11108614,
  author={Sun, Xiuxiu and Zhang, Peng and Zhao, Mingming and He, Sijia},
  booktitle={2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)}, 
  title={Impact of Large-Model Temperature on Generative Relation Extraction Results}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper investigates the impact of temperature parameters in large language models (LLMs) on relation extraction. Through experimental studies, we analyze the performance of LLMs in terms of accuracy, completeness, and consistency for relation extraction tasks under different temperature settings. The experiments show that, in terms of accuracy, at low temperatures ($T<1$), accuracy first increases and then decreases as the temperature rises. In contrast, at high temperatures ($T \geq 1$), accuracy decreases as the temperature increases. At low temperatures, relation extraction demonstrates high precision for common relation types but suffers from low recall for rare relations. As the temperature increases, the recall rate for rare relations improves; however, overall accuracy declines due to an increase in misjudgments. Studying the performance of different model parameters at varying temperatures reveals the synergistic effect between “version iteration optimization” and “parameter scale dividend” in LLMs. Regarding model versions, Llama3.1-70b, and Llama3-70b exhibit better accuracy and completeness at low temperatures ($T<1$), while Llama3-70b shows the best performance in these aspects at high temperatures ($T \geq 1$). In terms of model parameters, increasing the parameter scale yields higher task accuracy, smaller fluctuations in the F1 score, and more stable performance. For consistency in multi-round extraction, low temperatures yield results with high similarity and strong consistency but weak capability to capture dynamic information. In contrast, high temperatures result in low similarity and poor consistency.},
  keywords={Accuracy;Fluctuations;Large language models;Machine vision;Data mining;Optimization;Large language models;Temperature parameter;Relation extraction},
  doi={10.1109/PRMVAI65741.2025.11108614},
  ISSN={},
  month={June},}@INPROCEEDINGS{11021974,
  author={Li, Ziyang and Wu, Xibao and Meng, Xintong and Shan, Zhenyu and Wu, Peiliang and Chen, Wenbai},
  booktitle={2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Application of a Push-Grasping Generative Residual Attention Network in Self-Supervised Systems}, 
  year={2024},
  volume={},
  number={},
  pages={435-439},
  abstract={Grasping and pushing actions based on reinforcement learning can effectively capture target objects. However, grasping a specific target in a cluttered environment remains a challenging problem. To enhance the efficiency of robotic grasping in environments with clutter and invisible targets, and to train a more effective grasping strategy network. This study proposes an action network embedding target location information, using a two-stage self-supervised learning approach to train the policy network. This paper initially describes the problem of self-supervised learning for robot pushing and grasping tasks in cluttered environments. It defines the decision-making process for robot pushing and grasping operations within the workspace as a Markov Decision Process (MDP). Subsequently, the study implements an end-to-end network framework that transitions from image capture to action generation. It utilizes two layers of Residual processing to extract features from images and generate feature images independent of the actions through a visual model. The study introduced a Coordination Attention Convolutional Neural Network (CACNN) module, based on a position attention mechanism, into the action network to enhance the FCN's capability for feature extraction of the grasping target. This enhancement more effectively achieves synergy between pushing and grasping actions. Finally, the study conducted experimental comparisons and analyses to validate the effectiveness of the proposed method.},
  keywords={Training;Visualization;Service robots;Robot kinematics;Training data;Grasping;Self-supervised learning;Reinforcement learning;Feature extraction;Robot learning;pushing-grasping skill learning;self-supervised learning;feature extraction;coordination attention},
  doi={10.1109/ACAIT63902.2024.11021974},
  ISSN={},
  month={Nov},}@INBOOK{11164765,
  author={Boudreau, Paul},
  booktitle={Applying Artificial Intelligence to Project Management}, 
  title={Chapter 9: Generative AI and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={111-118},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9781501519406},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11164765},}@INBOOK{11164683,
  author={Velu, Anitha and Ramamoorthy, Raghu and Manasa, S. M. and Prasanth, A.},
  booktitle={Generative AI and LLMs: Natural Language Processing and Generative Adversarial Networks}, 
  title={5 LLM Pretraining Methods}, 
  year={2024},
  volume={},
  number={},
  pages={93-116},
  abstract={Generative artificial intelligence (AI), an AI technique produces original text, sounds, 3D models, animation, and images. It is powered by large-scale machine learning (ML) models that leverage pretrained deep neural networks on massive datasets. Pretraining mostly aims to guess the next word in a sentence or fill in masked words inside the sequence. Through this unsupervised learning exercise, the model learns to comprehend the linguistic structures and statistical trends. Through pretraining, large language model (LLM) acquires a general understanding of syntax, grammar, and semantics. It helps in establishing a strong basis for language comprehension and enables the model to depict the relationships between words. The pretraining of data is primarily responsible for the extraordinary powers of LLMs. It is similar to bombarding the brain with vast volumes of data so that it learns the laws of language and the outside world before being taught particular skills. Next sentence prediction, contrastive learning, masked language modeling, sentence-level and document-level objectives, denoising autoencoders (DAEs), and other pretraining approaches of LLM are covered in this chapter along with their significance. Pretrained models increase efficiency and reduce the need for additional training by enabling information to be applied to subsequent tasks. Pretraining of data has a greater range of application areas like transfer learning, classification, and feature extraction.},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111425511},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11164683},}@INPROCEEDINGS{10814840,
  author={Canelón, Rodolfo and Cruz, Francisco and Barrios, Irisysleyer},
  booktitle={2024 IEEE Latin American Conference on Computational Intelligence (LA-CCI)}, 
  title={A text mining-based method for domains using AI generative text-to-text}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Software reuse is an important principle within a software development process model, that efficiently and effectively manages the different variations that may exist between products of the same family, centered on a domain. However most existing software systems are developed without a process model that clearly defines the disciplines, activities, artifacts, and the actors involved, without considering software development as an engineering discipline with predictable times, cost estimates and obtaining a quality product. An agile model is proposed and evaluated that enables a domain engineering process with the disciplines of analysis, design and implementation using generative artificial intelligence and domain engineering with software production lines to build families of natural language text analysis applications. The study has succeeded in demonstrating the feasibility and effectiveness of software reuse in the text mining domain. By applying domain engineering principles and software production lines, it has been possible to create families of text analysis applications more efficiently and effectively. The results obtained provide new evidence to support the usefulness of this approach in software development.},
  keywords={Text mining;Analytical models;Text analysis;Runtime;Recurrent neural networks;Soft sensors;Production;Software systems;Software reusability;Software development management;IA generative;Text mining;agile processes;domain engineering;software architecture},
  doi={10.1109/LA-CCI62337.2024.10814840},
  ISSN={2769-7622},
  month={Nov},}@ARTICLE{9912427,
  author={Aliyu, Ibrahim and Van Engelenburg, Sélinde and Mu’Azu, Muhammed Bashir and Kim, Jinsul and Lim, Chang Gyoon},
  journal={IEEE Access}, 
  title={Statistical Detection of Adversarial Examples in Blockchain-Based Federated Forest In-Vehicle Network Intrusion Detection Systems}, 
  year={2022},
  volume={10},
  number={},
  pages={109366-109384},
  abstract={The internet-of-Vehicle (IoV) can facilitate seamless connectivity between connected vehicles (CV), autonomous vehicles (AV), and other IoV entities. Intrusion Detection Systems (IDSs) for IoV networks can rely on machine learning (ML) to protect the in-vehicle network from cyber-attacks. Blockchain-based Federated Forests (BFFs) could be used to train ML models based on data from IoV entities while protecting the confidentiality of the data and reducing the risks of tampering with the data. However, ML models are still vulnerable to evasion, poisoning and exploratory attacks by adversarial examples. The BFF-IDS offers partial defence against poisoning but has no measure for evasion attacks, the most common attack/threat faced by ML models. Besides, the impact of adversarial examples transferability in CAN IDS has largely remained untested. This paper investigates the impact of various possible adversarial examples on the BFF-IDS. We also investigated the statistical adversarial detector’s effectiveness and resilience in detecting the attacks and subsequent countermeasures by augmenting the model with detected samples. Our investigation results established that BFF-IDS is very vulnerable to adversarial examples attacks. The statistical adversarial detector and the subsequent BFF-IDS augmentation (BFF-IDS(AUG)) provide an effective mechanism against the adversarial examples. Consequently, integrating the statistical adversarial detector and the subsequent BFF-IDS augmentation with the detected adversarial samples provides a sustainable security framework against adversarial examples and other unknown attacks.},
  keywords={Biological system modeling;Data models;Intrusion detection;Security;Adversarial machine learning;Detectors;Blockchains;Federated learning;Artificial intelligence;Internet of Vehicles;Connected vehicles;Vehicular ad hoc networks;Adversarial examples;artificial intelligent (AI);blockchain;controller area network (CAN);federated learning;intrusion detection system (IDS)},
  doi={10.1109/ACCESS.2022.3212412},
  ISSN={2169-3536},
  month={},}@ARTICLE{10697933,
  author={Hussain, Manzoor and Hong, Jang-Eui},
  journal={IEEE Transactions on Reliability}, 
  title={Evaluating and Improving Adversarial Robustness of Deep Learning Models for Intelligent Vehicle Safety}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Deep learning models have proven their effectiveness in intelligent transportation. However, their vulnerability to adversarial attacks poses significant challenges to traffic safety. Therefore, this article presents a novel technique to evaluate and improve the adversarial robustness of the deep learning models. We first proposed a deep-convolutional-autoencoder-based adversarial attack detector that identifies whether or not the input samples are adversarial. It serves as a preliminary step toward adversarial attack mitigation. Second, we developed a conditional generative adversarial neural network (c-GAN) to transform the adversarial images back to their original form to alleviate the adversarial attacks by restoring the integrity of perturbed images. We present a case study on the traffic sign recognition model to validate our approach. The experimental results showed the effectiveness of the adversarial attack mitigator, achieving an average structure similarity index measure of 0.43 on the learning interpretable skills abstractions (LISA)-convolutional neural network (CNN) dataset and 0.38 on the German traffic sign recognition benchmark (GTSRB)-CNN dataset. While evaluating the peak signal noise ratio, the c-GAN model attains an average of 18.65 on the LISA-CNN and 18.05 on the GTSRB-CNN dataset. Ultimately, the proposed method significantly enhanced the average detection accuracy of adversarial traffic signs, elevating it from 72.66% to 98% on the LISA-CNN dataset. In addition, an average of 28% improvement in accuracy was observed on the GTSRB-CNN.},
  keywords={Perturbation methods;Training;Robustness;Safety;Prevention and mitigation;Iterative methods;Computational modeling;Accuracy;Transportation;Roads;Adversarial attacks;adversarial defense;autoencoder;deep learning (DL);generative adversarial neural network;robustness;safety;trusted artificial intelligence},
  doi={10.1109/TR.2024.3458805},
  ISSN={1558-1721},
  month={},}@ARTICLE{10301699,
  author={Fei, Fan and Cheng, Yean and Zhu, Yongjie and Zheng, Qian and Li, Si and Pan, Gang and Shi, Boxin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SPLiT: Single Portrait Lighting Estimation via a Tetrad of Face Intrinsics}, 
  year={2024},
  volume={46},
  number={2},
  pages={1079-1092},
  abstract={This paper proposes a novel pipeline to estimate a non-parametric environment map with high dynamic range from a single human face image. Lighting-independent and -dependent intrinsic images of the face are first estimated separately in a cascaded network. The influence of face geometry on the two lighting-dependent intrinsics, diffuse shading and specular reflection, are further eliminated by distributing the intrinsics pixel-wise onto spherical representations using the surface normal as indices. This results in two representations simulating images of a diffuse sphere and a glossy sphere under the input scene lighting. Taking into account the distinctive nature of light sources and ambient terms, we further introduce a two-stage lighting estimator to predict both accurate and realistic lighting from these two representations. Our model is trained supervisedly on a large-scale and high-quality synthetic face image dataset. We demonstrate that our method allows accurate and detailed lighting estimation and intrinsic decomposition, outperforming state-of-the-art methods both qualitatively and quantitatively on real face images.},
  keywords={Lighting;Faces;Estimation;Reflectivity;Three-dimensional displays;Rendering (computer graphics);Pipelines;Face modeling;intrinsic image decomposition;lighting estimation},
  doi={10.1109/TPAMI.2023.3328453},
  ISSN={1939-3539},
  month={Feb},}@INPROCEEDINGS{10625574,
  author={Poongodi, K. and Nivesh, R. Abhi and Blessed Jeberson, J. Q. and Divakar, V. S. and Gowshikan, S.},
  booktitle={2024 2nd International Conference on Sustainable Computing and Smart Systems (ICSCSS)}, 
  title={Integrating Beck's Depression Inventory and Emotion Detection Using GAN for Comprehensive Depression Assessment and Intervention}, 
  year={2024},
  volume={},
  number={},
  pages={1038-1046},
  abstract={This research study suggests a novel approach to measure depression levels by integrating emotion recognition using Generative Adversarial Networks (GAN) and Beck's Depression Inventory (BDI). The challenge encountered here, was that relying solely on facial recognition could lead to inaccurate results, as individuals may manipulate their expressions. To address this, we integrated BDI score assessment to supplement emotion detection. Additionally, we faced the challenge of ensuring the authenticity of facial recognition amidst potential manipulation. Therefore, we implemented face recognition for each BDI question, ensuring accurate emotion detection by the completion of the questionnaire. Our technique combines state-of-the-art AI-driven emotion analysis with the widely used BDI psychological evaluation tool to offer a comprehensive and multimodal approach in diagnosing depression. This method provides a more complete image of the person's emotional state by analyzing self-reported BDI scores and emotional data produced by the GAN model. A thorough and accurate assessment of the degree of depression is produced when these tests are combined. Further, the system provides customized activities and intervention strategies for people with varying degrees of distress based on the determined depression level. This study represents a significant advancement in the use of technology for extensive mental health screening and treatment.},
  keywords={Emotion recognition;Analytical models;Accuracy;Face recognition;Mental health;Green computing;Depression;Depression assessment;Emotional expression recognition;Generative adversarial networks;Beck's depression inventory;Artificial intelligence;Degree of depression},
  doi={10.1109/ICSCSS60660.2024.10625574},
  ISSN={},
  month={July},}@ARTICLE{11114441,
  author={Cebrian Carcavilla, Adriano and Meribout, Mahmoud},
  journal={IEEE Sensors Journal}, 
  title={Algorithms on Electrical Impedance Tomography, Focusing on Deep Learning Architectures and Their Implementations: A Review}, 
  year={2025},
  volume={25},
  number={18},
  pages={34252-34274},
  abstract={Electrical impedance tomography (EIT) has garnered increasing attention in recent years, across different domains, as a promising alternative to traditional imaging techniques like X-rays due to its nonionizing nature. Despite its potential, improvements in data collection methods and postprocessing techniques are essential to enhance image quality and generate useful metrics. This article reviews several cutting-edge approaches that leverage deep learning to address these challenges, aiming to improve the resolution and accuracy of EIT images. Direct and indirect deep learning methods are systematically compared to establish a framework for method selection, guiding practitioners in choosing approaches based on specific application requirements. A key contribution of this work is the exploration of Bayesian learning as an effective method for standardizing and optimizing EIT systems, emphasizing its ability to incorporate model uncertainties and tolerances that are often neglected in conventional deep learning models. Additionally, this article highlights emerging trends in EIT, including the use of generative adversarial networks (GANs), variational autoencoders (VAEs), and transformers, highlighting their strengths and weaknesses under different conditions. The tradeoffs between computational speed and image quality are emphasized, underscoring the need to balance real-time processing demands with high-fidelity reconstructions in algorithm design. Furthermore, it presents solutions that are protected by patents, underscoring the industrial interest and commercial viability of EIT technology in various applications.},
  keywords={Electrical impedance tomography;Deep learning;Image reconstruction;Voltage measurement;Mathematical models;Reviews;Data models;Sensors;Conductivity;Jacobian matrices;Artificial intelligence;deep learning;electrical impedance tomography (EIT);generative adversarial network (GAN);transformer;variational autoencoder (VAE)},
  doi={10.1109/JSEN.2025.3589157},
  ISSN={1558-1748},
  month={Sep.},}@ARTICLE{11152576,
  author={Li, Zilin and Fu, Jidai and Li, Wentao and Li, Shuai and Tian, Biao and Xu, Shiyou},
  journal={IEEE Sensors Journal}, 
  title={PFDE-DCN: A Lightweight Class Incremental Learning Method for Radar HRRP Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Deep neural networks have been widely used in the field of high-resolution range profile (HRRP) radar automatic target recognition (RATR) and have achieved promising results. However, when a new target class appears, how the deep neural networks learns new knowledge and updates the model is still an emerging problem. Directly using the new target class samples to train the deep neural networks will cause the problem of “catastrophic forgetting”. In this paper, we propose a two-stage distillation learning method, PFDE-DCN, to solve the HRRP class incremental learning (CIL) problem. Firstly, we use a machine learning boosting algorithm to dynamically extend the network and a pooled distillation learning algorithm to enable knowledge migration between the old and new feature extraction networks. Then, we use the distillation learning method to compress the extended network. The distillation strategy keeps the critical network parameters and removes the redundant network parameters to avoid the infinite increase of model complexity. We conduct experiments on both simulated and measured aircraft HRRP datasets, and the experimental results show that our method PFDE-DCN obtains state-of-the-art performance.},
  keywords={Computational modeling;Target recognition;Feature extraction;Training;Phase frequency detectors;Generative adversarial networks;Atmospheric modeling;Artificial neural networks;Translation;Sensitivity;catastrophic forgetting;class incremental learning (CIL);knowledge distillation;high-resolution range profile (HRRP);radar target recognition (RATR)},
  doi={10.1109/JSEN.2025.3604333},
  ISSN={1558-1748},
  month={},}@INPROCEEDINGS{10836648,
  author={Arslan, Muhammad and Munawar, Saba and Mahdjoubi, Lamine and Manu, Patrick},
  booktitle={2024 International Conference on Decision Aid Sciences and Applications (DASA)}, 
  title={Decision Support for Building Thermal Comfort Monitoring with a Sustainable GenAI System}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Staff expenses can account for up to 70% of business costs, with indoor conditions playing a critical role in employee health, behavior, and productivity. Optimal thermal comfort, typically around 21°C with 40-70% humidity, maximizes productivity. However, effective monitoring requires comprehensive data, particularly as energy regulations push for smarter building management. Although Building Information Modeling (BIM, a digital representation of a building's physical and functional characteristics) and sensor integration support facilities management, many existing systems are proprietary, expensive, and inflexible. To address these challenges, this study introduces ThermalComfortBot, a sustainable Generative Artificial Intelligence (GenAI)-powered Chatbot designed as an advanced Information System (IS). Utilizing Large Language Models (LLMs, AI models for natural language understanding) and Retrieval-Augmented Generation (RAG, a method that combines data retrieval with LLMs-generated insights), ThermalComfortBot integrates data from BIM, sensors, and other relevant sources. Built on open-source technology, it is cost-effective and fully customizable, allowing users to tailor datasets to their needs. The Chatbot delivers actionable insights through a Question-Answering (QA) interface, enabling data-driven decisions on thermal comfort to improve workplace conditions and enhance operational efficiency.},
  keywords={Productivity;Temperature sensors;Temperature measurement;Buildings;Employment;Humidity;Thermal conductivity;Chatbots;Sustainable development;Monitoring;Thermal Comfort Monitoring;Generative Artificial Intelligence;Data-Driven Decision Support;Building Information Modeling;Sustainable Workplace Solutions},
  doi={10.1109/DASA63652.2024.10836648},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10902892,
  author={De Silva, Ulindu and Fernando, Leon and Pik Lik, Billy Lau and Koh, Zann and Joyce, Sam Conrad and Yuen, Belinda and Yuen, Chau},
  booktitle={TENCON 2024 - 2024 IEEE Region 10 Conference (TENCON)}, 
  title={Large Language Models for Video Surveillance Applications}, 
  year={2024},
  volume={},
  number={},
  pages={563-566},
  abstract={The rapid increase in video content production has resulted in enormous data volumes, creating significant challenges for efficient analysis and resource management. To address this, robust video analysis tools are essential. This paper presents an innovative proof of concept using Generative Artificial Intelligence (GenAI) in the form of Vision Language Models to enhance the downstream video analysis process. Our tool generates customized textual summaries based on user-defined queries, providing focused insights within extensive video datasets. Unlike traditional methods that offer generic summaries or limited action recognition, our approach utilizes Vision Language Models to extract relevant information, improving analysis precision and efficiency. The proposed method produces textual summaries from extensive CCTV footage, which can then be stored for an indefinite time in a very small storage space compared to videos, allowing users to quickly navigate and verify significant events without exhaustive manual review. Qualitative evaluations result in 80% and 70% accuracy in temporal and spatial quality and consistency of the pipeline respectively.},
  keywords={Analytical models;Reviews;Navigation;Large language models;Pipelines;Production;Manuals;Video surveillance;Resource management;IEEE Regions;Vision Language Models;Generative Artificial Intelligence;Video Analysis},
  doi={10.1109/TENCON61640.2024.10902892},
  ISSN={2159-3450},
  month={Dec},}@ARTICLE{11146403,
  author={Lu, Yuwu and Liu, Yinsheng and Wen, Jiajun and Zhang, Yang and Liang, Yingyi and Lai, Zhihui and Shen, Linlin},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={READ3D-Net: Residual Autoencoder and GAN-Based 3-D Convolutional Network for Anomaly Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Video anomaly detection (VAD) is of great importance for a variety of real-time applications in video surveillance. Most deep learning-based anomaly detection algorithms adopt a one-class learning scheme to train a classifier using only normal data to distinguish between normal and abnormal events during the test phase. However, these methods, whether they are reconstruction or prediction models, commonly face the challenge of the model’s overly strong representation capability, which leads to excessive fitting of abnormal events and thus limits the performance of the model in diverse scenarios. To address these challenges, this work develops a novel residual autoencoder and generative adversarial network-based 3-D convolutional network, called READ3D-net, for anomaly detection. An adaptive multimodal pseudoanomaly generator is developed to simulate and generate diverse pseudoanomalies, aiming to enhance the model’s ability to extract the features with regard to “abnormal” behaviors while reducing the interference of background on the detection performance. In addition, residual structures are incorporated into the design of a reconstruction-based autoencoder model to enhance its feature extraction and discriminative capabilities. To further improve the model’s reconstruction ability on normal data, a dynamic generative adversarial strategy is proposed for effective feature learning. Extensive experiments conducted on public benchmark datasets demonstrate that the proposed model is more competitive than the state-of-the-art methods in video anomaly detection tasks, fully validating the effectiveness and practicality of the proposed approach.},
  keywords={Training;Feature extraction;Anomaly detection;Videos;Adaptation models;Three-dimensional displays;Data models;Convolutional neural networks;Autoencoders;Spatiotemporal phenomena;End-to-end detection;generative adversarial learning;pseudoanomaly;video anomaly detection (VAD)},
  doi={10.1109/TII.2025.3598471},
  ISSN={1941-0050},
  month={},}@ARTICLE{10057176,
  author={Miao, Qinghai and Lv, Yisheng and Huang, Min and Wang, Xiao and Wang, Fei-Yue},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Parallel Learning: Overview and Perspective for Computational Learning Across Syn2Real and Sim2Real}, 
  year={2023},
  volume={10},
  number={3},
  pages={603-631},
  abstract={The virtual-to-real paradigm, i.e., training models on virtual data and then applying them to solve real-world problems, has attracted more and more attention from various domains by successfully alleviating the data shortage problem in machine learning. To summarize the advances in recent years, this survey comprehensively reviews the literature, from the viewport of parallel intelligence. First, an extended parallel learning framework is proposed to cover main domains including computer vision, natural language processing, robotics, and autonomous driving. Second, a multi-dimensional taxonomy is designed to organize the literature in a hierarchical structure. Third, the related virtual-to-real works are analyzed and compared according to the three principles of parallel learning known as description, prediction, and prescription, which cover the methods for constructing virtual worlds, generating labeled data, domain transferring, model training and testing, as well as optimizing the strategies to guide the task-oriented data generator for better learning performance. Key issues remained in virtual-to-real are discussed. Furthermore, the future research directions from the viewpoint of parallel learning are suggested.},
  keywords={Training;Computer vision;Taxonomy;Machine learning;Data models;Natural language processing;Task analysis;Machine learning;parallel learning;parallel systems;sim-to-real;syn-to-real;virtual-to-real},
  doi={10.1109/JAS.2023.123375},
  ISSN={2329-9274},
  month={March},}@INPROCEEDINGS{10687980,
  author={Ramu, K and Nijhawan, Ginni and Lalitha, G and Praveen and Asha, V and Fakher, Ahmed J},
  booktitle={2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0}, 
  title={Multidimensional Impacts of Generative AI and an In-Depth Analysis of LLMs with Their Expanding Horizons in Technology and Society}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Generative AI and large language models (LLMs) have transformed technology and society, as this paper details. Generative AI advanced AI greatly. Because they can read and write, LLMs, a form of Generative AI, have altered how humans and robots communicate. This study focuses on how these technologies influence schooling, healthcare, finance, and morals. Recent studies and case studies will help us understand how LLM is applied in many sectors. Our key concerns with these activities are efficiency, efficacy, and morality. The technique also polls and interviews professionals to learn about LLM usage and issues. LLM is quicker and more precise in translating languages, creating content, and analyzing data. Sadly, prejudice, privacy, and abuse issues persisted. LLMs are versatile enough to be utilized in odd fields like individualized learning and mental health treatment, according to the research. LLMs and creative AI can advance technology and society. Despite their benefits, they pose moral and practical issues that must be addressed. This study seeks a compromise between the two LLM methods. A more moral, research-focused use might make them more beneficial and less hazardous.},
  keywords={Surveys;Performance evaluation;Ethics;Privacy;Generative AI;Reviews;Large language models;Artificial Intelligence;Content Production;Data Analysis;Education;Efficiency;Ethical Implications;Healthcare;Large Language Models;Privacy Concerns;Technological Advancements},
  doi={10.1109/OTCON60325.2024.10687980},
  ISSN={},
  month={June},}@INPROCEEDINGS{9707072,
  author={Jung, Soo Hyun and Bok Lee, Tae and Heo, Yong Seok},
  booktitle={2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Deep Feature Prior Guided Face Deblurring}, 
  year={2022},
  volume={},
  number={},
  pages={884-893},
  abstract={Most recent face deblurring methods have focused on utilizing facial shape priors such as face landmarks and parsing maps. While these priors can provide facial geometric cues effectively, they are insufficient to contain local texture details that act as important clues to solve face deblurring problem. To deal with this, we focus on estimating the deep features of pre-trained face recognition networks (e.g., VGGFace network) that include rich information about sharp faces as a prior, and adopt a generative adversarial network (GAN) to learn it. To this end, we propose a deep feature prior guided network (DFPGnet) that restores facial details using the estimated the deep feature prior from a blurred image. In our DFPGnet, the generator is divided into two streams including prior estimation and deblurring streams. Since the estimated deep features of the prior estimation stream are learned from the VGGFace network which is trained for face recognition not for deblurring, we need to alleviate the discrepancy of feature distributions between the two streams. Therefore, we present feature transform modules at the connecting points of the two streams. In addition, we propose a channel-attention feature discriminator and prior loss, which encourages the generator to focus on more important channels for deblurring among the deep feature prior during training. Experimental results show that our method achieves state-of-the-art performance both qualitatively and quantitatively.},
  keywords={Training;Shape;Face recognition;Estimation;Transforms;Streaming media;Generative adversarial networks;Computational Photography;Image and Video Synthesis},
  doi={10.1109/WACV51458.2022.00096},
  ISSN={2642-9381},
  month={Jan},}@ARTICLE{9105088,
  author={Jiang, Jianmin and Fares, Ahmed and Zhong, Sheng-Hua},
  journal={IEEE Transactions on Multimedia}, 
  title={A Brain-Media Deep Framework Towards Seeing Imaginations Inside Brains}, 
  year={2021},
  volume={23},
  number={},
  pages={1454-1465},
  abstract={While current research on multimedia is essentially dealing with the information derived from our observations of the world, internal activities inside human brains, such as imaginations and memories of past events etc., could become a brand new concept of multimedia, for which we coin as “brain-media”. In this paper, we pioneer this idea by directly applying natural images to stimulate human brains and then collect the corresponding electroencephalogram (EEG) sequences to drive a deep framework to learn and visualize the corresponding brain activities. By examining the relevance between the visualized image and the stimulation image, we are able to assess the performance of our proposed deep framework in terms of not only the quality of such visualization but also the feasibility of introducing the new concept of “brain-media”. To ensure that our explorative research is meaningful, we introduce a dually conditioned learning mechanism in the proposed deep framework. One condition is analyzing EEG sequences through deep learning to extract a more compact and class-dependent brain features via exploiting those unique characteristics of human brains such as hemispheric lateralization and biological neurons myelination (neurons importance), and the other is to analyze the content of images via computing approaches and extract representative visual features to exploit artificial intelligence in assisting our automated analysis of brain activities and their visualizations. By combining the brain feature space with the associated visual feature space of those images that are candidates of the stimuli, we are able to generate a combined-conditional space to support the proposed dual-conditioned and lateralization-supported GAN framework. Extensive experiments carried out illustrate that our proposed deep framework significantly outperforms the existing relevant work, indicating that our proposed does provide a good potential for further research upon the introduced concept of “brain-media”, a new member for the big family of multimedia. To encourage more research along this direction, we make our source codes publicly available for downloading at GitHub.11https://github.com/aneeg/LS-GAN.},
  keywords={Electroencephalography;Visualization;Brain;Deep learning;Gallium nitride;Feature extraction;Decoding;EEG;image generation;deep learning;brain-media;bi-directional computation;variant LSTM},
  doi={10.1109/TMM.2020.2999183},
  ISSN={1941-0077},
  month={},}@ARTICLE{10436374,
  author={Liu, Ruiqi and Li, Fangting and Jiang, Weiping and Song, Chengfang and Chen, Qusen and Li, Zhao},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Generating Pixel Enhancement for Road Extraction in High-Resolution Aerial Images}, 
  year={2024},
  volume={9},
  number={10},
  pages={6313-6325},
  abstract={Road extraction is a powerful technique support to autonomous driving as it provides routable road information for motion planning algorithms. High-resolution aerial images offer comprehensive road information, facilitating the establishment of efficient and accurate road networks to monitor road changes in a timely manner. However, widespread occlusions and abundant local details pose challenges to highly accurate and continuous extraction, especially in areas with road bifurcations. To simultaneously improve both accuracy and connectivity of road extraction, in this paper, we propose a novel approach TPEGAN to integrate pixel-level segmentation and graph inference based on road pixel enhancement. By generating road pixels enhanced images, the generative adversarial network exploits the consistency among road pixels to embed pixel-level accuracy into the segmentation module. The multi-scale dual-branch segmentation module employs graph convolution reasoning to capture dependencies across different spatial regions, maintaining the connectivity of road networks. Extensive experiments on three public datasets demonstrate that TPEGAN outperforms SOTA methods with a considerable performance gap. As the complexity of road networks increases, the performance of TPEGAN degrades more slowly than SOTA method. Even in challenging urban scenes where the proportion of road pixels is more than 15%, TPEGAN retains its high performance and achieves a rIoU of 0.664 with an APLS of 72.81%, amounting to improvements of 4.1% and 2.62% over SOTA method, respectively.},
  keywords={Road traffic;Image segmentation;Feature extraction;Transformers;Cognition;Convolutional neural networks;Generative adversarial networks;Autonomous driving;Satellite images;Graph convolutional networks;Road extraction;pixel enhancement;autonomous driving;segmentation;aerial images;GAN;graph convolution},
  doi={10.1109/TIV.2024.3366021},
  ISSN={2379-8904},
  month={Oct},}@ARTICLE{10050165,
  author={Wang, Yonghao and Jiang, Bowu and Wei, Zhefeng and Lu, Wenkai},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Deep Velocity Generator: A Plug-In Network for FWI Enhancement}, 
  year={2023},
  volume={61},
  number={},
  pages={1-17},
  abstract={Known for its great potential for determining subsurface properties quantitatively, full-waveform inversion (FWI) is a hot topic in the field of exploration seismology. The success of FWI depends significantly on the accuracy of the starting model. Given that both the migration and velocity profiles originate from the same geological structure, the two should be morphologically consistent. Starting from the velocity-reflector depth tradeoff, we propose a deep learning approach with a new training paradigm for building a good starting model. A velocity model and the corresponding migration image are used to form two-channel inputs, and the generative adversarial network (GAN) is trained to minimize the difference between the output and the true velocity model. After the training, the velocity generator network becomes a plug-in component to enhance the FWI performance. The network can be well generalized to unseen data by training with only the synthetic data. We perform extensive experiments on our test dataset, the Marmousi model, the salt velocity model, and field data to demonstrate the effectiveness of our method. Besides, we briefly give an explanation of why our model produces such outputs in this article, making the proposed method more controllable and credible.},
  keywords={Data models;Computational modeling;Training;Generators;Generative adversarial networks;Deep learning;Geology;Deep learning (DL);depth migration;full-waveform inversion (FWI);velocity model},
  doi={10.1109/TGRS.2023.3247880},
  ISSN={1558-0644},
  month={},}@ARTICLE{11095422,
  author={Kang, Honggu and Cha, Seohyeon and Kang, Joonhyuk},
  journal={IEEE Transactions on Mobile Computing}, 
  title={GeFL: Model-Agnostic Federated Learning with Generative Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Federated learning (FL) is a distributed training paradigm that enables collaborative learning across clients without sharing local data, thereby preserving privacy. However, the increasing scale and complexity of modern deep models often exceed the computational or memory capabilities of edge devices. Furthermore, clients may be constrained to use heterogeneous model architectures due to hardware variability (e.g., ASICs, FPGAs) or proprietary requirements that prevent the disclosure or modification of local model structures. These practical considerations motivate the need for model-heterogeneous FL, where clients participate using distinct model architectures. In this work, we propose Generative Model-Aided Federated Learning (GeFL), a framework that enables cross-client knowledge sharing via a generative model trained in a federated manner. This generative model captures global data semantics and facilitates local training without requiring model homogeneity across clients. While GeFL achieves strong performance, empirical analysis reveals limitations in scalability and potential privacy leakage due to generative sample memorization. To address these concerns, we propose GeFL-F, which utilizes feature-level generative modeling. This approach enhances scalability to large client populations and mitigates privacy risks. Extensive experiments across image classification tasks demonstrate that both GeFL and GeFL-F offer competitive performance in heterogeneous settings. Code is available at [1].},
  keywords={Training;Data models;Computational modeling;Feature extraction;Servers;Federated learning;Adaptation models;Privacy;Computer architecture;Scalability;Federated learning;model heterogeneity;generative model;data augmentation},
  doi={10.1109/TMC.2025.3592483},
  ISSN={1558-0660},
  month={},}@INPROCEEDINGS{10730917,
  author={Ravale, Ujwala and Tattu, Riya Ramesh and Baban Bhoir, Ashish and Mahajan, Sneha Bhaskar},
  booktitle={2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC)}, 
  title={Deepfake Detection using InceptionResNetV2 Model}, 
  year={2024},
  volume={},
  number={},
  pages={873-879},
  abstract={The ability to create a nearly similar human-generated image—also known as a "deep fake"—has become very easy due to the increasing processing capacity of deep learning algorithms. It is easy to imagine scenarios where these deepfakes with morphing faces are the driving force behind terrorist acts, political turmoil, revenge porn, and private blackmail. This article presents an innovative deep learning-based technique that can reliably identify authentic pictures from phoney ones created by artificial intelligence. The technique is capable of automatically differentiating between replacement deep fakes and reproductions and leverages transfer learning by incorporating the InceptionResNetV2 pre-trained on ImageNet, a large image database. To replicate real-time scenarios and enhance the model's performance on real-time data, the method is evaluated on a substantial amount of balanced dataset. It also shows how the system may achieve competitive outcomes by employing a rather simple and dependable technique.},
  keywords={Training;Deepfakes;Accuracy;Social networking (online);Films;Terrorism;Transfer learning;Real-time systems;Data models;Resilience;Artificial Intelligence;InceptionResNetV2;deep-fake;generative adversarial networks;Convolutional layers;Global average pooling},
  doi={10.1109/AIC61668.2024.10730917},
  ISSN={},
  month={July},}@INPROCEEDINGS{9261687,
  author={Li, Chenyang and Mo, Lingfei and Yan, Ruqiang},
  booktitle={2020 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Rolling Bearing Fault Diagnosis Based on Horizontal Visibility Graph and Graph Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={275-279},
  abstract={The automatic extraction and learning features relying on artificial intelligence algorithms replace traditional manual features. More effective feature expression improves the performance of machine fault diagnosis with fewer requirements for labor and expertise. However, the present models only can process the data in Euclidean space. The relations between data points are ignored for a long time, which can play a significant role in distinguishing diverse faults patterns. To combat this issue, a novel model for bearing faults diagnosis is proposed by incorporating the horizontal visibility graph (HVG) and graph neural networks (GNN). In the proposed model, time series is converted to graph retaining invariant dynamic characteristics through the HVG algorithm, and the generated graphs are fed into a designed GNN model for feature learning and faults classification further. Finally, the proposed model is tested on two actual bearing datasets, and it shows state-of-the-art performance in the bearing faults diagnosis. The experimental results demonstrate that extracting relation information using HVG benefits bearing faults diagnosis.},
  keywords={Fault diagnosis;Feature extraction;Time series analysis;Signal processing algorithms;Task analysis;Rolling bearings;Data models;rolling bearing;fault diagnosis;horizontal visibility graph (HVG);graph neural networks (GNN)},
  doi={10.1109/ICSMD50554.2020.9261687},
  ISSN={},
  month={Oct},}@ARTICLE{10190603,
  author={Nie, Lihai and Shan, Xiaoyang and Zhao, Laiping and Li, Keqiu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={PKDGA: A Partial Knowledge-Based Domain Generation Algorithm for Botnets}, 
  year={2023},
  volume={18},
  number={},
  pages={4854-4869},
  abstract={Domain generation algorithms (DGAs) can be categorized into three types: zero-knowledge, partial-knowledge, and full-knowledge. While prior research merely focused on zero-knowledge and full-knowledge types, we characterize their anti-detection ability and practicality and find that zero-knowledge DGAs present low anti-detection ability against detectors, and full-knowledge DGAs suffer from low practicality due to the strong assumption that they are fully detector-aware. Given these observations, we propose PKDGA, a partial knowledge-based domain generation algorithm with high anti-detection ability and high practicality. PKDGA employs the reinforcement learning architecture, which makes it evolve automatically based only on the easily-observable feedback from detectors. We evaluate PKDGA using a comprehensive set of real-world datasets, and the results demonstrate that it reduces the detection performance of existing detectors from 91.7% to 52.5%. We further apply PKDGA to the Mirai malware, and the evaluations show that the proposed method is quite lightweight and time-efficient.},
  keywords={Detectors;Chatbots;Servers;Botnet;IP networks;Feature extraction;Registers;Domain generation algorithms;botnet networks;reinforcement learning},
  doi={10.1109/TIFS.2023.3298229},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10050455,
  author={AL-Sammarraie, Yahya Qusay and AL-Qawasmi, Khaled and AL-Mousa, Mohammad Rasmi and Desouky, Sameh F.},
  booktitle={2022 International Engineering Conference on Electrical, Energy, and Artificial Intelligence (EICEEAI)}, 
  title={Image Captions and Hashtags Generation Using Deep Learning Approach}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={social media are fantastic tools for public communication. Social media has become an integral part of our everyday lives, and an increasing number of individuals use it for marketing and communication. Social networking enables you to demonstrate your skills and knowledge without leaving home. Companies exert significant efforts to make social media more controlled and valuable while avoiding negative repercussions. They accomplish this with artificial intelligence (AI), which enables them to develop unique applications and algorithms. It can eliminate inappropriate information or spam automatically, for instance. The description and hashtags that grab the reader's attention are among the most critical aspects of a social media post's success. Typically, individuals generate multiple captions and hashtags before selecting the optimal content for a post. Occasionally, they employ content writers, which requires time, effort, and money. The suggested method makes correct captions and hashtags using conventional neural networks (CNN) trained on image datasets containing captions},
  keywords={Training;Knowledge engineering;Deep learning;Social networking (online);Neural networks;Multimedia Web sites;Production;social media;AI;Conventional Neural Networks (CNN);Instagram;Hashtags;captions},
  doi={10.1109/EICEEAI56378.2022.10050455},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9458997,
  author={Yu, Chen and Qiong, Cai and Huang, Qianqian and Chen, GuoQing and Fu, Xingbao},
  booktitle={2021 4th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={An Image Defog Network Based on Multi-scale Feature Extraction and Weighting}, 
  year={2021},
  volume={},
  number={},
  pages={423-427},
  abstract={The research on image defogging is a long-running topic. With the wave of artificial intelligence and deep learning sweeping the world, the research on image defogging based on deep learning has gradually reached a new height. However, the following problems are still emerging, such as severely distorted image color after defogging, image jagging, and image blurring and so on. In response to these problems, this paper proposes an improved image defogging network based on the traditional CycleGAN. Different from the single feature extraction of traditional CycleGAN, this paper adopts a multi-scale feature extraction method. Combining small-scale convolution and large-scale convolution with residual network for in-depth feature extraction, and at the same time, the extracted features are weighted and fused through the attention module to make the extracted network information more complete. Finally, a clear and fog-free image is reconstructed through upsampling. Compared with the traditional defogging network, the improved network can better avoid the problems of image color distortion and blurring.},
  keywords={Deep learning;Convolution;Image color analysis;Big Data;Feature extraction;Distortion;Data mining;image defogging;CycleGAN;multi-scale features;residual network;attention module},
  doi={10.1109/ICAIBD51990.2021.9458997},
  ISSN={},
  month={May},}@INPROCEEDINGS{10277789,
  author={Sun, Jifeng and Lin, Yibin and Zhao, Shuai},
  booktitle={2023 2nd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)}, 
  title={The Colorization Based on Self-Attention Mechanism and GAN}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Grayscale image colorization is a process of adding reasonable color information to an image, and converting grayscale images into color images is an important and difficult image processing task. The process of colorization is to predict the color information corresponding to the grayscale image by the colorization model. In this paper, the proposed multi-scale input adversarial generative network coloring model with multi-scale input is the colorization scheme based on self-attention mechanism and GAN is proposed in this paper. The experimental result on the colorization of grayscale cartoon images shows the effectiveness of the proposed scheme.},
  keywords={Image color analysis;Color;Gray-scale;Predictive models;Generative adversarial networks;Task analysis;Information technology;component;colorization;self-attention mechanism;transformer;GAN;upgrade network},
  doi={10.1109/AICIT59054.2023.10277789},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11005059,
  author={Nath, Manoj and Ethirajan, Lakshmi and Joshi, Janya},
  booktitle={2025 Conference on Artificial Intelligence x Multimedia (AIxMM)}, 
  title={Multilingual Infographics Generator: A language-agnostic visual summarizer}, 
  year={2025},
  volume={},
  number={},
  pages={14-18},
  abstract={In a globalized world where people speak different languages and create data in multiple languages, it can become challenging to share information. Traditional methods of using textual representations to summarize data not only consume time but can also increase the cognitive load. It is proven that visual representations are easier & faster to comprehend. Building on these findings, we propose a novel multilingual infographics generator that leverages the best state-of-the-art generative AI models to create an intuitive visual summary of data in any language. This solution accepts input in various formats, such as video, audio, or text, and in any language, uses an AI-powered pre-trained language translation model with automatic language detection to convert the input data into English text. Using this English text as input, the summary, keywords, and title are extracted for it. The keywords are used to identify the icons and the background template file. If the user prefers a language other than English, then the summary and the title go through a pre-trained language translation model again, resulting in a final infographic generated in the user's preferred language. Further, with the regeneration feature, it is possible to generate infographics in multiple languages for the same content. The proposed multilingual infographics generator enables users who are not fluent in the original language to quickly grasp the information in a language of their choice. This makes the data available to a wider audience, reduces the language barrier, and improves the language diversity.},
  keywords={Visualization;Translation;Generative AI;Buildings;Cognitive load;Generators;Data models;User experience;Multilingual;Load modeling;multilingual;infographics;pre-trained language model;language translation;visual summary;user experience},
  doi={10.1109/AIxMM62960.2025.00009},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11050591,
  author={Jang Bahadur, Sunil Kumar and Dhar, Gopala and Nigam, Lavi},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={GenAI Security: Outsmarting the Bots with a Proactive Testing Framework}, 
  year={2025},
  volume={},
  number={},
  pages={610-612},
  abstract={The increasing sophistication and integration of Generative AI (GenAI) models into diverse applications introduce new security challenges that traditional methods struggle to address. This research explores the critical need for proactive security measures to mitigate the risks associated with malicious exploitation of GenAI systems. We present a framework encompassing key approaches, tools, and strategies designed to outmaneuver even advanced adversarial attacks, emphasizing the importance of securing GenAI innovation against potential liabilities. We also empirically prove the effectiveness of the said framework by testing it against the SPML Chatbot Prompt Injection Dataset. This work highlights the shift from reactive to proactive security practices essential for the safe and responsible deployment of GenAI technologies,},
  keywords={Technological innovation;Correlation;Generative AI;Prevention and mitigation;Organizations;Chatbots;Complexity theory;Security;Faces;Testing;GenAI;Security;Agents;Prompt Injection;Red Teaming;Blue Teaming;LLM},
  doi={10.1109/CAI64502.2025.00112},
  ISSN={},
  month={May},}
