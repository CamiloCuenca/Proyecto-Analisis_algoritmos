@ARTICLE{9371694,
  author={Mao, Cong and Huang, Lizhen and Xiao, Yongsheng and He, Fengshou and Liu, Yufan},
  journal={IEEE Access}, 
  title={Target Recognition of SAR Image Based on CN-GAN and CNN in Complex Environment}, 
  year={2021},
  volume={9},
  number={},
  pages={39608-39617},
  abstract={In recent years, with the rapid development of deep learning, the research of radar image automatic target recognition (ATR) has made great progress. However, because of the complex environments and special imaging principles, Synthetic Aperture Radar (SAR) image still have the problems of sample scarcity and strong speckle noise, which affects the target recognition performance. To solve the above problems, we proposed a target recognition method of SAR image based on Constrained Naive Generative Adversarial Networks (CN-GAN) and Convolutional Neural Network (CNN). Combining Least Squares Generative Adversarial Networks (LSGAN) and Image-to-Image Translation (Pix2Pix), CN-GAN can overcome these problems of low Signal-to-Clutter-Noise Ratio (SCNR), model instability and the excessive freedom degree of the output, which are produced by conventional naive GAN. Besides, we adopted a shallow network structure design in CNN, which can effectively improve the generalization ability of the model and avoid the problem of model overfitting. The experimental results in this paper demonstrate that CN-GAN has achieved the data generation and data enhancement, the SCNR of generated data is higher than the origin data set and data sets gained by other forms of GANs, the recognition performance based on the extended data set is better than the origin data set, and the recognition rate of data set enhanced by CN-GAN is higher than that of other common data enhancement methods.},
  keywords={Radar polarimetry;Gallium nitride;Generative adversarial networks;Target recognition;Synthetic aperture radar;Image recognition;Speckle;Convolutional neural network;generative adversarial networks;synthetic aperture radar;target recognition},
  doi={10.1109/ACCESS.2021.3064362},
  ISSN={2169-3536},
  month={},}@ARTICLE{9866699,
  author={Khan, Wasim and Haroon, Mohammad and Khan, Ahmad Neyaz and Hasan, Mohammad Kamrul and Khan, Asif and Mokhtar, Umi Asma and Islam, Shayla},
  journal={IEEE Access}, 
  title={DVAEGMM: Dual Variational Autoencoder With Gaussian Mixture Model for Anomaly Detection on Attributed Networks}, 
  year={2022},
  volume={10},
  number={},
  pages={91160-91176},
  abstract={A significant aspect of today’s digital information is attributed networks, which combine multiple node attributes with the basic network topology to extract knowledge. Anomaly Detection on attributed networks has recently drawn significant attention from researchers and is widely used in several high-impact areas. Most current approaches focus on shallow learning methods such as community analysis, ego network or selection of subspace method. These approaches have network sparsity and data nonlinearity problems, and they do not even capture the intricate relationships between various information sources. Deep learning approaches like graph autoencoders are utilized to perform anomaly detection through obtaining node embeddings while dealing with the network nonlinearity and sparsity issues. However, they suffer from the problem of ignoring the latent codes’ embedding distribution, which results in poor representation in many instances. In this paper, we propose a new framework called DVAEGMM to detect anomalies on attributed networks. First, our framework utilizes a dual variational autoencoder for capturing the complex cross-modality relationships between node attributes and network structure, like vanilla autoencoders, but it also considers the potential data distribution and makes use of a generative adversarial network (GAN) for an adversarial regularization approach. An adversarial mechanism makes the encoder make more accurate estimates of how potential features might be distributed. As a result, decoders can make graphs that are more like the original graph. Each input data point is represented by a low-dimensional representation and a probability of reconstruction by the algorithm. Lastly, the Gaussian Mixture Model, a distinct estimation network, is used to approximate the latent vector density, resulting in the detection of anomalies from measuring sample energy. They are trained jointly as an end-to-end framework. DVAEGMM helps in the simultaneous optimization of the mixture model, generative adversarial network, and variational autoencoder parameters. The joint optimization balances the reconstruction probability, the latent representation density approximation, and regularization. Extensive experiments on attributed networks prove that DVAEGMM significantly beats the existing methods, proving the efficiency of the presented approach. The AUC scores of our proposed framework for the BlogCatalog, Flickr, Enron, and Amazon datasets are 0.89380, 0.87130, 0.72480, and 0.75102, respectively.},
  keywords={Anomaly detection;Social networking (online);Generative adversarial networks;Network topology;Gaussian mixture model;Deep learning;Data models;Anomaly detection;attributed networks;deep learning;dual variational autoencoder;Gaussian mixture model;graph convolution network;unsupervised learning;generative adversarial network},
  doi={10.1109/ACCESS.2022.3201332},
  ISSN={2169-3536},
  month={},}@ARTICLE{8976082,
  author={Yang, Le and Jiang, Dongmei and Sahli, Hichem},
  journal={IEEE Access}, 
  title={Feature Augmenting Networks for Improving Depression Severity Estimation From Speech Signals}, 
  year={2020},
  volume={8},
  number={},
  pages={24033-24045},
  abstract={Depression disorder has become one of the major psychological diseases endangering human health. Researcher in the affective computing community is supporting the development of reliable depression severity estimation system, from multiple modalities (speech, face, text), to assist doctors in their diagnosis. However, the limited amount of annotated data has become the main bottleneck restricting the study on depression screening, especially when deep learning models are used. To alleviate this issue, in this work we propose to use Deep Convolutional Generative Adversarial Network (DCGAN) for features augmentation to improve depression severity estimation from speech. To the best of our knowledge, this approach is the first attempt to apply the Generative Adversarial Network for depression severity estimation from speech. Besides, to measure the quality of the augmented features, we propose three different measurement criteria, characterizing the spatial, frequency and representation learning of the augmented features. Finally, the augmented features are used to train depression estimation models. Experiments are carried out on speech signals from the Audio Visual Emotion Challenge (AVEC2016) depression dataset, and the relationship between the model performance and data size is explored. Our experimental results show that: 1) The combination of the three proposed evaluation criteria can effectively and comprehensively evaluate the quality of the augmented features. 2) When increasing the size of the augmented data, the performance of depression severity estimation gradually improves and the model converges to a certain stable state. 3) The proposed DCGAN based data augmentation approach effectively improves the performance of depression severity estimation, with the root mean square error (RMSE) reduced to 5.520 and mean absolute error (MAE) reduced to 4.634, which is better than most of the state of the art results on AVEC 2016.},
  keywords={Depression;Estimation;Generative adversarial networks;Gallium nitride;Data models;Visualization;Frequency measurement;Depression estimation;audio features;data augmentation;deep convolutional generative adversarial network;spatial domain;frequency domain;deep learning aspect},
  doi={10.1109/ACCESS.2020.2970496},
  ISSN={2169-3536},
  month={},}@ARTICLE{9954019,
  author={Adeboye, Olayinka and Dargahi, Tooska and Babaie, Meisam and Saraee, Mohamad and Yu, Chia-Mu},
  journal={IEEE Access}, 
  title={DeepClean: A Robust Deep Learning Technique for Autonomous Vehicle Camera Data Privacy}, 
  year={2022},
  volume={10},
  number={},
  pages={124534-124544},
  abstract={Autonomous Vehicles (AVs) are equipped with several sensors which produce various forms of data, such as geo-location, distance, and camera data. The volume and utility of these data, especially camera data, have contributed to the advancement of high-performance self-driving applications. However, these vehicles and their collected data are prone to security and privacy attacks. One of the main attacks against AV-generated camera data is location inference, in which camera data is used to extract knowledge for tracking the users. A few research studies have proposed privacy-preserving approaches for analysing AV-generated camera data using powerful generative models, such as Variational Auto Encoder (VAE) and Generative Adversarial Network (GAN). However, the related work considers a weak geo-localisation attack model, which leads to weak privacy protection against stronger attack models. This paper proposes DeepClean, a robust deep-learning model that combines VAE and a private clustering technique. DeepClean learns distinct labelled object structures of the image data as clusters and generates a more visual representation of the non-private object clusters, e.g., roads. It then distorts the private object areas using a private Gaussian Mixture Model (GMM) to learn distinct cluster structures of the labelled object areas. The synthetic images generated from our model guarantee privacy and resist a robust location inference attack by less than 4% localisation accuracy. This result implies that using DeepClean for synthetic data generation makes it less likely for a subject to be localised by an attacker, even when using a robust geo-localisation attack. The overall image utility level of the generated synthetic images by DeepClean is comparable to the benchmark studies.},
  keywords={Cameras;Privacy;Data models;Generative adversarial networks;Data privacy;Videos;Neural networks;Autonomous vehicles;Autonomous vehicle;data privacy;data utility;deep clustering;generative model},
  doi={10.1109/ACCESS.2022.3222834},
  ISSN={2169-3536},
  month={},}@ARTICLE{9237923,
  author={Theis, Julian and Darabi, Houshang},
  journal={IEEE Access}, 
  title={Adversarial System Variant Approximation to Quantify Process Model Generalization}, 
  year={2020},
  volume={8},
  number={},
  pages={194410-194427},
  abstract={In process mining, process models are extracted from event logs using process discovery algorithms and are commonly assessed using multiple quality metrics. While the metrics that measure the relationship of an extracted process model to its event log are well-studied, quantifying the level by which a process model can describe the unobserved behavior of its underlying system falls short in the literature. In this paper, a novel deep learning-based methodology called Adversarial System Variant Approximation (AVATAR) is proposed to overcome this issue. Sequence Generative Adversarial Networks are trained on the variants contained in an event log with the intention to approximate the underlying variant distribution of the system behavior. Unobserved realistic variants are sampled either directly from the Sequence Generative Adversarial Network or by leveraging the Metropolis-Hastings algorithm. The degree by which a process model relates to its underlying unknown system behavior is then quantified based on the realistic observed and estimated unobserved variants using established process model quality metrics. Significant performance improvements in revealing realistic unobserved variants are demonstrated in a controlled experiment on 15 ground truth systems. Additionally, the proposed methodology is experimentally tested and evaluated to quantify the generalization of 60 discovered process models with respect to their systems.},
  keywords={Measurement;Generative adversarial networks;Avatars;Computational modeling;Petri nets;Manufacturing;Neural networks;Process mining;process models;petri nets;conformance checking;generalization;sequence generative adversarial networks;deep learning;metropolis-hastings algorithm;variant estimation},
  doi={10.1109/ACCESS.2020.3033450},
  ISSN={2169-3536},
  month={},}@ARTICLE{10325493,
  author={Li, Guohou and Li, Jia and Chen, Gongchao and Wang, Zhibin and Jin, Songlin and Ding, Chang and Zhang, Weidong},
  journal={IEEE Access}, 
  title={Delving Deeper Into Image Dehazing: A Survey}, 
  year={2023},
  volume={11},
  number={},
  pages={131759-131774},
  abstract={Images captured under foggy or hazy weather conditions are affected by the scattering of atmospheric particles, resulting in decreased contrast and color variation, thereby limiting their practical applications. In recent years, deep learning methods showcase significant advancements in image dehazing. However, the complexity and degradation factors in hazy images challenge the generalization capacity of dehazing methods. This paper comprehensively reviews the recent developments in single-image dehazing techniques based on deep learning. From the perspectives of Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN), different models are introduced and classified into four categories: Encoder-Decoder, Multi-Module, Multi-Branch, and Dual-Generative Adversarial Networks. The robustness and effectiveness of deep learning models are analyzed by comparing their performance and model complexity on public datasets. Additionally, limitations of current benchmark datasets and evaluation metrics are identified, and unresolved issues and future research directions are discussed. Our efforts in this paper will serve as a comprehensive reference for future research and call for further development in deep learning-based image dehazing.},
  keywords={Atmospheric modeling;Scattering;Feature extraction;Deep learning;Decoding;Analytical models;Convolutional neural networks;Image analysis;Generative adversarial networks;Image dehazing;deep learning;convolutional neural networks (CNNs);generative adversarial networks (GANs)},
  doi={10.1109/ACCESS.2023.3335618},
  ISSN={2169-3536},
  month={},}@ARTICLE{10345606,
  author={Dai, Yang and Zhang, Lin and Fan, Fu-You and Wu, Ya-Juan and Zhao, Ze-Kuan},
  journal={IEEE Access}, 
  title={SCGAN: Extract Features From Normal Semantics for Unsupervised Anomaly Detection}, 
  year={2023},
  volume={11},
  number={},
  pages={137957-137968},
  abstract={Anomaly detection within the realm of industrial products seeks to identify regions of image semantics that deviate from established normal patterns. Given the inherent challenges associated with collecting anomaly samples, we exclusively extract features from normal semantics. Our proposed solution involves a Semantic CopyPaste based Generative Adversarial Network (SCGAN) for unsupervised anomaly detection. To enable the comprehensive acquisition of semantic features within intricate real-world images, we embrace an encoder-decoder-encoder as the fundamental network structure. In practical terms, our approach commences with the input image being subjected to the CopyPaste augmentation module. Here, we strategically copy N patches, each constituting 1% of the image’s area, from normal samples. These patches are then randomly pasted into different regions of the original image. Subsequently, a generative adversarial network is trained to facilitate sample reconstruction. A noteworthy augmentation to the network’s channel attention capabilities entails the incorporation of a multi-scale channel attention module within the first encoder. This module serves to emphasize contextual features across varying scales within the image. During the test, we detect anomalous regions by meticulously comparing residuals between the input image and its reconstructed counterpart. Our methodology is rigorously validated through diverse experiments conducted on challenging MVTec and BTAD public datasets. The results conclusively affirm the state-of-the-art performance achieved by our proposed method in the domain of anomaly detection.},
  keywords={Anomaly detection;Semantics;Image reconstruction;Feature extraction;Generative adversarial networks;Training;Task analysis;Channel estimation;Anomaly detection;generative adversarial networks;channel attention;multi-scale channel attention module},
  doi={10.1109/ACCESS.2023.3339780},
  ISSN={2169-3536},
  month={},}@ARTICLE{10149323,
  author={Wang, Yanqi and Sun, Yanguo and Lan, Zhenping and Sun, Fengxue and Zhang, Nianchao and Wang, Yuru},
  journal={IEEE Access}, 
  title={Occluded Person Re-Identification by Multi-Granularity Generation Adversarial Network}, 
  year={2023},
  volume={11},
  number={},
  pages={59612-59620},
  abstract={In order to address the problem that the detailed features of pedestrians are not prominent and the pedestrian pictures are obscured in unique environments in the process of person re-recognition, we propose a person re-recognition method with a multi-grain size generative adversarial network. Firstly, we use the generative adversarial network to recover the occluded pedestrian pictures; secondly, we improve the traditional multi-granularity network by adding an Efficient Channel Attention for Deep Convolutional Neural Networks (ECA-Net) on the coarse-grained branch to focus on the feature information in the pedestrian pictures and use the High-Resolution Net (HRNet) for pose estimation on the fine-grained branch to divide the pedestrian pictures into nine parts, to enhance the network’s learning of more detailed features of pedestrians, and thus improve the accuracy of pedestrian re-recognition learning, which in turn improves the accuracy of person re-identification.},
  keywords={Generative adversarial networks;Generators;Feature extraction;Training;Identification of persons;Image restoration;Pose estimation;Person re-identification;generative adversarial networks;random occlusion;attention mechanism},
  doi={10.1109/ACCESS.2023.3285798},
  ISSN={2169-3536},
  month={},}@ARTICLE{10783437,
  author={Jia, Yizhen and Xie, Yumeng and An, Ping and Tian, Zhen and Hua, Xia},
  journal={IEEE Signal Processing Letters}, 
  title={DiffHSR: Unleashing Diffusion Priors in Hyperspectral Image Super-Resolution}, 
  year={2025},
  volume={32},
  number={},
  pages={236-240},
  abstract={Hyperspectral images provide rich spectral information and have been widely applied in numerous computer vision tasks. However, their low spatial resolution often limits their use in applications such as image segmentation and recognition. In previous works, generating high-resolution hyperspectral (HR-HS) images required the use of low-resolution hyperspectral (LR-HS) images and high-resolution RGB (HR-RGB) images as priors, which increases the cost of data collection and may lead to measurement and calibration errors in practical applications. Although the currently popular CNN-based single hyperspectral image super-resolution (single HS-SR) methods have improved performance, they are not flexible enough to process images with different degradation. From a visual perspective, the generated super-resolution images exhibit a significant smudging effect due to the loss of information. Leveraging multi-modal techniques and generative prior, we propose DiffHSR that marks a significant leap in LR-HS images super-restoration without HR-RGB. Additionally, we have established a connection between hyperspectral images and the RGB image-based generative model tasks using low-cost data and fine-tuning approaches, which creates a novel paradigm. Comprehensive experiments have demonstrated that our proposed method achieves strong visual performance and competitive results in term of quantitative metrics and perceptive quality.},
  keywords={Hyperspectral imaging;Superresolution;Degradation;Visualization;Lighting;Image reconstruction;Diffusion models;Computational modeling;Adaptation models;Three-dimensional displays;Generative prior;multi-modal techniques;single hyperspectral image super-resolution},
  doi={10.1109/LSP.2024.3512371},
  ISSN={1558-2361},
  month={},}@ARTICLE{10732016,
  author={Reddy Pulakurthi, Prasanna and Mozaffari, Mahsa and Dianat, Sohail A. and Heard, Jamison and Rao, Raghuveer M. and Rabbani, Majid},
  journal={IEEE Access}, 
  title={Enhancing GANs With MMD Neural Architecture Search, PMish Activation Function, and Adaptive Rank Decomposition}, 
  year={2024},
  volume={12},
  number={},
  pages={174222-174244},
  abstract={Generative Adversarial Networks (GANs) have gained considerable attention owing to their impressive ability to generate high-quality, realistic images from a desired data distribution. This research introduces advancements in GANs by developing an improved activation function, a novel training strategy, and an adaptive rank decomposition method to compress the network. The proposed activation function, called Parametric Mish (PMish), automatically adjusts a trainable parameter to control the smoothness and shape of the activation function. Our method employs a Neural Architecture Search (NAS) to discover the optimal architecture for image generation while using the Maximum Mean Discrepancy (MMD) repulsive loss for adversarial training. The proposed novel training strategy improves performance by progressively increasing the upper bound of the bounded MMD-GAN repulsive loss. Finally, the proposed Adaptive Rank Decomposition (ARD) method reduces the complexity of the network with minimal impact on its generative performance, thus enabling efficient deployment on resource-limited platforms. The effectiveness of these advancements is rigorously tested on standard benchmark datasets such as CIFAR-10, CIFAR-100, STL-10, and CelebA, where significant improvements over existing techniques are demonstrated. The implementation code is available at: https://github.com/PrasannaPulakurthi/MMD-PMish-NAS},
  keywords={Generative adversarial networks;Training;Generators;Image coding;Acute respiratory distress syndrome;Tensors;Standards;Neural networks;Image synthesis;Adaptive systems;Activation function;generative adversarial network;maximum mean discrepancy;neural architecture search;tensor decomposition},
  doi={10.1109/ACCESS.2024.3485557},
  ISSN={2169-3536},
  month={},}@ARTICLE{11004126,
  author={Shwe Sin Khine, Win and Siritanawan, Prarinya and Kotani, Kazunori},
  journal={IEEE Access}, 
  title={Complex Emotion Estimation Using Analysis-by-Synthesis of Facial Expression Images}, 
  year={2025},
  volume={13},
  number={},
  pages={88731-88746},
  abstract={This research proposes an approach for recognizing facial expressions for complex emotion estimation through the utilization of generated facial images. The proposed approach consists of two main parts: facial image generation and facial expression recognition. In the first part, we introduce Conditioned Emotion Generative Adversarial Networks (cEmoGANs) to synthesize images that convey complex facial expressions. Unlike previous methods, our generative model maintains face identity information and expresses various types of complex emotions. This capability encourages the generator to have control over the image generation process, resulting in enhanced image quality, reduced distortion, and increased diversity of generated images. The second part involves the design of a multiple-label classification based on a convolutional neural network trained on the complex facial expression images generated from the first part, which is employed in the recognition of complex facial expressions for emotion estimation. Our model, using images generated by cEmoGANs, demonstrates a notable performance surpassing the capabilities of the previous models in comparative evaluations.},
  keywords={Face recognition;Emotion recognition;Image recognition;Estimation;Vectors;Image synthesis;Generators;Generative adversarial networks;Feature extraction;Deep learning;Complex emotions;analysis-by-synthesis;conditioned emotion generative adversarial networks;emotions estimation},
  doi={10.1109/ACCESS.2025.3570167},
  ISSN={2169-3536},
  month={},}@ARTICLE{10197424,
  author={Yang, Lingyi and Xu, Yang and Zhang, Sicong and Zhang, Xinyu},
  journal={IEEE Access}, 
  title={A Robust CycleGAN-L2 Defense Method for Speaker Recognition System}, 
  year={2023},
  volume={11},
  number={},
  pages={82771-82783},
  abstract={With the rapid development of voice technology, speaker recognition is becoming increasingly prevalent in our daily lives. However, with its increased usage, security issues have become more apparent. The adversarial attack poses a significant security risk to the speaker recognition model by making small changes to the input and thus causing the neural network model to produce an incorrect output. Nevertheless, there are currently limited defense techniques for speaker recognition models. To this end, we propose a robust CycleGAN-L2(CYC-L2) defense method. The method automatically adjusts the size of the dataset according to the learning of the generative adversarial networks on the dataset, and uses L2 loss functions to constrain the generative adversarial networks for better and faster training. In this paper, we will compare the effectiveness of defense against white-box attacks using existing defenses and the defenses proposed. The experimental results show that our defense method not only plays a better defense effect than the other defense methods mentioned under the x-vector model but also does not reduce the accuracy of benign examples in closed-set identification.},
  keywords={Speaker recognition;Perturbation methods;Band-pass filters;Training;Generative adversarial networks;Robustness;Feature extraction;Adversarial attack;adversarial defense;speaker recognition;generative adversarial networks},
  doi={10.1109/ACCESS.2023.3300031},
  ISSN={2169-3536},
  month={},}@ARTICLE{9966583,
  author={Hussain, Aftab and Yaseen, Muhammad Usman and Imran, Muhammad and Waqar, Muhammad and Akhunzada, Adnan and Al-Ja’afreh, Mohammad and Saddik, Abdulmotaleb El},
  journal={IEEE Access}, 
  title={An Attention-Based ResNet Architecture for Acute Hemorrhage Detection and Classification: Toward a Health 4.0 Digital Twin Study}, 
  year={2022},
  volume={10},
  number={},
  pages={126712-126727},
  abstract={Due to the advancement of digital twin (DT) technology, Health 4.0 applications have become reality and starting to take roots. In this article, we focus on intracranial hemorrhage (ICH) which is a life-threatening emergency that needs immediate diagnosis and treatment. ICH is caused by bleeding inside the skull or brain. Radiologists typically examine computed tomography (CT) scans of the patients to determine the ICH and its subtype. But the manual assessment of the CT scan is a complex and time-consuming task. The existing pre-trained convolutional neural network (CNN) models are state-of-the-art for ICH classification. However, they employ poor feature extraction techniques which hinder overall model performance. Furthermore, they suffer from the curse of dimensionality and use redundant and noisy features. The problem of imbalanced data is also crucial for achieving model generalization. This paper proposes a hybrid attention-based ResNet architecture for ICH detection and classification. An attention mechanism allows the model to focus on a specific region and extract relevant features. Principal component analysis (PCA) is used for dimensionality reduction and redundant feature removal whereas deep convolutional generative adversarial network (DCGAN) is used for resolving the class imbalance problem. The proposed model is evaluated using the dataset assembled during the Radiologist Society of North America (RSNA) ICH detection challenge 2019. The results show that our proposed model outperforms existing state-of-the-art models in terms of accuracy and F1-score. ICH classification achieved accuracies of 99.2%, 97.1%, 96.7%, 96.7% and 96.1%, for detecting epidural hemorrhage (EH), intraparenchymal hemorrhage (IH), intraventricular hemorrhage (IVH), subdural hemorrhage (SH), and subarachnoid hemorrhage (SAH) subtypes respectively. The F1-score of 96.1% for EH subtype is also best when compared with the benchmark models.},
  keywords={Computed tomography;Hemorrhaging;Feature extraction;Brain modeling;Deep learning;Principal component analysis;Machine learning;Computed tomography;data augmentation;deep convolutional generative adversarial network;deep learning;digital twin;health 4.0;intracranial hemorrhage;principal component analysis;ResNet-152V2},
  doi={10.1109/ACCESS.2022.3225671},
  ISSN={2169-3536},
  month={},}@ARTICLE{10839314,
  author={Liang, Ruihuai and Yang, Bo and Chen, Pengyu and Li, Xianjin and Xue, Yifan and Yu, Zhiwen and Cao, Xuelin and Zhang, Yan and Debbah, Mérouane and Vincent Poor, H. and Yuen, Chau},
  journal={IEEE Internet of Things Journal}, 
  title={Diffusion Models as Network Optimizers: Explorations and Analysis}, 
  year={2025},
  volume={12},
  number={10},
  pages={13183-13193},
  abstract={Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions. We provide code and data at https://github.com/qiyu3816/DiffSG.},
  keywords={Optimization;Diffusion models;Internet of Things;Probability distribution;Data models;Linear programming;Fitting;Complexity theory;Noise reduction;Computational modeling;Diffusion models;generative artificial intelligence (GAI);Internet of Things (IoT);network optimization},
  doi={10.1109/JIOT.2025.3528955},
  ISSN={2327-4662},
  month={May},}@INPROCEEDINGS{10969929,
  author={K, Vinutha and N, Manjunath T and Nithyashri, S. and Gopal, Anaya Singh and R P, Sinchana},
  booktitle={2025 International Conference on Computing for Sustainability and Intelligent Future (COMP-SIF)}, 
  title={Dynamic Content Generation for Learning Management System Using Generative AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The following paper presents the newly developed concept of Artificial Intelligence-based LMS that intends to revolutionize contemporary education and enhance the flow of education. The system also has built-in tools that help the user to filter courses on different aspects of goal - orientation, for specific coding exams or for professional development, as well as for users with particular levels of difficulty. Such features as quizzes, flashcards, chapter wise course with added YouTube videos, chatbot integrated in the Q&A module. The system empowers users to tailor their courses based on specific goals, such as preparing for job coding exams or professional skill development, while also adjusting for individual difficulty levels. Core features include interactive quizzes, flashcards, chapter-wise course enriched with YouTube videos, chatbot feature, question and-answer module. Another component to increase interaction is testing module which allows users to practice their knowledge and check the result by passing the questions and answers. Additionally, it offers two standout capabilities powered by Gemini AI: a math aid, which involves a virtual teacher that offers solutions to uploaded mathematics questions and a guide that helps users go through information on the Internet using conversion. The most remarkable one is the mock interview module, which along with analysing user responses to the questions presented by AI. Thus, it links the educational process with the opportunities for career growth, providing its users with the resources to address actual difficulties. Based on modern technologies, the LMS utilizes the Neon database for data management, Inngest for the event-based execution of actions, Clerk for authentication, and a fast frontend based on Next.js, React, and Tailwind CSS. The following paper aims to discuss the system, its application and the ways in which it is poised to revolutionise learning in the era of the web.},
  keywords={Video on demand;Neon;Engineering profession;Education;Chatbots;Encoding;Web sites;Artificial intelligence;Interviews;Videos;AI LMS with course material;YouTube;notes;Q&A;quizzes;chatbot;mock interview helper;virtual math tutor;automatically generated questions;Gemini AI API;built with NextJS;ReactJS;Tailwind CSS;Inngest triggers;unctions;Neon (Drizzle ORM) and Clerk authentication},
  doi={10.1109/COMP-SIF65618.2025.10969929},
  ISSN={},
  month={March},}@ARTICLE{10816641,
  author={Ur Rehman Ahmed, Naveed and Badshah, Afzal and Adeel, Hanan and Tajammul, Ayesha and Daud, Ali and Alsahfi, Tariq},
  journal={IEEE Access}, 
  title={Visual Deepfake Detection: Review of Techniques, Tools, Limitations, and Future Prospects}, 
  year={2025},
  volume={13},
  number={},
  pages={1923-1961},
  abstract={In recent years, rapid advancements in deepfakes (incorporating Artificial Intelligence (AI), machine, and deep learning) have updated tools and techniques for manipulating multimedia. Though technology has primarily been utilized for beneficial purposes, such as education and entertainment, it is also used for malicious or unethical tasks to spread disinformation or ruin someone’s dignity, even if it encompasses harassing and blackmailing victims. Deepfakes refer to high-quality and realistic multimedia-manipulated content that has been digitally modified or synthetically generated. We conducted a systematic literature review of deepfake detection to offer an updated overview of existing research work that initially describes the widely accessible deepfake generation tools, classifications, and detection process. We highlighted recent techniques in visual deepfake detection based on the feature representations, grouped into four domains: spatial, temporal, frequency, and spatio-temporal, including their key features and limitations by providing details of existing datasets, together with the potentials of deepfake and its future directions. This study tried to add an updated repository of technological change in deepfake, which could help researchers to develop robust deepfake models.},
  keywords={Deepfakes;Face recognition;Deep learning;Faces;Training;Generators;Decoding;Social networking (online);Generative adversarial networks;Feature extraction;Deepfake detection;machine learning;deep learning;deepfake applications;deepfake datasets;deepfake generation tools},
  doi={10.1109/ACCESS.2024.3523288},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11050467,
  author={Neuman, W. Russell and Coleman, Chad and Shah, Manan and Dasdan, Ali},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Ethical Logic in Six Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={290-295},
  abstract={This study examines the ethical reasoning of six prominent generative large language models: OpenAI's GPT4o, Meta's LLaMA 3.1, Perplexity, Anthropic's Claude 3.5 Sonnet, Google's Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and the Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist-deontological distinction, Moral Foundations Theory, and Kohlberg's Stages of Moral Development. Findings reveal that LLMs exhibit convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in training and architecture, nuanced differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and selfawareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.},
  keywords={Training;Ethics;Philosophical considerations;Large language models;Minimization;Cognition;Internet;Logic;Artificial Intelligence;Ethical Logic;Value Alignment;Human Centered AI;Explainability;Transparency},
  doi={10.1109/CAI64502.2025.00052},
  ISSN={},
  month={May},}@INPROCEEDINGS{11011699,
  author={Trijono, Janette Mirna Putri and Sanjaya, Febrian and Zanetta, Meily and Hidayaturrahman},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Personalized Music Recommendations Using Retrieval Augmented Generation: Evaluating Data Processing Strategies on Spotify Data}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent years, music consumption has transitioned from physical formats to streaming platforms such as Spotify. Spotify, one of the leading platforms, has seen remarkable growth, surpassing 574 million active users globally by 2024. This shift has made music more accessible than ever before, but as the volume of available content increases, discovering personalized music recommendations has become increasingly complex. To address this challenge, the development of AI-powered generative recommendation systems has become essential. This research investigates the application of Retrieval-Augmented Generation (RAG) to provide personalized music recommendations by utilizing the Spotify dataset. We evaluate the impact of data preprocessing, converting numerical data into categorical data, on system performance. The RAG system, powered by OpenAI’s text-embedding-ada-002 model, processes the data, stores embeddings in Chroma DB, and generates recommendations using GPT-4o-mini. Experimental results show that the converted dataset improves efficiency, reducing both processing time and token usage. User feedback from 43 participants reveals an 80% satisfaction rate, demonstrating the effectiveness and user appeal of the proposed RAG-based music recommendation system.},
  keywords={Accuracy;Computational modeling;System performance;Retrieval augmented generation;Data retrieval;Data models;Real-time systems;Numerical models;Recommender systems;Faces;Music Recommendation;Retrieval Augmented Generation;Open AI;Artificial Intelligence;Spotify},
  doi={10.1109/ICDSAAI65575.2025.11011699},
  ISSN={},
  month={March},}@ARTICLE{9381216,
  author={Wang, Zhenfei and Wang, Hongju},
  journal={IEEE Access}, 
  title={Global Data Distribution Weighted Synthetic Oversampling Technique for Imbalanced Learning}, 
  year={2021},
  volume={9},
  number={},
  pages={44770-44783},
  abstract={Imbalanced learning is a common problem in data mining. There is a different distribution of data samples among other classes in the imbalanced datasets. It’s a challenge for standard algorithms designed for balanced class distributions. Although there are various strategies to solve this problem, generating artificial data to achieve a relatively balanced class distribution is universal rather than directly modifying specific classification algorithms. The oversampled data can be combined with any user-specified algorithm without any restrictions. In this paper, we present a novel oversampling method, Global Data Distribution Weighted Synthetic Oversampling Technique (GDDSYN). By applying clustering, optimizing the selection criteria of the minority class samples that are used to generate synthetic samples, avoiding generating more noise samples. GDDSYN assigns weights for the number of synthetic samples to tackle the within-class imbalance and between-class imbalance simultaneously, according to the informative level of the sample and the sparsity of the cluster to which the sample belongs. The use of scores with Silhouette Coefficient and Mutual Information helps the k-means algorithm set a reasonable number of clusters for the minority and majority classes respectively so that the clustering effect can be guaranteed. Next, by using clustering information, synthetic samples’ generation path is improved to avoid class overlap. Additionally, GDDSYN has been evaluated extensively on 10 artificial and 10 real-world data sets. The empirical results show that our method is outperforms or comparable with some other existing methods in terms of assessment metrics when artificial data generated by GDDSYN are used.},
  keywords={Clustering algorithms;Classification algorithms;Partitioning algorithms;Standards;Gallium nitride;Training;Imbalanced learning;synthetic sample generation;oversampling;clustering;classification;within-class imbalance},
  doi={10.1109/ACCESS.2021.3067060},
  ISSN={2169-3536},
  month={},}@ARTICLE{10918960,
  author={Liang, Xiao and Jasmina Khaw, Yen-Min and Liew, Soung-Yue and Tan, Tien-Ping and Qin, Donghong},
  journal={IEEE Access}, 
  title={Toward Low-Resource Languages Machine Translation: A Language-Specific Fine-Tuning With LoRA for Specialized Large Language Models}, 
  year={2025},
  volume={13},
  number={},
  pages={46616-46626},
  abstract={In the field of computational linguistics, addressing machine translation (MT) challenges for low-resource languages remains crucial, as these languages often lack extensive data compared to high-resource languages. General large language models (LLMs), such as GPT-4 and Llama, primarily trained on monolingual corpora, face significant challenges in translating low-resource languages, often resulting in subpar translation quality. This study introduces Language-Specific Fine-Tuning with Low-rank adaptation (LSFTL), a method that enhances translation for low-resource languages by optimizing the multi-head attention and feed-forward networks of Transformer layers through low-rank matrix adaptation. LSFTL preserves the majority of the model parameters while selectively fine-tuning key components, thereby maintaining stability and enhancing translation quality. Experiments on non-English centered low-resource Asian languages demonstrated that LSFTL improved COMET scores by 1-3 points compared to specialized multilingual machine translation models. Additionally, LSFTL’s parameter-efficient approach allows smaller models to achieve performance comparable to their larger counterparts, highlighting its significance in making machine translation systems more accessible and effective for low-resource languages.},
  keywords={Translation;Machine translation;Computational modeling;Adaptation models;Generative Pre-trainer transformer;Transformers;Training;Multilingual;Context modeling;Computational efficiency;Machine translation;low-resource languages;large language models;parameter-efficient fine-tuning;LoRA},
  doi={10.1109/ACCESS.2025.3549795},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10833458,
  author={Lima, Ines Katia Huaman and Arapa, E. Gladys Cutipa},
  booktitle={2024 IEEE XXXI International Conference on Electronics, Electrical Engineering and Computing (INTERCON)}, 
  title={Integration of Deep Learning Models and Explanatory Models to Detect Anomalous Periods in Vital Signs of Mining Truck Engines}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={“Latex The Peruvian mining industry is beginning to adopt IoT technologies. The increasing complexity of modern engines makes it crucial to implement predictive maintenance systems to ensure reliability and performance optimization. This study is based on data obtained from an open-pit copper mine in southern Peru. The use of advanced machine learning models is proposed, evaluating between Long Short-Term Memory with Autoencoders (LSTM-AE) and Artificial Neural Network (ANN) for anomaly detection in multivariate time series for diesel engine health monitoring. This project addresses three issues of the current analysis methodology: first, point anomalies are used to monitor potential failures, generating a high rate of false alarms, while the proposed method identifies abnormal time periods; second, the threshold for alerts is manually calculated and requires expert intervention periodically, whereas the proposed methodology calculates the threshold in an unsupervised manner. Third, deep learning models are often considered “black box” models since it is not easy to interpret how the model reached a conclusion, which is why model explainability is important for its adoption within the industry. The objective is to develop a predictive model that identifies potential failures early, allowing for the planning of preventive maintenance and reducing unplanned downtime. The results show that the combination of these models significantly improves the accuracy in failure prediction, facilitating timely and precise interventions.},
  keywords={Deep learning;Accuracy;Autoencoders;Time series analysis;Predictive models;Data models;Long short term memory;Anomaly detection;Monitoring;Engines;Predictive maintenance;LSTM;ANN;anomaly detection;machine learning;multivariate time series;artificial intelligence;deep learning;big data},
  doi={10.1109/INTERCON63140.2024.10833458},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10100361,
  author={Ligi, S. Vineth and Kumar, R. and Dhanalakshmi, Samiappan},
  booktitle={2023 International Conference on Intelligent Systems for Communication, IoT and Security (ICISCoIS)}, 
  title={COVID-19 Detection from Pulmonary CT Images using Neural Networks based on Dropout-Driven Hidden Layers}, 
  year={2023},
  volume={},
  number={},
  pages={310-316},
  abstract={Enhanced diagnosis with considerably good sensitivity and specificity is highly indispensable for COVID-19 diagnosis using radiological data to combat hazardous viral infection. Accuracy of diagnosis is a very important part that helps in further triaging and disease management. Artificial intelligent techniques using Convolutional Neural Networks and their modified alternatives have been recognized to be the salvation in chaotic situations and emergencies. Despite their immense ability to give quality results, they suffer from overfitting problems which have to be reduced by regularizing the networks. Dropout is one such regularization that modifies the network to achieve improved performance by discarding the unwanted nodes in the network layers. A simple neural network architecture inspired by former renowned architectures with dropout-driven hidden layers, CVDNN is built and experimented with for various dropout probabilities (0.1, 0.25, 0.5 and 0.75). The model was also tested with different numbers of dense layers: CVDNN1 with a single dense layer and CVDNN2 with two dense layers of a fixed dropout probability of 0.5 in it. The models are trained and tested with pulmonary computed tomography images to distinguish COVID-19 abnormality against normal cases. The CVDNN2 model presents better functioning with improved performance measures than CVDNN1 with an accuracy of 92.86 % accuracy, 90.21% sensitivity and a specificity of 95.52% for the dataset used. Dropout probabilities of 0.25 and 0.5 present reliable and better results compared to the other values experimented with. Hence a dropout-driven hidden layer can enhance the neural network's performance by choosing either 0.25 or 0.5 preferably for different applications.},
  keywords={COVID-19;Sensitivity;Computed tomography;Computational modeling;Neural networks;Lung;Computer architecture;COVID-19;Artificial Intelligence;Convolutional Neural Network;Dropout;Hidden layers},
  doi={10.1109/ICISCoIS56541.2023.10100361},
  ISSN={},
  month={Feb},}@ARTICLE{10124764,
  author={Qiu, Chenmeng and Li, Yuzhi and Kang, Mingyu and Chen, Duxin and Yu, Wenwu},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={CDSTTN: A Data Imputation Method for Cyber-Physical Systems by Causal Dense Spatial–Temporal Transformer Network}, 
  year={2023},
  volume={13},
  number={3},
  pages={851-860},
  abstract={Data-driven technique of artificial intelligence is a main receipt to model the uncertainty of cyber-physical systems. Based on large-scale sensor network, big data can be collected, and used to represent, understand and control the system. However, due to the issue of communication failures and cyberattacks, the collected data is often damaged and corrupted, which leads to terrible model training and low performance. To address this issue, a data missing imputation method is proposed based on causal dense spatial-temporal transformer network (CDSTTN). This model specially designs a module to recognize causal network on the observation variables, and then combine the network with densely connected spatial-temporal transformer modules. The results show that the causal module improves the performance of CDSTTN, and provides more causal interpretability. Moreover, extensive ablation studies are also conducted to show the contribution of CDSTTN to accelerating the model training process. Through comprehensive experimental comparison, the proposed model reaches the state-of-the-art performance compared to the other baseline models, which indicates it may provide some insight into the commonly encountered data missing problems in cyber-physical systems.},
  keywords={Data models;Tensors;Transformers;Training;Feature extraction;Cyber-physical systems;Uncertainty;Data missing;causal network;transformer;spatial-temporal modal;cross-modal},
  doi={10.1109/JETCAS.2023.3276641},
  ISSN={2156-3365},
  month={Sep.},}@ARTICLE{9904835,
  author={Wang, Jiangong and Wang, Xiao and Shen, Tianyu and Wang, Yutong and Tian, Yonglin and Wang, Fei-Yue},
  journal={IEEE Journal of Radio Frequency Identification}, 
  title={A Long-Tail Regularization Method for Traffic Sign Recognition Based on Parallel Vision}, 
  year={2022},
  volume={6},
  number={},
  pages={957-961},
  abstract={The long-tail effect is prevalent in the actual world, and the resulting long-tail problem hinders the development of intelligent vehicles towards a more reliable and safe direction. In previous studies, the theory of Long-tail Regularization (LoTR) based on parallel vision was proposed, and further research and experiments on the long-tail problem of traffic sign recognition are conducted in this paper. Specifically, in the process of long-tail regularization for traffic sign recognition, a novel generative adversarial network is designed and improved to generate images. By feeding the category-specific image from standard gallery into the proposed network, it can generate a large number of virtual images with the same category. Finally, the generated virtual images are added to the original dataset to regularize the long tail, and then the recognition accuracy is significantly improved.},
  keywords={Image recognition;Task analysis;Traffic control;Radiofrequency identification;Diversity reception;Parallel vision;long tail;traffic sign regularization},
  doi={10.1109/JRFID.2022.3209157},
  ISSN={2469-7281},
  month={},}@ARTICLE{10213444,
  author={Hu, Bo and Zhu, Guang and Li, Leida and Gan, Ji and Li, Weisheng and Gao, Xinbo},
  journal={IEEE Transactions on Multimedia}, 
  title={Blind Image Quality Index With Cross-Domain Interaction and Cross-Scale Integration}, 
  year={2024},
  volume={26},
  number={},
  pages={2729-2739},
  abstract={With the assistance of Convolutional Neural Networks (CNNs), Image Quality Assessment (IQA) models have made great progress in evaluating both simulated distortion and authentic distortion. However, most of the existing IQA models only learn the features of distorted images, and thus do not make full use of the available feature representation of other domains. Furthermore, the common multi-scale fusion strategies are relatively simple, such as downsampling and concatenating, which further limits the prediction performance. To this end, we propose a novel blind image quality index with cross-domain interaction and cross-scale integration, which is designed based on the combination of CNN and Transformer. First, the hierarchical spatial-domain and gradient-domain representations are obtained through a typical CNN architecture. Then, based on the proposed gradient-query cross-attention, these two types of features are fully interacted in the Cross-Domain Interaction (CDI) module. To represent the distortion information more comprehensively, the Cross-Scale Integration (CSI) module is proposed to combine the information between different scales progressively. Finally, the quality score is obtained through a simple regression module. The experimental results on five public IQA databases of both simulated and authentic scenes show that the proposed model outperforms the compared state-of-the-art metrics. In addition, cross-database experiments show that the proposed model has strong generalization performance.},
  keywords={Measurement;Feature extraction;Distortion;Image quality;Transformers;Databases;Indexes;Image quality assessment;deep learning;cross-domain interaction;cross-scale integration},
  doi={10.1109/TMM.2023.3303725},
  ISSN={1941-0077},
  month={},}@ARTICLE{10492674,
  author={Pastor-Galindo, Javier and Nespoli, Pantaleone and Ruipérez-Valiente, José A.},
  journal={IEEE Security & Privacy}, 
  title={Large-Language-Model-Powered Agent-Based Framework for Misinformation and Disinformation Research: Opportunities and Open Challenges}, 
  year={2024},
  volume={22},
  number={3},
  pages={24-36},
  abstract={The affordances that generative artificial intelligence can have in mis/disinformation contexts are major threats to our digitalized society. We present a research framework to generate customized agent-based social networks for disinformation simulations that would enable understanding and evaluating the phenomena.},
  keywords={Information integrity;Social networking (online);Fake news;Context modeling;Integrated circuit modeling;Fraud;Visualization;Generative AI;Large language models;Artificial intelligence},
  doi={10.1109/MSEC.2024.3380511},
  ISSN={1558-4046},
  month={May},}@INPROCEEDINGS{9747059,
  author={Wang, Chao and Gu, Yi and Li, Jie and He, Xinlei and Zhang, Zirui and Gao, Yuting and Wu, Chentao},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Iterative Learning for Distorted Image Restoration}, 
  year={2022},
  volume={},
  number={},
  pages={2085-2089},
  abstract={Deep generative networks have achieved great success on distorted image restoration. However, existing deep learning approaches mainly focus on delicate module structure while ignoring the saturation problem. In this paper, we study the influence of different learning schemes on fitting capability and tackle the problem by proposing a novel iterative learning scheme. It accumulates weight importance from past episodes and guides the network to search for the optimal of current episodes based on obtained knowledge. Since public available datasets contain very few distortion types, we also release a new benchmark to explore this task. Extensive experimental evaluations on the benchmarks demonstrate that our learning approach significantly outperforms all other methods and achieves new state-of-the-art results.},
  keywords={Learning systems;Knowledge engineering;Deep learning;Acoustic distortion;Fitting;Benchmark testing;Image restoration;Image Restoration;Iterative Learning;Benchmark},
  doi={10.1109/ICASSP43922.2022.9747059},
  ISSN={2379-190X},
  month={May},}@INPROCEEDINGS{9373248,
  author={Bu, Seok-Jun and Moon, Hyung-Jun and Cho, Sung-Bae},
  booktitle={2021 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Adversarial Signal Augmentation for CNN-LSTM to Classify Impact Noise in Automobiles}, 
  year={2021},
  volume={},
  number={},
  pages={60-64},
  abstract={The classification of impact noise on vehicle steering gear mainly addresses the issue of modeling the transient and impulsive signals. In particular, variations between the steering systems arising from the differences in manufacturing processes according to the vehicle types extremely limit the conventional deep acoustic models. Focusing on the fact that the major hurdles addressed can be mitigated by generating and modeling the virtual impact noise, we propose an adversarial signal augmentation method for the vehicle noise modeling. The impact noise is represented based on the Fourier transform and the variance between vehicle types is alleviated using a generative adversarial network with an auxiliary classifier in order to improve the generalization performance of the model. Experiments with the dataset of 134,400,000 time-series collected from a global motor corporation show that the proposed method has more than 3% of accuracy improvement against the conventional approaches.},
  keywords={Training;Recurrent neural networks;Steering systems;Inspection;Acoustics;Transient analysis;Spectrogram;Signal augmentation;Convolutional recurrent neural network;Deep acoustic modeling;In-vehicle noise classification},
  doi={10.1109/BigComp51126.2021.00020},
  ISSN={2375-9356},
  month={Jan},}@INPROCEEDINGS{10446185,
  author={Zhao, Mengyi and Liu, Mengyuan and Ren, Bin and Dai, Shuling and Sebe, Nicu},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Denoising Diffusion Probabilistic Models for Action-Conditioned 3D Motion Generation}, 
  year={2024},
  volume={},
  number={},
  pages={4225-4229},
  abstract={Diffusion-based generative models have proven to be highly effective in various domains of synthesis. In this work, we propose a conditional paradigm utilizing the denoising diffusion probabilistic model (DDPM) to address the challenge of realistic and diverse action-conditioned 3D skeleton-based motion generation. The proposed method leverages bidirectional Markov chains to generate samples by inferring the reversed Markov chain based on the learned distribution mapping during the forward diffusion process. To the best of our knowledge, our work is the first to employ DDPM to synthesize a variable number of motion sequences conditioned on a categorical action. The proposed method is evaluated on the NTU RGB+D dataset and the NTU RGB+D two-person dataset, showing significant improvements over state-of-the-art motion generation methods.},
  keywords={Solid modeling;Three-dimensional displays;Image synthesis;Noise reduction;Probabilistic logic;Task analysis;Speech processing;Motion Generation;Diffusion Model;Skeleton Data;Conditional Motion Generation},
  doi={10.1109/ICASSP48485.2024.10446185},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10214083,
  author={Yi, Jiangyan and Tao, Jianhua and Fu, Ruibo and Wang, Tao and Zhang, Chu Yuan and Wang, Chenglong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Adversarial Multi-Task Learning for Mandarin Prosodic Boundary Prediction With Multi-Modal Embeddings}, 
  year={2023},
  volume={31},
  number={},
  pages={2963-2973},
  abstract={Prosodic boundaries are still crucial to the naturalness of end-to-end speech synthesis systems. This article proposes to use adversarial multi-task learning to predict prosodic boundaries. Adversarial multi-task learning is utilized to transfer knowledge from an auxiliary POS tagging task to a prosodic boundary prediction task. Furthermore, multi-modal embeddings are composed of contextual word and speech embedding features obtained from the pre-trained bidirectional encoder representations from transformers (BERT) model and Speech2Vec. We can utilize linguistic and acoustic information from large amounts of external text and speech data without prosodic boundary labels. At the inference stage, the prosodic boundary predicting model can use the syntactic features learnt from the POS tagging task without any extra computation cost due to only employing the prosodic boundary predicting task to decode. We conducted experiments on Mandarin datasets. The results show that the models using multi-modal embeddings from the pre-trained BERT and Speech2Vec outperform the models trained with single modal embedding. Furthermore, the models trained with adversarial training obtain further performance gains by up to 2.95% in $F_{1}$ score.},
  keywords={Training;Costs;Computational modeling;Bit error rate;Predictive models;Linguistics;Tagging;Adversarial training;multi-task learning;prosodic boundaries;speech synthesis;multi-modal embeddings},
  doi={10.1109/TASLP.2023.3301235},
  ISSN={2329-9304},
  month={},}@ARTICLE{10597619,
  author={Zhang, Lei and Jiang, Yunzhe and Ma, Yazhou and Mao, Shiwen and Huang, Wenyuan and Yu, Zhiyong and Zheng, Xiao and Shu, Lin and Fan, Xiaochen and Xu, Guangquan and Dong, Changyu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Toward Robust and Effective Behavior Based User Authentication With Off-the-Shelf Wi-Fi}, 
  year={2024},
  volume={19},
  number={},
  pages={8731-8746},
  abstract={Behavior-based Wi-Fi user authentication has gained popularity in user-centered smart systems. However, its wide adoption has been hindered by certain critical issues, including significant performance degradation when the environment changes, the inability to handle unknown activities, and weak security due to basing authentication on the recognition of a single, one-off activity. In this paper, we propose Wi-Dist, which authenticates a user using a behavior password, i.e. a pre-chosen sequence of activities. Wi-Dist addressed the previously mentioned technical challenges through a cross-layer joint optimization framework. In particular, we address environment dependency by incorporating adversarial learning and optimizing both the signal layer and the domain adaptation layer. This enhances the performance of the learned model across various environments. To effectively handle unknown behaviors, we utilize an adversarial learning-based network. This network establishes a pseudo-decision boundary between samples from known and unknown sources, ensuring robust authentication. Additionally, for authentication using continuous activities, we employ double-sliding windows activity monitoring. This approach, coupled with activity state correction, partitions activities for accurate recognition. We also conducted extensive experiments in indoor environments to demonstrate that Wi-Dist is effective and robust.},
  keywords={Authentication;Wireless fidelity;Passwords;Libraries;Computational modeling;Adaptation models;Training data;Wi-Fi;channel state information;action recognition;cross-environment},
  doi={10.1109/TIFS.2024.3428367},
  ISSN={1556-6021},
  month={},}@INBOOK{9749418,
  author={Buchanan, Ben and Imbrie, Andrew},
  booktitle={The New Fire: War, Peace, and Democracy in the Age of AI}, 
  title={8 LYING}, 
  year={2022},
  volume={},
  number={},
  pages={183-207},
  abstract={The letter to the editor that ran in India's Patriot newspaper on July 16, 1983, began with a bold claim: "AIDS, the deadly mysterious disease which has caused havoc in the U.S., is believed to be the result of the Pentagon's experiments to develop new and dangerous biological weapons."1 The letter outlined the mysteries of the disease's spread and the horrors it inflicted upon its victims. The writer accurately quoted Pentagon and CIA statements from years before about the American biological weapons program, and referenced the CIA's aggressive medical experimentation with mind-altering substances in the top secret MKULTRA project. There was no name attached to the note, but it was signed by a "well-known American scientist and anthropologist" based in New York.},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262368599},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9749418},}@INPROCEEDINGS{10584855,
  author={Gupta, Anjani and Jain, Dhyanendra and Phoghat, Parul},
  booktitle={2024 International Conference on Computational Intelligence and Computing Applications (ICCICA)}, 
  title={Optimizing Handwritten Digit Recognition with CNN and Data Augmentation Strategies}, 
  year={2024},
  volume={1},
  number={},
  pages={470-475},
  abstract={Convolutional Neural networks (CNN) are very effective in recognizing handwritten digits/ words by extracting distinct features from images and are more suitable to solve handwriting recognition problems. This paper presents an experimental investigation and testing of an improved CNN technique for recognizing handwritten digits and solve the computation problem with higher accuracy. We propose a deep learning framework focused on refining CNN concepts, aiming for precise identification and evaluation of user-provided handwritten mathematical images. Our objective is to achieve absolute accuracy and efficiency. We choose the Convolutional Neural Network for its computational efficiency and potential for improved accuracy over existing models. We introduce a method that outperforms current models in terms of computational cost, speed, and accuracy while prioritizing user accessibility and ease of computation. The key enhancement lies in the use of an improved CNN, which simplifies user access and computation compared to traditional CNN or Artificial Neural Networks (ANN) approaches.},
  keywords={Handwriting recognition;Analytical models;Accuracy;Computational modeling;Education;Feature extraction;Mathematical models;Convolutional Neural Network (CNN);Deep Learning;Artificial Neural Network (ANN)},
  doi={10.1109/ICCICA60014.2024.10584855},
  ISSN={},
  month={May},}@INPROCEEDINGS{10969193,
  author={Uddin, Main and Fu, Zhangjie and Zhang, Xiang and Arnob, Abu Bakor Hayat},
  booktitle={2025 3rd International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)}, 
  title={Low Resolution Deepfake Face Detection using Multi-Scale Discrete Cosine Transform and Vision Transformer}, 
  year={2025},
  volume={},
  number={},
  pages={1134-1139},
  abstract={Deepfake technology uses generative adversarial networks to produce highly authentic counterfeit images and videos, which raises a significant global security concern by disseminating fake information through online platforms, especially on social media. Researchers have developed sophisticated models for deepfake detection, demonstrating high performance on high-resolution images. However, their effectiveness significantly diminishes when applied to low-resolution images. We propose a simple framework that utilizes multi-scale discrete cosine transform and vision transformer for low-resolution deepfake face detection. This framework introduces a multi-scale frequency filter fusion approach to extract subtle frequency features in the frequency domain. Our proposed network has been tested on the FaceForensics++ and Celeb-DF datasets, and it outperforms existing models, achieving superior AUC and F1 scores.},
  keywords={Deepfakes;Image resolution;Social networking (online);Frequency-domain analysis;Transformers;Feature extraction;Discrete cosine transforms;Face detection;Security;Intelligent systems;Deepfake Face Detection;Discrete Cosine Transform;Vision Transformer;Computer Vision},
  doi={10.1109/ISACC65211.2025.10969193},
  ISSN={},
  month={Feb},}@INBOOK{9072002,
  author={Lee, Edward Ashford},
  booktitle={The Coevolution: The Entwined Futures of Humans and Machines}, 
  title={4 SAY WHAT YOU MEAN}, 
  year={2020},
  volume={},
  number={},
  pages={61-78},
  abstract={I do quite a bit of public speaking. I have tried writing a script and memorizing it, but that rarely works. I think the only time it worked for me was when the US National Science Foundation asked me to give a ninety- second talk about my research. It is surprisingly difficult to say so little when you want to convey so much, and I couldn't figure out how to stay within the ninety-second limit without a word-for-word script.1 On a few other occasions, I tried writing a script for a longer talk and reading from it. This was a disaster, yielding a stilted, soporific speech. So most of the time, I do not plan what to say. I have a general idea, of course, and I use PowerPoint slides to prompt me and provide visual stimulus to the audience, but what words will come out of mouth, I cannot predict. I have to trust my brain to synthesize the right words on the fly. Most of the time, it does a reasonable job.},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262358378},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9072002},}@INPROCEEDINGS{10939946,
  author={Das, Subhranil and Kumari, Rashmi and Tripathi, Utkarsh and Parashar, Shivi and Dubey, Adetya and Yadav, Akanksha and Singh, Raghwendra Kishore},
  booktitle={2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)}, 
  title={Enhanced DeepFake Detection Using CNN and EfficientNet-Based Ensemble Models for Robust Facial Manipulation Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={677-681},
  abstract={Deepfakes are generally powered by Generative adversarial networks (GANs) that has been newly introduced a new dimension of realism in the manipulation of digital media particularly in some delicate features such as eyes, lips, and skin texture. These sophisticated manipulations pose significant threats to digital security, privacy, and trust in media. Detecting these alterations, especially in still images, has become a critical focus of research. This paper presents a Convolutional neural network (CNN)-based approach to detecting facial feature manip- ulations in deepfake images. Leveraging CNN architectures like ResNet, we focus on identifying subtle pixel-level inconsistencies in facial regions. Preprocessing steps, including face cropping, alignment, and normalization, are applied to enhance the model's focus on manipulated features. Datasets such as Face Forensics++ are utilized for training and evaluation. Our model demonstrates a high level of accuracy in distinguishing real from fake images based on facial feature manipulations, outperforming baseline models in several key metrics, including precision and recall. The results highlight the effectiveness of CNNs in detecting fine- grained alterations and their potential for real-world application in combating deepfake threats.This research contributes to the ongoing efforts in deepfake detection by proposing a robust and scalable method to detect facial manipulations, ultimately enhancing the security and authenticity of digital media. Future work will explore the integration of multimodal detection tech- niques to further improve the robustness of the system.},
  keywords={Training;Deepfakes;Privacy;Media;Skin;Robustness;Convolutional neural networks;Security;Facial features;Faces;component;formatting;style;styling;insert},
  doi={10.1109/CE2CT64011.2025.10939946},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10650568,
  author={Jin, Yicheng},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={GaniX: Advancing GANimation through ConvNeXt features}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Recent advancements in Generative Adversarial Networks (GANs) have revolutionized facial expression synthesis, pushing the boundaries of what is achievable in this field. One of the most notable breakthrough architectures is GANimation, which leverages a conditioning mechanism using images of Action Units (AUs) annotations to control the intensity of individual AUs and combine multiple AUs seamlessly. However, existing methods for generating realistic expressions are limited, often resulting in flaws and blurring in regions with intense expressions. Additionally, transitioning from one emotion to another, like grief to indignation, may lead to unwanted overlapping artifacts. In contrast, ConvNeXts have displayed exceptional performance across a wide array of vision tasks, making integrating Transformer-inspired training techniques a practical avenue for improving convolutional networks. In this paper, we reevaluate GANimation in light of ConvNeXt's design principles and present GaniX, a novel approach that significantly augments the network's capacity to extract features and diversify expressions. This enhancement not only mitigates artifacts but also makes generated facial images more lifelike and expressive. GaniX, composed entirely of standard convolutional layers, exhibits superior performance in facial expression editing, as demonstrated through experiments on widely used public datasets and real-world images.},
  keywords={Training;Convolution;Neural networks;Psychology;Entertainment industry;Feature extraction;Transformers;Facial Expression Synthesis;Action Units;ConvNeXts},
  doi={10.1109/IJCNN60899.2024.10650568},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10699185,
  author={Deepa, N. and Alkhayyat, Ahmad and Lande, Jayapal and Armoogum, Sheeba and Bhoomika, S S},
  booktitle={2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={Fraud Detection using Enhanced Secure Machine Learning Algorithm for Wireless Communication}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={In recent years of technology, the wide usage and expansion of credit cards causing significant improvement in the fraud cases. Which are affecting the financial conditions, so it is very important to aware and stop the fraud transactions. In this paper, Fraud detection for wireless communication is implemented by using a proposed method called as eXtreme Gradient Boosting (XGBoost) is based on Gradient-boosted Decision Tree (GDBT) and it increases the training speed and to improve accuracy by making multiple decision trees iteratively, and reducing each iterations loss function. Initially, the proposed method implemented on dataset which is collected from Kaggle. That contains the credit card transactions made by the European credit cardholders in the year of 2013 September for 2 days. In data preprocessing stage Synthetic Minority Oversampling Technique (SMOTE) is used to balance the imbalanced class distribution in the dataset by generating synthetic samples that will improve the classification accuracy. And for dimensionality reduction Auto encoders are used significantly there are two phases namely encoders and decoders, which reduces the size of input data. In the classification phase, XGBoost algorithm is used as it achieves high accuracy of 98% when compared with all the start-of-arts namely Variational Autoencoder Generative Adversarial Network (VAEGAN) and Convolutional Neural Network, Bi directional- Long Short Term Memory (CNN-Bi-LSTM).},
  keywords={Wireless communication;Training;Accuracy;Machine learning algorithms;Data preprocessing;Credit cards;Fraud;Decoding;Decision trees;Long short term memory;auto encoders;fraud detection;gradient-boosted decision tree;synthetic minority oversampling technique;XGboost},
  doi={10.1109/NMITCON62075.2024.10699185},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10424840,
  author={Li, Guangyu and Zhang, Jinghan and Yang, Boyu and Yi, Hongbin and Zhang, Yingbo and Wang, Yao and Chang, Shaocong and Zhou, Yusen},
  booktitle={2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)}, 
  title={Visual Servo Technology for Fault Detection in Power Inspection UAVs}, 
  year={2023},
  volume={},
  number={},
  pages={376-381},
  abstract={This paper takes deep learning as the technical background, and aims at the problems of small number of fault samples, poor image quality and low target detection accuracy in practical applications, to carry out the construction of power system UAV inspection image dataset, the research on the low illumination image enhancement method based on deep learning, and the research on the intelligent detection technology of power system fault based on improved EifficientDet-D0.},
  keywords={Deep learning;Visualization;Target recognition;Object detection;Inspection;Servomotors;Power system faults;drone inspection;scene images;power target recognition},
  doi={10.1109/ICICML60161.2023.10424840},
  ISSN={},
  month={Nov},}@ARTICLE{11008525,
  author={Meng, Ming and Mu, Ke and Zhu, Yonggui and Zhu, Zhe and Sun, Haoyu and Yan, Heyang and Fan, Zhaoxin},
  journal={Computational Visual Media}, 
  title={VarGes: Improving variation in co-speech 3D gesture generation via StyleCLIPS}, 
  year={2025},
  volume={},
  number={},
  pages={1-17},
  abstract={Generating expressive and diverse human gestures from audio is crucial in fields like human-computer interaction, virtual reality, and animation. While existing methods have achieved remarkable performance, they often exhibit limitations due to constrained dataset diversity and the restricted amount of information derived from audio inputs. To address these challenges, we present VarGes, a novel variationdriven framework designed to enhance co-speech gesture generation by integrating visual stylistic cues while maintaining naturalness. Our approach begins with a variation-enhanced feature extraction module, which seamlessly incorporates style-reference video data into a 3D human pose estimation network to extract StyleCLIPS, thereby enriching the input with stylistic information. Subsequently, we employ a variation-compensation style encoder, a transformer-style encoder equipped with an additive attention mechanism pooling layer, to robustly encode diverse StyleCLIPS representations and effectively manage stylistic variations. Finally, a variation-driven gesture predictor module fuses MFCC audio features with StyleCLIPS encodings via cross-attention, injecting this fused data into a cross-conditional autoregressive model to modulate 3D human gesture generation based on audio input and stylistic clues. The efficacy of our approach is validated on benchmark datasets, on which it outperforms existing methods in terms of gesture diversity and naturalness. Our code and video results are publicly available at https://github.com/mookerr/VarGES/.},
  keywords={Three-dimensional displays;Feature extraction;Codes;Shape;Visualization;Hidden Markov models;Hands;Adaptation models;Deep learning;Transformers;gesture generation;variation enhancement;multi-modal fusion;autoregressive modeling},
  doi={10.26599/CVM.2025.9450477},
  ISSN={2096-0662},
  month={},}@INPROCEEDINGS{10332712,
  author={Park, Hyunkyu and Kang, Sungho and Park, YeongHyeon and Lee, Yeonho and Lee, Hanbyul and Bae, Seho and Yi, Juneho},
  booktitle={2023 IEEE 6th International Conference on Knowledge Innovation and Invention (ICKII)}, 
  title={Unsupervised Image-to-Image Translation Based on Bidirectional Style Transfer}, 
  year={2023},
  volume={},
  number={},
  pages={671-676},
  abstract={Image-to-image translation (I2I) is an image synthesis technique to map a source image to the style of the target domain while preserving its content information. Existing image-to-image translation study results showed excellent image synthesis performance using generative adversarial network (GAN) based models, but they are not capable of efficiently handling the style of the target domain. To overcome this limitation, a bidirectional style transfer network has been developed to perform image synthesis by intersecting images of two domains with each other's styles, but the type of applicable dataset is limited due to supervised learning-based training. We proposed an unsupervised image-to-image translation method by employing a bidirectional style transfer network with a cyclic collaborative loss to train the model. Experimental results showed that the proposed network accurately reflected the style of the target domain in the image synthesis task.},
  keywords={Training;Knowledge engineering;Image quality;Technological innovation;Visualization;Image synthesis;Collaboration;bidirectional network;unsupervised image-to-image translation (I2I);style transfer},
  doi={10.1109/ICKII58656.2023.10332712},
  ISSN={2770-4785},
  month={Aug},}@INPROCEEDINGS{11100409,
  author={Loo, Jonathan and Chen, Yue and Chai, Kok Keong},
  booktitle={2025 6th International Conference on Information Technology and Education Technology (ITET)}, 
  title={Transforming Embedded Systems Design Course: GenAI-empowered CDIO-based Authentic Assessment with Challenge-Based Learning}, 
  year={2025},
  volume={},
  number={},
  pages={30-35},
  abstract={Traditional assessment methods, such as written exams and programming assignments, often inadequate in evaluating the diverse competencies required for real-world embedded systems engineering. This paper presents the design of an innovative assessment framework for a Year 3 Embedded Systems Design course in a four-year engineering programme. The framework integrates Generative Artificial Intelligence (GenAI) tools within a CDIO-based Authentic Assessment (AA) framework with Challenge-Based Learning (CBL). It emphasises group-based, open-ended projects that address complex, real-world challenges, encouraging students to apply and extend their knowledge beyond the course syllabus. Central to our framework is the integration of GenAI tools, enabling students to acquire new knowledge, evaluate design options, assist with implementation, troubleshoot issues, and optimise performance, fostering active learning and higher-order skills such as critical thinking, problem-solving, and creativity. The assessment strategy holistically evaluates both project processes and outcomes, incorporating peer assessments to reflect individual contributions. It employs iterative tasks with feedback, reflective activities, and clear criteria to assess technical proficiency, collaboration, and practical application of knowledge. Aligned with Bloom’s Taxonomy, the framework progressively develops students’ cognitive skills from foundational understanding to higher-order synthesis and evaluation. By immersing students in real-world challenges, this framework equips them with essential professional skills and prepares them for industry demands, fostering lifelong learning and adaptability in modern engineering contexts.},
  keywords={Industries;Embedded systems;Generative AI;Taxonomy;Education;Learning (artificial intelligence);Problem-solving;Iterative methods;Information technology;Programming profession;GenAI;Authentic Assessment;CDIO;CBL;Higher-Order Skills;Bloom’s Taxonomy},
  doi={10.1109/ITET65804.2025.11100409},
  ISSN={},
  month={May},}@ARTICLE{10887290,
  author={Moudoud, Hajar and Houda, Zakaria Abou El and Brik, Bouziane},
  journal={IEEE Consumer Electronics Magazine}, 
  title={LLMs to Secure Consumer Networks: Open Problems and Future Directions}, 
  year={2025},
  volume={14},
  number={5},
  pages={51-59},
  abstract={The increasing complexity of consumer networks, characterized by the rapid adoption of Internet of Things devices and smart home technologies, exposes significant limitations in traditional security mechanisms. These challenges drive a growing interest in innovative solutions, such as generative artificial intelligence, including large language models (LLMs) to ensure the security of consumer networks from evolving cyber threats. In this article, we present a forward-looking perspective on the role of LLMs in securing consumer networks. Then, we present a comprehensive study of attacks targeting LLMs and current defense mechanisms/strategies. Finally, we present a set of core research problems and a comprehensive research agenda that identifies future directions to advance LLM capabilities for consumer network security.},
  keywords={Security;Filtering;Closed box;Training;Safety;Filters;Ethics;Data mining;Context modeling;Glass box;Internet of Things;Large language models;Generative AI;Artificial intelligence},
  doi={10.1109/MCE.2025.3542247},
  ISSN={2162-2256},
  month={Sep.},}@ARTICLE{11008616,
  author={Lv, Qingxuan and Dong, Junyu and Li, Yuezun and Chen, Sheng and Yu, Hui and Zhang, Shu and Wang, Wenhan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={UWStereo: A Large Synthetic Dataset for Underwater Stereo Matching}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Despite recent advances in stereo matching, the extension to intricate underwater settings remains unexplored, primarily owing to: 1) the reduced visibility, low contrast, and other adverse effects of underwater images; 2) the difficulty in obtaining ground truth data for training deep learning models, i.e. simultaneously capturing an image and estimating its corresponding pixel-wise depth information in underwater environments. To enable further advance in underwater stereo matching, we introduce a large synthetic dataset called UWStereo. Our dataset includes 29,568 synthetic stereo image pairs with dense and accurate disparity annotations for left view. We design four distinct underwater scenes filled with diverse objects such as corals, ships and robots. We also induce additional variations in camera model, lighting, and environmental effects. In comparison with existing underwater datasets, UWStereo is superior in terms of scale, variation, annotation, and photo-realistic image quality. To substantiate the efficacy of the UWStereo dataset, we undertake a comprehensive evaluation compared with eleven state-of-the-art algorithms as benchmarks. The results indicate that current models still struggle to generalize to new domains. Hence, we design a new strategy that learns to reconstruct cross domain masked images before stereo matching training and integrate a cross view attention enhancement module that aggregates long-range content information to enhance the generalization ability.},
  keywords={Training;Synthetic data;Annotations;Accuracy;Costs;Three-dimensional displays;Marine vehicles;Iterative methods;Benchmark testing;Image reconstruction;Underwater stereo matching dataset;Stereo matching;Masked image learning},
  doi={10.1109/TCSVT.2025.3572044},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{10766486,
  author={Shi, Yu-Zhe and Li, Haotian and Ruan, Lecheng and Qu, Huamin},
  booktitle={2024 IEEE VIS Workshop on Data Storytelling in an Era of Generative AI (GEN4DS)}, 
  title={Constraint Representation Towards Precise Data-Driven Storytelling}, 
  year={2024},
  volume={},
  number={},
  pages={4-12},
  abstract={Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. In this viewpoint, we propose integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. We identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, we explore the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. We believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.},
  keywords={Systematics;Limiting;Shape;Grounding;Generative AI;Taxonomy;Manuals;Programming;Hybrid power systems;Domain specific languages;Data-driven storytelling;structural representation;domain-specific language;constraint programming;constraint programming},
  doi={10.1109/GEN4DS63889.2024.00006},
  ISSN={},
  month={Oct},}@ARTICLE{9540700,
  author={Shehzad, Faisal and Javaid, Nadeem and Almogren, Ahmad and Ahmed, Abrar and Gulfam, Sardar Muhammad and Radwan, Ayman},
  journal={IEEE Access}, 
  title={A Robust Hybrid Deep Learning Model for Detection of Non-Technical Losses to Secure Smart Grids}, 
  year={2021},
  volume={9},
  number={},
  pages={128663-128678},
  abstract={For dealing with the electricity theft detection in the smart grids, this article introduces a hybrid deep learning model. The model tackles various issues such as class imbalance problem, curse of dimensionality and low theft detection rate of the existing models. The model integrates the benefits of both GoogLeNet and gated recurrent unit (GRU). The one dimensional electricity consumption (EC) data is fed into GRU to remember the periodic patterns of electricity consumption. Whereas, GoogLeNet model is leveraged to extract the latent features from the two dimensional weekly stacked EC data. Furthermore, the time least square generative adversarial network (TLSGAN) is proposed to solve the class imbalance problem. The TLSGAN uses unsupervised and supervised loss functions to generate fake theft samples, which have high resemblance with real world theft samples. The standard generative adversarial network only updates the weights of those points that are available at the wrong side of the decision boundary. Whereas, TLSGAN even modifies the weights of those points that are available at the correct side of decision boundary that prevent the model from vanishing gradient problem. Moreover, dropout and batch normalization layers are utilized to enhance model’s convergence speed and generalization ability. The proposed model is compared with different state-of-the-art classifiers including multilayer perceptron (MLP), support vector machine, naive bayes, logistic regression, MLP-long short term memory network and wide and deep convolutional neural network. It outperforms all classifiers by achieving 96% and 97% precision-recall area under the curve and receiver operating characteristics area under the curve, respectively.},
  keywords={Feature extraction;Logic gates;Smart grids;History;Time series analysis;Support vector machines;Generative adversarial networks;Electricity theft detection;gated recurrent unit;GoogLeNet;non-technical losses;smart grids;SGCC},
  doi={10.1109/ACCESS.2021.3113592},
  ISSN={2169-3536},
  month={},}@ARTICLE{10043650,
  author={Moysis, Lazaros and Iliadis, Lazaros Alexios and Sotiroudis, Sotirios P. and Boursianis, Achilles D. and Papadopoulou, Maria S. and Kokkinidis, Konstantinos-Iraklis D. and Volos, Christos and Sarigiannidis, Panagiotis and Nikolaidis, Spiridon and Goudos, Sotirios K.},
  journal={IEEE Access}, 
  title={Music Deep Learning: Deep Learning Methods for Music Signal Processing—A Review of the State-of-the-Art}, 
  year={2023},
  volume={11},
  number={},
  pages={17031-17052},
  abstract={The discipline of Deep Learning has been recognized for its strong computational tools, which have been extensively used in data and signal processing, with innumerable promising results. Among the many commercial applications of Deep Learning, Music Signal Processing has received an increasing amount of attention over the last decade. This work reviews the most recent developments of Deep Learning in Music signal processing. Two main applications that are discussed are Music Information Retrieval, which spans a plethora of applications, and Music Generation, which can fit a range of musical styles. After a review of both topics, several emerging directions are identified for future research.},
  keywords={Deep learning;Multiple signal classification;Speech recognition;Signal processing;Music information retrieval;Instruments;Databases;Machine learning;Neural networks;Deep learning;music signal processing;music information retrieval;music generation;neural networks;machine learning},
  doi={10.1109/ACCESS.2023.3244620},
  ISSN={2169-3536},
  month={},}@ARTICLE{10988839,
  author={Shen, Haibo},
  journal={IEEE Access}, 
  title={Anterior Cruciate Ligament Tear Detection Based on Combination of Convolutional Neural Network Enhanced by Improved Human Evolutionary Algorithm}, 
  year={2025},
  volume={13},
  number={},
  pages={88445-88457},
  abstract={Anterior Cruciate Ligament (ACL) tears are prevalent injuries in sports and physical activities that necessitate prompt and precise diagnosis for optimal treatment and re-habilitation. Conventional diagnostic techniques like physical examination and MRI, may be subjective and protracted. This study proposes a new efficient technique for detecting tears of ACL based on the integration of a Convolutional Neural Network (CNN) and an improved version of Human Evolutionary Algorithm (IHEA). The purpose of the suggested IHEA is to enhance the hyperparameters of the CNN to improve its performance in detecting ACL rupture from MRI scans. The suggested technique has been validated by assessing it on a standard case study and comparing its results with some other advanced methods, including the Convolutional Neural Network (CNN), Generative Adversarial Network (GAN), Generative Adversarial Network (GAN2), Gated Recurrent Unit combined with Flexible Fitness Dependent Optimizer (GRU/FFDO), and GRU optimized by Hybrid Tasmanian Devil Optimization (GRU/HTDO). Final results showed the superiority of the proposed model in diagnosing of the ACL tear.},
  keywords={Magnetic resonance imaging;Computer architecture;Convolutional neural networks;Microprocessors;Injuries;Accuracy;Ligaments;Feature extraction;Data augmentation;Generative adversarial networks;Knee;anterior cruciate ligament;healthcare;diagnosis;convolutional neural network;improved human evolutionary algorithm},
  doi={10.1109/ACCESS.2025.3567303},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8954096,
  author={Liang, Jian and He, Ran and Sun, Zhenan and Tan, Tieniu},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Distant Supervised Centroid Shift: A Simple and Efficient Approach to Visual Domain Adaptation}, 
  year={2019},
  volume={},
  number={},
  pages={2970-2979},
  abstract={Conventional domain adaptation methods usually resort to deep neural networks or subspace learning to find invariant representations across domains. However, most deep learning methods highly rely on large-size source domains and are computationally expensive to train, while subspace learning methods always have a quadratic time complexity that suffers from the large domain size. This paper provides a simple and efficient solution, which could be regarded as a well-performing baseline for domain adaptation tasks. Our method is built upon the nearest centroid classifier, seeking a subspace where the centroids in the target domain are moderately shifted from those in the source domain. Specifically, we design a unified objective without accessing the source domain data and adopt an alternating minimization scheme to iteratively discover the pseudo target labels, invariant subspace, and target centroids. Besides its privacy-preserving property (distant supervision), the algorithm is provably convergent and has a promising linear time complexity. In addition, the proposed method can be readily extended to multi-source setting and domain generalization, and it remarkably enhances popular deep adaptation methods by borrowing the learned transferable features. Extensive experiments on several benchmarks including object, digit, and face recognition datasets validate that our methods yield state-of-the-art results in various domain adaptation tasks.},
  keywords={Learning systems;Deep learning;Visualization;Computer vision;Closed-form solutions;Face recognition;Artificial neural networks;Benchmark testing;Minimization;Time complexity;Recognition: Detection;Categorization;Retrieval;Statistical Learning},
  doi={10.1109/CVPR.2019.00309},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9956763,
  author={Loddo, Andrea and Di Ruberto, Cecilia and Armano, Giuliano and Manconi, Andrea},
  journal={IEEE Access}, 
  title={Automatic Monitoring Cheese Ripeness Using Computer Vision and Artificial Intelligence}, 
  year={2022},
  volume={10},
  number={},
  pages={122612-122626},
  abstract={Ripening is a very important process that contributes to cheese quality, as its characteristics are determined by the biochemical changes that occur during this period. Therefore, monitoring ripening time is a fundamental task to market a quality product in a timely manner. However, it is difficult to accurately determine the degree of cheese ripeness. Although some scientific methods have also been proposed in the literature, the conventional methods adopted in dairy industries are typically based on visual and weight control. This study proposes a novel approach aimed at automatically monitoring the cheese ripening based on the analysis of cheese images acquired by a photo camera. Both computer vision and machine learning techniques have been used to deal with this task. The study is based on a dataset of 195 images (specifically collected from an Italian dairy industry), which represent Pecorino cheese forms at four degrees of ripeness. All stages but the one labeled as “day 18”, which has 45 images, consist of 50 images. These images have been handled with image processing techniques and then classified according to the degree of ripening, i.e., 18, 22, 24, and 30 days. A 5-fold cross-validation strategy was used to empirically evaluate the performance of the models. During this phase, each training fold was augmented online. This strategy allowed to use 624 images for training, leaving 39 original images per fold for testing. Experimental results have demonstrated the validity of the approach, showing good performance for most of the trained models.},
  keywords={Dairy products;Monitoring;Task analysis;Feature extraction;Machine learning;Computer vision;Principal component analysis;Image classification;Cheese ripening;image analysis;image processing;machine learning;image classification;deep learning},
  doi={10.1109/ACCESS.2022.3223710},
  ISSN={2169-3536},
  month={},}@ARTICLE{10742575,
  author={Rong, Yi and Mao, Yingchi and He, Xiaoming and Chen, Mingkai},
  journal={IEEE Internet of Things Magazine}, 
  title={Large-Scale Traffic Flow Forecast with Lightweight LLM in Edge Intelligence}, 
  year={2025},
  volume={8},
  number={1},
  pages={12-18},
  abstract={Large-scale traffic flow forecasting affiliated with the time is valuable for the management in Intelligent Transportation Systems (ITS). Recently, Large Language Models (LLMs) have shown the prominence on this issue. Unfortunately, the existing LLMs cannot forecast the entire road network, including two problems, i.e., (1) training the large-scale data generated by a road network on the central cloud can impose computational pressure. (2) LLMs fail to capture the spatiotemporal correlations in the road network. To overcome the challenges, we propose a novel architecture named Light-weight Spatio-temporal Generative Large Language Model on Edges (LSGLLM-E). Specifically, we first decompose the entire large-scale road network into several subparts, and deploy the Rode-side Unit (RSU) as an edge on each subpart, thus avoiding the computational pressure on the central cloud. In addition, we design an LLM-based method named Light-weight Spatio-temporal Generative Large Language Model (LSGLLM) to extract the spatiotemporal correlations, which compensates for the lack of spatiotemporal features. Finally, the experimental results illustrate that the LSGLLM-E is superior to some advanced baselines in terms of the accuracy and efficiency of the prediction.},
  keywords={Cloud computing;Correlation;Training;Predictive models;Time series analysis;Forecasting;Edge computing;Large language models;Feature extraction;Traffic control;Road traffic;Intelligent transportation systems;Artificial intelligence},
  doi={10.1109/IOTM.001.2400047},
  ISSN={2576-3199},
  month={January},}@INPROCEEDINGS{10990788,
  author={Hemalatha, K and Deepika, V and M Mallika, R and Babu, Kona Mahesh},
  booktitle={2024 2nd International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)}, 
  title={Large Language Model based Personalized Learning Assistant for Career-Oriented Skills}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Large Language Models (LLMs) are recently emerged as a variant of Artificial Intelligence algorithms in Natural Language Processing task such as analyzing extensive datasets, generating, summarizing content and predicting new information. LLMs fall under Generative AI, designed specifically for text generation. Their exceptional training and numerous model parameters significantly enhance the LLMs capacity to mimic human-like performances in natural language understanding. The advent of LLMs has revolutionized the interaction between humans and machines. In contrast to conventional recommendation systems and search engines, LLMs are the adoptive active user engagement. This interactive capability opens up new possibilities for the users towards personalization, enabling the provision of personalized services based on individualized information. Despite their potential, the applications of LLMs in personalization remain largely unexplored. Consequently, this study explores into the domain of personalization within education, aiming to uncover the potential contributions of Large Language Models (LLMs). In the context of this research, an innovative personalized LLM-based Human Machine Interface is crafted to individualize learning experiences for each learner. This system is adept at crafting personalized learning plans and adjusting learning materials accordingly. Additionally, it formulates assessments to analyze user strengths and weaknesses. These assessments undergo automatic evaluation, offering learners instant feedback. This feedback mechanism allows learners to promptly identify and understand their mistakes, fostering a more efficient learning process and aiding them in successfully completing their courses. The proposed personalized LLM based Assistant enhances the learner's unique journey by creating adaptive leaning plans, real time assessments with instant feedback.},
  keywords={Training;Generative AI;Large language models;Speech recognition;Signal processing;Search engines;Prediction algorithms;Natural language processing;Real-time systems;Recommender systems;Large Language Model;Human-Machine Interaction;Personalized systems;Generative AI;Personalized content;Learning Experiences},
  doi={10.1109/SCOPES64467.2024.10990788},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11106634,
  author={Zhou, Tianyu and Wan, Yuwei and Liu, Ying and Kumar, Maneesh},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Enabling Interactive AI in Industry 5.0 with RAG-Enhanced GenAI Chatbots}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Industry 5.0 advances sustainable development through human-machine collaboration and personalised manufacturing. The increase in intelligent industrial equipment creates data scalability challenges for human workers who face difficulties in making decisions based on relevant data sources. Advanced interactive AI systems, capable of integrating diverse data sources and delivering real-time, context-aware insights, present promising solutions to the challenges of the industrial environment. This research introduces a retrieval-augmented generation (RAG)-enhanced Generative artificial intelligence (GenAI) chatbot to address these challenges. The system integrates a variety of information sources, including government reports, news websites, academic studies, and industry reports. This industry 5.0 chatbot aims to offer users extensive knowledge of the industrial sector through a Question-and-Answer interface. It provides relevant and accurate information through intuitive, context-aware interactions to reduce cognitive load for users, which improves decision-making efficiency and user experience. Through experimental evaluation, the RAG-enhanced GenAI chatbot significantly improves accuracy, relevance and user satisfaction, outperforming models like ChatGPT-4o. This system presents an innovative practical solution to tackle Industry 5.0 core issues particularly in enhancing human-machine collaboration and decision-making efficiency. This research contributes to the theoretical and practical development of RAG-enhanced AI systems, laying a foundation for future investigations of industrial AI interaction.},
  keywords={Accuracy;Generative AI;Human-machine systems;Soft sensors;Retrieval augmented generation;Decision making;Collaboration;Chatbots;Cognitive load;Fifth Industrial Revolution;Industry 5.0;Interactive AI;Retrieval-Augmented Generation;Generative AI;Human-Machine Collaboration;Knowledge Retrieval;Cognitive Load Reduction},
  doi={10.1109/ICE/ITMC65658.2025.11106634},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{8539530,
  author={Chang, Jiho and Choi, Yoonsung and Lee, Taegyoung and Cho, Junhee},
  booktitle={2018 International Conference on Information and Communication Technology Convergence (ICTC)}, 
  title={Reducing MAC operation in convolutional neural network with sign prediction}, 
  year={2018},
  volume={},
  number={},
  pages={177-182},
  abstract={Due to recent researches on artificial neural net- work algorithms and machine learning, the accuracy of image recognition and natural language processing has increased to the level of human beings in specific fields. Especially, researches to improve the accuracy of algorithms are being actively conducted, and researches on hardware accelerators that implement such algorithms quickly and efficiently are actively under way. In order to utilize artificial intelligence reasoning ability as well as computing speed in mobile or embedded environment, it is necessary to reduce the power consumption and memory usage of artificial intelligence hardware. In this paper, we propose a algorithm to reduce the computational complexity in designing the CNN accelerator. We tried to reduce the MAC computation by encoding the inputs and predicting the sign of the MAC oper- ation. We confirmed the performance improvement by evaluating the sign predictor through the simulation results.},
  keywords={Neural networks;Convolution;Encoding;Hardware;Two dimensional displays;Image coding;Arrays;CNN;Hardware accelerator;Sign prediction;activation function},
  doi={10.1109/ICTC.2018.8539530},
  ISSN={2162-1233},
  month={Oct},}@INPROCEEDINGS{10646794,
  author={Noppel, Maximilian and Wressnegger, Christian},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={SoK: Explainable Machine Learning in Adversarial Environments}, 
  year={2024},
  volume={},
  number={},
  pages={2441-2459},
  abstract={Modern deep learning methods have long been considered black boxes due to the lack of insights into their decision-making process. However, recent advances in explainable machine learning have turned the tables. Post-hoc explanation methods enable precise relevance attribution of input features for otherwise opaque models such as deep neural networks. This progression has raised expectations that these techniques can uncover attacks against learning-based systems such as adversarial examples or neural backdoors. Unfortunately, current methods are not robust against manipulations themselves. In this paper, we set out to systematize attacks against post-hoc explanation methods to lay the groundwork for developing more robust explainable machine learning. If explanation methods cannot be misled by an adversary, they can serve as an effective tool against attacks, marking a turning point in adversarial machine learning. We present a hierarchy of explanation-aware robustness notions and relate existing defenses to it. In doing so, we uncover synergies, research gaps, and future directions toward more reliable explanations robust against manipulations.},
  keywords={Deep learning;Privacy;Taxonomy;Decision making;Artificial neural networks;Turning;Robustness;Explainable Machine Learning;XAI;Attacks;Defenses;Robustness Notions},
  doi={10.1109/SP54263.2024.00021},
  ISSN={2375-1207},
  month={May},}@ARTICLE{10443885,
  author={Khan, Hikmat and Bouaynaya, Nidhal Carla and Rasool, Ghulam},
  journal={IEEE Access}, 
  title={Brain-Inspired Continual Learning: Robust Feature Distillation and Re-Consolidation for Class Incremental Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={34054-34073},
  abstract={Artificial intelligence and neuroscience have a long and intertwined history. Advancements in neuroscience research have significantly influenced the development of artificial intelligence systems that have the potential to retain knowledge akin to humans. Building upon foundational insights from neuroscience and existing research in adversarial and continual learning fields, we introduce a novel framework that comprises two key concepts: feature distillation and re-consolidation. The framework distills continual learning (CL) robust features and rehearses them while learning the next task, aiming to replicate the mammalian brain’s process of consolidating memories through rehearsing the distilled version of the waking experiences. Furthermore, the proposed framework emulates the mammalian brain’s mechanism of memory re-consolidation, where novel experiences influence the assimilation of previous experiences via feature re-consolidation. This process incorporates the new understanding of the CL model after learning the current task into the CL-robust samples of the previous task(s) to mitigate catastrophic forgetting. The proposed framework, called Robust Rehearsal, circumvents the limitations of existing CL frameworks that rely on the availability of pre-trained Oracle CL models to pre-distill CL-robustified datasets for training subsequent CL models. We conducted extensive experiments on three datasets, CIFAR10, CIFAR100, and real-world helicopter attitude datasets, demonstrating that CL models trained using Robust Rehearsal outperform their counterparts’ baseline methods. In addition, we conducted a series of experiments to assess the impact of changing memory sizes and the number of tasks, demonstrating that the baseline methods employing robust rehearsal outperform other methods trained without robust rehearsal. Lastly, to shed light on the existence of diverse features, we explore the effects of various optimization training objectives within the realms of joint, continual, and adversarial learning on feature learning in deep neural networks. Our findings indicate that the optimization objective dictates feature learning, which plays a vital role in model performance. Such observation further emphasizes the importance of rehearsing the CL-robust samples in alleviating catastrophic forgetting. In light of our experiments, closely following neuroscience insights can contribute to developing CL approaches to mitigate the long-standing challenge of catastrophic forgetting.},
  keywords={Task analysis;Neuroscience;Brain modeling;Adaptation models;Training;Biological system modeling;Robustness;Memory;Learning systems;Research and development;Continual learning;neuroscience-inspired;brain-inspired;catastrophic forgetting;feature distillation;feature re-consolidation;class-incremental learning;rehearsal-based learning strategies},
  doi={10.1109/ACCESS.2024.3369488},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10406035,
  author={Das, Sanjay and Kundu, Shamik and Basu, Kanad},
  booktitle={2023 IEEE 66th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
  title={Bottlenecks in Secure Adoption of Deep Neural Networks in Safety-Critical Applications}, 
  year={2023},
  volume={},
  number={},
  pages={801-805},
  abstract={The widespread proliferation of deep learning models has resulted in their extensive adoption in various domains, ranging from image recognition, Natural Language Processing (NLP), medical diagnosis, autonomous driving etc. Features such as high accuracy, quick inference, high scalability and adaptability have been instrumental in this revolution. Nonetheless, there are several elements that pose a threat to secure and reliable implementation of these models in safety-critical applications. Three primary reasons can be attributed to engender this bottleneck. First, Deep Neural Networks (DNN) are vulnerable to attacks that manipulate inputs and/or network parameters, which can compromise their performance. Additionally, faults in the DNN hardware can have adverse effects on the network accuracy. Next, error and uncertainty quantification in DNN-based inference are complex, which can undermine the dependability of their decisions. Finally, the lack of transparency and explainability in black-box models impedes their adoption in highly critical environments. This paper offers a comprehensive exploration of these issues, which must be addressed for improving the safety and reliability in adoption of machine learning models in mission-critical applications.},
  keywords={Deep learning;Uncertainty;Mission critical systems;Artificial neural networks;Natural language processing;Safety;Reliability;Deep learning;DNN accelerator;reliability;security;model uncertainty;explainability},
  doi={10.1109/MWSCAS57524.2023.10406035},
  ISSN={1558-3899},
  month={Aug},}@INPROCEEDINGS{9750755,
  author={Liang, Yiwen and Han, Ziqi},
  booktitle={2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI)}, 
  title={Intelligent Love Letter Generator Based on GPT-2 Model}, 
  year={2022},
  volume={},
  number={},
  pages={562-567},
  abstract={The confession wall is one of the most popular virtual communities in colleges and universities. It has the functions of expressing love, information release, academic exchange and thought guidance, among which the most important is expressing love. In this paper, an intelligent love letter generator is invented using GPT-2 mode in NLP (Natural Language Processing) based on the words collected from confession wall of college. In addition, emojis can be automatically selected which could match the love letter generated by the model. This smart love letter generator fits the fashion trend of contemporary young people, with a wide audience and good practical significance.},
  keywords={Training;Technological innovation;Computational modeling;Writing;Transformers;Market research;Generators;GPT (Generative Pre-training Transformer);text generation;love lotter},
  doi={10.1109/IWECAI55315.2022.00115},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10733691,
  author={Kaimakamidis, Anestis and Pitas, Ioannis},
  booktitle={2024 IEEE Symposium on Computers and Communications (ISCC)}, 
  title={Leveraging Collective Knowledge for Forest Fire Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper presents a novel Fire Classification Multi-Agent (FCMA) framework that utilizes peer-to-peer learning and distributed learning techniques to disseminate knowledge within the agent community. Furthermore, we define and introduce the architecture of a Deep Neural Network (DNN) agent, which can infinitely interact with other DNN agents and the external environment upon deployment. The FCMA framework is suitable for natural disaster management systems where multiple agents are required to run autonomously and foster the community’s knowledge. The FCMA provides two options for knowledge transfer, a peer-to-peer and a federated one. The experimental results display the effective knowledge transfer using both options and also compare the two options with each other in a forest fire classification setting.},
  keywords={Computers;Computer aided instruction;Federated learning;Distance learning;Disasters;Artificial neural networks;Forestry;Disaster management;Peer-to-peer computing;Knowledge transfer;Fire Classification;Multi-Agent Framework;Knowledge Distillation;Federated Learning},
  doi={10.1109/ISCC61673.2024.10733691},
  ISSN={2642-7389},
  month={June},}@INPROCEEDINGS{10854418,
  author={Zhu, Xinglong and Liao, Huitong and Chen, Lili and Zhang, Zhuohong},
  booktitle={6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024)}, 
  title={Anchorless target tracking based on region generation}, 
  year={2024},
  volume={2024},
  number={},
  pages={238-244},
  abstract={With the development of artificial intelligence in recent years, target tracking algorithms based on deep learning have been more and more widely used in the fields of unmanned aerial vehicles (UAVs), autonomous driving, sports events, and public safety. Although current target tracking algorithms have multiple solutions for problems such as occlusion, deformation, jitter, and fast movement, the accuracy of the tracker's position prediction is greatly reduced when the target undergoes deformation in a complex environment. In this paper, we propose to introduce an anchorless method based on a regional generative twin network to solve the problem of long computation time consumed by the model while performing the tracking task and the problem of high tracking failure rate when the object undergoes deformation. In the branch of predicting the object position, by directly predicting the distance between the centre point and the perimeter of the border, the number of parameters is five times less compared to the original anchor point-based target tracking algorithm, effectively shortening the computation time required for target tracking, while the accuracy remains almost the same. Experimental results on the VOT dataset show that this method is higher than the traditional method model in terms of accuracy and speed.},
  keywords={},
  doi={10.1049/icp.2024.4232},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10545413,
  author={Das, Sanjay and Kundu, Shamik and Basu, Kanad},
  booktitle={2024 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)}, 
  title={Explainability to the Rescue: A Pattern-Based Approach for Detecting Adversarial Attacks}, 
  year={2024},
  volume={},
  number={},
  pages={160-170},
  abstract={The widespread adoption of deep neural networks (DNNs) has resulted in their integration into critical autonomous systems. However, recent studies have emphasized the vulnerability of DNNs to fault injection attacks, specifically adversarial weight bit-flips. These attacks exploit memory systems to manipulate network parameters to compromise the performance of DNNs. Consequently, it is crucial to develop efficient online detectors to ensure the reliable and secure operation of DNNs on edge devices. In this paper, we propose a functional pattern-based approach, that efficiently detects attacks on DNNs. The methodology employs functional test patterns crafted by a post-hoc black-box model explanation method for executing run-time integrity checks on the model. To the best of our knowledge, this is the first piece of work that utilizes explainability to improve the security of edge DNN architectures in mission mode. The effectiveness of the proposed approach has been validated through extensive experiments conducted using various standard model-dataset configurations, underscoring its broad applicability. The results indicate that the suggested technique can attain a detection rate of up to 100 % with zero false positives using as few as a single pattern.},
  keywords={Explainable AI;Image edge detection;Computational modeling;Artificial neural networks;Predictive models;Transformers;Robustness;In-memory computing;Network-on-chip;Deep neural networks;DNN accelerator;Flooding attacks},
  doi={10.1109/HOST55342.2024.10545413},
  ISSN={2765-8406},
  month={May},}@INPROCEEDINGS{10911942,
  author={Alotaibi, Sarah},
  booktitle={2024 17th International Conference on Development in eSystem Engineering (DeSE)}, 
  title={Revealing the Unseen: An Insightful Survey on Explainable AI for Deepfake Face Detection}, 
  year={2024},
  volume={},
  number={},
  pages={526-531},
  abstract={A deepfake face is a technique that uses deep learning to create extremely realistic forged faces, making it challenging for humans to distinguish between real and fake faces. Existing deepfake face detection approaches show promise but heavily rely on complex deep neural networks (DNNs) and are often treated as black boxes. To address this, eXplainable AI (XAI) approaches have gained attention to enhance model interpretability and transparency. This survey reviews the most advanced XAI models used for detecting face deepfakes in videos and images. Our review examines the literature adapting to these XAI algorithms: Feature Visualization, Feature Attribution Methods, Attention Mechanisms, model-agnostic methods, interpretability through prototypes, and hybrid approaches. We perform a comparative analysis of different XAI methods and highlight their strengths and weaknesses. We also look ahead and highlight exciting, unsolved challenges that need to be addressed. This survey serves as a primary reference for researchers to enhance their understanding of neural networks using XAI techniques in the domain of deepfake detection in facial images and videos.},
  keywords={Surveys;Deep learning;Deepfakes;Adaptation models;Explainable AI;Reviews;Prototypes;Artificial neural networks;Face detection;Faces;Deepfake;Faces;Explainable AI;Deep Neural Networks},
  doi={10.1109/DeSE63988.2024.10911942},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10911410,
  author={Mao, Yiming},
  booktitle={2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)}, 
  title={Emotion-Aware Systems for Hazardous Operations: Integrating Mobile Sensors with LLM}, 
  year={2024},
  volume={},
  number={},
  pages={913-916},
  abstract={This study proposes a mobile device sensor-based emotion sensing system for hazard-involved operational action environments. In this study, the system utilizes accelerometers and gyroscopes on mobile devices for actor gait data acquisition. The obtained gait data is classified using an artificial intelligence algorith to classify the emotional state of the operator. At the same time, the system combines the gestures recognized by the sensors to identify critical operations, and combines the generative large language model (LLM) to provide personalized reminders and safety recommendations in high-risk operations. Through data analysis, the prediction accuracy of the regression model reaches 68% in anxiety, anger or relaxation, real-time safety reminders based on emotional state. Edge computing-based processing analyzes sensor data in real-time and combines emotions and actions to generate adaptive reminder content. This approach demonstrates the potential to develop low-cost, real-time methods for monitoring an individual’s mental health using LLMs.},
  keywords={Emotion recognition;Accuracy;Large language models;Real-time systems;Sensor systems;Mobile handsets;Sensors;Safety;Gyroscopes;Testing;Emotion recognition;gait analysis;LLM;SVM;gesture recognition},
  doi={10.1109/RICAI64321.2024.10911410},
  ISSN={},
  month={Dec},}@INBOOK{10952765,
  author={Bolojan, Daniel},
  booktitle={Diffusions in Architecture: Artificial Intelligence and Image Generators}, 
  title={Design as a Latent Condition}, 
  year={2024},
  volume={},
  number={},
  pages={109-119},
  abstract={Summary <p>Prompt encoding involves the deconstruction of the design task into parts, that could represent design intentions, levels of abstractions or a specific architectural system, developing different concepts and design intents separately and evaluating them before combining and layering them. The proposed approach considers design as a latent condition and facilitates its exploration by human designers and artificial intelligence (AI) agents through a collaborative interaction process. As the usage of generative AI models continues to proliferate, it is becoming increasingly evident that a future dominated by a singular, all&#x2010;encompassing AI model in architecture is unlikely. Instead, a multitude of specialized AI models designed for specific tasks and domains will interact with one another, including highly accurate task&#x2010;specific models and industry&#x2010;specific domain&#x2010;specific models, as well as general&#x2010;purpose models with broader functionality but potentially lower accuracy.</p>},
  keywords={Encoding;Prompt engineering;Process control;Text to image;Periodic structures;Iterative methods;Diffusion processes;Diffusion models;Complexity theory},
  doi={10.1002/9781394191802.ch13},
  ISSN={},
  publisher={Wiley},
  isbn={9781394191796},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952765},}@INPROCEEDINGS{9591016,
  author={Bentley, Peter J. and Lim, Soo Ling and Jindal, Shrey and Narang, Sid},
  booktitle={2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={Generating Synthetic Energy Usage Data to Enable Machine Learning for Sustainable Accommodation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Machine Learning has the potential to discover new correlations between energy usage in apartments and variables such as seasonality, apartment location, size, efficiency and details of those staying in the apartments, thus helping apartments to become more sustainable and helping those who stay in them to use less energy. The biggest impedance to creating such ML tools is lack of viable data - without the data, the tools cannot be created - yet it is not feasible to wait for several years' worth of good data before creating the tools. Here we present a solution to this problem: the use of a digital twin to generate synthetic data. This approach is viable even when there is no existing data, but when expert knowledge about the relationship between systems exist. To achieve this, we develop a new agent-based synthetic data generator (ASDG) and explore a case study with a corporate housing and luxury alternate accommodation marketplace called TheSqua.re. We show that unlimited quantities of realistic data can be automatically generated, including data for different scenarios, and that it can be used by Machine Learning to discover the underlying correlations.},
  keywords={Correlation;Mechatronics;Digital twin;Computational modeling;Machine learning;Tools;Generators;big data;computational modelling;digital twin;synthetic data generator;energy;machine learning;artificial intelligence;sustainability},
  doi={10.1109/ICECCME52200.2021.9591016},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10756362,
  author={Adama, Samaké and Soukoura, Diassana Fatoumata dite and Ismaël, Koné and Lahsen, Boulmane},
  booktitle={2024 Sixth International Conference on Intelligent Computing in Data Sciences (ICDS)}, 
  title={Reliable Medical Data Augmentation for Deep Learning: a Case Study on Breast Cancer Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The limited availability of data poses a challenge for the effective application of deep learning techniques. To address this, various data augmentation methods have been developed. However, a significant limitation of these techniques is that they do not always ensure the integrity of the generated data. This means there is a risk of producing data that does not accurately reflect the original, potentially misrepresenting the phenomenon being studied. This drawback is particularly relevant when applying these augmentation techniques to medical images. In this paper, we propose an implicit data augmentation approach for classification problems, regardless of the data type. Starting with any classification problem, our approach first reformulates it into a binary classification task. Then, for this new task, we generate a new dataset composed of combinations of examples from the original dataset. As a result, the dataset size increases substantially, leading to implicit data augmentation. To solve this newly formulated classification problem, we define a generic neural network architecture. Preliminary results from a case study using the BreastMNIST dataset show promising im-provements. Specifically, we observed a significant performance increase due to our implicit data augmentation approach. A key advantage of our method is that it preserves the integrity of the training data, offering greater reliability for applying deep learning models to medical data.},
  keywords={Deep learning;Computational modeling;Neural networks;Training data;Computer architecture;Data augmentation;Data models;Breast cancer;Reliability;Biomedical imaging;Data augmentation;deep learning;image classification;medical data;reliable artificial intelligence;breast cancer prediction},
  doi={10.1109/ICDS62089.2024.10756362},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10824407,
  author={Zou, Chenjun and Feng, Zhuoyan and Guan, Hongwei and Xing, Chuanjun},
  booktitle={2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)}, 
  title={Chain-of-Thought Enhanced Content Detection in Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={610-617},
  abstract={This paper proposes and explores a content detection method for large language models based on the Chain of Thought (CoT) logic chain. By incrementally extracting logic chains from text or code and converting them into feature vectors, this method enables logical plagiarism detection and content verification for complex tasks. Particularly for classical algorithmic problems, such as dynamic programming, this approach effectively mitigates plagiarism attempts through variable renaming. The paper first delves into the theoretical foundation of CoT logic chains and analyzes their application mechanisms within large language models. It then provides a detailed account of the steps for feature extraction and similarity detection algorithms, including how to efficiently convert logic chains into feature vectors for comparison. To validate the effectiveness of this method, several classical algorithmic instances were selected for experimentation. Results indicate that the CoT logic chain-based content detection method offers significant advantages in detecting originality in academic papers and code. This approach not only substantially improves the detection rate of academic misconduct but also assists editors and reviewers in more accurately identifying AI-generated content, thereby holding broad applications in academic publishing, code review, and educational fields. Additionally, the paper discusses the potential application of this method in other domains, such as patent plagiarism detection and legal document similarity analysis.},
  keywords={Codes;Text analysis;Reviews;Publishing;Plagiarism;Large language models;Feature extraction;Vectors;Logic;Software engineering;Content Detection;Similarity Detection;Algorithmic Plagiarism Detection;Chain of Thought (CoT);Artificial Intelligence-Generated Content (AIGC)},
  doi={10.1109/CBASE64041.2024.10824407},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10066740,
  author={Brocki, Lennart and Dyer, George C. and Gładka, Anna and Chung, Neo Christopher},
  booktitle={2023 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Deep Learning Mental Health Dialogue System}, 
  year={2023},
  volume={},
  number={},
  pages={395-398},
  abstract={Mental health counseling remains a major challenge in modern society due to cost, stigma, fear, and unavailability. We posit that generative artificial intelligence (AI) models designed for mental health counseling could help improve outcomes by lowering barriers to access. To this end, we have developed a deep learning (DL) dialogue system called Serena. The system consists of a core generative model and post-processing algorithms. The core generative model is a 2.7 billion parameter Seq2Seq Transformer [26] fine-tuned on thousands of transcripts of person-centered-therapy (PCT) sessions. The series of postprocessing algorithms detects contradictions, improves coherency, and removes repetitive answers. Serena is implemented and deployed on https://serena.chat, which currently offers limited free services. While the dialogue system is capable of responding in a qualitatively empathetic and engaging manner, occasionally it displays hallucination and long-term incoherence. Overall, we demonstrate that a deep learning mental health dialogue system has the potential to provide a low-cost and effective complement to traditional human counselors with less barriers to access.},
  keywords={Deep learning;Employee welfare;Costs;Mental health;Transformer cores;Big Data;Transformers;Deep Learning;Artificial Intelligence;Transformers;Mental Health;Chatbot;Dialogue System},
  doi={10.1109/BigComp57234.2023.00097},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10675965,
  author={Ma, Peipeng and Dou, Jinhua and Jia, Jiayi and Ma, Xinrong},
  booktitle={2024 International Conference on Culture-Oriented Science & Technology (CoST)}, 
  title={AIGC-assisted geospatial generative design for classical Chinese literary works}, 
  year={2024},
  volume={},
  number={},
  pages={22-27},
  abstract={Visualizing the geographic space in the text is a good way to reveal the spatial structure of the narrative. The spatial maps in current fictional works require the creator to carefully read and analyze the entire text of the literary work and then draw it manually, which consumes a lot of time and manpower.This study proposes a systematic method that combines natural language processing (NLP) and artificial intelligence generated content (AIGC) to transform the geographic space described in classical texts into three-dimensional (3D) visualization maps. The study first designs a three-stage NLP method of “positioning-divided-judgment” to accurately extract geographic entities in the text; secondly, formulates and applies rules for converting literary descriptions into quantified spatial coordinates; finally, the 3D-AIGC model is used to combine descriptive poems to generate geographic entity models and arrange them into maps according to their spatial coordinates. The text-geospace rapid conversion method proposed in this study can assist the author’s literary creation and the reader’s reading comprehension, and provides new ideas in the fields of text digital display and film and television adaptation, showing the application potential of AI in the fields of literature and digital humanities.},
  keywords={Visualization;Humanities;Three-dimensional displays;TV;Systematics;Costs;Transforms;Literary Geography;Chinese Classical Literary;NLP;Place Name Extraction;AI Prompt Methods;Multimodal - AIGC},
  doi={10.1109/CoST64302.2024.00014},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9009560,
  author={Shocher, Assaf and Bagon, Shai and Isola, Phillip and Irani, Michal},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={InGAN: Capturing and Retargeting the “DNA” of a Natural Image}, 
  year={2019},
  volume={},
  number={},
  pages={4491-4500},
  abstract={Generative Adversarial Networks (GANs) typically learn a distribution of images in a large image dataset, and are then able to generate new images from this distribution. However, each natural image has its own internal statistics, captured by its unique distribution of patches. In this paper we propose an "Internal GAN'' (InGAN) - an image-specific GAN - which trains on a single input image and learns its internal distribution of patches. It is then able to synthesize a plethora of new natural images of significantly different sizes, shapes and aspect-ratios - all with the same internal patch-distribution (same "DNA'') as the input image. In particular, despite large changes in global size/shape of the image, all elements inside the image maintain their local size/shape. InGAN is fully unsupervised, requiring no additional data other than the input image itself. Once trained on the input image, it can remap the input to any size or shape in a single feedforward pass, while preserving the same internal patch distribution. InGAN provides a unified framework for a variety of tasks, bridging the gap between textures and natural images.},
  keywords={Gallium nitride;Shape;Generators;Task analysis;DNA;Visualization;Image reconstruction},
  doi={10.1109/ICCV.2019.00459},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{7487237,
  author={Rosen, David M. and Mason, Julian and Leonard, John J.},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Towards lifelong feature-based mapping in semi-static environments}, 
  year={2016},
  volume={},
  number={},
  pages={1063-1070},
  abstract={The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, “everything changes and nothing stands still, ” and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
  keywords={Feature extraction;Detectors;Computational modeling;Adaptation models;Simultaneous localization and mapping;Bayes methods},
  doi={10.1109/ICRA.2016.7487237},
  ISSN={},
  month={May},}@ARTICLE{9127831,
  author={Huang, Xin and McGill, Stephen G. and DeCastro, Jonathan A. and Fletcher, Luke and Leonard, John J. and Williams, Brian C. and Rosman, Guy},
  journal={IEEE Robotics and Automation Letters}, 
  title={DiversityGAN: Diversity-Aware Vehicle Motion Prediction via Latent Semantic Sampling}, 
  year={2020},
  volume={5},
  number={4},
  pages={5089-5096},
  abstract={Vehicle trajectory prediction is crucial for autonomous driving and advanced driver assistant systems. While existing approaches may sample from a predicted distribution of vehicle trajectories, they lack the ability to explore it – a key ability for evaluating safety from a planning and verification perspective. In this work, we devise a novel approach for generating realistic and diverse vehicle trajectories. We first extend the generative adversarial network (GAN) framework with a low-dimensional approximate semantic space, and shape that space to capture semantics such as merging and turning. We then sample from this space in a way that mimics the predicted distribution, but allows us to control coverage of semantically distinct outcomes. We validate our approach on a publicly available dataset and show results that achieve state-of-the-art prediction performance, while providing improved coverage of the space of predicted trajectory semantics.},
  keywords={Trajectory;Semantics;Cognition;Vehicles;Taxonomy;Generators;Planning;Intelligent Transportation Systems;Representation Learning;Computer Vision for Transportation},
  doi={10.1109/LRA.2020.3005369},
  ISSN={2377-3766},
  month={Oct},}@ARTICLE{10385176,
  author={Cao, Chentao and Cui, Zhuo-Xu and Wang, Yue and Liu, Shaonan and Chen, Taijin and Zheng, Hairong and Liang, Dong and Zhu, Yanjie},
  journal={IEEE Transactions on Medical Imaging}, 
  title={High-Frequency Space Diffusion Model for Accelerated MRI}, 
  year={2024},
  volume={43},
  number={5},
  pages={1853-1865},
  abstract={Diffusion models with continuous stochastic differential equations (SDEs) have shown superior performances in image generation. It can serve as a deep generative prior to solving the inverse problem in magnetic resonance (MR) reconstruction. However, low-frequency regions of  ${k}$ -space data are typically fully sampled in fast MR imaging, while existing diffusion models are performed throughout the entire image or  ${k}$ -space, inevitably introducing uncertainty in the reconstruction of low-frequency regions. Additionally, existing diffusion models often demand substantial iterations to converge, resulting in time-consuming reconstructions. To address these challenges, we propose a novel SDE tailored specifically for MR reconstruction with the diffusion process in high-frequency space (referred to as HFS-SDE). This approach ensures determinism in the fully sampled low-frequency regions and accelerates the sampling procedure of reverse diffusion. Experiments conducted on the publicly available fastMRI dataset demonstrate that the proposed HFS-SDE method outperforms traditional parallel imaging methods, supervised deep learning, and existing diffusion models in terms of reconstruction accuracy and stability. The fast convergence properties are also confirmed through theoretical and experimental validation. Our code and weights are available at https://github.com/Aboriginer/HFS-SDE.},
  keywords={Image reconstruction;Diffusion processes;Convergence;Mathematical models;Magnetic resonance imaging;Perturbation methods;Kernel;Diffusion models;MRI;image reconstruction;inverse problem},
  doi={10.1109/TMI.2024.3351702},
  ISSN={1558-254X},
  month={May},}@ARTICLE{10325535,
  author={An, Tai and Xue, Bin and Huo, Chunlei and Xiang, Shiming and Pan, Chunhong},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Efficient Remote Sensing Image Super-Resolution via Lightweight Diffusion Models}, 
  year={2024},
  volume={21},
  number={},
  pages={1-5},
  abstract={With the emergence of diffusion models, the image generation has experienced a significant advancement. In super-resolution tasks, diffusion models surpass generative adversarial network (GAN)-based methods in generating more realistic samples. However, these models come with significant costs: denoising networks rely on large U-Net, making them computationally intensive for high-resolution (HR) images, and the extensive sampling steps in diffusion models lead to prolonged inference time. This complexity limits their application in remote sensing, due to the high demand for high-resolution images in such scenarios. To address this, we propose a lightweight diffusion model (LWTDM), which simplifies the denoising network and efficiently incorporates conditional information using a cross-attention-based encoder–decoder architecture. Furthermore, LWTDM serves as the pioneering model that incorporates the accelerated sampling technique from denoising diffusion implicit models (DDIMs). This integration involves the meticulous selection of sampling steps, ensuring the quality of the generated images. The experiments confirm that LWTDM strikes a favorable balance between precision and perceptual quality, while its faster inference speed makes it suitable for diverse remote sensing scenarios with specific requirements. The source code is available at: https://github.com/Suanmd/LWTDM.},
  keywords={Noise reduction;Mathematical models;Superresolution;Remote sensing;Data models;Computational modeling;Gaussian distribution;Cross-attention mechanism;lightweight diffusion models (LWTDM);remote sensing super-resolution;satellite imagery},
  doi={10.1109/LGRS.2023.3335421},
  ISSN={1558-0571},
  month={},}@ARTICLE{10659917,
  author={Lim, Se-Heon and Kim, Taegeun and Lee, Kyeong-Yeong and Song, Kyung-Min and Yoon, Sung-Guk},
  journal={IEEE Access}, 
  title={Two-Stage Fault Classification Algorithm for Real Fault Data in Transmission Lines}, 
  year={2024},
  volume={12},
  number={},
  pages={121156-121168},
  abstract={Fault classification in power transmission lines is important in distance relaying for identifying the accurate phases implicated in the fault occurrence. Generally, the accuracy of fault classification algorithms is evaluated by simulation data, which shows quite different characteristics from real fault data. Also, most of the previous works on fault classification used a single-stage method such as a rule-based algorithm or machine learning-based algorithm. Because of the diverse characteristics of real fault data, the performance of the single-stage method is limited. To address these issues, this paper proposes a novel two-stage algorithm that combines the strengths of rule-based and machine-learning algorithms to improve the accuracy of real fault data. A case study using real fault data shows that the proposed two-stage algorithm outperforms other conventional single-stage algorithms.},
  keywords={Classification algorithms;Artificial neural networks;Accuracy;Random forests;Machine learning algorithms;Reviews;Prediction algorithms;Root mean square;Power transmission lines;Two-stage algorithm;rule-based;artificial neural network;root mean square},
  doi={10.1109/ACCESS.2024.3452188},
  ISSN={2169-3536},
  month={},}@ARTICLE{10644094,
  author={Li, Ming and Yang, Zhaoli and Wang, Tao and Zhang, Yushu and Wen, Wenying},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dual Protection for Image Privacy and Copyright via Traceable Adversarial Examples}, 
  year={2024},
  volume={34},
  number={12},
  pages={13401-13412},
  abstract={In recent years, the uploading of massive personal images has increased the security risks, mainly including privacy breaches and copyright infringement. Adversarial examples provide a novel solution for protecting image privacy, as they can evade the detection by deep neural network (DNN)-based recognizers. However, the perturbations in the adversarial examples typically meaningless and therefore cannot be extracted as traceable information to support copyright protection. In this paper, we designed a dual protection scheme for image privacy and copyright via traceable adversarial examples. Specifically, a traceable adversarial model is proposed, which can be used to embed the invisible copyright information into images for copyright protection while fooling DNN-based recognizers for privacy protection. Inspired by the training method of generative adversarial networks (GANs), a new dynamic adversarial training strategy is designed, which allows our model for achieving stable multi-objective learning. Experimental results show that our scheme is exceptionally robust in the face of a variety of noise conditions and image processing methods, while exhibiting good model migration and defense robustness.},
  keywords={Watermarking;Privacy;Perturbation methods;Protection;Copyright protection;Robustness;Training;Adversarial examples;copyright;deep neural networks;privacy protection;robust watermarking},
  doi={10.1109/TCSVT.2024.3448351},
  ISSN={1558-2205},
  month={Dec},}@ARTICLE{10535047,
  author={Zhu, Xingjian and Dong, Jiankuo and Qi, Jin and Zhou, Zhenguo and Dong, Zhenjiang and Sun, Yanfei and Wang, Moyu},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={AUTH: An Adversarial Autoencoder Based Unsupervised Insider Threat Detection Scheme for Multisource Logs}, 
  year={2024},
  volume={20},
  number={9},
  pages={10954-10965},
  abstract={Deep learning has shown broad research prospects in addressing insider threats, a serious problem currently facing industrial information systems. Although deep learning is able to capture effective feature representations from complex multidimensional data, there are still issues such as strong stealth of insider threat behavior and the imbalance data that need to be solved. Therefore, we propose an adversarial Autoencoder based Unsupervised insider Threat detection scHeme (AUTH). Compared to other methods, AUTH fully considers the role of time feature and event feature in threat detection. In addition, in order to improve the performance of autoencoder models to detect covert threat behaviors, AUTH drives a temporal convolutional network and long short-term memory network-based Adversarial Autoencoder (TL-AAE). Generative Adversarial Theory is introduced to solve the problem of uncertainty in the latent feature of the encoder. Finally, with the sufficient experiments on public datasets, we demonstrate that the usefulness of adding time features and the proposed TL-AAE model to improve threat detection performance. Compared with the baseline, AUTH obtains the area under curve value of 0.932, which is 4.95% higher than the highest result obtained by the baseline. In addition, AUTH obtains the EER value of 0.146, which is 12.57% lower than the lowest result of the baseline.},
  keywords={Feature extraction;Threat assessment;Machine learning;Long short term memory;Hidden Markov models;Training;Deep learning;Adversarial autoencoder;insider threat detection;time series data;unsupervised learning},
  doi={10.1109/TII.2024.3393491},
  ISSN={1941-0050},
  month={Sep.},}@INPROCEEDINGS{11092461,
  author={Yuan, Chao and Zhang, Guiwei and Ma, Changxiao and Zhang, Tianyi and Niu, Guanglin},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization}, 
  year={2025},
  volume={},
  number={},
  pages={24409-24418},
  abstract={Person re-identification (ReID) aims to extract accurate identity representation features. However, during feature extraction, individual samples are inevitably affected by noise (background, occlusions, and model limitations). Considering that features from the same identity follow a normal distribution around identity centers after training, we propose a Training-Free Feature Centralization ReID framework (Pose2ID) by aggregating the same identity features to reduce individual noise and enhance the stability of identity representation, which preserves the feature’s original distribution for following strategies such as re-ranking. Specifically, to obtain samples of the same identity, we introduce two components: ➀Identity-Guided Pedestrian Generation: by leveraging identity features to guide the generation process, we obtain high-quality images with diverse poses, ensuring identity consistency even in complex scenarios such as infrared, and occlusion. ➁Neighbor Feature Centralization: it explores each sample’s potential positive samples from its neighborhood. Experiments demonstrate that our generative model exhibits strong generalization capabilities and maintains high identity consistency. With the Feature Centralization framework, we achieve impressive performance even with an ImageNet pre-trained model without ReID training, reaching mAP/Rank-1 of 52.81/78.92 on Market1501. Moreover, our method sets new state-of-the-art results across standard, cross-modality, and occluded ReID tasks, showcasing strong adaptability.},
  keywords={Training;Adaptation models;Pedestrians;Noise;Gaussian distribution;Feature extraction;Stability analysis;Pattern recognition;Standards;Identification of persons;person re-identification;controllable pedestrian generation;feature centralization},
  doi={10.1109/CVPR52734.2025.02273},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{11021042,
  author={Xu, Yuan and Chen, Yi and Liang, Xue},
  booktitle={2025 IEEE 18th Pacific Visualization Conference (PacificVis)}, 
  title={MTvis: Understanding and Optimizing Microbial Time-series Data Augmentation Model via Interactive Visualization}, 
  year={2025},
  volume={},
  number={},
  pages={160-170},
  abstract={Microbial time-series data usually need to be obtained by professionals through biological experiments, and the amount of data obtained from such manual experiments is very limited. Deep learning generative models leverage the superior learning capabilities of neural networks to generate high-quality synthetic data, which provides a promising approach to address the above shortcomings. However, the black-box nature of deep learning models makes it difficult for domain experts to trust the generated data, limiting the application of these techniques. Thus, we propose MTvis, an interactive visualization system designed to help experts understand and optimize data generated by MT-GAN, which is a microbial time-series data augmentation model proposed by us. MT-GAN not only introduces the temperature lag effect but also captures the dynamics of microbial time-series data via adversarial and joint learning. MTvis provides two exploration modes which enable users to gain insights into the structure of the model and adjust hyperparameters dynamically. The system also provides real-time observation of distribution differences between generated and real data. Experimental results show that MT-GAN effectively improves the fidelity of the generated data, while MTvis enhances the credibility and practicality of synthetic data for domain experts.},
  keywords={Deep learning;Temperature distribution;Computational modeling;Biological system modeling;Visual analytics;Data visualization;Data augmentation;Data models;Real-time systems;Synthetic data;Deep learning;Information visualization;Interactive visual analytics;Time-series data augmentation},
  doi={10.1109/PacificVis64226.2025.00022},
  ISSN={2165-8773},
  month={April},}@ARTICLE{11026001,
  author={Zhao, Jiangting and Zhang, Xiaoyu and Yang, Yandong},
  journal={IEEE Internet of Things Journal}, 
  title={Visual SLAM Dynamic Disturbance Suppression Algorithm Combining Blur Denoising and Object Detection}, 
  year={2025},
  volume={12},
  number={15},
  pages={32259-32270},
  abstract={Visual simultaneous localization and mapping (SLAM) is a crucial technology for solving the navigation problem of unmanned systems in unknown environments, playing an important role in Internet of Things (IoT) applications. However, the dynamic objects in the environment bring great challenges to visual SLAM. The dynamic objects and the blur errors caused by the motion seriously affect the feature extraction quality and localization accuracy of the system. To address these problems, this article proposes a dynamic disturbance suppression algorithm combining blur denoising and object detection, referred to as DO-SLAM, which improves the front end of ORB-SLAM2 to make the system more robust in dynamic environments. First, DO-SLAM employs a deblurring algorithm based on generative adversarial networks (GANs) to reduce the blur noise caused by dynamic objects or the jitter of unmanned platforms. Then, the optimized object detection is combined with the geometry constraint and the overlapping box strategy to comprehensively eliminate the dynamic feature points. Finally, the remaining static feature points are used to complete the subsequent pose estimation. Local and global experiments in TUM dynamic datasets demonstrate that DO-SLAM greatly improves the performance of the system in dynamic environments compared with ORB-SLAM2 and has better robustness than other similar methods.},
  keywords={Dynamics;Feature extraction;Heuristic algorithms;Simultaneous localization and mapping;Accuracy;Detectors;Semantic segmentation;Geometry;Training;Sensors;Blur denoising;dynamic environments;unmanned systems;visual simultaneous localization and mapping (SLAM)},
  doi={10.1109/JIOT.2025.3576861},
  ISSN={2327-4662},
  month={Aug},}@INPROCEEDINGS{10963161,
  author={Muthugala, Nethmi and Yunpeng, Li},
  booktitle={2025 5th International Conference on Advanced Research in Computing (ICARC)}, 
  title={Enhanced Anomaly Detection in Aerial Imagery using Text to Image Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This study investigates the use of generative AI models, like unCLIP for synthetic aerial image generation to improve anomaly detection in the field of aerial imagery. The research demonstrates how unCLIP can generate realistic aerial images from textual descriptions, addressing challenges related to the limited availability of real-world data. These artificially generated images with anomalies are then used to train and optimize the YOLOv8 model for anomaly detection, with a focus on detecting environmental and infrastructural anomalies. The study explores the entire process—from prompt design and image generation to data preprocessing, anomaly labeling, and model training—while evaluating the effectiveness of synthetic data qualitatively and quantitatively in enhancing model performance. While the results indicate that the generated images significantly improve anomaly detection capabilities, challenges in achieving high image fidelity, accuracy and realism still exist. All the findings of this research work underscore the potential of synthetic data generation in training robust machine learning models and provide a foundation for future research to improve the scalability, accuracy, and generalizability of anomaly detection systems.},
  keywords={Training;Accuracy;Image synthesis;Scalability;Text to image;Machine learning;Data models;Labeling;Anomaly detection;Synthetic data;Anomaly Detection;Aerial Imagery;Synthetic Images;unCLIP;YOLOv8},
  doi={10.1109/ICARC64760.2025.10963161},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10582044,
  author={Liu, Yexiang and Liu, Jin and Cao, Jie and Duan, Junxian and He, Ran},
  booktitle={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={PortraitDAE: Line-Drawing Portraits Style Transfer from Photos via Diffusion Autoencoder with Meaningful Encoded Noise}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The line-drawing portrait is a kind of highly abstract art that contains a sparse set of continuous graphical elements such as lines to capture a person's facial features. Due to their abstract artistic form, common style transfer methods fail to synthesize high-quality line-drawing portraits from photos. Previous works mostly concentrate on GANs, often requiring pre-calculated landmarks acquired by other models and using extra classifiers with complicated structures to capture local facial features. We propose a novel idea without these extra operations based on diffusion models, which is more flexible and stable than GAN-based methods. We utilize the diffusion-based decoder in the Diffusion Autoencoder to encode the input image to an encoded noise that contains much meaningful stochastic information by running the deterministic generative process backward. By fully utilizing the encoded noise, our method can effectively preserve the identity information and better capture facial details. We also improve the loss function to alleviate the interference of the background color. Several experiments show that our method can produce better samples with smoother lines that look more like the corresponding person, outperforming state-of-the-art methods both qualitatively and quantitatively. Our method can also be generalized to other styles such as sketch.},
  keywords={Image color analysis;Face recognition;Noise;Stochastic processes;Interference;Gesture recognition;Diffusion models},
  doi={10.1109/FG59268.2024.10582044},
  ISSN={2770-8330},
  month={May},}@ARTICLE{9760126,
  author={Wang, Chaoli and Han, Jun},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization}, 
  year={2023},
  volume={29},
  number={8},
  pages={3714-3733},
  abstract={Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey articles on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this article, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The article concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.},
  keywords={Task analysis;Generative adversarial networks;Neural networks;Measurement;Deep learning;Convolutional neural networks;Three-dimensional displays;Scientific visualization;deep learning;survey},
  doi={10.1109/TVCG.2022.3167896},
  ISSN={1941-0506},
  month={Aug},}@INPROCEEDINGS{9799865,
  author={Keydal, Duygu and Oymak, Erencan and Demir, Kadir Batuhan and Yilmaz, Guray and Sahingoz, Ozgur Koray},
  booktitle={2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)}, 
  title={The Art of Machine Learning as Fashion Stylish for Designing Clothes}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Over the years, designers have come to the fore with their originality and personal styles and have shaped the fashion industry with their designs. However, due to the progress of time, designers have become unable to meet the demands of all consumers. Since it takes a lot of time to produce an original design, the production process progresses slowly, and customers are uncomfortable with this situation. As in many other industries, designers are trying to solve this problem with the help of artificial intelligence, which is indispensable in the fields of commerce, art, and security. It first entered the fashion sector with drawing programs in the 1950s and has started to change the fashion sector since the 2000s. In the 1950s, artificial intelligence was used only to create a virtual drawing environment When the designer makes a mistake, he can simply erase the mistake and continue working on the design without having to start the whole design from scratch. These programs have greatly facilitated the work of designers. Designers are now able to draw their designs in a much shorter time. But even this shortened period is not enough for the whole fashion industry. Designers could still not keep up with the demands of all customers. Thanks to researchers who added different perspectives to artificial intelligence in the early 2000s with its usage not only for drawing but also for designing, Therefore, in this paper, it is aimed at producing some original designs by preserving the designer's style with the use of Aí techniques. With the proposed model, it has become able to produce ready-made designs by using features such as object detection and visual processing. The experimental results showed that Aí techniques are very successful for combining different patterns for producing an original fashion style.},
  keywords={Industries;Visualization;Art;Shape;Supply chains;Software algorithms;Production;Artificial intelligence;Art;Pattern;Design;Technology},
  doi={10.1109/HORA55278.2022.9799865},
  ISSN={},
  month={June},}@INPROCEEDINGS{10410677,
  author={Naeem, Ammar Muhammad and Gad, Abdalla and Alhalabi, Marah and Yaghi, Maha and Khelifi, Adel and Ghazal, Mohammed},
  booktitle={2023 10th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={AI-Based Mobile Paper Grading: Trends and Challenges}, 
  year={2023},
  volume={},
  number={},
  pages={404-409},
  abstract={Many universities, schools, and other educational institutions embraced distance learning due to the Covid-19 Pandemic in previous years. Because some instructors are unfamiliar with it, creating exams for students using online platforms is not always simple. Therefore, it takes a lot of time and effort to create online assessments and manually correct them. Additionally, grading many tests can burden instructors, especially if they are unfamiliar with online technology. Exams can be made as multiple-choice questions, which can help instructors overcome human error in judging exam papers, as a temporary fix for this rather than creating lengthy questions. This, however, is not a long-term fix because instructors still have to spend time grading the multiple-choice questions. Artificial intelligence can therefore be quite useful in this case. This paper offers an overview of notable implementations that autonomously grade and compute exam paper outcomes using vision-based and Artificial Intelligence-based mobile approaches. Additionally, different techniques offer a variety of methods, each with its benefits and limitations. In this paper, each strategy's methodologies, as well as its results performance, are discussed. Also, it discusses which approach is more appropriate for an automated Grading system. Based on the results reviewed from different sources, it was found that Artificial Intelligence-based approaches and vision-based approaches are accurate in detecting the objectives of this project. However, the AI-based approach has more efficiency due to the combination of various models, resulting in higher accuracy. Finally, both approaches' advantages and limitations are described throughout the report.},
  keywords={Performance evaluation;Pandemics;Computational modeling;Predictive models;Mobile handsets;Mobile applications;Standards;Optical Character Recognition;Deep Learning;Vision-based approach;Artificial Intelligence-based approach;Mobile;Yolo},
  doi={10.1109/FiCloud58648.2023.00065},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10541709,
  author={Liu, Xiaorong and Meng, Sa and Zheng, Danyang},
  booktitle={2024 7th World Conference on Computing and Communication Technologies (WCCCT)}, 
  title={Cost Optimization for In-Network Computing Services Deployment in Edge-Cloud Networks}, 
  year={2024},
  volume={},
  number={},
  pages={267-272},
  abstract={In edge-cloud computing networks, clients can conveniently offload their computation tasks to the servers along the traffic flow, the process of which is known as in-network computing (INC). Notably, within the landscape of cutting-edge artificial intelligence, big data, and virtualization techniques, the computational demands of an INC service can range from relatively modest demands, for instance, image processing, to highly demanding tasks, such as generative pre-training. Since the edge-cloud computing networks’ pricing structure is associated with computational resources availability and the service location, a compelling quandary emerges concerning the deployment of INC services in edge-cloud networks. In this work, our focal point is the optimization of the cost-effectiveness pertaining to the deployment of INC services in edge-cloud computing networks. To begin, we mathematically formalize the In-Network Computing services Deployment (INCD) problem with the objective of cost optimization. In addressing the INCD problem, we propose an efficient heuristic algorithm known as Optimal In-Network Computing Services Deployment (Opt-INCD), which is proved to be optimal when deploying an INC service. Extensive simulation results show that the proposed Opt-INCD algorithm outperforms the greedy-based benchmarks by an average of 35.42% and 43.46%, respectively.},
  keywords={Costs;Runtime;Simulation;Image edge detection;Pricing;Benchmark testing;Servers;Edge-cloud computing networks;In-network computing;Cost optimization},
  doi={10.1109/WCCCT60665.2024.10541709},
  ISSN={},
  month={April},}@ARTICLE{10173738,
  author={Yuan, Xiaofeng and Huang, Lingfeng and Li, Lin and Wang, Kai and Wang, Yalin and Ye, Lingjian and Shen, Feifan},
  journal={IEEE Sensors Journal}, 
  title={Multiscale Dynamic Feature Learning for Quality Prediction Based on Hierarchical Sequential Generative Network}, 
  year={2023},
  volume={23},
  number={17},
  pages={19561-19570},
  abstract={In industrial processes, long short-term memory (LSTM) is usually used for temporal dynamic modeling of soft sensor. The process data usually have various temporal correlations under different time scales due to the continuous physical and chemical reactions. However, LSTM model can only extract the dynamic features at a specific time scale, which affects the feature learning capability and modeling accuracy. In this article, a new hierarchical sequential generative network (HSGN) is proposed for mining multiscale dynamic features using large amount of unlabeled process data for soft sensor. To extract multiscale dynamic features for quality prediction, the process data are resampled with different sampling rates and then used to pretrain the corresponding self-learning LSTM models at different time scales. Subsequently, they can used to calculate the multiscale hidden feature states for labeled samples, which are further integrated with the original input information and input into a deep belief network (DBN) to construct the prediction model for the output variable. Thus, the HSGN method can take advantage of large number of unlabeled samples to mine multiscale dynamic hidden features and overcome the irregular sampling problem in industrial processes. The application in a real industrial scene shows the effectiveness of the proposed method.},
  keywords={Feature extraction;Data models;Soft sensors;Sensors;Predictive models;Logic gates;Deep learning;Deep learning;hierarchical sequential generative network (HSGN);multiscale dynamics;quality prediction;soft sensor},
  doi={10.1109/JSEN.2023.3290163},
  ISSN={1558-1748},
  month={Sep.},}@INPROCEEDINGS{10560442,
  author={Sheba, M.Vanitha and Sadagopan, S.},
  booktitle={2024 International Conference on Computing and Data Science (ICCDS)}, 
  title={Enhanced Deep Learning Techniques for Multimodal Medical Image Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Multimodal Medical Image Fusion is the process of integrating data from various imaging modalities to produce a full and improved analysis of a patient's medical condition. MRI, CT, and PE T scans are examples of medical imaging modalities that can be used to produce a more thorough and informative image for better disease monitoring, treatment planning, and diagnosis. Because deep learning can understand the links between the input modalities and the intended fused image, it has become a potent tool for medical image fusion. Deep learning is a subfield of artificial intelligence. Computed tomography (CT) and magnetic resonance imaging (MRI) are two popular medical imaging technologies that provide complementary insights into the human body's internal components. While CT employs X-rays to provide cross-sectional images of bones, tissues, and organs with high spatial resolution and quick imaging, MRI uses magnetic fields and radio waves to make detailed images of soft tissues and organs with great contrast resolution. This paper explains a few of the deep learning techniques like DeepFuse technique, ResN et architecture, Generative Adversarial Networks (GANs), Convolutional Neural Network-Recurrent Neural Network (CNN-RNN) model, Autoencoders, and Attention mechanics are some of the deep learning methods for MRI and CT image fusion that are currently in use. Better visualization and delineation of anatomical features are made possible by these deep learning-based fusion approaches, which eventually improve patient diagnosis, treatment, and overall health care results.},
  keywords={Deep learning;Magnetic resonance imaging;Computed tomography;X-rays;Planning;Convolutional neural networks;Medical diagnostic imaging;Multimodal image fusion;deep learning techniques;MRI;CT images},
  doi={10.1109/ICCDS60734.2024.10560442},
  ISSN={},
  month={April},}@ARTICLE{10274679,
  author={Zhao, Min and Dobigeon, Nicolas and Chen, Jie},
  journal={IEEE Transactions on Image Processing}, 
  title={Guided Deep Generative Model-Based Spatial Regularization for Multiband Imaging Inverse Problems}, 
  year={2023},
  volume={32},
  number={},
  pages={5692-5704},
  abstract={When adopting a model-based formulation, solving inverse problems encountered in multiband imaging requires to define spatial and spectral regularizations. In most of the works of the literature, spectral information is extracted from the observations directly to derive data-driven spectral priors. Conversely, the choice of the spatial regularization often boils down to the use of conventional penalizations (e.g., total variation) promoting expected features of the reconstructed image (e.g., piece-wise constant). In this work, we propose a generic framework able to capitalize on an auxiliary acquisition of high spatial resolution to derive tailored data-driven spatial regularizations. This approach leverages on the ability of deep learning to extract high level features. More precisely, the regularization is conceived as a deep generative network able to encode spatial semantic features contained in this auxiliary image of high spatial resolution. To illustrate the versatility of this approach, it is instantiated to conduct two particular tasks, namely multiband image fusion and multiband image inpainting. Experimental results obtained on these two tasks demonstrate the benefit of this class of informed regularizations when compared to more conventional ones.},
  keywords={Task analysis;Imaging;Spatial resolution;Image restoration;Optimization;Learning systems;Hyperspectral imaging;Multiband imaging;inverse problems;deep learning;deep image prior;guided image;deep generative regularization},
  doi={10.1109/TIP.2023.3321460},
  ISSN={1941-0042},
  month={},}@BOOK{10162713,
  author={Jha, Ashish Ranjan and Pillai, Dr. Gopinath},
  booktitle={Mastering PyTorch: Build powerful neural network architectures using advanced PyTorch 1.x features},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Master advanced techniques and algorithms for deep learning with PyTorch using real-world examplesKey FeaturesUnderstand how to use PyTorch 1.x to build advanced neural network modelsLearn to perform a wide range of tasks by implementing deep learning algorithms and techniquesGain expertise in domains such as computer vision, NLP, Deep RL, Explainable AI, and much moreBook DescriptionDeep learning is driving the AI revolution, and PyTorch is making it easier than ever before for anyone to build deep learning applications. This PyTorch book will help you uncover expert techniques to get the most out of your data and build complex neural network models. The book starts with a quick overview of PyTorch and explores using convolutional neural network (CNN) architectures for image classification. You'll then work with recurrent neural network (RNN) architectures and transformers for sentiment analysis. As you advance, you'll apply deep learning across different domains, such as music, text, and image generation using generative models and explore the world of generative adversarial networks (GANs). You'll not only build and train your own deep reinforcement learning models in PyTorch but also deploy PyTorch models to production using expert tips and techniques. Finally, you'll get to grips with training large models efficiently in a distributed manner, searching neural architectures effectively with AutoML, and rapidly prototyping models using PyTorch and fast.ai. By the end of this PyTorch book, you'll be able to perform complex deep learning tasks using PyTorch to build smart artificial intelligence models.What you will learnImplement text and music generating models using PyTorchBuild a deep Q-network (DQN) model in PyTorchExport universal PyTorch models using Open Neural Network Exchange (ONNX)Become well-versed with rapid prototyping using PyTorch with fast.aiPerform neural architecture search effectively using AutoMLEasily interpret machine learning (ML) models written in PyTorch using CaptumDesign ResNets, LSTMs, Transformers, and more using PyTorchFind out how to use PyTorch for distributed training using the torch.distributed APIWho this book is forThis book is for data scientists, machine learning researchers, and deep learning practitioners looking to implement advanced deep learning paradigms using PyTorch 1.x. Working knowledge of deep learning with Python programming is required.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781789616408},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10162713},}@INPROCEEDINGS{9319109,
  author={Gruosso, Monica and Capece, Nicola and Erra, Ugo and Angiolillo, Francesco},
  booktitle={2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={A Preliminary Investigation into a Deep Learning Implementation for Hand Tracking on Mobile Devices}, 
  year={2020},
  volume={},
  number={},
  pages={380-385},
  abstract={Hand tracking is an essential component of computer graphics and human-computer interaction applications. The use of RGB camera without specific hardware and sensors (e.g., depth cameras) allows developing solutions for a plethora of devices and platforms. Although various methods were proposed, hand tracking from a single RGB camera is still a challenging research area due to occlusions, complex backgrounds, and various hand poses and gestures. We present a mobile application for 2D hand tracking from RGB images captured by the smartphone camera. The images are processed by a deep neural network, modified specifically to tackle this task and run on mobile devices, looking for a compromise between performance and computational time. Network output is used to show a 2D skeleton on the user's hand. We tested our system on several scenarios, showing an interactive hand tracking level and achieving promising results in the case of variable brightness and backgrounds and small occlusions.},
  keywords={Two dimensional displays;Cameras;Three-dimensional displays;Neural networks;Heating systems;Computational modeling;Performance evaluation;Deep Learning;Human-Computer Interaction;Image Processing;Hand Tracking},
  doi={10.1109/AIVR50618.2020.00079},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10189724,
  author={Fu, Hanbo and Zhou, Zhengqing and Zeng, Zeng and Sang, Tong and Zhu, Yaxin and Zheng, Xiaoyan},
  booktitle={2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)}, 
  title={Surface Defect Detection Based on ResNet Classification Network with GAN Optimized}, 
  year={2022},
  volume={},
  number={},
  pages={1568-1575},
  abstract={Surface defect detection plays an important role in industrial domains. The defect failed to report and the defect missed to report will have a serious impact on the whole industrial production process, such as production stagnation. Traditional defect detection methods are based on image processing or shallow machine learning techniques, but these can only detect defects under specific inspection conditions. And the related methods of artificial intelligence still have some flaws and challenges, including small data sets, poor model accuracy, over-fitting generalization, and others. In this study, we offer a novel method for surface defect detection based on GAN optimized ResNet classification training on data sets, which creatively integrates the two networks. In specifically, the ResNet network training model is used together with the generative adversarial network (GAN) for defect detection and classification. The GAN is used to first supplement the original data, which is limited in quantity and category. Then the data and two models are organically integrated. We find that this GAN-optimized strategy outperforms direct classification using the ResNet network alone after conducting several 200-epoch runs on a variety of big defect pictures.},
  keywords={Training;Production;Machine learning;Network architecture;Inspection;Generative adversarial networks;Data models;Defect Detection;GAN;ResNet;Deep Learning},
  doi={10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00225},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9792726,
  author={V V, Nagendra Kumar and D, Rajeswari},
  booktitle={2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Analyzing Prognosis Methods using Machine Learning Algorithms for Detecting COPD}, 
  year={2022},
  volume={},
  number={},
  pages={595-602},
  abstract={Chronic Obstructive Pulmonary Disorder (COPD) is a significant challenge encountered by healthcare professionals. With millions of people being affected by COPD every year, there is a need for better diagnosis conditions. Many of the earlier studies on the facets of the machine learning model's application to COPD have discussed the success in terms of accuracy, the optimal classifiers resulting in success, etc. This study, with the objective of a contemporary literature review, has focused on the earlier contributions of critical models of machine learning for COPD detection. However, based on the information collated from the models, while some classifiers are resourceful in training the system and improved accuracy is a need to address the feature selection issues. Some of the non-technical and purely medical-related studies reviewed in this study have impressed the facts about diagnosis state as false positives or false negatives (misdiagnosis) of COPD conditions. Both conditions are challenging for offering the right treatment to the patients. Thus, considering the existing patterns and the dynamics, this study has discussed three significant gaps in feature selection, the need for multiple layers of diagnosis, and the lifestyle conditions based on COPD detection models as a prospective domain for future research.},
  keywords={Training;Machine learning algorithms;Bibliographies;Lung;Machine learning;Medical services;Organizations;Chronic Obstructive Pulmonary Disorder;Prognosis;Machine Learning;Computer-Aided Diagnosis},
  doi={10.1109/ICAAIC53929.2022.9792726},
  ISSN={},
  month={May},}@ARTICLE{10494809,
  author={Yang, Handuo and Huyan, Ju and Ma, Tao and Song, Yitao and Han, Chengjia},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Novel Applicable Shadow Resistant Neural Network Model for High-Efficiency Grid-Level Pavement Crack Detection}, 
  year={2024},
  volume={5},
  number={9},
  pages={4535-4549},
  abstract={To address two key challenges—limited grid-level detection capability and difficulty in detecting pavement cracks in complex environments, this study proposes a novel neural network model called CrackcellNet. This innovative model incorporates an output structure that enables end-to-end grid recognition and a module that enhances shadow image data to enhance crack detection. The model relies on the design of consecutive pooling layers to achieve adaptive target size grid output. By utilizing image fusion techniques, it enhances the quantity of shadow data in road surface detection. The results of ablation experiments indicate that the optimal configuration for CrackcellNet includes V-block and shadow augmentation operations, dilation rates of 1 or 2, and a convolutional layer in the CBA module. Through extensive experimentation, we have demonstrated that our model achieved an accuracy rate of 94.5% for grid-level crack detection and a F1 value of 0.839. Furthermore, practical engineering validation confirms the model's efficacy with an average PCIe of 0.045, providing valuable guidance for road maintenance decisions.},
  keywords={Roads;Cameras;Lighting;Surface cracks;Computational modeling;Convolution;Complexity theory;Computer vision;deep convolutional neural networks (DCNN);grid level detection;pavement crack detection;shadow augmentation},
  doi={10.1109/TAI.2024.3386149},
  ISSN={2691-4581},
  month={Sep.},}@ARTICLE{10313058,
  author={Nguyen, Hung and Chang, J. Morris},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Synthetic Information Toward Maximum Posterior Ratio for Deep Learning on Imbalanced Data}, 
  year={2024},
  volume={5},
  number={6},
  pages={2790-2804},
  abstract={This work explores how class-imbalanced data affect deep learning and propose a data balancing technique for mitigation by generating more synthetic data for the minority class. In contrast to random-based oversampling techniques, our approach prioritizes balancing the most informative region by finding high entropy samples. This approach is opportunistic and challenging because well-placed synthetic data points can boost machine learning algorithms’ accuracy and efficiency, whereas poorly placed ones can cause a higher misclassification rate. In this study, we present an algorithm for maximizing the probability of generating a synthetic sample in the correct region of its class by placing it toward maximizing the class posterior ratio. In addition, to preserve data topology, synthetic data are closely generated within each minority sample neighborhood. Overall, experimental results on forty-one datasets show that our technique significantly outperforms experimental methods in terms of boosting deep-learning performance.},
  keywords={Deep learning;Machine learning algorithms;Synthetic data;Data models;Topology;Entropy;Training;Data imbalance;deep learning;high entropy samples;maximum posterior ratio},
  doi={10.1109/TAI.2023.3330949},
  ISSN={2691-4581},
  month={June},}@INPROCEEDINGS{10465446,
  author={Ngom, Charles Abdoulaye and Ba, Mandicou and Sarr, Simon Antoine and Diop, Idy and Bah, Alassane and Diao, Maboury},
  booktitle={2023 First International Conference on the Advancements of Artificial Intelligence in African Context (AAIAC)}, 
  title={Data Generation Strategies for Enhanced Atrial Fibrillation Prediction in Pacemaker Patients}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Atrial Fibrillation (AF) is a significant concern for patients with pacemakers, as it poses a heightened risk of cardiovascular complications, including strokes and limb ischemia. Accurate prediction of AF in this vulnerable population is crucial for early intervention and prevention of these serious outcomes. However, the scarcity of digitized and labeled electrocardiogram (ECG) data has presented a challenge for Deep Learning based prediction models. In this paper, we undertake a comprehensive exploration of innovative data generation strategies specifically tailored for pacemaker patients’ ECG data. Our study meticulously investigates conditional generation and standard augmentation techniques, systematically augmenting the dataset. Notably, the conditional DeepFake generation emerges as a standout technique, significantly improving data distribution, as evidenced by MMD = 8.10−3 and MMD = 4.10−4. This augmentation approach yields data with acceptable FD, RMSE, and PRD scores when compared to a baseline. These compelling findings offer valuable insights into the enhancement of AF prediction in pacemaker wearers, underscoring the pivotal role of data augmentation in advancing cardiac risk assessment.},
  keywords={Deep learning;Deepfakes;Pacemakers;Atrial fibrillation;Electrocardiography;Predictive models;Data augmentation;Deep Learning;Atrial Fibrillation;Pacemaker Patients;ECG Data Augmentation;Conditional DeepFake Generation},
  doi={10.1109/AAIAC60008.2023.10465446},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10551232,
  author={Li, Biao and Sun, Guang and Sheng, Haoyu and Liu, Junjun},
  booktitle={2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Application of Deep Learning Neural Networks in Sports Shoe Brand Classification Task}, 
  year={2023},
  volume={},
  number={},
  pages={152-158},
  abstract={In recent years, deep learning has gained widespread attention and importance due to its outstanding performance in image feature learning. The rapid development of machine learning and deep learning has provided a powerful tool for handling complex classification problems and has achieved significant results. The development of neural network algorithms has also benefited from the progress in deep learning research. Deep learning is a machine learning method based on multi-layer neural network structures, which improves the modeling capability for complex data and tasks by increasing the depth and complexity of the network. For the task of classifying shoe brands, deep learning can help neural network models better understand features such as appearance, texture, and brand logos of shoes, as well as identify other everyday items to assist in their classification. After identifying the items, using different optimizers can improve the accuracy of classification, enabling more effective classification decisions.},
  keywords={Deep learning;Representation learning;Machine learning algorithms;Footwear;Data models;Complexity theory;Classification algorithms;deep learning;convolutional neural networks;image classification;image recognition;optimizer},
  doi={10.1109/ICCBD-AI62252.2023.00034},
  ISSN={},
  month={Dec},}@INBOOK{10785838,
  author={Campesato, Oswald},
  booktitle={Artificial Intelligence, Machine Learning, and Deep Learning}, 
  title={Chapter 4: Deep Learning Introduction}, 
  year={2020},
  volume={},
  number={},
  pages={95-126},
  abstract={},
  keywords={Predictive models;Deep learning;Neural networks;Codes;Training;Testing;Logic arrays;Fitting;Convolutional neural networks;Training data},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9781683924661},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10785838},}@INPROCEEDINGS{10064895,
  author={Ramkumar, N. and Renuka, D. Karthika and Ashok Kumar, L.},
  booktitle={2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)}, 
  title={An Approach on BCI based Silent Speech Interface for Automatic Speech Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Paralysis is the loss of muscle function in a part of the body. It occurs when the transmission of messages between brain and muscles is disrupted. Approximately 5.4 million people have some form of paralysis. . Depending on the site of the damage, paralysis may be accompanied by a lack of sensation. The majority of paralysis is caused by strokes or spinal cord injury. The Brain computer interface (BCI) is an interface between the Brain and the computer. It is an emerging field wherein it’s a fast-growing emerging technology, in which researchers acquire data in the form for signals by using various sensors. It uses artificially produced electrical signals to stimulate the brain, transfer sensory information to the brain, or restore sensory function. Motor speech dysfunction paralyzes people and prevents them from speaking. Since paralyzed people are unable to speak or move, it is very difficult to fulfill the basic requirements. One way to approach the problem of voice recognition using brain waves is to quantify the brain signals from an individual human being. By examining the EEG signals from an individual person, and to investigating the feasibility of detecting syllable level units.},
  keywords={Text recognition;Machine learning;Muscles;Brain modeling;Speech;Electroencephalography;Brain-computer interfaces;Brain Computing Interface;EEG;Voice Recognition;Motor Speech dysfunction},
  doi={10.1109/AIST55798.2022.10064895},
  ISSN={},
  month={Dec},}@INBOOK{10952693,
  author={Patel, Hrishitva and Raman, Ramakrishnan and Jawarneh, Malik and Ansari, Arshiya S. and Pallathadka, Hriakumar and Sanchez, Domenic T.},
  booktitle={Conversational Artificial Intelligence}, 
  title={Machine Learning for Automatic Speech Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={147-168},
  abstract={Summary <p>It is very important to be able to use speech signals correctly so that you can share your feelings, thoughts, and information about the real world, as well as for your day&#x2010;to&#x2010;day work. Speech is the most natural way for people to talk to each other verbally. With the help of speech recognition technology, computers can now understand human languages and respond to spoken commands. At the heart of the field of voice recognition is the development of methods and platforms for analyzing spoken and natural language. It is put into the computer these days so that the right processing and rearranging can happen. Over the past 10 years, automatic speech recognizers have gotten much better at what they do. Most studies have looked at the many technologies that are now available to help people who have trouble speaking. Research shows that voice recognition applications for people who have trouble speaking are well within the capabilities of modern technology. However, there is not a lot of work that looks at the human side of things, which is the main thing holding back progress in this field. Concerns have been raised about a number of things that have to do with people. Speech recognition has the most potential to change the lives of people who have trouble talking. This chapter presents a machine learning&#x2010;based framework for automatic speech recognition. Speech acquisition, speech preprocessing, feature extraction, and classification are the main components of the proposed framework.</p>},
  keywords={Speech recognition;Vocabulary;Accuracy;Automatic speech recognition;Feature extraction;Noise;Neurons;Natural languages;Mel frequency cepstral coefficient;Hidden Markov models},
  doi={10.1002/9781394200801.ch10},
  ISSN={},
  publisher={Wiley},
  isbn={9781394200795},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952693},}
