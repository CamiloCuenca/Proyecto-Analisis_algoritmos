@INPROCEEDINGS{11016608,
  author={González Pérez, Maria Magdalena and Figarola, Alfredo Figarola and Villegas, Ernesto Reyes},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Leveraging Emerging Digital Technologies in Climate Change Education: A National-Level Case Study}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The urgent challenge of climate change demands innovative educational strategies that not only disseminate scientific knowledge but also foster critical thinking and active engagement, particularly on large groups that need to be taught online. This paper presents the redesign and implementation of the online course Causes, Consequences, and Solutions to Climate Change, delivered to 137 students across diverse regions of Mexico. Leveraging advanced technological tools such as generative AI and immersive metaverse experiences, the course aimed to enhance student engagement, learning outcomes, and environmental awareness. The curriculum was structured around core components of the climate system, including the atmosphere, biosphere, hydrosphere, and the anthropogenic drivers of climate change. Guided by principles of active learning and project-based education, the redesign emphasized interactive, technologyenhanced learning experiences. One of the highlights was a metaverse-based group project, where students collaborated to propose solutions for mitigating climate change impacts in their local regions. This activity encouraged teamwork, creativity, and the practical application of scientific knowledge to real-world environmental challenges. To evaluate the impact of this course, pre- and post-course assessments were conducted to measure prior knowledge of the students about climate change and their familiarity with the innovative technological tools. Initial results revealed a lack of understanding in both areas. However, postcourse evaluations showed a 20% improvement in knowledge of climate science and technological platforms. These findings highlight the effectiveness of integrating advanced digital tools into environmental education.},
  keywords={Climate change;Technological innovation;Climatology;Metaverse;Generative AI;Biosphere;Teamwork;Engineering education;Creativity;Meteorology;Innovative teaching;climate change education;emerging technologies;AI;online learning;student engagement;higher education;educational innovation},
  doi={10.1109/EDUCON62633.2025.11016608},
  ISSN={2165-9567},
  month={April},}@ARTICLE{11074348,
  author={Cheng, Xiang and Liu, Boxun and Liu, Xuanyu and Liu, Ensong and Huang, Ziwei},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration}, 
  year={2025},
  volume={},
  number={},
  pages={1-21},
  abstract={To support future intelligent multifunctional sixthgeneration (6G) wireless communication networks, Synesthesia of Machines (SoM) is proposed as a novel paradigm for artificial intelligence (AI)-native intelligent multi-modal sensingcommunication integration. However, existing SoM system designs rely on task-specific AI models and face challenges such as scarcity of massive high-quality datasets, constrained modeling capability, poor generalization, and limited universality. Recently, foundation models (FMs) have emerged as a new deep learning paradigm and have been preliminarily applied to SoM-related tasks, but a systematic design framework is still lacking. In this paper, we for the first time present a systematic categorization of FMs for SoM system design, dividing them into generalpurpose FMs, specifically large language models (LLMs), and SoM domain-specific FMs, referred to as wireless foundation models. Furthermore, we derive key characteristics of FMs in addressing existing challenges in SoM systems and propose two corresponding roadmaps, i.e., LLM-based and wireless foundation model-based design. For each roadmap, we provide a framework containing key design steps as a guiding pipeline and several representative case studies of FM-empowered SoM system design. Specifically, we propose LLM-based path loss generation (LLM4PG) and scatterer generation (LLM4SG) schemes, and wireless channel foundation model (WiCo) for SoM mechanism exploration, LLM-based wireless multi-task SoM transceiver (LLM4WM) and wireless foundation model (WiFo) for SoMenhanced transceiver design, and wireless cooperative perception foundation model (WiPo) for SoM-enhanced cooperative perception, demonstrating the significant superiority of FMs over taskspecific models. Finally, we summarize and highlight potential directions for future research.},
  keywords={Sensors;Frequency modulation;Wireless communication;Wireless sensor networks;Foundation models;Transceivers;Radio frequency;Training;System analysis and design;Point cloud compression;Intelligent multi-modal sensing-communication integration;Synesthesia of Machines (SoM);foundation models (FMs);large language models (LLMs);wireless foundation models},
  doi={10.1109/TNSE.2025.3587238},
  ISSN={2327-4697},
  month={},}@INBOOK{10296185,
  author={Munir, Arslan and Kong, Joonho and Qureshi, Mahmood Azhar},
  booktitle={Accelerators for Convolutional Neural Networks}, 
  title={Introduction}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  abstract={Deep neural networks (DNNs) have enabled the deployment of artificial intelligence (AI) in many modern applications including autonomous driving, image recognition, and speech processing. Convolutional neural networks (CNNs) are a type of DNNs, which are most commonly used for computer vision tasks. Invention of CNNs has revolutionized the field of computer vision and has enabled many applications of computer vision to go mainstream. CNNs have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, object detection, activity recognition, natural language processing, brain&#x2013;computer interfaces, and financial time&#x2010;series prediction. This chapter discusses history of DNNs with emphasis on domains that are particularly revolutionized by DNNs, such as speech processing, autonomous driving, medical, and security. The chapter then elaborates some of the pitfalls of high&#x2010;accuracy DNNs/CNNs focusing on compute and energy bottlenecks, and the effect of sparsity of high&#x2010;accuracy models on throughput and hardware utilization.},
  keywords={Training;Image edge detection;Computer vision;Biomedical imaging;Artificial intelligence;Recurrent neural networks;Performance evaluation},
  doi={10.1002/9781394171910.ch1},
  ISSN={},
  publisher={IEEE},
  isbn={9781394171897},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10296185},}@INBOOK{10953008,
  author={Singh, Shiv},
  booktitle={Marketing with AI For Dummies}, 
  title={Making Connections: Machine Learning and Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={77-92},
  abstract={Summary <p>People probably overuse the terms machine learning and neural networks when they talk about the fundamentals of artificial intelligence (AI). This chapter presents neural networks as one method of accomplishing machine learning &#x2014; by using human brain&#x2010;inspired algorithms to recognize patterns in data. Understanding machine learning and neural networks is crucial for marketers. These technologies enable personalized marketing, predictive analytics, and automation, which enhance marketing efficiency and customer insights. They help in optimizing content, detecting fraud, and improving customer support. In unsupervised learning, the algorithm in the AI model receives data without explicit instructions on what the data represents or what to do with it. In the vast, dynamic cityscape of machine learning, reinforcement learning is a type of machine learning where an AI agent learns to make decisions by receiving rewards or penalties for its actions. Convolutional neural networks with their specialized, layered architecture, revolutionized machine vision.</p>},
  keywords={Machine learning;Biological neural networks;Machine learning algorithms;Data models;Urban areas;Training;Computational modeling;Training data;Supervised learning;Overfitting},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394237210},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953008},}@ARTICLE{10859278,
  author={Yang, Yongjie and Chen, Tao and Shangguan, Longfei},
  journal={IEEE Pervasive Computing}, 
  title={Toward Next-Generation Human–Computer Interface Based on Earables}, 
  year={2024},
  volume={23},
  number={4},
  pages={50-52},
  abstract={We introduce a new type of human–computer interface—mobile acoustic field (MAF). MAF explores a variety of onboard sensors, such as speaker transducers, inertial measurement unit (IMU), and feedforward and feedback microphones, on earphones/earbuds to capture a wide range of acoustic activities both inside-out and outside-in. We anticipate that the exponential advancement of generative AI, powered by the sheer size of cloud computing resources, will enable personal agents to sense, understand, and further interact with this MAF, offering many exciting mobile services to users. In this article, we summarize our preliminary results, discuss the downstream applications, technical challenges, and our vision on this exciting research field.},
  keywords={Pervasive computing;Cloud computing;Transducers;Measurement units;Generative AI;Next generation networking;Microphones;Human computer interaction;Mobile computing;Biosensors;Sensor systems;Inertial sensors;Headphones;Acoustic devices;Feedforward systems;Feedback;Artificial intelligence;Cloud computing},
  doi={10.1109/MPRV.2024.3500112},
  ISSN={1558-2590},
  month={Oct},}@INPROCEEDINGS{10702283,
  author={Li, Qiang and Zhao, Feng and Ouyang, Hong and Xiang, Hui and Zheng, Jianning and Zhao, Linlin and Zhang, Hongbing and Guo, Ting},
  booktitle={2024 20th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
  title={Interactive Emotional Learning in Emotional Intelligent Dialogue Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={With the continuous advancement of artificial intelligence in the field of conversational systems, users have raised higher demands for the emotional expression capabilities of dialogue agents. To address this issue, we proposed an Interactional Emotional Learning model (TIEL) tailored for task-oriented dialogue, aimed at enhancing the emotional prediction accuracy of dialogue agents and addressing the issue of missing target text. The TIEL model achieved a deep understanding of multi-turn dialogue contexts and accurate prediction of emotional categories through three modules: knowledge-enhanced dialogue context encoding, latent emotional response learning, and interactive emotional prediction. Experimental results on the DailyDialog dataset demonstrated a significant performance improvement of the TIEL model in emotion prediction tasks.},
  keywords={Emotion recognition;Accuracy;Computational modeling;Predictive models;Knowledge discovery;Encoding;Emotional responses;Artificial intelligence;Context modeling;Fuzzy systems;Task-oriented Dialogue;Emotional Prediction;Knowledge Enhancement;Emotional Intelligent Dialogue Systems},
  doi={10.1109/ICNC-FSKD64080.2024.10702283},
  ISSN={},
  month={July},}@INBOOK{10950640,
  author={Ammanath, Beena},
  booktitle={Trustworthy AI: A Business Guide for Navigating Trust and Ethics in AI}, 
  title={Secure}, 
  year={2022},
  volume={},
  number={},
  pages={93-109},
  abstract={Summary <p>Artificial intelligence (AI) security is one element fueling the field of adversarial machine learning (AML). As it relates to security, AML attempts to penetrate, compromise, or corrupt a system so as to learn how to remedy vulnerabilities. This chapter looks more closely at some of the attack vectors bad actors might use to compromise AI systems. To understand the threat landscape, it considers the attack paths and how unsecure AI can yield negative outcomes for the enterprise. These attack paths include: data poisoning, transfer learning attack, reverse engineering the code, and exploiting system errors. Across every industry, as more aspects of any enterprise use and become reliant on AI, they will likely become increasingly attractive targets for cybercriminals. Some of the reasons bad actors may target a system include: data exposure, loss of intellectual property, bypassing filters, liability and regulatory fines, and user trust in AI.</p>},
  keywords={Artificial intelligence;Security;Unsolicited e-mail;Training;Data models;Companies;Automated machine learning;Vectors;Training data;Taxonomy},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119867968},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950640},}@INPROCEEDINGS{11131927,
  author={Bernetic, J. and Katic, M. Asenbrener and Candrlic, S.},
  booktitle={2025 MIPRO 48th ICT and Electronics Convention}, 
  title={Developing a Child-Friendly Chatbot Using GPT and OpenAI API: A Case Study}, 
  year={2025},
  volume={},
  number={},
  pages={2145-2150},
  abstract={The development of modern technology has led to significant advancements in various aspects of society, including the field of artificial intelligence (AI). One of the key breakthroughs in this area is the development of large language models (LLMs), based on massive amounts of textual data. These models enable computers to comprehend and generate human language with an exceptional level of coherence and accuracy. Examples of popular language models include OpenAI's ChatGPT systems, which have found wide-ranging applications across industries-from automating customer support to creating creative content. Their ability to generate contextually appropriate responses makes them suitable for diverse scenarios. This paper provides a detailed description of the process of creating a chatbot based on the GPT-3.5 model. It outlines the steps involved in building the chatbot and connecting the model via OpenAI's API, using prompt engineering techniques. The outcome is a customized chatbot capable of generating responses suitable for children, with a special emphasis on comprehensibility and an adjusted linguistic style.},
  keywords={Ethics;Large language models;Computational modeling;Education;Oral communication;User interfaces;Linguistics;Chatbots;Prompt engineering;Protection;chatbot;gpt;child friendly;LLM},
  doi={10.1109/MIPRO65660.2025.11131927},
  ISSN={1847-3938},
  month={June},}@INPROCEEDINGS{10835964,
  author={Zhang, Zurui},
  booktitle={2024 International Conference on Power, Electrical Engineering, Electronics and Control (PEEEC)}, 
  title={Research and Application of Image Recognition and Automatic Translation Technology in English Translation Software}, 
  year={2024},
  volume={},
  number={},
  pages={998-1002},
  abstract={With the rapid development of artificial intelligence technology, image recognition and automatic translation technology are more and more widely used in English translation software. This paper aims to explore how the combination of these two technologies can improve translation efficiency and accuracy. First, the basic concept of image recognition and its main applications in translation software are outlined, including real-time image translation and optical character recognition (OCR) technology. Second, it analyzes the current status, advantages and challenges of the current automatic translation technology, and explores the potential opportunities brought by its deep integration with image recognition. Finally, it looks forward to the future development direction of this field, and believes that technological innovation and user experience optimization will be the key factors to promote the continuous progress of English translation software. Through this study, it is expected to provide reference and reference for academic research and practical application in related fields.},
  keywords={Technological innovation;Translation;Image recognition;Accuracy;Optical character recognition;User experience;Real-time systems;Character recognition;Artificial intelligence;Optimization;Image recognition;automatic translation;English translation software;optical character recognition (OCR);real-time translation},
  doi={10.1109/PEEEC63877.2024.00185},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10650761,
  author={Nimmy, Sonia Farhana and Kamal, Md Sarwar and Hussain, Omar Khadeer and Chakrab, Ripon},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Interpretability in Mapping Weeds and Crops from Drone Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Agriculture and food production constantly struggle with tracking the growth of crops and controlling weeds. Weeds take away important resources like water, nutrients, and sunlight from crops. This can cause a big decrease in how many crops grow if the weeds are not taken care of properly. Modern agriculture is increasingly using artificial intelligence (AI) based systems to effectively monitor and manage weeds. Mapping weeds with remote sensing or image processing helps effectively measure their impact. Extensive research indicates that machine learning or AI is effective in assessing the impact of weeds and quantifying their presence within crops. However, the internal decision-making processes of some AI approaches are complex, making them difficult to understand even for AI experts. In agriculture, it’s essential that these decision-making approaches are easy to understand for people who are not familiar with AI. In this research, we propose an interpretable method for identifying crops and weeds from images captured by unmanned aerial vehicles (UAVs), or drones. First, we used U-net segmentation on UAV datasets to filter out noise in the images. U-net is effective in extracting detailed local information, like textures, and learning the connections between pixels in an image. The filtered images are then processed by Vision Transformers (ViT), which extract both local and global contextual information about weeds and crops from them. This information aids in measuring the quantity of weeds in the fields. Finaly we apply Explainable AI (XAI) approaches layer-wise relevance propagation (LRP) and pixel density analysis (PDA). These techniques demonstrate the step-by-step decision-making process in measuring the amount of weeds from the images.},
  keywords={Accuracy;Explainable AI;Decision making;Crops;Autonomous aerial vehicles;Transformers;Information filters;UAV images;Vision Transformer;dynamic obstacles;explainable-AI;LRP;PDA},
  doi={10.1109/IJCNN60899.2024.10650761},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11070410,
  author={Pradhan, Rajeshwari and Devi, Monika and Kaur, Inderdeep and Mogha, Harsh and Singh, Shivpratap},
  booktitle={2025 4th OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 5.0}, 
  title={Deepfake Forensics: Detection and Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The term "deepfake" describes the production of incredibly lifelike and frequently misleading media, including photos or videos, that change material using sophisticated algorithms and artificial intelligence. Because deep fake technology can be used maliciously for things like disseminating false information, assuming identities, or damaging reputations, it poses serious risks to people’s and organisations’ confidence, security, and privacy. Consequently, there is a need for efficient and successful techniques as well as instruments to identify and lessen the effects of deepfake media. We offer software for the deep fake analyser and detector in this study. This method uses GenAI, natural language processing and cutting-edge machine learning algorithms to detect and reveal deepfake media. We exhibit its excellent robustness, speed, and precision. We also talk about the limitations and possible uses of our system, along with the moral and societal implications of deepfake technology. This software is a useful tool to protect against the malicious use of modified content, especially as deepfake risks keep growing.},
  keywords={Deepfakes;Technological innovation;Privacy;Machine learning algorithms;Production;Machine learning;Software;Natural language processing;Robustness;Security;Deepfake technology;Deepfake detection;Machine learning;GenAI;Privacy-preserving;Security;Media manipulation;NLP},
  doi={10.1109/OTCON65728.2025.11070410},
  ISSN={},
  month={April},}@INPROCEEDINGS{10270778,
  author={Pokale, Sairaj and Taware, Karan and Fernandes, Gavin and Kangane, Sakshi and Bhosale, Parth and Bewoor, Laxmi},
  booktitle={2023 3rd Asian Conference on Innovation in Technology (ASIANCON)}, 
  title={Text Summarization: GPT Perspective}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The field of artificial intelligence (AI) has witnessed significant advancements in recent years, particularly in the areas of machine learning (ML), deep learning (DL), and transformers. These technologies have revolutionized various industries, including healthcare, finance, and transportation. This study provides an extensive examination of GPT-based models, especially GPT-4, in their ability to effectively condense text. We investigate the architectural elements, training approaches, and multiple strategies used by the model to generate high-quality summaries. Our analysis offers valuable insights into the strengths and possible enhancements for GPT-based models in text summarization tasks.},
  keywords={Training;Measurement;Industries;Analytical models;Technological innovation;Transportation;Finance;Transformers;GPT;Summarization},
  doi={10.1109/ASIANCON58793.2023.10270778},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11046271,
  author={Spilski, Jan and Satchuk-Patricio, Nathalia and Zylowski, Thorsten and Hekele, Felix and Hettmann, Wladimir and Wölfel, Matthias and Lachmann, Thomas and Henning, Peter},
  booktitle={2025 13th International Conference on Information and Education Technology (ICIET)}, 
  title={Evaluating the General Technology Acceptance Model (GTAM) in the Education Context: an Analysis of AI-Based Chatbot Acceptance Among Schoolchildren}, 
  year={2025},
  volume={},
  number={},
  pages={31-38},
  abstract={The rapid development of AI technologies, in particular generative AI chatbots like ChatGPT, is changing education, especially in the areas of teaching, learning, and career planning. As these technologies become more widespread, questions of acceptance among schoolchildren are becoming increasingly important. To address this issue, the generic General Technology Acceptance Model (GTAM) was evaluated to measure the acceptance of AI-based technologies in education. The GTAM combines elements of the Technology Acceptance Model (TAM), the Theory of Planned Behavior (TPB) and the Hierarchical Attitude Models (HAM) to assess both general attitudes and specific beliefs regarding AI. In two studies, the model showed good validity, reliability and predictive quality, with an explained variance of 52.22 % in study 1 and 58.83 % in study 2 for usage intention. The results suggest that GTAM is a practical and costeffective method for evaluating the acceptance of AI technologies in educational contexts and provides developers with a reliable approach for efficiently evaluating AI applications.},
  keywords={Technology acceptance model;Engineering profession;Generative AI;Education;Predictive models;Reliability theory;Chatbots;Planning;chatbots;career assistant;schoolchildren;general technology acceptance model;education context},
  doi={10.1109/ICIET66371.2025.11046271},
  ISSN={},
  month={April},}@ARTICLE{10972178,
  author={Guo, Shuaishuai and Zhao, Jinqiu and Ye, Jia and Zhang, Peng and Bai, Zhiquan},
  journal={IEEE Wireless Communications}, 
  title={Reconfigurable Intelligent Surface Empowered Simultaneous Communication, Localization, and Mapping for Vertical Applications}, 
  year={2025},
  volume={32},
  number={5},
  pages={134-141},
  abstract={As 6G communication continues to advance rapidly, it is opening up new frontiers in diverse domains such as smart cities, autonomous transportation, and industrial automation. A key enabler of these advancements is the ability to perform simultaneous communication, localization, and mapping (SCLAM) tasks. Reconfigurable intelligent surfaces (RIS) have emerged as a transformative technology to enhance communication, localization, and mapping capabilities, thereby improving system efficiency and reliability. However, the full potential of RIS in enabling these simultaneous capabilities has yet to be thoroughly explored. This article proposes innovative approaches to address real-world challenges across vertical industries, including smart cities, autonomous vehicles, and industrial automation, by integrating RIS with SCLAM systems. We discuss the anticipated benefits and technical challenges of this integration, particularly the tradeoffs between communication efficiency and the Cramér-Rae bound (CRB) for parameter estimation. Additionally, we highlight the crucial role of artificial intelligence (AI) and machine learning in achieving these goals and offer clear directions for future research in RIS-powered SCLAM systems.},
  keywords={Simultaneous localization and mapping;Location awareness;Real-time systems;Reconfigurable intelligent surfaces;Smart cities;Heuristic algorithms;Reliability;Accuracy;Transceivers;Machine learning},
  doi={10.1109/MCOM.002.2400480},
  ISSN={1558-0687},
  month={October},}@INPROCEEDINGS{10829340,
  author={Kamble, Asmita and Chaudhari, Chetan},
  booktitle={2024 International Conference on Intelligent Systems and Advanced Applications (ICISAA)}, 
  title={Beyond Faces: A Conceptual Framework for Responsible use of Deepfake Algorithm in Advertising.}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Deepfake is a type of Artificial intelligence in which content is created using deep learning which is not real. Nowadays most of the marketers getting benefits from Deepfake advertisements as it makes highly personalised content for customers and viewers. But there are some blurred line between ethical and unethical content making by using this technology. This study introducing a conceptual framework for responsible use of Deepfake algorithm in advertising for marketer. With the consideration of proposed conceptual framework this study makes argument for balancing three spheres (i.e. Government, Business and Customers). And it is hoped that this study will provide a good base for future research and “New desire” for marketer.},
  keywords={Deep learning;Deepfakes;Ethics;Government;Advertising;Intelligent systems;Faces;Business;Deepfake;advertising;government;business;customers},
  doi={10.1109/ICISAA62385.2024.10829340},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11013422,
  author={Al Shafi, Abdullah and Muntakim, Abdul and Shill, Pintu Chandra and Zannat, Rowzatul and Al-Amin, Abdullah},
  booktitle={2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)}, 
  title={Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Skin cancer can be identified through dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), leveraging annotated skin images and Convolutional Neural Networks (CNNs), enhances diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. Three benchmark datasets—HAM10000, ISIC 2016, and ISIC 2019—were used in this research. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32%, 90.86%, and 93.92% for the three datasets. The system’s performance was evaluated using established skin lesion detection metrics, yielding impressive results.},
  keywords={Image segmentation;Accuracy;Filtering;Transfer learning;Reliability engineering;Skin;Lesions;Convolutional neural networks;Artificial intelligence;Skin cancer;Convolution Neural Networks(CNNs);Transfer Learning;Deep Neural Network;Skin Lesion Classification;Segmentation;Filtering;Ensemble Network},
  doi={10.1109/ECCE64574.2025.11013422},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{8725767,
  author={Zolotukhin, Mikhail and Hämäläinen, Timo},
  booktitle={2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
  title={On Artificial Intelligent Malware Tolerant Networking for IoT}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={With the recent progress in development of low-budget sensors and machine-to-machine communication, the Internet of Things (IoT) has attracted considerable attention. Unfortunately, many of today's IoT devices are rushed to market with little consideration for basic security and privacy protection making them easy targets for various attacks. As a result, number of malware and their variants designed for IoT devices has been constantly increasing. Traditional intrusion detection approaches are unsuitable for IoT networks due to limited computational capacity of smart devices and diversity in their technology. In this paper, we propose a defense system for IoT networks based on software-defined networking and network function virtualization. The defense system core component is a reinforcement machine learning agent that evaluates risks of potential attack and takes the most optimal action in order to mitigate it.},
  keywords={Reinforcement learning;Malware;Intrusion detection;Neural networks;Conferences;Network security;intrusion detection;IoT;SDN;NFV;machine learning;reinforcement learning},
  doi={10.1109/NFV-SDN.2018.8725767},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8836918,
  author={Thieling, Jörn and Elspas, Philip and Roßmann, Jürgen},
  booktitle={2019 IEEE International Systems Conference (SysCon)}, 
  title={Neural Networks for End-to-End Refinement of Simulated Sensor Data for Automotive Applications}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={The rising use of Artificial Intelligence (AI) for Advanced Driver Assistance Systems (ADAS) and Autonomous Vehicles (AVs) comes with the need of comprehensive tests, verification and validation. This is hardly achievable in real test drives alone, but validation with simulated sensors in Virtual Testbeds (VTBs) becomes a popular supplement. To reduce the gap between simulation and reality, Digital Twins of real sensors need to generate data as realistic as possible. Instead of classical methods such as rasterization or ray tracing, a novel approach based on neural networks is developed and evaluated. Based on the concept of Generative Adversarial Networks (GANs) a classification network is trained to distinguish real from simulated images. At the same time the classifier is used as critic to improve a generation network that refines simulated sensor images to look more realistic. This contribution gives an overview of recent research in image-to-image translation with GANs and suggests a framework to generate more realistic sensor images for-but not limited to-automotive applications. State of the art image-to-image translation architectures are evaluated and several methods are suggested to deal with drawbacks and shortcomings. An evaluation metric according to the subjective assessment of a more realistic color distribution in the refined sensor images is introduced. Finally, the potential of the novel approach to be used in VTBs is analyzed and discussed.},
  keywords={Gallium nitride;Training;Ray tracing;Cameras;Decoding;Neural networks;Automotive applications},
  doi={10.1109/SYSCON.2019.8836918},
  ISSN={2472-9647},
  month={April},}@ARTICLE{10472503,
  author={Choi, Wonyoung and Nam, Gi Pyo and Cho, Junghyun and Kim, Ig-Jae and Ko, Hyeong-Seok},
  journal={IEEE Access}, 
  title={Integrating Pretrained Encoders for Generalized Face Frontalization}, 
  year={2024},
  volume={12},
  number={},
  pages={43530-43539},
  abstract={In the field of face frontalization, the model obtained by training on a particular dataset often underperforms on other datasets. This paper presents the Pre-trained Feature Transformation GAN (PFT-GAN), which is designed to fully utilize diverse facial feature information available from pre-trained face recognition networks. For that purpose, we propose the use of the feature attention transformation (FAT) module that effectively transfers the low-level facial features to the facial generator. On the other hand, in the hope of reducing the pre-trained encoder dependency, we attempt a new FAT module organization that accommodates the features from all pre-trained face recognition networks employed. This paper attempts evaluating the proposed work using the “independent critic” as well as “dependent critic”, which enables objective judgments. Experimental results show that the proposed method significantly improves the face frontalization performance and helps overcome the bias associated with each pre-trained face recognition network employed.},
  keywords={Face recognition;Feature extraction;Generative adversarial networks;Decoding;Training;Generators;Pose estimation;Generative adversarial networks;Encoding;Face frontalization;face pose normalization;face recognition;generative modeling},
  doi={10.1109/ACCESS.2024.3377220},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10506885,
  author={Wang, Yijin and Liang, Jiajun},
  booktitle={2023 Asia Conference on Cognitive Engineering and Intelligent Interaction (CEII)}, 
  title={A Novel Chess Interaction System Based on Visual Recognition and Generative Models: Virtual Gestures, Intelligent Assistant, and Multimodal Feedback}, 
  year={2023},
  volume={},
  number={},
  pages={20-23},
  abstract={With the rapid development of computer vision algorithms and artificial intelligence generative models, interaction methods have become more diverse. This paper proposes a novel international chess interaction system that integrates virtual gesture recognition, large language model (LLM) intelligent assistant, and image-text generation technology to upgrade the traditional chess game experience in three aspects: interaction, decision-making, and feedback layers. We conducted a user survey on the prototype experience system and analyzed it in terms of intelligence and interactivity. In terms of interactivity, our user survey results showed that multimodal generation significantly improves interaction, while virtual interaction technology still needs optimization in usability. In terms of intelligence, the LLM intelligent assistant demonstrated certain usability and potential for higher win-rate improvement.},
  keywords={Surveys;Visualization;Decision making;Prototypes;Games;Gesture recognition;Market research;Gesture Interaction;LLM Intelligent Assistant;Multimodal Generation;Chessboard System},
  doi={10.1109/CEII60565.2023.00012},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10527530,
  author={Gahlot, Alok Singh and Vyas, Ruchi},
  booktitle={2023 IEEE International Conference on Paradigm Shift in Information Technologies with Innovative Applications in Global Scenario (ICPSITIAGS)}, 
  title={Game Theory and Deep Learning Approach to Implement Scrabble}, 
  year={2023},
  volume={},
  number={},
  pages={207-211},
  abstract={Integrating deep learning into the Scrabble board game introduces a level of intricacy and sophistication to the overall gaming experience. This study delves into the application of deep learning methodologies in developing a Scrabble game. Traditionally, Scrabble games have relied on rule-based systems or rudimentary algorithms. However, this research takes a groundbreaking approach by integrating deep learning to elevate the game's intelligence and adaptability. The central focus revolves around employing neural network architectures to discern and comprehend patterns within Scrabble board layouts and word placements. The study explores the potential of utilizing a convolutional neural network, specifically the U-Net architecture, to scrutinize the game board. This involves the model learning intricate relationships between letter distributions, vacant spaces, and valid word formations. The deep learning model undergoes training using a diverse dataset that encompasses various Scrabble board states, each annotated with corresponding valid letter distributions. The goal is to empower the model to predict optimal letter placements on the board while considering the inherent constraints and rules of the Scrabble game.},
  keywords={Deep learning;Training;Adaptation models;Neural networks;Layout;Games;Predictive models;Scrabble;Deep Learning;Convolutional neural network;UNet architecture},
  doi={10.1109/ICPSITIAGS59213.2023.10527530},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10378344,
  author={Shi, Liushuai and Wang, Le and Zhou, Sanping and Hua, Gang},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Trajectory Unified Transformer for Pedestrian Trajectory Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={9641-9650},
  abstract={Pedestrian trajectory prediction is an essential link to understanding human behavior. Recent work achieves state-of-the-art performance gained from hand-designed post-processing, e.g., clustering. However, this post-processing suffers from expensive inference time and neglects the probability that the predicted trajectory disturbs downstream safety decisions. In this paper, we present Trajectory Unified TRansformer, called TUTR, which unifies the trajectory prediction components, social interaction, and multimodal trajectory prediction, into a transformer encoder-decoder architecture to effectively remove the need for post-processing. Specifically, TUTR parses the relationships across various motion modes using an explicit global prediction and an implicit mode-level transformer encoder. Then, TUTR attends to the social interactions with neighbors by a social-level transformer decoder. Finally, a dual prediction forecasts diverse trajectories and corresponding probabilities in parallel without post-processing. TUTR achieves state-of-the-art accuracy performance and improvements in inference speed of about 10× - 40× compared to previous well-tuned state-of-the-art methods using post-processing.},
  keywords={Computer vision;Pedestrians;Clustering algorithms;Transformers;Prediction algorithms;Data structures;Inference algorithms},
  doi={10.1109/ICCV51070.2023.00887},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9583233,
  author={Luo, Mandi and Ma, Xin and Li, Zhihang and Cao, Jie and He, Ran},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Partial NIR-VIS Heterogeneous Face Recognition With Automatic Saliency Search}, 
  year={2021},
  volume={16},
  number={},
  pages={5003-5017},
  abstract={Near-infrared-visual (NIR-VIS) heterogeneous face recognition (HFR) aims to match NIR face images with the corresponding VIS ones. It is a challenging task due to the sensing gaps among different modalities. Occlusions in the input face images make the task extremely complex. To tackle these problems, we present a Saliency Search Network (SSN) to extract domain-invariant identity features. We propose to automatically search the efficient parts of face images in a modality-aware manner, and remove redundant information. Moreover, the searching process is guided by an information bottleneck network, which mitigates the overfitting problems caused by small datasets. Extensive experiments on both complete and partial NIR-VIS HFR on multiple datasets demonstrate the effectiveness and robustness of the proposed method to modality discrepancy and occlusions.},
  keywords={Face recognition;Task analysis;Visualization;Image recognition;Lighting;Feature extraction;Training data;Heterogeneous face recognition;near infrared-visible matching;information bottleneck;neural architecture search},
  doi={10.1109/TIFS.2021.3122072},
  ISSN={1556-6021},
  month={},}@ARTICLE{10716673,
  author={Pu, Yan and Gong, Maoguo and Liu, Tongfei and Zhang, Mingyang and Gao, Tianqi and Jiang, Fenlong and Hu, Xiaobo},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Adversarial Feature Equilibrium Network for Multimodal Change Detection in Heterogeneous Remote Sensing Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-17},
  abstract={Change detection (CD) methods have been crucial in exploring geo-environmental science. With the advancement of remote sensing (RS) technology, multimodal images acquired from different platforms and sensors are widely used for CD tasks. As an emerging task, multimodal CD (MCD) aims to achieve more comprehensive and precise detection of land cover changes through complementary information in multimodal images. However, there are significant differences between modalities, particularly in heterogeneous images. How to deal with modal differences while effectively integrating change information remains a challenge in MCD. In this article, we propose a novel adversarial feature equilibrium network (AFENet), which establishes an additional adversarial optimization to solve the equilibrium problem between modal differences and land cover changes. Our AFENet aligns the features and reduces the modal gap through a multiscale adversarial domain adaptation (MADA) approach. Meanwhile, a divergence-aware contrastive module (DCM) is designed as a regularization term for adversarial optimization. DCM affects the sensitivity of feature extractors by constraining the mutual information between changed and unchanged pixels. In this case, AFENet can maintain the consistency of feature representation while maximizing the discriminability of change targets. The features extracted from AFENet will then be integrated by our multistream feature fusion (MFF) module and utilized to generate change maps. The effectiveness of our approach is demonstrated on two scene-level multimodal RS datasets. Compared with existing methods, our AFENet achieves state-of-the-art (SOTA) performance on both datasets and outperforms the second-best  $F1$  score by 4.64% and 1.1%, respectively.},
  keywords={Feature extraction;Land surface;Data mining;Accuracy;Deep learning;Standards;Sensitivity;Semantics;Optimization;Optical sensors;Change detection (CD);domain adaptation (DA);heterogeneous image;multimodal;remote sensing (RS)},
  doi={10.1109/TGRS.2024.3480091},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10450377,
  author={Wang, SiYu and Hu, ChangHui},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Residual-Unet Generative Flow Model for Low-Light Image Enhancement}, 
  year={2023},
  volume={},
  number={},
  pages={2046-2051},
  abstract={Low-light image enhancement (LLIE) task based on deep learning learns the mapping from low-light image to normal exposure image by using supervised or unsupervised training. In the majority of current methods, the utilization of Convolutional Neural Networks (CNNs) substantially constrains the network's capacity to assimilate comprehensive global information. Simultaneously, the reconstruction loss or adversarial loss commonly employed tends to inadequately gauge the visual disparity between predictions and targets, resulting in the emergence of blurred regions in the enhanced results. In this paper, we therefore propose a Residual-Unet Generative Flow Model for Low-light Image Enhancement (RUGF), which consist of a feature extraction module and a flow learning based reconstruction module. We first propose a residual-based Unet encoder during deep feature encoding in the feature extraction module, which can compensate for the shortcomings in global information processing and has advantages in processing depth information. In addition, our network adopts a negative logarithmic likelihood loss function, which can better constrain the model to learn the complex distribution of the reference image, thereby avoiding the local area blurring that exists in existing methods. We conducted extensive testing on two benchmark datasets, and the actual results confirmed the superiority and effectiveness of our proposed method.},
  keywords={Training;Deep learning;Visualization;Neural networks;Information processing;Feature extraction;Encoding;Low-light image enhancement(LLIE);Residual;Unet;Conditional Normalizing Flow(CNF)},
  doi={10.1109/CAC59555.2023.10450377},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10512514,
  author={Zhu, Zixuan and Xia, Xinqian and Wang, Zikai and Chen, Junjie and Guo, Zelin and Li, Yiyan},
  booktitle={2023 IEEE 7th Conference on Energy Internet and Energy System Integration (EI2)}, 
  title={Exploring GAN and Their Derivatives in Power Data Generation: A Review}, 
  year={2023},
  volume={},
  number={},
  pages={2039-2044},
  abstract={In the continuously evolving and increasingly complex environment of power systems, utilizing Generative Adversarial Networks (GAN) to process and optimize multidimensional electric power data has grad-ually become a hot research topic. This paper explores the applications and challenges of utilizing GAN for large-scale electric power data processing, highlighting their ability to generate synthetic data amidst challenges like cost, equipment limitations, and data privacy in the power domain. While GAN showcase significant potential in enhancing time-series and image data sam-ples, they exhibit drawbacks, such as complex training and uncontrollable results. Therefore, various improved models, such as Conditional GAN, Wasserstein GAN, Wasserstein GAN-gradient penalty, etc. have been derived to address these issues, and are elaborated upon in the latter part of the review. Ultimately, this paper summarizes the current application challenges and future research directions of GAN in electric power systems.},
  keywords={Training;Electric potential;Data privacy;Costs;Reviews;Energy Internet;System integration;Generative Adversarial Networks;Electric Power Systems;Data Generation},
  doi={10.1109/EI259745.2023.10512514},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10981078,
  author={Xie, Yifan and Wang, Jingge and Feng, Tao and Ma, Fei and Li, Yang},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={CCIS-DIFF: A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Colonoscopy is crucial for identifying adenomatous polyps and preventing colorectal cancer. However, developing robust models for polyp detection is challenging by the limited size and accessibility of existing colonoscopy datasets. While previous efforts have attempted to synthesize colonoscopy images, current methods suffer from instability and insufficient data diversity. Moreover, these approaches lack precise control over the generation process, resulting in images that fail to meet clinical quality standards. To address these challenges, we propose CCIS-DIFF, a Controlled generative model for high-quality Colonoscopy Image Synthesis based on a Diffusion architecture. Our method offers precise control over both the spatial attributes (polyp location and shape) and clinical characteristics of polyps that align with clinical descriptions. Specifically, we introduce a blur mask weighting strategy to seamlessly blend synthesized polyps with the colonic mucosa, and a text-aware attention mechanism to guide the generated images to reflect clinical characteristics. Notably, to achieve this, we construct a new multi-modal colonoscopy dataset that integrates images, mask annotations, and corresponding clinical text descriptions. Experimental results demonstrate that our method generates high-quality, diverse colonoscopy images with fine control over both spatial constraints and clinical consistency, offering valuable support for downstream segmentation and diagnostic tasks.},
  keywords={Image segmentation;Attention mechanisms;Image synthesis;Shape;Large language models;Colonoscopy;Process control;Colorectal cancer;Standards;Biomedical imaging;Colonoscopy Image Synthesis;Controlled Synthesis;Stable Diffusion},
  doi={10.1109/ISBI60581.2025.10981078},
  ISSN={1945-8452},
  month={April},}@ARTICLE{11045936,
  author={Cheng, Lei and Huang, Xiaowen and Sang, Jitao and Yu, Jian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Towards Robust Recommendation: A Review and an Adversarial Robustness Evaluation Library}, 
  year={2025},
  volume={37},
  number={9},
  pages={5679-5698},
  abstract={Recently, recommender system has achieved significant success. However, due to the openness of recommender systems, they remain vulnerable to malicious attacks. Additionally, natural noise in training data and issues such as data sparsity can also degrade the performance of recommender systems. Therefore, enhancing the robustness of recommender systems has become an increasingly important research topic. In this survey, we provide a comprehensive overview of the robustness of recommender systems. Based on our investigation, we categorize the robustness of recommender systems into adversarial robustness and non-adversarial robustness. In the adversarial robustness, we introduce the fundamental principles and classical methods of recommender system adversarial attacks and defenses. In the non-adversarial robustness, we analyze non-adversarial robustness from the perspectives of data sparsity, natural noise, and data imbalance. Additionally, we summarize commonly used datasets and evaluation metrics for evaluating the robustness of recommender systems. Finally, we also discuss the current challenges in the field of recommender system robustness and potential future research directions. Additionally, to facilitate fair and efficient evaluation of attack and defense methods in adversarial robustness, we propose an adversarial robustness evaluation library–ShillingREC, and we conduct evaluations of basic attack models and recommendation models.},
  keywords={Recommender systems;Robustness;Reviews;Predictive models;Data models;Prediction algorithms;Measurement;Taxonomy;Noise;Libraries;Recommender systems;adversarial robustness;no-adversarial robustness;adversarial robustness evaluation library},
  doi={10.1109/TKDE.2025.3581553},
  ISSN={1558-2191},
  month={Sep.},}@INPROCEEDINGS{10649924,
  author={Luo, Yuling and Li, Yuanze and Zhang, Shunsheng and Liu, Junxiu and Qin, Sheng},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A watermark-based framework to actively protect deep neural networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Deep Neural Networks (DNNs) have made significant progress in the field of artificial intelligence, and machine learning as a service has become a service that can be profitable. Intellectual property protection for DNNs has become an important issue. These DNNs models may be misappropriated by third-party, thus causing financial losses to the model owners. In the existing white-box and black-box approaches, protected models cannot be actively verified for ownership, and they can only be verified by sending a specific sample after the model has been stolen. In this work, a black-box DNNs watermark protection framework is proposed for protecting intellectual property of image classification models, which embeds watermarks in DNNs models. The proposed framework is applicable to models for image classification, and only images containing secret watermarks can be correctly recognized by the model. Besides, the proposed framework can correctly identify the ownership of the model, resist pruning operations of the model, and resist malicious tampering of the input image. Experimental results show that the watermarking framework is robust and effective.},
  keywords={Text categorization;Closed box;Watermarking;Resists;Intellectual property;Artificial neural networks;Protection;Deep Neural Networks;Intellectual Property;Watermark Protection Framework;Watermarked Image},
  doi={10.1109/IJCNN60899.2024.10649924},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10498465,
  author={Rao, K Rama and Swapna Kumari, Manthena and Eklarker, Ravindra and Shaker Reddy, Pundru Chandra and Muley, Kirti and Burugari, Vijay Kumar},
  booktitle={2024 International Conference on Integrated Circuits and Communication Systems (ICICACS)}, 
  title={An Adaptive Deep Learning Framework for Prediction of Agricultural Yield}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Agriculture and its large yields are vital to human survival. Recently, machine learning and deep learning have been applied to determine agricultural output. This study estimated apple production utilizing a unique deep-learning(DL)-based technique. A study-designed convolutional neural network (CNN) classified golden and royal gala apple photos. Classifying CNN-extracted pictures with traditional machine-learning(ML) methods was a hybrid application. Datasets A is used to evaluate the performance of the proposed model. Based on Dataset A accuracy findings, the CNN-SVM hybrid model performed best. CNN-SVM and CNN-Gradient Boosting hybrid models performed best and accurately for Dataset A. The more balanced distribution of train, test, and validation sizes, the suggested hybrid CNN model, and the model's assessment made dataset C the best choice. The hybrid model was 99.70% accurate for Dataset A. Precision, recall, f-measure, and Cohen kappa were 99%. The study found that hybrid models could accurately predict apple fruit yield using golden and royal gala photos.},
  keywords={Deep learning;Adaptation models;Communication systems;Production;Predictive models;Prediction algorithms;Generative adversarial networks;Yield-Prediction;DL;Ensemble-Methods;ML;Smart-farms},
  doi={10.1109/ICICACS60521.2024.10498465},
  ISSN={},
  month={Feb},}@ARTICLE{11165334,
  author={Lijin, P. and Nair, Madhu S.},
  journal={IEEE Access}, 
  title={A Dual-Path Self-Supervised Framework for Polyp Segmentation Using Vision Transformers and CNNs}, 
  year={2025},
  volume={13},
  number={},
  pages={162191-162203},
  abstract={In the field of medical image analysis, accurate polyp segmentation remains a significant challenge due to the limited availability of labeled training data. This paper introduces a novel approach that addresses this issue by leveraging a self-supervised learning framework combined with deep supervision and a dual path architecture of Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs). Our proposed model consists of two paths: Path 1 employs Vision Transformer blocks, with the initial stage utilizing a pre-trained backbone, while subsequent stages further refine these features. Path 2 incorporates self-supervised learning via the Barlow Twins method to capture global context, followed by ResNet blocks and attention-augmented convolution blocks to extract and enhance local features. The outputs from both paths are fused using a cross-attention module, concatenated, and processed through multiple MLP heads. To enhance the segmentation accuracy, deep supervision is applied at multiple decoder stages, utilizing skip connections from the MobileNetV2 backbone. Additionally, we leverage a modified Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic polyp images for Barlow Twins pre-training. Experimental results show that our approach outperforms existing methods in polyp segmentation, achieving higher accuracy and robustness in challenging scenarios. Specifically, our model achieves an average Dice score of 94% and an average Intersection over Union (IoU) of 89%, significantly surpassing state-of-the-art methods.},
  keywords={Transformers;Image segmentation;Computer vision;Feature extraction;Computer architecture;Self-supervised learning;Convolutional neural networks;Accuracy;Medical diagnostic imaging;Data models;Barlow twins;convolutional neural networks (CNN);deep convolutional generative adversarial network;polyp segmentation;self-supervised learning},
  doi={10.1109/ACCESS.2025.3610453},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10930112,
  author={Kim, Kyumin and Lee, Hanyong and Lee, Jeasung},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={GoodGPT: Counseling-Chat}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={In this study, we introduced GoodGPT, a korean generative neural network system designed to enhance counseling interactions by combining retrieval-augmented generation with advanced safeguard mechanisms. By leveraging a database of real counseling records and employing Dense Passage Retrieval, GoodGPT effectively retrieves contextually relevant information to inform its generative processes. This approach not only improves the relevance and appropriateness of the generated responses but also aligns them with professional counseling fields.},
  keywords={Employee welfare;Databases;Neural networks;Retrieval augmented generation;Mental health;Information retrieval;Safety;Consumer electronics;neural network;generative model;information retrieval;mental health;safety},
  doi={10.1109/ICCE63647.2025.10930112},
  ISSN={2158-4001},
  month={Jan},}@ARTICLE{8633862,
  author={Lee, Hong Joo and Kim, Seong Tae and Lee, Hakmin and Ro, Yong Man},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Lightweight and Effective Facial Landmark Detection using Adversarial Learning with Face Geometric Map Generative Network}, 
  year={2020},
  volume={30},
  number={3},
  pages={771-780},
  abstract={Facial landmark detection plays an important role in face analysis tasks. Moreover, it is used as a prerequisite in many facial related applications, the simplicity, as well as effectiveness, is essential in the facial landmark detection. In this paper, we propose an effective facial landmark detection network and an associated learning framework with the geometric prior-generative adversarial network. The geometric prior-generative adversarial network consists of one generator and two discriminators. The generator consists of an encoder and two decoders. The encoder predicts facial landmark points. The decoders generate a facial inner and contour geometric map from predicted landmark points. Generating face geometric maps from predicted landmark points helps the predicted landmark points to represent the face geometric information, including shape and configuration. The discriminators determine that the given geometric maps are generated from actual landmark points or estimated landmark points. Our proposed network is end-to-end trainable, and only the encoder part is used simply as the facial landmark detector in the testing stage. To verify the effectiveness of the proposed method, we have conducted comprehensive experiments with benchmark data sets. The results have shown that the proposed method achieves comparable performances over recently proposed facial landmark detection methods with a simple and effective facial landmark detection network.},
  keywords={Face;Generators;Shape;Geometry;Task analysis;Decoding;Gallium nitride;Facial landmark detection;geometric prior-generative adversarial network;face geometric map},
  doi={10.1109/TCSVT.2019.2897243},
  ISSN={1558-2205},
  month={March},}@ARTICLE{10937235,
  author={Ma, Jun and Ruotsalo, Tuukka},
  journal={IEEE Transactions on Human-Machine Systems}, 
  title={Brain-Supervised Conditional Generative Modeling}, 
  year={2025},
  volume={55},
  number={3},
  pages={383-393},
  abstract={Present machine learning approaches to steer generative models rely on the availability of manual human input. We propose an alternative approach to supervising generative machine learning models by directly detecting task-relevant information from brain responses. That is, requiring humans only to perceive stimulus and react to it naturally. Brain responses of participants (N=30) were recorded via electroencephalography (EEG) while they perceived artificially generated images of faces and were instructed to look for a particular semantic feature, such as “smile” or “young”. A supervised adversarial autoencoder was trained to disentangle semantic image features by using EEG data as a supervision signal. The model was subsequently conditioned to generate images matching users' intentions without additional human input. The approach was evaluated in a validation study comparing brain-conditioned models to manually conditioned and randomly conditioned alternatives. Human assessors scored the saliency of images generated from different models according to the target visual features (e.g., which face image is more “smiling” or more “young”). The results show that brain-supervised models perform comparably to models trained with manually curated labels, without requiring any manual input from humans.},
  keywords={Brain modeling;Electroencephalography;Semantics;Visualization;Functional magnetic resonance imaging;Faces;Brain;Manuals;Image reconstruction;Training data;Electroencephalography (EEG);generative modeling;neuroimaging},
  doi={10.1109/THMS.2025.3537339},
  ISSN={2168-2305},
  month={June},}@ARTICLE{11002741,
  author={Tzelepakis, Anastasios and Dimitriou, Nikolaos and Charalampous, Paschalis and Tsongas, Konstantinos and Tzetzis, Dimitrios and Tzovaras, Dimitrios},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Generative Models Training Based on FEM Simulations Toward Improved Defect Detection}, 
  year={2025},
  volume={74},
  number={},
  pages={1-9},
  abstract={The defect prediction and quality control are critical in manufacturing, where convolutional neural networks (CNNs) demonstrate significant potential. However, traditional finite element method (FEM) simulations, despite their accuracy, are hindered by high computational demands. This article introduces a novel framework that integrates FEM simulations with generative CNNs to predict strain distributions during antenna manufacturing. By employing physics-informed dataset generation from FEM simulations, the proposed method trains a generative CNN to predict strain distributions during antenna manufacturing, enabling physically consistent strain predictions. Validated against FEM-calculated data, the framework demonstrates its efficacy in defect prevention, while addressing the limitations of traditional offline FEM capabilities. Furthermore, a comprehensive analysis of weight initialization and cost function choices, along with the experimental validation, highlights the method’s efficiency, establishing a cost effective and practical approach to integrate numerical simulations with CNN-based deep learning in manufacturing.},
  keywords={Finite element analysis;Antennas;Predictive models;Bending;Manufacturing;Strain;Solid modeling;Numerical models;Deformation;Convolutional neural networks;Defect prediction;finite element method (FEM);generative AI;physics-informed dataset generation;simulation},
  doi={10.1109/TIM.2025.3568966},
  ISSN={1557-9662},
  month={},}@ARTICLE{9527262,
  author={Tao, Yuechuan and Qiu, Jing and Lai, Shuying},
  journal={IEEE Transactions on Transportation Electrification}, 
  title={A Data-Driven Management Strategy of Electric Vehicles and Thermostatically Controlled Loads Based on Modified Generative Adversarial Network}, 
  year={2022},
  volume={8},
  number={1},
  pages={1430-1444},
  abstract={As part of the transportation system electrification and decarbonization, electric vehicles (EVs) have been experiencing a steady growth in some countries in recent years. However, the uncoordinated charging of EVs brings negative effects to the power grid. Therefore, a new intelligent energy management strategy needs to be involved in the transportation electrification progress to address the variability of flexible loads and energy storage in the active distribution network. In this article, we have proposed a coordinated dispatch strategy of EVs and thermostatically controlled loads (TCLs) based on a modified generative adversarial network (GAN). TCLs are utilized to complement the limits of EVs’ driving behaviors. EVs are modeled as battery energy storage systems (BESSs), and TCLs are modeled as virtual energy storage systems (VESSs). Machine learning is integrated into a bilevel optimization problem to determine the steady-state power dispatch and the energy storage control of the VESS. The proposed method is verified on the IEEE 33-bus system. Based on the simulation results, it can be concluded that the proposed data-driven method can outperform the conventional model-based method in terms of accuracy. Also, the modified GAN helps the training process to be less affected by the missing data. Comparative studies are conducted to show that the coordinated dispatch strategy of EVs and TCLs can maintain the voltage stability in the distribution system by compensating for the drawbacks of each other.},
  keywords={Generative adversarial networks;Transportation;Mathematical model;Load flow;Optimization;Load modeling;Demand response;Electric vehicles (EVs);energy management;machine learning;power dispatch;thermostatically controlled loads (TCLs)},
  doi={10.1109/TTE.2021.3109671},
  ISSN={2332-7782},
  month={March},}@ARTICLE{9558836,
  author={Chen, Jiawei and Zhang, Ziqi and Xie, Xinpeng and Li, Yuexiang and Xu, Tao and Ma, Kai and Zheng, Yefeng},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Beyond Mutual Information: Generative Adversarial Network for Domain Adaptation Using Information Bottleneck Constraint}, 
  year={2022},
  volume={41},
  number={3},
  pages={595-607},
  abstract={Medical images from multicentres often suffer from the domain shift problem, which makes the deep learning models trained on one domain usually fail to generalize well to another. One of the potential solutions for the problem is the generative adversarial network (GAN), which has the capacity to translate images between different domains. Nevertheless, the existing GAN-based approaches are prone to fail at preserving image-objects in image-to-image (I2I) translation, which reduces their practicality on domain adaptation tasks. In this regard, a novel GAN (namely IB-GAN) is proposed to preserve image-objects during cross-domain I2I adaptation. Specifically, we integrate the information bottleneck constraint into the typical cycle-consistency-based GAN to discard the superfluous information (e.g., domain information) and maintain the consistency of disentangled content features for image-object preservation. The proposed IB-GAN is evaluated on three tasks—polyp segmentation using colonoscopic images, the segmentation of optic disc and cup in fundus images and the whole heart segmentation using multi-modal volumes. We show that the proposed IB-GAN can generate realistic translated images and remarkably boost the generalization of widely used segmentation networks (e.g., U-Net).},
  keywords={Generative adversarial networks;Task analysis;Image segmentation;Adaptation models;Training;Biomedical imaging;Mutual information;Information bottleneck;image translation;domain adaptation},
  doi={10.1109/TMI.2021.3117996},
  ISSN={1558-254X},
  month={March},}@ARTICLE{4492763,
  author={Zhu, Shenghuo and Wang, Dingding and Yu, Kai and Li, Tao and Gong, Yihong},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Feature Selection for Gene Expression Using Model-Based Entropy}, 
  year={2010},
  volume={7},
  number={1},
  pages={25-36},
  abstract={Gene expression data usually contain a large number of genes but a small number of samples. Feature selection for gene expression data aims at finding a set of genes that best discriminate biological samples of different types. Using machine learning techniques, traditional gene selection based on empirical mutual information suffers the data sparseness issue due to the small number of samples. To overcome the sparseness issue, we propose a model-based approach to estimate the entropy of class variables on the model, instead of on the data themselves. Here, we use multivariate normal distributions to fit the data, because multivariate normal distributions have maximum entropy among all real-valued distributions with a specified mean and standard deviation and are widely used to approximate various distributions. Given that the data follow a multivariate normal distribution, since the conditional distribution of class variables given the selected features is a normal distribution, its entropy can be computed with the log-determinant of its covariance matrix. Because of the large number of genes, the computation of all possible log-determinants is not efficient. We propose several algorithms to largely reduce the computational cost. The experiments on seven gene data sets and the comparison with other five approaches show the accuracy of the multivariate Gaussian generative model for feature selection, and the efficiency of our algorithms.},
  keywords={Gene expression;Entropy;Biological system modeling;Gaussian distribution;Mutual information;Machine learning;Proteins;Sequences;DNA;Distributed computing;Feature selection;multivariate Gaussian generative model;entropy.;Feature extraction or construction;Data mining;Bioinformatics (genome or protein) databases},
  doi={10.1109/TCBB.2008.35},
  ISSN={1557-9964},
  month={Jan},}@INPROCEEDINGS{8950791,
  author={Wang, Ruoyu and Li, Guangyao and Chu, Dongsheng},
  booktitle={2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM)}, 
  title={Capsules Encoder and Capsgan for Image Inpainting}, 
  year={2019},
  volume={},
  number={},
  pages={325-328},
  abstract={Convolutional neural networks (CNN) have solved a lot of tasks in the field of computer vision because of its powerful processing capacity in recent years. Many different kinds of CNN are used to implement in the image restoration tasks and achieved outstanding results. Hinton propose a new network architecture with dynamic routing called Capsule networks which has shown remarkable results for image classification on MNIST data. Its advantage over CNN is that the networks can preserve more information from original image feature map with dynamic routing and compress the image into a vector, which CNN process the image as a scalar. We expand the capsule networks to the task of image restoration for the first time. Our idea is based on the structure of encoder-decoder by replacing convolutional neural networks with capsules networks in encoder aspect. Especially, we add Generative Adversarial Capsule Network (CapsuleGAN) in decoder to constraint parameters for expanding generative space. We evaluate our method on the ImageNet datasets and achieved acceptable results.},
  keywords={Computer vision;Computer architecture;Transforms;Network architecture;Routing;Vectors;Image restoration;Manufacturing;Convolutional neural networks;Image reconstruction;deep learning, capsule networks, computer vision, image inpainting},
  doi={10.1109/AIAM48774.2019.00071},
  ISSN={},
  month={Oct},}@ARTICLE{9559963,
  author={Hono, Yukiya and Takaki, Shinji and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
  journal={IEEE Access}, 
  title={PeriodNet: A Non-Autoregressive Raw Waveform Generative Model With a Structure Separating Periodic and Aperiodic Components}, 
  year={2021},
  volume={9},
  number={},
  pages={137599-137612},
  abstract={This paper presents PeriodNet, a non-autoregressive (non-AR) waveform generative model with a new model structure for modeling periodic and aperiodic components in speech waveforms. Non-AR raw waveform generative models have enabled the fast generation of high-quality waveforms. However, the variations of waveforms that these models can reconstruct are limited by training data. In addition, typical non-AR models reconstruct a speech waveform from a single Gaussian input despite the mixture of periodic and aperiodic signals in speech. These may significantly affect the waveform generation process in some applications such as singing voice synthesis systems, which require reproducing accurate pitch and natural sounds with less periodicity, including husky and breath sounds. PeriodNet uses a parallel or series model structure to model a speech waveform to tackle these problems. Two sub-generators connected in parallel or in series take an explicit periodic and aperiodic signal (sine wave and Gaussian noise) as an input. Since PeriodNet models periodic and aperiodic components by focusing on whether these input signals are autocorrelated or not, it does not require external periodic/aperiodic decomposition during training. Experimental results show that our proposed structure improves the naturalness of generated waveforms. We also show that speech waveforms with a pitch outside of the training data range can be generated with more naturalness.},
  keywords={Generators;Vocoders;Periodic structures;Training;Computational modeling;Acoustics;Training data;Generative adversarial network;neural vocoder;signal processing;singing voice synthesis;waveform generative model},
  doi={10.1109/ACCESS.2021.3118033},
  ISSN={2169-3536},
  month={},}@ARTICLE{10816095,
  author={Ayaz, Muhammad and Khan, Mustaqeem and Saqib, Muhammad and Khelifi, Adel and Sajjad, Muhammad and Elsaddik, Abdulmotaleb},
  journal={IEEE Consumer Electronics Magazine}, 
  title={MedVLM: Medical Vision–Language Model for Consumer Devices}, 
  year={2025},
  volume={14},
  number={5},
  pages={75-83},
  abstract={Generative artificial intelligence (GenAI) has enabled significant advancements in healthcare by supporting complex medical tasks through multimodal data processing. However, existing models often lack the adaptability required for diverse medical applications and are limited by their large size, hindering real-time deployment on consumer and edge devices. This article presents MedVLM, a novel vision–language model optimized for medical applications, such as visual question–answering (VQA) and medical report generation. MedVLM integrates the Florence-2 visual model with the LLaMA-2 language model using low-rank adaptation, reducing the number of trainable parameters to support efficient, real-time analysis across various imaging modalities, including X-rays, CT scans, and MRIs. Our evaluation includes extensive benchmarking against both specialized (Open-Flamingo, MedVInT, and Med-Flamingo) and generalist (Qwen-VL, PaLM-E) models, with results showing MedVLM’s superior performance in diagnostic accuracy and VQA tasks, achieving 0.51% accuracy on the RadVQA dataset. We also validate MedVLM’s outputs through collaboration with radiologists, who rated 74% of its generated medical reports as high quality. This work bridges the gap between GenAI advancements and practical radiological needs, providing a versatile tool that can streamline workflows and enhance diagnostic accuracy across various clinical settings.},
  keywords={Visualization;Computational modeling;Adaptation models;Medical diagnostic imaging;Accuracy;Training;Medical services;Data models;Consumer electronics;Computer architecture;Generative AI;Artificial intelligence},
  doi={10.1109/MCE.2024.3522521},
  ISSN={2162-2256},
  month={Sep.},}@ARTICLE{10643104,
  author={Zeng, Xiangyun and Tan, Siok Yee and Nasrudin, Mohammad Faidzul},
  journal={IEEE Access}, 
  title={Adapt-Net: A Unified Object Detection Framework for Mobile Augmented Reality}, 
  year={2024},
  volume={12},
  number={},
  pages={120788-120803},
  abstract={Object detection is a crucial task in mobile augmented reality (MAR), where achieving both speed and accuracy with limited computational resources is essential. However, applying object detection models to new domains or reducing the model size tends to lower their performance. To address this problem, this research introduced a unified object detection framework called Adapt-Net. This framework incorporates contrastive learning techniques for unsupervised domain adaptation, a teacher-student structure generative compressed model with masking, and deep mutual learning of student models, all built upon the YOLOv8 architecture. Adapt-Net’s key novelty lies in its unified framework that combines three models: two student models and one teacher model. Each model comprises a feature-extracting backbone and an adapter network. The student models backbone are trained using deep mutual learning and contrastive learning loss to ensure domain-invariant feature generation. Unsupervised domain adaptation and masked generative knowledge distillation modules facilitate knowledge transfer from the teacher to the student models, enhancing their ability to generalize to unfamiliar objects. The use of masked generative knowledge distillation, which guides the student models to reconstruct the teacher’s features from a masked input in a generative manner, rather than merely imitating the output. This generative approach improves the student models’ representation capabilities. Adapt-Net enables the student models to not only learn domain-invariant features but also enhance their generalization capabilities to new objects. Extensive experiments conducted on benchmark datasets demonstrate that our proposed approach surpasses state-of-the-art object detection methods by 6.8 mAP score in terms of detection accuracy on the Microsoft COCO dataset. Notably, the model size remains a compact 3.2M, enabling fast inference speeds, lower computational resource consumption, and enhanced resilience to domain variations. Adapt-Net represents a promising and efficient approach to object detection that combines accuracy with efficiency.},
  keywords={Adaptation models;Object detection;Computational modeling;Accuracy;Data models;Feature extraction;Training;Contrastive learning;Deep learning;Mutual information;Knowledge management;Augmented reality;Contrastive learning;deep mutual learning;masked generative knowledge distillation;mobile augmented reality;object detection;unsupervised domain adaptation},
  doi={10.1109/ACCESS.2024.3447043},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10470682,
  author={Sanida, Theodora and Sideris, Argyrios and Sanida, Maria Vasiliki and Dossis, Michael and Dasygenis, Minas},
  booktitle={2023 8th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM)}, 
  title={Acceleration of GANs for Potato Crop Disease Identification via FPGA}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Effective management of plant diseases is essential for ensuring food protection, as a wide range of diseases can significantly reduce crop yields. Potatoes, a staple crop worldwide, are particularly susceptible to various diseases. The timely identification and early warning systems are pivotal in curbing the proliferation of potato diseases and ultimately increasing crop productivity. In recent times, Generative Adversarial Networks (GANs) have emerged as revolutionary technology within the realm of artificial intelligence and machine learning. These networks provide a diverse dataset of synthetic images, thereby facilitating the development of robust computer vision algorithms capable of accurately discerning and categorizing various potato crop diseases. In the scope of our work, we have implemented and accelerated GAN on a Xilinx FPGA SoC. This platform has exceptional efficiency in handling such challenges in terms of power consumption and performance. Our method has also achieved superior power and performance efficiency compared to conventional GPU and CPU setups, boasting an impressive 0.015 ms average time per image in the FPGA's configuration.},
  keywords={Productivity;Plant diseases;Power demand;Machine learning algorithms;Social networking (online);Graphics processing units;Machine learning;Potato crop diseases;Identification;GANs;Hardware;FPGA;Generative Adversarial Networks},
  doi={10.1109/SEEDA-CECNSM61561.2023.10470682},
  ISSN={},
  month={Nov},}@ARTICLE{8598736,
  author={Mao, Wentao and Liu, Yamin and Ding, Ling and Li, Yuan},
  journal={IEEE Access}, 
  title={Imbalanced Fault Diagnosis of Rolling Bearing Based on Generative Adversarial Network: A Comparative Study}, 
  year={2019},
  volume={7},
  number={},
  pages={9515-9530},
  abstract={Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.},
  keywords={Gallium nitride;Fault diagnosis;Generative adversarial networks;Feature extraction;Training;Rolling bearings;Neural networks;Generative adversarial network;fault diagnosis;imbalanced fault;SDAE},
  doi={10.1109/ACCESS.2018.2890693},
  ISSN={2169-3536},
  month={},}@ARTICLE{9000871,
  author={Shaker, Abdelrahman M. and Tantawi, Manal and Shedeed, Howida A. and Tolba, Mohamed F.},
  journal={IEEE Access}, 
  title={Generalization of Convolutional Neural Networks for ECG Classification Using Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={35592-35605},
  abstract={Electrocardiograms (ECGs) play a vital role in the clinical diagnosis of heart diseases. An ECG record of the heart signal over time can be used to discover numerous arrhythmias. Our work is based on 15 different classes from the MIT-BIH arrhythmia dataset. But the MIT-BIH dataset is strongly imbalanced, which impairs the accuracy of deep learning models. We propose a novel data-augmentation technique using generative adversarial networks (GANs) to restore the balance of the dataset. Two deep learning approaches-an end-to-end approach and a two-stage hierarchical approach-based on deep convolutional neural networks (CNNs) are used to eliminate hand-engineering features by combining feature extraction, feature reduction, and classification into a single learning method. Results show that augmenting the original imbalanced dataset with generated heartbeats by using the proposed techniques more effectively improves the performance of ECG classification than using the same techniques trained only with the original dataset. Furthermore, we demonstrate that augmenting the heartbeats using GANs outperforms other common data augmentation techniques. Our experiments with these techniques achieved overall accuracy above 98.0%, precision above 90.0%, specificity above 97.4%, and sensitivity above 97.7% after the dataset had been balanced using GANs, results that outperform several other ECG classification methods.},
  keywords={Electrocardiography;Feature extraction;Heart beat;Heart rate variability;Gallium nitride;Machine learning;Generative adversarial networks;Class imbalance;convolution neural networks (CNNs);ECG classification;generative adversarial networks (GANs)},
  doi={10.1109/ACCESS.2020.2974712},
  ISSN={2169-3536},
  month={},}@ARTICLE{9090171,
  author={Dong, Fei and Zhang, Yu and Nie, Xiushan},
  journal={IEEE Access}, 
  title={Dual Discriminator Generative Adversarial Network for Video Anomaly Detection}, 
  year={2020},
  volume={8},
  number={},
  pages={88170-88176},
  abstract={Video anomaly detection is an essential task because of its numerous applications in various areas. Because of the rarity of abnormal events and the complicated characteristic of videos, video anomaly detection is challenging and has been studied for a long time. In this paper, we propose a semi-supervised approach with a dual discriminator-based generative adversarial network structure. Our method considers more motion information in video clips compared with previous approaches. Specifically, in the training phase, we predict future frames for normal events via a generator and attempt to force the predicted frames to be similar to their ground truths. In addition, we utilize both a frame discriminator and motion discriminator to adverse the generator to generate more realistic and consecutive frames. The frame discriminator attempts to determine whether the input frames are generated or original frames sampled from the normal video. The motion discriminator attempts to determine whether the given optical flows are real or fake. Fake optical flows are estimated from generated frames and adjacent frames, and real optical flows are estimated from the real frames sampled from original videos. Then, in the testing phase, we evaluate the quality of predicted frames to obtain the regular score, and we consider those frames with lower prediction qualities as abnormal frames. The results of experiments on three publicly available datasets demonstrate the effectiveness of our proposed method.},
  keywords={Anomaly detection;Generators;Training;Gallium nitride;Generative adversarial networks;Testing;Image reconstruction;Anomaly detection;generative adversarial network;dual discriminator},
  doi={10.1109/ACCESS.2020.2993373},
  ISSN={2169-3536},
  month={},}@ARTICLE{8941140,
  author={Iliyasu, Auwal Sani and Deng, Huifang},
  journal={IEEE Access}, 
  title={Semi-Supervised Encrypted Traffic Classification With Deep Convolutional Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={118-126},
  abstract={Network traffic classification serves as a building block for important tasks such as security and quality of service management. The field has been studied for a long time, with many techniques such as classical machine learning and deep learning methods currently available. However, the emergence of stronger encryption protocols has led to the rise of new challenges. One of the challenges is capturing and labeling a large amount of encrypted traffic data especially for training deep learning classifiers, as current techniques rely on deep packet inspection tools (DPI) which perform poorly on encrypted traffic. In this paper, we propose a semi-supervised learning approach using Deep Convolutional Generative Adversarial Network (DCGAN). The basic idea is to utilize the samples generated by DCGAN generators as well as unlabeled data to improve the performance of a classifier trained on a few labeled samples. Thus, alleviating the difficulties associated with large dataset collecting and labeling. To demonstrate the efficacy of our approach, we evaluated our model using a self-collected dataset of the recently established QUIC protocol as well as publicly available ISCX VPN-NonVPN dataset. Our approach is able to achieve 89% and 78% accuracy with a very small number of labeled samples (just 10% of the dataset) on both QUIC and ISCX VPN-NonVPN datasets respectively.},
  keywords={Generative adversarial networks;Deep learning;Generators;Payloads;Protocols;Encryption;Deep convolutional generative adversarial network;encrypted traffic classification;semi-supervised learning},
  doi={10.1109/ACCESS.2019.2962106},
  ISSN={2169-3536},
  month={},}@ARTICLE{9177091,
  author={Man, Rui and Yang, Ping and Xu, Bowen},
  journal={IEEE Access}, 
  title={Classification of Breast Cancer Histopathological Images Using Discriminative Patches Screened by Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={155362-155377},
  abstract={Computer-aided diagnosis (CAD) systems of breast cancer histopathological images automated classification can help reduce the manual observation workload of pathologists. In the classification of breast cancer histopathology images, due to the small number and high-resolution of the training samples, the patch-based image classification methods have become very necessary. However, adopting a patches-based classification method is very challenging, since the patch-level datasets extracted from whole slide images (WSIs) contain many mislabeled patches. Existing patch-based classification methods have paid little attention to addressing the mislabeled patches for improving the performance of classification. To solve this problem, we propose a novel approach, named DenseNet121-AnoGAN, for classifying breast histopathological images into benign and malignant classes. The proposed approach consists of two major parts: using an unsupervised anomaly detection with generative adversarial networks (AnoGAN) to screen mislabeled patches and using densely connected convolutional network (DenseNet) to extract multi-layered features of the discriminative patches. The performance of the proposed approach is evaluated on the publicly available BreaKHis dataset using 5-fold cross validation. The proposed DenseNet121-AnoGAN can be better suited to coarse-grained high-resolution images and achieved satisfactory classification performance in 40X and 100X images. The best accuracy of 99.13% and the best  $F1_{score}$  of 99.38% have been obtained at the image level for the 40X magnification factor. We have also investigated the performance of AnoGAN on the other classification networks, including AlexNet, VGG16, VGG19, and ResNet50. Our experiments show that whether it is at the patient-level accuracy or at the image-level accuracy, the classification networks with AnoGAN have provided better performance than the classification networks without AnoGAN.},
  keywords={Breast cancer;Feature extraction;Training;Generative adversarial networks;Machine learning;Breast cancer histopathological images;densely connected convolutional networks;discriminative patches;generative adversarial networks;image classification},
  doi={10.1109/ACCESS.2020.3019327},
  ISSN={2169-3536},
  month={},}@ARTICLE{9220113,
  author={Zhang, Hongliang and Wang, Rui and Pan, Ruilin and Pan, Haiyang},
  journal={IEEE Access}, 
  title={Imbalanced Fault Diagnosis of Rolling Bearing Using Enhanced Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={185950-185963},
  abstract={Machinery fault diagnosis tasks have been well addressed when sufficient and abundant data are available. However, the data imbalance problem widely exists in real-world scenarios, which leads to the performance deterioration of fault diagnosis markedly. To solve this problem, we present a novel imbalanced fault diagnosis method based on the enhanced generative adversarial networks (GAN). By artificially generating fake samples, the proposed method can mitigate the loss caused by the lack of real fault data. Specifically, in order to improve the quality of generated samples, a new discriminator is designed using spectrum normalization (SN) strategy and a two time-scale update rule (TTUR) method is used to stabilize the training process of GAN. Then, an enhanced Wasserstein GAN with gradient penalty is developed to generate high-quality synthetic samples for the fault samples set. Finally, a deep convolutional classifier is constructed to carry out fault classification. The performance and effectiveness of the proposed method are validated on the Case Western Reserve University bearing dataset and rolling bearing dataset acquired from our laboratory. The simulation results show that the proposed method has a superior performance than other methods for imbalanced fault diagnosis tasks.},
  keywords={Generative adversarial networks;Fault diagnosis;Training;Feature extraction;Generators;Rolling bearings;Convolution;Fault diagnosis;rolling bearing;generative adversarial networks;imbalanced data;convolutional neural networks},
  doi={10.1109/ACCESS.2020.3030058},
  ISSN={2169-3536},
  month={},}@ARTICLE{9131783,
  author={Chi, Jianning and Wu, Chengdong and Yu, Xiaosheng and Ji, Peng and Chu, Hao},
  journal={IEEE Access}, 
  title={Single Low-Dose CT Image Denoising Using a Generative Adversarial Network With Modified U-Net Generator and Multi-Level Discriminator}, 
  year={2020},
  volume={8},
  number={},
  pages={133470-133487},
  abstract={Low-dose CT (LDCT) images have been widely applied in the medical imaging field due to the potential risk of exposing patients to X-ray radiations. Given the fact that reducing the radiation dose may result in increased noise and artifacts, methods that can eliminate the noise and artifacts in the LDCT image have drawn increasing attentions and produced impressive results over the past decades. However, recent proposed methods mostly suffer from noise remaining, over-smoothing structures or false lesions derived from noise. In this paper, we propose a generative adversarial network (GAN) with novel architecture and loss function for restoring the LDCT image. Firstly, the inception-residual block and residual mapping are incorporated in the U-Net structure. The modified U-Net is applied as the generator of the GAN network so that the noise feature can be eliminated during the forward propagation. Secondly, a novel multi-level joint discriminator is designed by concatenating multiple convolutional neural networks (CNNs) where the output of each deconvolutional layer in the generator is compared with the corresponding down-sampled ground truth image. The adversarial training can be sensitive to noise and artifacts in different scales with this discriminator. Thirdly, we novely define a loss function consisting of the least square adversarial loss, VGG based perceptual loss, MSE based pixel loss and the noise loss, so that the differences in pixel, visual perception and noise distribution are comprehensively considered to optimize the network. Experimental results on both simulated and official simulated clinical images have demonstrated that the proposed method can provide superior performance to the state-of-the-art methods in noise removal, structure preservation and false lesions elimination.},
  keywords={Computed tomography;Generative adversarial networks;Gallium nitride;Generators;Image reconstruction;Training;Image denoising;Low-dose CT image denoising;deep learning;generative adversarial network;inception block;residual mapping;joint loss},
  doi={10.1109/ACCESS.2020.3006512},
  ISSN={2169-3536},
  month={},}@ARTICLE{9057608,
  author={Rong, Changle and Zhang, Xingming and Lin, Yubei},
  journal={IEEE Access}, 
  title={Feature-Improving Generative Adversarial Network for Face Frontalization}, 
  year={2020},
  volume={8},
  number={},
  pages={68842-68851},
  abstract={Face frontalization can boost the performance of face recognition methods and has made significant progress with the development of Generative Adversarial Networks (GANs). However, many GAN-based face frontalization methods still perform relatively weak on face recognition tasks under large face poses. In this paper, we propose Feature-Improving GAN (FI-GAN) for face frontalization, which aims to improve the recognition performance under large face poses. We assume that there is an inherent mapping between the frontal face and profile face, and their discrepancy in deep representation space can be estimated. The generation module of FI-GAN has a compact module named Feature-Mapping Block that helps to map the features of profile face images to the frontal space. Moreover, we produce a feature discriminator that can distinguish the features of profile face images from those of ground true frontal face images, which guide the generation module to provide high-quality features of profile faces. We conduct experiments on the MultiPIE, Labeled Faces in the Wild (LFW), and Celebrities in Frontal-Profile (CFP) databases. Our method is comparable to state-of-the-art methods under small poses and outperforms them on large pose face recognition.},
  keywords={Face;Face recognition;Generative adversarial networks;Task analysis;Databases;Training;Gallium nitride;Face frontalization;face recognition;generative adversarial network},
  doi={10.1109/ACCESS.2020.2986079},
  ISSN={2169-3536},
  month={},}@ARTICLE{10225306,
  author={Shahbazian, Reza and Greco, Sergio},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks Assist Missing Data Imputation: A Comprehensive Survey and Evaluation}, 
  year={2023},
  volume={11},
  number={},
  pages={88908-88928},
  abstract={Missing data imputation is a technique to deal with incomplete datasets. Since many models and algorithms cannot be applied to data containing missing values, a pre-processing step needs to be performed to remove incomplete data or to estimate the missing values. This is a well-known problem referred to as the data imputation problem. Several approaches have been designed for data imputation. These algorithms can be divided into two main categories: statistical and machine learning-based algorithms. As machine learning algorithms are optimized, they usually have better performance compared with statistical ones. In this paper, we review the most recent literature related to missing data imputation based on generative adversarial networks (GANs) that have gained tremendous attention in dealing with missing values. We examine the structures of GANs for missing data imputation and discuss the commonly used datasets and metrics for evaluation. We also cover the influence of the missing datatype, the effect of the missing data fraction, and the algorithm-related problems on data imputation performance. We conduct experiments on two publicly available datasets and evaluate the performance of GAIN, a missing data imputation algorithm to that of existing state-of-the-art approaches, demonstrating that the GAN-based algorithm outperforms the others in terms of RMSE and FID.},
  keywords={Machine learning algorithms;Surveys;Measurement;Generative adversarial networks;Machine learning;Clustering algorithms;Principal component analysis;Data acquisition;Information integrity;Generative adversarial networks;missing data;data imputation;semi-supervised learning;data cleaning},
  doi={10.1109/ACCESS.2023.3306721},
  ISSN={2169-3536},
  month={},}@ARTICLE{8585006,
  author={Liu, Jing and Sun, Wanning and Li, Mengjie},
  journal={IEEE Access}, 
  title={Recurrent Conditional Generative Adversarial Network for Image Deblurring}, 
  year={2019},
  volume={7},
  number={},
  pages={6186-6193},
  abstract={Nowadays, there is an increasing demand for images with high definition and fine textures, but images captured in natural scenes usually suffer from complicated blurry artifacts, caused mostly by object motion or camera shaking. Since these annoying artifacts greatly decrease image visual quality, deblurring algorithms have been proposed from various aspects. However, most energy-optimization-based algorithms rely heavily on blur kernel priors, and some learning-based methods either adopt pixel-wise loss function or ignore global structural information. Therefore, we propose an image deblurring algorithm based on a recurrent conditional generative adversarial network (RCGAN), in which the scale-recurrent generator extracts sequence spatio–temporal features and reconstructs sharp images in a coarse-to-fine scheme. To thoroughly evaluate the global and local generator performance, we further propose a receptive field recurrent discriminator. Besides, the discriminator takes blurry images as conditions, which helps to differentiate reconstructed images from real sharp ones. Last but not least, since the gradients are vanishing when training the generator with the output of the discriminator, a progressive loss function is proposed to enhance the gradients in back propagation and to take full advantage of discriminative features. Extensive experiments prove the superiority of RCGAN over state-of-the-art algorithms both qualitatively and quantitatively.},
  keywords={Image reconstruction;Generators;Feature extraction;Image edge detection;Image restoration;Generative adversarial networks;Kernel;Image deblurring;conditional generative adversarial network;receptive field recurrent;coarse-to-fine},
  doi={10.1109/ACCESS.2018.2888885},
  ISSN={2169-3536},
  month={},}@ARTICLE{8986554,
  author={Zhang, Ning and Wang, Yongcheng and Zhang, Xin and Xu, Dongdong and Wang, Xiaodong},
  journal={IEEE Access}, 
  title={An Unsupervised Remote Sensing Single-Image Super-Resolution Method Based on Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={29027-29039},
  abstract={Image super-resolution (SR) technique can improve the spatial resolution of images without upgrading the imaging system. As a result, SR promotes the development of high resolution (HR) remote sensing image applications. Many remote sensing image SR algorithms based on deep learning have been proposed recently, which can effectively improve the spatial resolution under the constraints of HR images. However, images acquired by remote sensing imaging devices typically have lower resolution. Hence, an insufficient number of HR remote sensing images are available for training deep neural networks. In view of this problem, we propose an unsupervised SR method that does not require HR remote sensing images. The proposed method introduces a generative adversarial network (GAN) that obtains SR images through the generator; then, the SR images are downsampled to train the discriminator with low resolution (LR) images. Our method outperformed several methods in terms of the quality of the obtained SR images as measured by 6 evaluation metrics, which proves the satisfactory performance of the proposed unsupervised method for improving the spatial resolution of remote sensing images.},
  keywords={Remote sensing;Image reconstruction;Generative adversarial networks;Training;Gallium nitride;Image super-resolution;unsupervised learning;remote sensing;generative adversarial network},
  doi={10.1109/ACCESS.2020.2972300},
  ISSN={2169-3536},
  month={},}@ARTICLE{9087855,
  author={Li, Fusheng and Lin, Dan and Yu, Tao},
  journal={IEEE Access}, 
  title={Improved Generative Adversarial Network-Based Super Resolution Reconstruction for Low-Frequency Measurement of Smart Grid}, 
  year={2020},
  volume={8},
  number={},
  pages={85257-85270},
  abstract={There is a universal trend toward a data-driven smart grid, which aims to realize two-way communication of energy flow and data flow between various agents across power generation side, transmission&distribution side, electricity retailors and end users. However, the low frequency electrical measurement data accumulated over a long period of time is insignificant for intelligent agents. This paper presents a machine learning method for reconstructing the low frequency electrical measurement data in smart grid. Firstly, the electrical measurement data is converted into electrical images, and then the low frequency electrical measurement data is reconstructed into high frequency electrical measurement data by generative adversarial network to improve the training stability, Wasserstein distance is introduced into the reconstruction mechanism. In addition, by designing the deep residual network based generator, the deep convolutional network based discriminator as well as the perception loss function, the reconstruction accuracy and the high-frequency detail reduction ability are improved. The proposed method is tested on three publicly available datasets and compared with the traditional data reconstruction method, justifying that this method not only can restore high-frequency details with less error, but also can be generalized to different datasets at one location and to datasets at different locations with satisfactory accuracy.},
  keywords={Electric variables measurement;Image reconstruction;Generative adversarial networks;Frequency measurement;Smart grids;Gallium nitride;Data-driven;super-resolution reconstruction;generative adversarial network;electrical measurement data},
  doi={10.1109/ACCESS.2020.2992836},
  ISSN={2169-3536},
  month={},}@ARTICLE{9380376,
  author={Kato, Sorachi and Fukushima, Takeru and Murakami, Tomoki and Abeysekera, Hirantha and Iwasaki, Yusuke and Fujihashi, Takuya and Watanabe, Takashi and Saruwatari, Shunsuke},
  journal={IEEE Access}, 
  title={CSI2Image: Image Reconstruction From Channel State Information Using Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={47154-47168},
  abstract={This study aims to determine the upper limit of the wireless sensing capability of acquiring physical space information. This is a challenging objective because, at present, wireless sensing studies continue to succeed in acquiring novel phenomena. Thus, although we have still not obtained a complete answer, a step is taken toward it herein. To achieve this, CSI2Image, a novel channel state information (CSI)-to-image conversion method based on generative adversarial networks (GANs), is proposed. The type of physical information acquired using wireless sensing can be estimated by checking whether the reconstructed image captures the desired physical space information. We demonstrate three types of learning methods: generator-only learning, GAN-only learning, and hybrid learning. Evaluating the performance of CSI2Image is difficult because both the clarity of the image and the presence of the desired physical space information must be evaluated. To solve this problem, we propose a quantitative evaluation methodology using an image-based object detection system. CSI2Image was implemented using IEEE 802.11ac compressed CSI, and the evaluation results show that CSI2Image successfully reconstructs images. The results demonstrate that generator-only learning is sufficient for simple wireless sensing problems; however, in complex wireless sensing problems, GANs are essential for reconstructing generalized images with more accurate physical space information.},
  keywords={Wireless communication;Location awareness;Wireless sensor networks;Generative adversarial networks;Sensors;Hybrid learning;Image reconstruction;Wireless sensing;channel state information;deep learning;generative adversarial networks;image reconstruction},
  doi={10.1109/ACCESS.2021.3066158},
  ISSN={2169-3536},
  month={},}@ARTICLE{9378517,
  author={Yin, Xiang and Han, Yanni and Sun, Hongyu and Xu, Zhen and Yu, Haibo and Duan, Xiaoyu},
  journal={IEEE Access}, 
  title={Multi-Attention Generative Adversarial Network for Multivariate Time Series Prediction}, 
  year={2021},
  volume={9},
  number={},
  pages={57351-57363},
  abstract={Multivariate Time series data play important roles in our daily life. How to use these data in the process of prediction is a highly attractive study for many researchers. To achieve this goal, in this paper, we present a novel multivariate time series prediction method based on multi-attention generative adversarial network. This method includes three phases to explore multivariate time series prediction. Firstly, the encoder stage consists of two modules, from which the input-attention and self-attention can encode the exogenous sequence into latent space. Secondly, the decoder stage consists of the temporal-convolution-attention module, which can extract long-term temporal patterns. To solve the problem of low accuracy in long-term prediction, inspired by the weight clipping method, we design an improved discrimination network finally. The experiment results indicate that multi-attention mechanism is useful and the discrimination network can improve the performance in multivariate time series prediction. We also tested extensive empirical studies with five real world datasets (NASDAQ100, SML2010, Energy, EEG and Air Quality) demonstrate the effectiveness and robustness of our proposed approach.},
  keywords={Time series analysis;Predictive models;Autoregressive processes;Generative adversarial networks;Data models;Market research;Correlation;Multivariate data;time series prediction;multi-attention;generative adversarial network},
  doi={10.1109/ACCESS.2021.3065969},
  ISSN={2169-3536},
  month={},}@ARTICLE{9513282,
  author={Huang, Jinjie and Yang, Guihua and Li, Biao and He, Yongjun and Liang, Yani},
  journal={IEEE Access}, 
  title={Segmentation of Cervical Cell Images Based on Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={115415-115428},
  abstract={The segmentation of cervical cell in liquid-based smear image plays an important role in cervical cancer detection. Despite of research for many years, it is still a challenge for the complexity of cell images such as poor contrast, cell irregularity, and overlapping. To solve this problem, a novel method is proposed based on Cell-GAN - a generative adversarial network. Firstly, the Cell-GAN is trained to learn a probability distribution of cell morphology by comparing the difference between the generated single-cell images and annotated single-cell images. Thus, the Cell-GAN has the ability to judge the integrity of a cell and treat other cellular information of a cell image, except for overlapping parts, as the background. Then, a complete single-cell image is generated by the trained Cell-GAN for each cell, which is located by a guide factor. The guide factor is constructed by a part of the cell to be segmented, such as the nucleus, to help Cell-GAN locate the cell and avoid generating a multi-cell image in the presence of overlapping, which means the contours of cells still cannot be distinguished. Finally, the segmentation line is defined by the contour of the generated cell, and the input image is cropped using the cell size information. The cropped image is reused for image generation until the area of generated cell varies within a small range.The proposed method is evaluated on the segmentation of single-cell images and overlapping cell images and obtained significant values of 94.3% DC, 7.9% FNRo for single-cell images and 89.9% DC, 6.4% FNRo for overlapping cell images respectively. The experimental results indicate that the proposed method can adaptively approach the boundary lines of cells to handle with different cases of overlapping in cervical cell images through what learned by the Cell-GAN. The proposed method outperforms most current methods in both segmentation accuracy and robustness.},
  keywords={Image segmentation;Generative adversarial networks;Convolutional neural networks;Shape;Level set;Cervical cancer;Probability distribution;Deep learning;Generative Adversarial Networks;cervical cell image segmentation;overlapping cells},
  doi={10.1109/ACCESS.2021.3104609},
  ISSN={2169-3536},
  month={},}@ARTICLE{9110873,
  author={Zhao, Huan and Xiao, Yufeng and Zhang, Zixing},
  journal={IEEE Access}, 
  title={Robust Semisupervised Generative Adversarial Networks for Speech Emotion Recognition via Distribution Smoothness}, 
  year={2020},
  volume={8},
  number={},
  pages={106889-106900},
  abstract={Despite the recent great achievements in speech emotion recognition (SER) with the development of deep learning, the performance of SER systems depends strongly on the amount of labeled data available for training. Obtaining sufficient annotated data, however, is often extremely time consuming and costly and sometimes even prohibitive because of privacy and ethical concerns. To address this issue, this article proposes the semisupervised generative adversarial network (SSGAN) for SER to capture underlying knowledge from both labeled and unlabeled data. The SSGAN is derived from a GAN, but the discriminator of the SSGAN can not only classify its input samples as real or fake but also distinguish their emotional class if they are real. Thus, the distribution of realistic inputs can be learned to encourage label information sharing between labeled and unlabeled data. This article proposes two advanced methods, i.e., the smoothed SSGAN (SSSGAN) and the virtual smoothed SSGAN (VSSSGAN), which, respectively, smooth the data distribution of the SSGAN via adversarial training (AT) and virtual adversarial training (VAT). The SSSGAN smooths the conditional label distribution given inputs using labeled examples, while the VSSSGAN smooths the conditional label distribution without label information (“virtual” labels). To evaluate the effectiveness of the proposed methods, four publicly available and frequently used corpora are selected to conduct experiments in intradomain and interdomain situations. The results illustrate that the proposed methods are superior to the state-of-the-art methods. Specifically, in experimental settings with mismatched and semimismatched unlabeled training sets, the SSSGAN and VSSSGAN are more robust than the SSGAN because of the distributional smoothness.},
  keywords={Training;Semisupervised learning;Speech recognition;Emotion recognition;Supervised learning;Generative adversarial networks;Feature extraction;Semisupervised learning;generative adversarial network;adversarial training;speech emotion recognition},
  doi={10.1109/ACCESS.2020.3000751},
  ISSN={2169-3536},
  month={},}@ARTICLE{8876596,
  author={Jiang, Mingfeng and Yuan, Zihan and Yang, Xu and Zhang, Jucheng and Gong, Yinglan and Xia, Ling and Li, Tieqiang},
  journal={IEEE Access}, 
  title={Accelerating CS-MRI Reconstruction With Fine-Tuning Wasserstein Generative Adversarial Network}, 
  year={2019},
  volume={7},
  number={},
  pages={152347-152357},
  abstract={Compressed sensing magnetic resonance imaging (CS-MRI) is a time-efficient method to acquire MR images by taking advantage of the highly under-sampled k-space data to accelerate the time consuming acquisition process. In this paper, we proposed a de-aliasing fine-tuning Wasserstein generative adversarial network (DA-FWGAN) for imaging reconstruction of highly under-sampled k-space data in CS-MRI. In the architecture, we used the fine-tuning method for accurate training of the neural network parameters and the Wasserstein distance as the discrepancy measure between the real and reconstructed images. Furthermore, for better preservation of the fine structures in the reconstructed images, we incorporated perceptual loss, image and frequency loss into the loss function for training the network. With experimental results from 3 different sampling schemes and 3 levels of sampling rates, we compared the reconstruction performance of the DA-FWGAN method with other state-of-the-art deep learning methods for CS-MRI reconstruction, including ADMM-Net, Pixel-GAN, and DAGAN. The proposed DA-FWGAN method outperforms all other methods and can provide superior reconstruction with improved peak signal-to-noise ratio (PSNR) and structural similarity index measure.},
  keywords={Image reconstruction;Magnetic resonance imaging;Generators;Gallium nitride;Generative adversarial networks;Neural networks;Fine-tuning;image reconstruction;magnetic resonance image (MRI);Wasserstein generative adversarial network (WGAN)},
  doi={10.1109/ACCESS.2019.2948220},
  ISSN={2169-3536},
  month={},}@ARTICLE{9729829,
  author={El-Shal, Ibrahim H. and Fahmy, Omar M. and Elattar, Mustafa A.},
  journal={IEEE Access}, 
  title={License Plate Image Analysis Empowered by Generative Adversarial Neural Networks (GANs)}, 
  year={2022},
  volume={10},
  number={},
  pages={30846-30857},
  abstract={Although the majority of existing License Plate (LP) recognition techniques have significant improvements in accuracy, they are still limited to ideal situations in which training data is correctly annotated with restricted scenarios. Moreover, images or videos are frequently used in monitoring systems that have Low Resolution (LR) quality. In this work, the problem of LP detection in digital images is addressed in the images of a naturalistic environment. Single-stage character segmentation and recognition are combined with adversarial Super-Resolution (SR) approaches to improve the quality of the LP by processing the LR images into High-Resolution (HR) images. This work proposes effective changes to the SRGAN network regarding the number of layers, an activation function, and the appropriate loss regularization using Total Variation (TV) loss. The main paper contribution can be summarized into presenting an end-to-end deep learning framework based on generative adversarial networks (GAN), which is able to generate realistic super-resolution images. Also, proposed adding a TV regularization to the loss function to help the model enhance the resolution of images. The proposed SRGAN can handle tiny  $72\times 72$  images of LPs. The paper explores how SRGAN performed over different datasets from many aspects, such as visual analysis, PSNR, SSIM, and Optical Character Recognition (OCR). The experiments demonstrate that the suggested SRGAN can generate high-resolution images that improve the accuracy of the license plate recognition stage compared to other systems.},
  keywords={Image edge detection;Computer vision;Generative adversarial networks;Feature extraction;Image color analysis;Superresolution;Optical character recognition software;License plate recognition;Computer vision;deep learning;generative adversarial networks;image reconstruction;license plate recognition;single image super-resolution;total variation loss},
  doi={10.1109/ACCESS.2022.3157714},
  ISSN={2169-3536},
  month={},}@ARTICLE{10226215,
  author={Liu, Zhipeng and Hu, Junyi and Liu, Yang and Roy, Kaushik and Yuan, Xiaohong and Xu, Jinsheng},
  journal={IEEE Access}, 
  title={Anomaly-Based Intrusion on IoT Networks Using AIGAN-a Generative Adversarial Network}, 
  year={2023},
  volume={11},
  number={},
  pages={91116-91132},
  abstract={Adversarial attacks have threatened the credibility of machine learning models and cast doubts over the integrity of data. The attacks have created much harm in the fields of computer vision, and natural language processing. In this paper, we focus on the adversarial attack, in particular the poisoning attack, against the network intrusion detection system (NIDS), which is often viewed as the first line of defense against cyber threats. We develop a generative adversarial network (GAN) in AIGAN, which uses deep learning techniques to generate adversarial data and to conduct an anomaly attack on IoT networks. To evaluate the effectiveness of our generator, we measure the similarities between real and fake data using the Jaccard similarity index, in addition comparing the F1-scores from four generic algorithms: multilayer perception, logistic regression, decision tree, random forest. We contrast the performance of ten machine learning classifiers experimented on two real IoT datasets and their fake adversarial samples. Our work highlights a vulnerable side of NIDS created by machine learning when attacked with adversarial perturbation.},
  keywords={Data models;Generative adversarial networks;Biological system modeling;Training data;Perturbation methods;Data augmentation;Internet of Things;Intrusion detection;Toxicology;Generative adversarial network;the IoT;machine learning;network intrusion detection system;poisoning attack},
  doi={10.1109/ACCESS.2023.3307463},
  ISSN={2169-3536},
  month={},}@ARTICLE{8910526,
  author={Zhuang, Haojie and Zhang, Weibin},
  journal={IEEE Access}, 
  title={Generating Semantically Similar and Human-Readable Summaries With Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={169426-169433},
  abstract={The application of neural networks in natural language processing, including abstractive text summarization, is increasingly attractive in recent years. However, teaching a neural network to generate a human-readable summary that reflects the core idea of the original source text (i.e., semantically similar) remains a challenging problem. In this paper, we explore using generative adversarial networks to solve this problem. The proposed model contains three components: a generator that encodes the long input text into a shorter representation; a discriminator to teach the generator to create human-readable summaries and another discriminator to restrict the output of the generator to reflect the core idea of the input text. The main training process can be carried out in an adversarial learning process. To solve the non-differentiable problem caused by the words sampling process, we use the policy gradient algorithm to optimize the generator. We evaluate the proposed model on the CNN/Daily Mail summarization task. The experimental results show that the model outperforms previous state-of-the-art models.},
  keywords={Generators;Generative adversarial networks;Training;Decoding;Postal services;Recurrent neural networks;Abstractive text summarization;generative adversarial networks;natural language processing},
  doi={10.1109/ACCESS.2019.2955087},
  ISSN={2169-3536},
  month={},}@ARTICLE{9503382,
  author={Wang, Cong and Zhang, Yin and Zhang, Yongqiang and Tian, Rui and Ding, Mingli},
  journal={IEEE Access}, 
  title={Mars Image Super-Resolution Based on Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={108889-108898},
  abstract={High-resolution (HR) Mars images have great significance for studying the land-form features of Mars and analyzing the climate on Mars. Nowadays, the mainstream image super-resolution methods are based on deep learning or CNNs, which are better than traditional methods. However, these deep learning based methods obtain low-resolution(LR) images usually by using an ideal down-sampling method (e.g. bicubic interpolation). There are two limitations in the existing SR methods: 1) The paired LR-HR data by using such methods can achieve a satisfactory results when tested on an ideal datasets. But, these methods always fail in real Mars image super-resolution, since real Mars images rarely obey an ideal down-sampling rule. 2) The LR images obtained by ideal down-sampling methods have no noise while real Mars images usually have noise, which leads to the super-resolved images are not realistic in texture details. To solve the above-mentioned problems, in this article, we propose a novel two-step framework for Mars image super-resolution. Specifically, to address limitation 1), we focus on designing a new degradation framework by estimating blur-kernels. To address limitation 2), a Generative Adversarial Network (GAN) is trained to generate noise distribution. Extensive experiments on the Mars32k dataset demonstrate the effectiveness of the proposed method, and we achieve better qualitative and quantitative results compared to other SOTA methods.},
  keywords={Mars;Superresolution;Earth;Generative adversarial networks;Kernel;Feature extraction;Interpolation;Generative adversarial network;kernel estimation;mars image super-resolution;noise model},
  doi={10.1109/ACCESS.2021.3101858},
  ISSN={2169-3536},
  month={},}@ARTICLE{8918423,
  author={Yang, Hongtao and Shi, Ping and Zhong, Dixiu and Pan, Da and Ying, Zefeng},
  journal={IEEE Access}, 
  title={Blind Image Quality Assessment of Natural Distorted Image Based on Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={179290-179303},
  abstract={Most existing image quality assessment (IQA) methods focus on improving the performance of synthetic distorted images. Although these methods perform well on the synthetic distorted IQA database, once they are applied to the natural distorted database, the performance will severely decrease. In this work, we propose a blind image quality assessment based on generative adversarial network (BIQA-GAN) with its advantages of self-generating samples and self-feedback training to improve network performance. Three different BIQA-GAN models are designed according to the target domain of the generator. Comprehensive experiments on popular benchmarks show that our proposed method significantly outperforms the previous state-of-the-art methods for authentically distorted images, which also has good performances on synthetic distorted benchmarks.},
  keywords={Image quality;Distortion;Feature extraction;Generative adversarial networks;Gallium nitride;Databases;Benchmark testing;Generative adversarial networks;deep learning;image quality assessment;no-reference/blind image quality assessment;natural distorted image},
  doi={10.1109/ACCESS.2019.2957235},
  ISSN={2169-3536},
  month={},}@ARTICLE{9256206,
  author={Liu, Jianyi and Tian, Yu and Zhang, Ru and Sun, Youqiang and Wang, Chan},
  journal={IEEE Access}, 
  title={A Two-Stage Generative Adversarial Networks With Semantic Content Constraints for Adversarial Example Generation}, 
  year={2020},
  volume={8},
  number={},
  pages={205766-205777},
  abstract={Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples, and these manipulated instances can mislead DNN into making false predictions. The existing methods of generating adversarial examples include pixel-level perturbation or spatial transformation of images, which cannot consider concurrently with the semantic quality of adversarial examples or success rate of attack. These methods are computationally bulky and slow to generate the adversarial examples. To solve this kind of issue, a two-stage generative adversarial networks (TSGAN) with semantic content constraints is proposed in this paper. The first-stage uses the original example dataset to train generator  $G$ , which can help the generator learn the distribution of real examples. Then, the example semantic quality constraint loss function, the adversarial loss function and the distance loss function are adopted in the second-stage, so that the generator  $G$  can continue to learn to search the distribution of the adversarial examples, and train the new generator  $G_{adv} $ . The adversarial examples generated by generator  $G_{adv} $  are better fit the distribution of real examples, and have targeted black-box attack capability. The experiments show that the adversarial examples generated by TSGAN can achieve the success rate of attack at 98.40% in target model, 29.40% success rate in defense-oriented model. And 77.58% success rate is obtained in the transfer test attack. The results show that the adversarial examples generated by the proposed model, which has a highly attack success rate and more difficult to defense. Meanwhile, the improved adversarial examples have stronger transfer ability than the existing models. The proposed model can effectively reduce the expression of target category features of the adversarial examples, and the generated adversarial examples have better semantic quality than others.},
  keywords={Training;Generators;Feature extraction;Semantics;Generative adversarial networks;Gallium nitride;Perturbation methods;Generative adversarial networks;adversarial example attack;semantic content},
  doi={10.1109/ACCESS.2020.3037329},
  ISSN={2169-3536},
  month={},}@ARTICLE{8868179,
  author={Yanagi, Rintaro and Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  journal={IEEE Access}, 
  title={Query is GAN: Scene Retrieval With Attentional Text-to-Image Generative Adversarial Network}, 
  year={2019},
  volume={7},
  number={},
  pages={153183-153193},
  abstract={Scene retrieval from input descriptions has been one of the most important applications with the increasing number of videos on the Web. However, this is still a challenging task since semantic gaps between features of texts and videos exist. In this paper, we try to solve this problem by utilizing a text-to-image Generative Adversarial Network (GAN), which has become one of the most attractive research topics in recent years. The text-to-image GAN is a deep learning model that can generate images from their corresponding descriptions. We propose a new retrieval framework, “Query is GAN”, based on the text-to-image GAN that drastically improves scene retrieval performance by simple procedures. Our novel idea makes use of images generated by the text-to-image GAN as queries for the scene retrieval task. In addition, unlike many studies on text-to-image GANs that mainly focused on the generation of high-quality images, we reveal that the generated images have reasonable visual features suitable for the queries even though they are not visually pleasant. We show the effectiveness of the proposed framework through experimental evaluation in which scene retrieval is performed from real video datasets.},
  keywords={Generative adversarial networks;Task analysis;Gallium nitride;Visualization;Videos;Image resolution;Semantics;Scene retrieval;deep learning;generative adversarial network;text-to-image translation},
  doi={10.1109/ACCESS.2019.2947409},
  ISSN={2169-3536},
  month={},}@ARTICLE{9358203,
  author={Islam, Naeem Ul and Park, Jaebyung},
  journal={IEEE Access}, 
  title={Depth Estimation From a Single RGB Image Using Fine-Tuned Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={32781-32794},
  abstract={Estimating the depth map from a single RGB image is important to understand the nature of the terrain in robot navigation and has attracted considerable attention in the past decade. The existing approaches can accurately estimate the depth from a single RGB image, considering a highly structured environment. The problem becomes more challenging when the terrain is highly dynamic. We propose a fine-tuned generative adversarial network to estimate the depth map effectively for a given single RGB image. The proposed network is composed of a fine-tuned generator and a global discriminator. The encoder part of the generator takes input RGB images and depth maps and generates their joint distribution in the latent space. Subsequently, the decoder part of the generator decodes the depth map from the joint distribution. The discriminator takes real and fake pairs in three different configurations and then guides the generator to estimate the depth map from the given RGB image accordingly. Finally, we conducted extensive experiments with a highly dynamic environment dataset for verifying the effectiveness and feasibility of the proposed approach. The proposed approach could decode the depth map from the joint distribution more effectively and accurately than the existing approaches.},
  keywords={Estimation;Generators;Training;Shape;Robots;Generative adversarial networks;Three-dimensional displays;Generative adversarial network;convolutional neural network;image translation;auto-encoders},
  doi={10.1109/ACCESS.2021.3060435},
  ISSN={2169-3536},
  month={},}@ARTICLE{8794508,
  author={Shi, Zaifeng and Li, Jinzhuo and Li, Huilong and Hu, Qixing and Cao, Qingjie},
  journal={IEEE Access}, 
  title={A Virtual Monochromatic Imaging Method for Spectral CT Based on Wasserstein Generative Adversarial Network With a Hybrid Loss}, 
  year={2019},
  volume={7},
  number={},
  pages={110992-111011},
  abstract={Spectral computed tomography (CT) has become a popular clinical diagnostic technique because of its unique advantage in material distinction. Specifically, it can perform virtual monochromatic imaging to obtain accurate tissue composition with less beam hardening artifacts. It is an ill-posed problem that monochromatic images are acquired by material decomposition matrix, suffering from amplified noise due to various uncertain factors. Aiming at modeling spatial and spectral correlations, this paper proposes a Wasserstein generative adversarial network with a hybrid loss (WGAN-HL) for monochromatic imaging instead of voxel-by-voxel decomposition. A min-max concept about the optimal transport is introduced in WGAN to make a tradeoff between generated images and target images where the authenticity of data cannot be distinguished anymore by network. The hybrid loss focuses on the data distribution of the generated images and target images from voxel space together with feature space to meet clinical requirements. Thereby, the proposed network can generate robust monochromatic images with accurate decomposition at any energy, while identifying and removing noise and artifacts. The advantages of this method are demonstrated in CT value measurement, beam hardening, and metal artifacts removal. Simulations and real tests prove that the WGAN-HL method preserves the important tissue details with less noise and it can reconstruct more accurate CT value. Both qualitative and quantitative comparisons show that the network is superior to other monochromatic imaging method.},
  keywords={Computed tomography;Image reconstruction;Generative adversarial networks;Attenuation;Data models;Gallium nitride;Spectral CT;generative adversarial network;monochromatic imaging;hybrid loss},
  doi={10.1109/ACCESS.2019.2934508},
  ISSN={2169-3536},
  month={},}@ARTICLE{9001139,
  author={Sun, Xudong and Zhao, Zhenxi and Zhang, Song and Liu, Jintao and Yang, Xinting and Zhou, Chao},
  journal={IEEE Access}, 
  title={Image Super-Resolution Reconstruction Using Generative Adversarial Networks Based on Wide-Channel Activation}, 
  year={2020},
  volume={8},
  number={},
  pages={33838-33854},
  abstract={In recent years, residual learning has shown excellent performance on convolutional neural network (CNN)-based single-image super-resolution (SISR) tasks. However, CNN-based SISR approaches have focused mainly on the design of deep architectures, and the rectified linear units (ReLUs) used in these networks hinder shallow-to-deep information transfer. As a result, these methods are unable to utilize some shallow information, and improving model performance is difficult. To solve the above issues, this paper proposes an image SR reconstruction method based on a generative adversarial network with a residual dense architecture. First, before ReLU activation, the number of feature channels is expanded by a factor of 6~9 using a 1 × 1 convolutional layer, which improves the utilization of shallow information. Next, the original discriminator is replaced with a relativistic average discriminator, thereby improving the authenticity of the discriminative network. Finally, preactivation features are used to improve the perceptual loss, thus providing stronger monitoring for brightness consistency and texture restoration. Experimental results show that the proposed algorithm improves the utilization of shallow information in a deep network. Structural similarity (SSIM) index evaluations show that the overall utilization of shallow information is increased by 105.52%. In addition, the average runtime is 0.42 sec/frame, nearly 3.6 times faster than those of traditional methods. Moreover, the recovered images have an average natural image quality evaluator value of 3.4 and high perceptual quality, showing that the proposed method is suitable for image reconstruction applications in fields such as agriculture and medicine.},
  keywords={Image reconstruction;Gallium nitride;Generative adversarial networks;Image resolution;Brightness;Convolutional neural networks;Monitoring;Super-resolution;residual block;relativistic average discriminator;generative adversarial network;perceived quality},
  doi={10.1109/ACCESS.2020.2974759},
  ISSN={2169-3536},
  month={},}@ARTICLE{9163112,
  author={Zheng, Jieying and Wu, Yahong and Song, Wanru and Xu, Ran and Liu, Feng},
  journal={IEEE Access}, 
  title={Multi-Scale Feature Channel Attention Generative Adversarial Network for Face Sketch Synthesis}, 
  year={2020},
  volume={8},
  number={},
  pages={146754-146769},
  abstract={Face sketch synthesis for photos is an applied research topic and it is critical for criminal investigation. However, sketch synthesis remains some challenges because of the blur and artifacts in the generated face sketches. To mitigate these problems in face sketch synthesis, we propose a fast Generative Adversarial Network with fast Multi-scale feature channel Attention, namely MAGAN. In the generator network, multi-scale features are extracted by proposed multi-scale feature extraction to produce detailed sketches. Then, a channel attention mechanism is applied to emphasize the significance of important feature channels, further enhancing the synthesized sketches. Besides, the loss of patch-wise high-layer features from the VGG-19 network is applied to supervise the generator to synthesize more realistic sketches. To accelerate the training process, the features from the pooling layers are adopted to calculate the pseudo sketch feature loss. The experimental results demonstrate that our MAGAN can achieve better performance in both visual evaluations and quantitative evaluations (in terms of feature similarity and learned perceptual image patch similarity), compared with the state-of-the-art methods.},
  keywords={Face;Feature extraction;Gallium nitride;Generators;Training;Generative adversarial networks;Image reconstruction;Face sketch synthesis;generative adversarial network;channel attention;multi-scale features},
  doi={10.1109/ACCESS.2020.3015312},
  ISSN={2169-3536},
  month={},}@ARTICLE{9269971,
  author={Wu, Yun and Lan, Lin and Long, Huiyun and Kong, Guangqian and Duan, Xun and Xu, Changzhuan},
  journal={IEEE Access}, 
  title={Image Super-Resolution Reconstruction Based on a Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={215133-215144},
  abstract={In the field of computer vision, super-resolution reconstruction techniques based on deep learning have undergone considerable advancement; however, certain limitations remain, such as insufficient feature extraction and blurred image generation. To address these problems, we propose an image super-resolution reconstruction model based on a generative adversarial network. First, we employ a dual network structure in the generator network to solve the problem of insufficient feature extraction. The dual network structure is divided into an upsample subnetwork and a refinement subnetwork, which upsample and optimize a low-resolution image, respectively. In a scene with large upscaling factors, this structure can reduce the negative effect of noise and enhance the utilization of high-frequency details, thereby generating high-quality reconstruction results. Second, to generate sharper super-resolution images, we use the perceptual loss, which exhibits a fast convergence and excellent visual effect, to guide the generator network training. We apply the ResNeXt-50-$32\times 4\text{d}$ network, which has few parameters and a large depth, to calculate the loss to obtain a reconstructed super-resolution image that is highly realistic. Finally, we introduce the Wasserstein distance into the discriminator network to enhance the discrimination ability and stability of the model. Specifically, this distance is employed to eliminate the activation function in the last layer of the network and avoid the use of the logarithm in calculating the loss function. Extensive experiments on the DIV2K, Set5, Set14, and BSD100 datasets demonstrate the effectiveness of the proposed model.},
  keywords={Image segmentation;Semantics;Feature extraction;Generative adversarial networks;Generators;Stability analysis;Image reconstruction;Deep learning;dual network structure;generative adversarial network;perceptual loss;super-resolution},
  doi={10.1109/ACCESS.2020.3040424},
  ISSN={2169-3536},
  month={},}@ARTICLE{8513818,
  author={Chen, Tiao and Song, Xuehua and Wang, Changda},
  journal={IEEE Access}, 
  title={Preserving-Texture Generative Adversarial Networks for Fast Multi-Weighted MRI}, 
  year={2018},
  volume={6},
  number={},
  pages={71048-71059},
  abstract={Traditional magnetic resonance imaging (MRI) acquires three contrasts of T1, T2, and proton density (PD), but only one contrast can be highlighted in an imaging process, which not only restricts the reference standard for disease but also increases the discomfort and medical expenses of the patients due to requiring two different weighted MRI. In order to solve such a problem, we proposed a method based on deep learning technology to provide two MRI contrasts after one signal acquisition. In this paper, a new model (PTGAN) based on generative adversarial networks is devised to convert T2-weighted MRI images into PD-weighted MRI images. In addition, we have devised four different network structures as the reference model of PTGAN, by which the different brain dissection MRI images, different noise MRI images, knee cartilage MRI images, and pathological MRI images from different body parts are used to test PTGAN. The research results show that the proposed PTGAN can effectively preserve the structure and texture and improve resolution in the conversion. Moreover, each T2-weighted MRI conversion takes only about 4 ms and can provide more information for disease diagnosis through different image contrasts.},
  keywords={Magnetic resonance imaging;Generative adversarial networks;Gallium nitride;Generators;Magnetization;Training;Mathematical model;T₂-weighted MRI;PD-weighted MRI;deep learning;generative adversarial networks (GAN);fast conversion;accurate diagnosis;preserve texture},
  doi={10.1109/ACCESS.2018.2877932},
  ISSN={2169-3536},
  month={},}@ARTICLE{9044843,
  author={Hu, Zhihang and Turki, Turki and Wang, Jason T. L.},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks for Stochastic Video Prediction With Action Control}, 
  year={2020},
  volume={8},
  number={},
  pages={63336-63348},
  abstract={The ability of predicting future frames in video sequences, known as video prediction, is an appealing yet challenging task in computer vision. This task requires an in-depth representation of video sequences and a deep understanding of real-word causal rules. Existing approaches for tackling the video prediction problem can be classified into two categories: deterministic and stochastic methods. Deterministic methods lack the ability of generating possible future frames and often yield blurry predictions. On the other hand, although current stochastic approaches can predict possible future frames, their models lack the ability of action control in the sense that they cannot generate the desired future frames conditioned on a specific action. In this paper, we propose new generative adversarial networks (GANs) for stochastic video prediction. Our framework, called VPGAN, employs an adversarial inference model and a cycle-consistency loss function to empower the framework to obtain more accurate predictions. In addition, we incorporate a conformal mapping network structure into VPGAN to enable action control for generating desirable future frames. In this way, VPGAN is able to produce fake videos of an object moving along a specific direction. Experimental results show that the combination of VPGAN with a pre-trained image segmentation model outperforms existing stochastic video prediction methods.},
  keywords={Stochastic processes;Predictive models;Gallium nitride;Video sequences;Task analysis;Image segmentation;Generative adversarial networks;Cycle consistency;deep learning;generative adversarial networks;video prediction},
  doi={10.1109/ACCESS.2020.2982750},
  ISSN={2169-3536},
  month={},}@ARTICLE{9503409,
  author={Khattak, Gul Rukh and Vallecorsa, Sofia and Carminati, Federico and Khan, Gul Muhammad},
  journal={IEEE Access}, 
  title={High Energy Physics Calorimeter Detector Simulation Using Generative Adversarial Networks With Domain Related Constraints}, 
  year={2021},
  volume={9},
  number={},
  pages={108899-108911},
  abstract={Generative Adversarial Networks (GANs) have gained notoriety by generating highly realistic images. The present work explores GAN for simulating High Energy Physics detectors, interpreting detector output as three-dimensional images. The demands and requirements of a scientific simulation are quite stringent, as compared to the domain of visual images. Image characteristics such as pixel intensity and sparsity, for example, have very different distributions. Moreover, detector simulation requires conditioning on physics inputs, and domain knowledge becomes essential. We, therefore, adjust the pre-processing and incorporate physics-based constraints in the loss function. We also introduce a multi-step training process based on transfer learning by breaking up the task complexity. Validation of the results primarily consists of a detailed comparison to full Monte Carlo in terms of several physics quantities where a high level of agreement is found (ranging from a few percent up to 10% across a large particle energy range). In addition, we assess the performance by physics unrelated metrics, thereby proving further the variability and pertinence through diverse standpoints. We have demonstrated that an image generation technique from vision can successfully simulate highly complex physics processes while achieving a speedup of more than three orders of magnitude in comparison to the standard Monte Carlo.},
  keywords={Generative adversarial networks;Detectors;Training;Monte Carlo methods;Three-dimensional displays;Generators;High energy physics;3D vision;fast simulation;generative adversarial networks;high energy physics;image processing and generation;transfer learning},
  doi={10.1109/ACCESS.2021.3101946},
  ISSN={2169-3536},
  month={},}@ARTICLE{9837875,
  author={Cen, Shengcai and Luo, Haokun and Huang, Jinghan and Shi, Wurui and Chen, Xueyun},
  journal={IEEE Access}, 
  title={Pre-Trained Feature Fusion and Multidomain Identification Generative Adversarial Network for Face Frontalization}, 
  year={2022},
  volume={10},
  number={},
  pages={77872-77882},
  abstract={The study of face frontalization is essential for improving face recognition accuracy in extreme pose scenarios. Mainstream methods like TP-GAN, CAPG-GAN, etc., have made meaningful contributions. However, they still suffer from two problems: the lack of extracted feature diversity and the blurred details in generated images. This paper proposes a pre-trained feature fusion and multi-domain identification generative adversarial network (PM-GAN) for face frontalization: the features of the model pre-trained on large-scale datasets are fused with the original features of the encoder to enhance the diversity and robustness of features. In order to fuse features more effectively, we design a novel feature fusion module (FFM). In addition, a group of global and local discriminators is introduced to reinforce the local details and realism of generated frontal faces. Experimental results show that our proposed method outperforms state-of-the-art methods on M2FPA and CAS-PEAL datasets.},
  keywords={Face recognition;Feature extraction;Generative adversarial networks;Faces;Fuses;Task analysis;Robustness;Face frontalization;transferring pre-trained network;feature fusion;detail optimization;generative adversarial network},
  doi={10.1109/ACCESS.2022.3193386},
  ISSN={2169-3536},
  month={},}@ARTICLE{9999217,
  author={Song, Dongmei and Tang, Yunhe and Wang, Bin and Zhang, Jie and Yang, Changlong},
  journal={IEEE Access}, 
  title={Two-Branch Generative Adversarial Network With Multiscale Connections for Hyperspectral Image Classification}, 
  year={2023},
  volume={11},
  number={},
  pages={7336-7347},
  abstract={Hyperspectral image (HSI) classification has always drawn great attention in the field of remote sensing. Various deep learning models are in the ascendant and gradually applied to HSI classification. Nevertheless, limited-labeled and class-imbalanced datasets largely make the classifier prone to overfitting. To address the above problem, this article proposes a two-branch generative adversarial network with multiscale connections (TBGAN), which includes two generators to produce the spectral and spatial samples, respectively. Thereinto, the spectral generator is imbued with the self-attention mechanism to maximumly capture the long-term dependencies across the spectral bands. And meanwhile, an elaborated discriminator with two branches is devised in TBGAN for extracting the joint spectral-spatial features. Besides, the multiscale connections are placed between the discriminator and two generators to alleviate the instability problems caused by the inherently backward propagation of gradients in GAN. Furthermore, a feature-matching term is added to the loss function to prevent the generators from overtraining upon the current discriminator, thereby further improving the stability of the network. Experiments upon three benchmark datasets demonstrate that TBGAN achieves an extremely competitive classification accuracy and exerts lower sensitivity to the training sample size compared with several state-of-the-art methods.},
  keywords={Generators;Feature extraction;Generative adversarial networks;Training;Convolutional neural networks;Hyperspectral imaging;Task analysis;Hyperspectral image classification;generative adversarial network;multiscale connections;joint spectral-spatial features},
  doi={10.1109/ACCESS.2022.3232152},
  ISSN={2169-3536},
  month={},}@ARTICLE{10065435,
  author={Sereethavekul, Wuttinan and Ekpanyapong, Mongkol},
  journal={IEEE Access}, 
  title={Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network}, 
  year={2023},
  volume={11},
  number={},
  pages={26667-26685},
  abstract={Many Convolutional Neural Networks (CNNs) methods have already surpassed traditional approaches to image restoration tasks. Those CNNs models were usually designed to enhance single tasks such as an image resolution (super-resolution) or image denoising, but we came up with unconventional goals, that is, multiple recovery tasks from a single network design. Although the Transformer design has recently gained attention in image recovery tasks, they are too slow. In order to work with license plate images from a traffic camera stream, the system has to be responsive. So, we proposed a fast and lightweight deep learning-based data recovery system using a Generative Adversarial Network (GAN) principle named License Plate Recovery GAN (LPRGAN). The design has a proposed encoder-decoder style inspired by an autoencoder aided by dual classification networks. This style suits problem-characteristic learning because strong contextual information is retrieved from the down-scaled representations. This proposed system has three main features such as identifying a problem, data recovery, and fail-safe mechanism. The core of system is a data recovery unit (LPRGAN), is used to recover license plate images from multiple degraded input images. Most existing image restoration systems do not have self-awareness, leading to an inefficiency problem. Unlike existing works, this system has anomaly detection and will only process on a degraded input, reducing workload overhead, improving efficiency and a fail-safe feature that prevents an unexpected bad output. Hence, it is light enough to deploy on a low-power machine such as edge computing devices, opening up new possibilities in on-device computing. Our proposed research can recover several degraded problems up to 720p resolution at 15 frames per second on a single graphic card,  $256\times 128$  resolution at 17 frames per second on a CPU-only workstation machine, or 7 frames per second on an ultra-low-power tablet PC.},
  keywords={Generative adversarial networks;Streaming media;License plate recognition;Monitoring;Bit rate;Task analysis;Image restoration;Data recovery;deep learning;generative adversarial networks;image and video recovery;machine learning;neural networks;video streaming},
  doi={10.1109/ACCESS.2023.3255641},
  ISSN={2169-3536},
  month={},}@ARTICLE{10433531,
  author={Wei, Zhiqiang and Wei, Xijia and Zhao, Xinghua and Hu, Zongtang and Xu, Chu},
  journal={IEEE Access}, 
  title={SGANFuzz: A Deep Learning-Based MQTT Fuzzing Method Using Generative Adversarial Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={27210-27224},
  abstract={As the Internet of Things (IoT) industry grows, the risk of network protocol security threats has also increased. One protocol that has come under scrutiny for its security vulnerabilities is MQTT (Message Queuing Telemetry Transport), which is widely used. To address this issue, an automated execution program called fuzz has been developed to verify the security of MQTT brokers. This program is provided with various random and unexpected input data and monitored for different responses, such as acknowledgments, crashes, failures, or memory leaks. To generate a significant number of realistic MQTT protocols, we have proposed a Generative Adversarial Networks (GAN)-based protocol fuzzer called SGANFuzz. Our experimental results show that SGANFuzz has successfully detected 6 vulnerabilities among 7 MQTT implementations, including 3 CVE bugs. Compared to the state-of-the-art fuzzing tools, SGANFuzz has proven to be the most efficient fuzzing tool in terms of vulnerability detection and has expanded the feedback coverage by receiving more unique network responses from MQTT brokers.},
  keywords={Protocols;Fuzzing;Training;Mathematical models;Generative adversarial networks;Lenses;Bidirectional control;Time series analysis;Transformers;Detection algorithms;MQTT;fuzz test;generative adversarial networks;time-series models;transformer;vulnerability detection},
  doi={10.1109/ACCESS.2024.3365712},
  ISSN={2169-3536},
  month={},}@ARTICLE{9509434,
  author={Liang, Zhiyao and Zhang, Shiru},
  journal={IEEE Access}, 
  title={Generating and Measuring Similar Sentences Using Long Short-Term Memory and Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={112637-112654},
  abstract={The two problems of measuring the semantic similarity (MSS) between two sentences and generating a similar sentence (GSS) for a given one are particularly challenging. Since these two problems naturally have logical connections, this article proposes algorithms to deal with them together. The main contributions of this article are in four aspects. 1) We propose a new algorithm called the syntactic and semantic long short-term memory (SSLSTM) for computing sentence similarity. The sentence model used by SSLSTM computes a representation vector of a given sentence by merging the results of separately running two LSTM networks, one with the given sentence and the other with a related sentence that is generated based on the semantic features of the words and syntactic features of the given sentence. The semantic similarity score of two sentences is calculated based on the distance between the two representations vectors. 2) A new GAN framework is proposed called the sentence similarity generative adversarial network (SSGAN). A GSS algorithm and an MSS algorithm are incorporated as modules in the generator and discriminator of SSGAN. A unique design of SSGAN is that, with one input triple to the GAN, the generator will produce three additional items, and the discriminator will process them. Three versions of SSGAN are proposed: the classic SSGAN (C-SSGAN), the hybrid SSGAN (H-SSGAN), and the black-box SSGAN (B-SSGAN). 3) Two new paradigms of GAN emerge from the design patterns of the black-box SSGAN and hybrid SSGAN, called the black-box GAN (B-GAN) and the hybrid GAN (H-GAN), respectively, which have the potentials to be generally applied to other NLP problems. 4) A series of experiments of different settings are designed to test the effects of B-SSGAN, and the results show that B-SSGAN has considerable boosting effects on both the chosen MSS and GSS algorithms. Several experiments are executed to compare SSLSTM with some representative and state-of-the-art MSS algorithms. The results show that SSLSTM has advantages in terms of the amount of error and the overall performance. There are new design features in these experiments. The performances of a GSS algorithm are measured by using an MSS algorithm. Multiple performance measures are considered to describe the algorithms’ performance holistically, including the efficiency of achieved performance relative to training time, which indicates that a CNN-based algorithm (SSCNN) is the most training-efficient in the comparison.},
  keywords={Semantics;Generative adversarial networks;Task analysis;Syntactics;Generators;Natural language processing;Computational modeling;Long short-term memory;generative adversarial networks;semantics;syntax;deep learning;neural network;generating similar sentence;measuring sentence similarity;natural language processing},
  doi={10.1109/ACCESS.2021.3103669},
  ISSN={2169-3536},
  month={},}@ARTICLE{10614315,
  author={Shams Siam, Zakaria and Tasnuva Hasan, Rubyat and Hossain Chowdhury, Moajjem and Shaheenur Islam Sumon, Md. and Bin Ibne Reaz, Mamun and Hamid Bin Md Ali, Sawal and Mushtak, Adam and Al-Hashimi, Israa and Bassam Zoghoul, Sohaib and Chowdhury, Muhammad E. H.},
  journal={IEEE Access}, 
  title={Improving MRI Resolution: A Cycle Consistent Generative Adversarial Network-Based Approach for 3T to 7T Translation}, 
  year={2024},
  volume={12},
  number={},
  pages={116498-116515},
  abstract={Brain magnetic resonance imaging (MRI) offers intricate soft tissue contrasts that are essential for diagnosing diseases and conducting neuroscience research. At 7 Tesla (7T) magnetic field intensity, MRI enables increased resolution, enhanced tissue contrast, and improved SNR, compared to MRI collected from the commonly employed 3 Tesla (3T) MRI scanners. However, the exorbitant expenses associated with 7T MRI scanners hinder their broad use in research and clinical facilities. Efforts are underway to develop algorithms that can generate 7T MRI from 3T MRI to achieve better image quality without the need for 7T MRI machines. In this study, we have adopted a cycle consistent generative adversarial network (CycleGAN)-based approach for 3T MRI to 7T MRI translation, and vice versa, using a recently published dataset of paired T1-weighted MR images collected at 3T and 7T from a total of ten subjects. Various CycleGAN architectures were experimented with and compared on this dataset. The best performing CycleGAN architecture successfully produced the reconstructed images with a high level of accuracy based on different quantitative and qualitative evaluation criteria. Utilizing a post-processing technique, the best performing model generated 7T MRI from 3T MRI with a structural similarity index measure (SSIM) of 83.80%, peak SNR (PSNR) of 26.25, normalized mean squared error (NMSE) of 0.0088 and normalized mean absolute error (NMAE) of 0.0630. Utilizing CycleGAN to convert images from 3T to 7T MRI has shown a substantial improvement in MRI resolution, setting the stage for advancements in more informative and precise diagnostic imaging.},
  keywords={Magnetic resonance imaging;Training;Image processing;Spatial resolution;Image quality;Generative adversarial networks;Superresolution;Data models;Cycle consistent generative adversarial network;image-to-image translation;magnetic resonance imaging;paired dataset;T1-weighted MRI},
  doi={10.1109/ACCESS.2024.3430968},
  ISSN={2169-3536},
  month={},}@ARTICLE{9680719,
  author={Nguyen, Tri Minh and Yoo, Myungsik},
  journal={IEEE Access}, 
  title={Wasserstein Generative Adversarial Network for Depth Completion With Anisotropic Diffusion Depth Enhancement}, 
  year={2022},
  volume={10},
  number={},
  pages={6867-6877},
  abstract={The objective of depth completion is to generate a dense depth map by upsampling a sparse one. However, irregular sparse patterns or the lack of groundtruth data caused by unstructured data make depth completion extremely challenging. Sensor fusion using both RGB and LIDAR sensors can help produce a more reliable context with higher accuracy. Compared with previous approaches, this method takes semantic segmentation images as additional input and develops an unsupervised loss function. Thus, when combined with supervised depth loss, the depth completion problem is considered as semi-supervised learning. We used an adapted Wasserstein Generative Adversarial Network architecture instead of applying the traditional autoencoder approach and post-processing process to preserve valid depth measurements received from the input and further enhance the depth value precision of the results. Our proposed method was evaluated on the KITTI depth completion benchmark, and its performance in depth completion was proven to be competitive.},
  keywords={Generators;Laser radar;Generative adversarial networks;Computer architecture;Feature extraction;Training;Semantics;Depth completion;LIDAR sparse depth;anisotropic diffusion;generative adversarial network;Wasserstein GAN},
  doi={10.1109/ACCESS.2022.3142916},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9190835,
  author={Ma, Jie and Wu, Hanlin and Zhang, Jue and Zhang, Libao},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={SD-FB-GAN: Saliency-Driven Feedback Gan for Remote Sensing Image Super-Resolution Reconstruction}, 
  year={2020},
  volume={},
  number={},
  pages={528-532},
  abstract={The visual characteristics of different regions in remote sensing images are significantly versatile, which poses a huge challenge to single image super-resolution. Although generative adversarial network (GAN) has shown great potential in generating photo-realistic results, it provides unsatisfactory performance in objective metrics owning to pseudo textures brought by adversarial learning. In this paper, we propose a new saliency-driven feedback GAN to cope with these problems. We design a saliency-driven feedback generator based on paired-feedback blocks (PFBBs) and recurrent structure to provide strong reconstruction ability. In the PFBB, the saliency map serves as an indicator to reflect the texture complexity, so different reconstruction principles can be applied to restore areas with varying levels of saliency. Besides, we propose to measure the visual quality of salient areas, non-salient areas, and the whole image with multi-discriminators, which can dramatically eliminate pseudo textures. Comprehensive evaluations and ablation studies validate the superiority of our proposal.},
  keywords={Indexes;Economic indicators;Zirconium;Image reconstruction;super-resolution;deep learning;GAN;saliency analysis},
  doi={10.1109/ICIP40778.2020.9190835},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10447304,
  author={Cai, Zhuotong and Xin, Jingmin and Dong, Siyuan and Onofrey, John A. and Zheng, Nanning and Duncan, James S.},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Symmetric Consistency with Cross-Domain Mixup for Cross-Modality Cardiac Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={1536-1540},
  abstract={Accurate cardiac segmentation in cross-modality images plays an important role in the quantitative analysis of the heart to diagnose cardiovascular diseases. However, achieving high performance in cross-modality segmentation is hindered by the time-consuming annotation and modality gap. While some approaches employ Unsupervised Domain Adaptation (UDA) through adversarial learning to address the issue, it still remains challenging due to the instability of the adversarial generative models. In this work, we propose Symmetric Consistency with Cross-Domain Mixup (SCCDM), integrated with the teacher-student model for cross-modality cardiac segmentation. Specifically, we introduce symmetric consistency across the domains for two mixed data to diversify the data distribution from both the source domain and target domain. Extensive experiments on a public cardiac dataset demonstrate that SCCDM achieves superior domain adaptation performance for cardiac segmentation compared to state-of-the-art methods.},
  keywords={Training;Heart;Image segmentation;Adaptation models;Statistical analysis;Annotations;Signal processing;Mixup;Cross-modality;Cardiac Segmentation;Unsupervised Domain Adaptation},
  doi={10.1109/ICASSP48485.2024.10447304},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10021322,
  author={Sheikhjafari, Ameneh and Noga, Michelle and Ahmed, Ahmed and Ray, Nilanjan and Punithakumar, Kumaradevan},
  journal={IEEE Access}, 
  title={GMCNet: A Generative Multi-Resolution Framework for Cardiac Registration}, 
  year={2023},
  volume={11},
  number={},
  pages={8185-8198},
  abstract={Deformable image registration plays a crucial role in estimating cardiac deformation from a sequence of images. However, existing registration methods primarily process images as pairs instead of processing all images in a sequence together. This study proposes a novel end-to-end learning-free generative multi-resolution convolutional neural network (GMCNet) with the primary focus of registering images in a sequence. Even though learning-based methods have yielded high performance for image registration, their performance depends on their ability to learn information from a large number of samples which are difficult to obtain and might bias the framework to the specific domain of data. The proposed learning-free method eliminates the need for a dedicated training set while exploiting the capabilities of neural networks to achieve accurate deformation fields. Due to its capability of parameter sharing through the architecture, the GMCNet can be used as a groupwise registration as well as pairwise registration. The proposed method was evaluated on three different clinical cardiac magnetic resonance imaging datasets and compared quantitatively against nine other state-of-the-art learning and optimization-based algorithms. The proposed method outperformed other methods in all comparisons and yielded average Dice metric values ranging from 0.85 to 0.88 for the datasets. Different aspects of the GMCNet are also explored by assessing 1) the robustness; 2) performance on pairwise registration; 3) the influence of spatial transformation in a controlled environment; and 4) the impact of different multi-resolution structures. The results demonstrate that using temporal information to estimate the deformation fields leads to more accurate registration results and improved robustness under different noise levels. Moreover, the proposed method does not need images for training, and therefore, its prediction is not domain-specific and can be applied to any sequence of images.},
  keywords={Strain;Image registration;Magnetic resonance imaging;Learning systems;Optimization;Convolutional neural networks;Biomedical imaging;Generative adversarial networks;Convolutional neural networks;cardiac cine MRI;deformable registration;generative network;learning-free framework;multi resolution},
  doi={10.1109/ACCESS.2023.3238058},
  ISSN={2169-3536},
  month={},}@ARTICLE{9641798,
  author={Pulford, Graham W. and Kondrashov, Kirill},
  journal={IEEE Access}, 
  title={Convergence and Optimality Analysis of Low-Dimensional Generative Adversarial Networks Using Error Function Integrals}, 
  year={2021},
  volume={9},
  number={},
  pages={165366-165384},
  abstract={Due to their success at synthesising highly realistic images, many claims have been made about optimality and convergence in generative adversarial networks (GANs). But what of vanishing gradients, saturation, and other numerical problems noted by AI practitioners? Attempts to explain these phenomena have so far been based on purely empirical studies or differential equations, valid only in the limit. We take a fresh look at these questions using explicit, low-dimensional models. We revisit the well known optimal discriminator result and, by construction of a counterexample, show that it is not valid in the case of practical interest: when the dimension of the latent variable is less than that of the data:  ${\mathrm{ dim}}({\mathbf{z}}) < {\mathrm{ dim}}({\mathbf{x}})$ . To examine convergence issues, we consider a 1-D least squares (LS) GAN with exponentially distributed data, a Rayleigh distributed latent variable, a square law generator and a discriminator of the form  $D(x)=(1+ {\text {erf}}(x))/2$  where erf is the error function. We obtain explicit representations of the cost (or loss) function and its derivatives. The representation is exact down to the evaluation of a well-behaved 1-D integral. We present analytical numerical examples of 2D and 4D parameter trajectories for gradient-based minimax optimisation. Although the cost function has no saddle points, it generally has a minimum, maximum and plateaux areas. The gradient algorithms typically converge to a plateau, where the gradients vanish and the cost function saturates. This is an undesirable setting with no implications of optimality for either the generator or discriminator. The analytical method is compared with stochastic gradient optimisation and proven to be a very accurate predictor of the latter’s performance. The quasi-deterministic framework we develop is a powerful analytical tool for understanding convergence behaviour of low-dimensional GANs based on least-squares cost criteria.},
  keywords={Generators;Generative adversarial networks;Cost function;Training;Convergence;Costs;Trajectory;Alternating optimization;change of variable;convergence;cost surface;counterexample;error function integral;GAN;generative adversarial network;gradient descent;least squares GAN;low-dimensional model;optimality;optimal discriminator;optimal generator;performance analysis},
  doi={10.1109/ACCESS.2021.3133762},
  ISSN={2169-3536},
  month={},}@ARTICLE{9758805,
  author={Goodman, Joel and Sarkani, Shahram and Mazzuchi, Thomas},
  journal={IEEE Access}, 
  title={A Generative Approach to Open Set Recognition Using Distance-Based Probabilistic Anomaly Augmentation}, 
  year={2022},
  volume={10},
  number={},
  pages={42232-42242},
  abstract={Machine learning (ML) algorithms that are used in decision support (DS) and autonomous systems commonly train on labeled categorical samples from a closed set. This, however, poses a problem for deployed DS and autonomous systems when they encounter an anomalous pattern that did not originate from the closed set distribution used for training. In this case, the ML algorithm that was trained only on closed set samples may erroneously identify an anomalous pattern as having originated from one of the categories in the closed set, sometimes with very high confidence. In this paper, we consider the problem of unknown pattern recognition from a generative perspective in which additional synthetic training samples that represent anomalies are added to the training data. These synthetic samples are generated to optimally balance the desire to place anomalies all along the boundary of the training set in feature space, while not adversely effecting core classification performance on the test set. We demonstrate the efficacy of distance-based probabilistic anomaly augmentation (DPAA) that is proposed in this paper for a diverse set of applications such as character recognition and intrusion detection, and compare its combined classification and identification performance to both recent open set and more traditional novelty detection approaches.},
  keywords={Training;Anomaly detection;Generative adversarial networks;Deep learning;Weibull distribution;Probabilistic logic;Image reconstruction;Machine learning;outlier and novelty detection;open set recognition;anomalies;generative and discriminative architectures},
  doi={10.1109/ACCESS.2022.3168003},
  ISSN={2169-3536},
  month={},}@ARTICLE{10876168,
  author={Kakati, Amayika and Li, Guoquan and Moustapha Diallo, Elhadj and Chiru Kawala, Lilian and Hussain, Nasir and Adam, Abuzar B. M.},
  journal={IEEE Open Journal of the Communications Society}, 
  title={Toward Proactive, Secure and Efficient Space-Air-Ground Communications: Generative AI-Based DRL Framework}, 
  year={2025},
  volume={6},
  number={},
  pages={1284-1298},
  abstract={The rapid growth of low-Earth-orbit (LEO) satellites has enabled integrated space-air-ground networks to provide seamless connectivity to mobile users. However, these networks face challenges such as physical layer security risks from line-of-sight channels and the energy constraints of high-altitude platforms (HAPs), necessitating solutions for secure communication and energy efficiency. In this work, we address the challenges of energy efficiency and secure communication in space-air-ground networks, which are becoming critical with the increasing deployment of LEO satellites to support high-mobility users. We propose a novel downlink architecture where high-altitude platforms (HAPs) assist the LEO satellite in serving ground users. To tackle the demands of secrecy energy efficiency (SEE) in this dynamic and complex network, we formulate a non-convex optimization problem that jointly considers HAP trajectory, user-HAP association, and beamforming. The problem’s non-convexity makes it computationally challenging to solve in polynomial time. To overcome these challenges, we introduce a generative artificial intelligence (GAI)-based deep reinforcement learning (DRL) framework, named Gen-DRL, which leverages generative adversarial networks to empower its agents. This framework dynamically predicts and adapts to changes in the space-air-ground network environment by optimizing key parameters such as channel states, HAP trajectories, user associations, and beamforming. Compared to conventional methods, the proposed Gen-DRL achieves significant improvements in SEE by effectively managing complex interdependencies among multiple agents and intelligently adapting to the network’s goals and constraints. Extensive simulation results demonstrate that Gen-DRL consistently outperforms existing state-of-the-art frameworks in terms of secrecy energy efficiency, robustness to dynamic user locations, and adaptability to varying network parameters. This work provides new insights into the design of secure and energy-efficient space-air-ground networks, highlighting the potential of GAI-based DRL for future communication systems.},
  keywords={Optimization;Energy efficiency;Security;Satellites;Low earth orbit satellites;Array signal processing;Space-air-ground integrated networks;Autonomous aerial vehicles;Trajectory;Resource management;Generative artificial intelligence;space-air-ground network;deep reinforcement learning;beamforming;user association;high altitude platform},
  doi={10.1109/OJCOMS.2025.3539355},
  ISSN={2644-125X},
  month={},}@ARTICLE{11142250,
  author={Gujarathi, Arnav and Oza, Pratham and Bera, Asish},
  journal={IEEE Access}, 
  title={Advanced Encryption Using Generative Adversarial Network for Enhancing Security of Non-Fungible Tokens (NFTs)}, 
  year={2025},
  volume={13},
  number={},
  pages={151137-151152},
  abstract={Non-fungible tokens (NFTs) represent a unique form of digital asset stored on a blockchain, encompassing a wide array of assets from digital art to real-world commodities. While NFTs offer robust security through decentralization and smart contract enforcement, they are not impervious to cyber threats. Traditionally, NFTs are transferred over blockchain platforms via smart contracts. The primary objective of this study is to introduce a cryptographic framework that integrates an advanced encryption algorithm layer to fortify NFT image transfers, reinforcing content protection in an evolving digital ecosystem. Various pixel-based encryption algorithms have been implemented using image processing techniques and compared considering their levels of encryption and execution times. Based on the comparison, an advanced encryption algorithm has been devised with an added level of encryption and fast execution time. The proposed encryption algorithm leverages alpha composition with a randomly generated image obtained using a Generative Adversarial Network (GAN). The GAN-generated image is infused with Gaussian noise to deter decryption by unauthorized algorithms, thereby enhancing resilience against cryptanalysis. The encrypted image metadata would be transferred through both on-chain (i.e., blockchain) and off-chain methods to facilitate efficiency and security during NFT transfers. The image decryption protocol mandates the new owner/receiver of NFT to possess the correct private identification, transaction key, and off-chain data, ensuring exclusive access while automatically revoking ownership from the previous holder upon successful transfer. This approach not only secures the transfer process but also minimizes the risk of data leaks, as intermediaries should not have access to the complete metadata. This cryptographic approach not only safeguards digital assets but also aligns with the futuristic vision of secure transactions in the metaverse and Web 3.0. The experimental analysis based on the security level and encryption time based on multiple encryption algorithms justifies the benefits of the proposed method.},
  keywords={Encryption;Nonfungible tokens;Security;Metadata;Smart contracts;Generative adversarial networks;Computer hacking;Protection;Media;Ecosystems;Applied cryptography;and alpha composition;and blockchain;and content protection;and encryption;and decryption;and Generative Adversarial Network (GAN);and non-fungible token (NFT)},
  doi={10.1109/ACCESS.2025.3603088},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10890403,
  author={Zhang, Yaoyun and Xu, Xuenan and Wu, Mengyue},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The video-to-audio (V2A) generation task has drawn attention in the field of multimedia due to the practicality in producing Foley sound. Semantic and temporal conditions are fed to the generation model to indicate sound events and temporal occurrence. Recent studies on synthesizing immersive and synchronized audio are faced with challenges on videos with moving visual presence. The temporal condition is not accurate enough while low-resolution semantic condition exacerbates the problem. To tackle these challenges, we propose Smooth-Foley, a V2A generative model taking semantic guidance from the textual label across the generation to enhance both semantic and temporal alignment in audio. Two adapters are trained to leverage pre-trained text-to-audio generation models. A frame adapter integrates high-resolution frame-wise video features while a temporal adapter integrates temporal conditions obtained from similarities of visual frames and textual labels. The incorporation of semantic guidance from textual labels achieves precise audio-video alignment. We conduct extensive quantitative and qualitative experiments. Results show that Smooth-Foley performs better than existing models on both continuous sound scenarios and general scenarios. With semantic guidance, the audio generated by Smooth-Foley exhibits higher quality and better adherence to physical laws. Homepage: https://danny-c-auditore.github.io/smoothfoley.github.io/},
  keywords={Adaptation models;Visualization;Accuracy;Semantics;Streaming media;Signal processing;Synchronization;Sustainable development;Speech processing;Videos;Video-to-Audio;Controllable Audio Generation;Multimodal Learning},
  doi={10.1109/ICASSP49660.2025.10890403},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10908580,
  author={Ichinomiya, Hiroki and Kawabe, Kenichi and Chaitusaney, Surachai},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks for Stochastic Grid Planning Considering Individual Distributed Energy Resource}, 
  year={2025},
  volume={13},
  number={},
  pages={39455-39465},
  abstract={To consider the future impact of distributed energy resources (DERs) on grid planning, certain transmission and distribution system operators assume future profiles of conventional loads (CLs) and DERs and calculate the net load by adding them together. As certain DERs such as energy storage (ES) are operated according to grid conditions and renewable energy generation, the profiles must be assumed considering the correlation between CLs and DERs for calculating the net load. In addition, a spatial correlation is vital to properly understanding grid constraints on lines and transformers by load flow analysis. In this study, we proposed a method using a generative adversarial network to generate various profiles that simulate three-dimensional correlations such as time, space, and type of DERs. The objective is to show that the proposed method can adequately quantify the uncertainty of the future load assumption for grid planning, which is becoming increasingly difficult with the expansion of DERs. It was shown that the proposed method can account for the peak and off-peak deviations of individual DERs and appropriately assume the probability distribution of future aggregated loads. In fact, we evaluated the validity of the probability distribution using a reliability diagram and found that the deviation from the perfect reliability line was reduced by 33% on average against the compared method. Furthermore, we performed load flow analysis using the load profiles generated by the proposed method and confirmed that the load flows on lines and transformers can also be assumed with appropriate probability distributions.},
  keywords={Energy resources;Correlation;Indexes;Generative adversarial networks;Generators;Solid modeling;Planning;Renewable energy sources;Load flow analysis;Three-dimensional displays;Distributed energy resources;generative adversarial networks;grid planning;scenario generation},
  doi={10.1109/ACCESS.2025.3546468},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10008852,
  author={Wu, Sibo and Xu, Mengqiu and Wu, Ming and Zhang, Chuang and Shen, Hua},
  booktitle={2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
  title={Identify, Guess and Reconstruct: Three Principles for Cloud Removal Task}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Remote sensing images serve a significant role in earth observation to tackle climate change and post-disaster reconstruction concerns. However, optical images are obscured by clouds or haze, preventing precise earth observation; hence, cloud removal has been a hot topic among concerned scholars. The objective of this article is to make cloud removal more efficient and explicable by proposing three principles: identifying clouds, guessing objects beneath the clouds, and reconstructing the cloudy area. In addition, a modified dual contrastive learning Generative Adversarial Network is proposed based on these three principles by adding cloud detection and weight sharing strategy to obtain cloud semantics. In particular, we align two datasets by forming a quaternary sample pair that includes not only optical pictures and SAR images, but also region information for a more precise reconstruction. Our experiment results on the integrated dataset reveal the superiority of proposed method over previous cloud removal methods and the effectiveness of added modules through ablation experiments, with PSNR and SSIM values of 26.2 and 0.728, respectively.},
  keywords={Earth;Visual communication;Clouds;Semantics;Optical imaging;Radar polarimetry;Optical sensors;Climate change;Cloud Removal;Dual Contrastive Learning GAN;Remote Sensing;Deep Learning},
  doi={10.1109/VCIP56404.2022.10008852},
  ISSN={2642-9357},
  month={Dec},}@ARTICLE{11053865,
  author={Villegas-Ch, William and Govea, Jaime and Buenaño-Fernandez, Diego and Mera-Navarrete, Aracely},
  journal={IEEE Access}, 
  title={Automating the Design of Scalable and Efficient IoT Architectures Using Generative Adversarial Networks and Model-Based Engineering for Industry 4.0}, 
  year={2025},
  volume={13},
  number={},
  pages={112271-112291},
  abstract={The increasing demand for efficient and scalable IoT systems in Industry 4.0 has driven the development of increasingly complex architectures. However, the design of these architectures presents significant challenges, particularly in dense networks where latency, energy consumption, and scalability constraints affect operational performance. Traditional approaches, such as heuristic and genetic algorithms, have proven insufficient in automating and optimizing large-scale IoT configurations, resulting in a high design and validation time cost. This work proposes an innovative approach based on integrating generative adversarial networks (GANs), model-based engineering (MBSE), and MATLAB simulations to automate and optimize the design of IoT systems. Using GANs to generate scalable architectures and validate them using MBSE, our approach automates both the creation and optimization of configurations, ensuring compliance with technical and operational constraints. The results show that the proposed design achieves an average latency between 30 and 70 ms, improving by 10% compared to the reference values of the IoT Benchmark Dataset. Energy consumption remains between 3.2 W and 15.6 W, representing a 12% reduction compared to traditional systems. In addition, packet loss remains below 7%, exceeding the average of conventional systems.},
  keywords={MATLAB;Internet of Things;Adaptation models;Computer architecture;Real-time systems;Fourth Industrial Revolution;Energy consumption;Generative adversarial networks;Energy efficiency;Benchmark testing;IoT design automation;generative adversarial networks (GANs);engineering-based modeling (MBSE);energy efficiency in IoT},
  doi={10.1109/ACCESS.2025.3583869},
  ISSN={2169-3536},
  month={},}@ARTICLE{10758629,
  author={Ahmed Mohamed, Alshaymaa and Saleh, Yasmine N. M. and Abdel-Hamid, Ayman A.},
  journal={IEEE Access}, 
  title={OptiSGD-DPWGAN: Integrating Metaheuristic Algorithms and Differential Privacy to Improve Privacy-Utility Trade-Off in Generative Models}, 
  year={2024},
  volume={12},
  number={},
  pages={176070-176086},
  abstract={Recently, the necessity to balance model performance with data confidentiality in synthetic data generation has become a significant challenge in deep learning analysis of medical databases. In this paper, the OptiSGD-DPWGAN model is proposed, that incorporates metaheuristic algorithms and differential privacy into the Wasserstein Generative Adversarial Network (WGAN) architecture to protect sensitive data during the training process. The integration of Simulated Annealing and Backtracking Line Search with Stochastic Gradient Descent (SGD) optimizes the exploration of the solution space of complex parameters in non-convex deep learning models, significantly avoiding local minima. In differentially private synthetic data generation, adjusting the epsilon value critically influences the trade-off between preserving privacy and maintaining the utility of the data. Typically, a lower epsilon value strengthens privacy guarantees but can inversely affect the model’s effectiveness due to increased noise in the data processing. Empirical results demonstrate that OptiSGD-DPWGAN effectively mitigates this trade-off. Compared to other schemes, OptiSGD-DPWGAN consistently achieves lower privacy costs without compromising the quality of the synthetic data generated. These results not only show the capability of OptiSGD-DPWGAN to set a new standard in privacy-preserving synthetic data generation but also highlight its potential to generate high-quality synthetic data crucial for the medical domain which requires strict confidentiality and high precision.},
  keywords={Privacy;Differential privacy;Noise;Deep learning;Data models;Training;Synthetic data;Generative adversarial networks;Computational modeling;Protection;Differential privacy;deep learning;generative adversarial network;privacy-utility tradeoff;stochastic gradient descent},
  doi={10.1109/ACCESS.2024.3502909},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11064938,
  author={Yang, Yikang and Deng, Yulin and Yi, Qiuxu},
  booktitle={2025 IEEE 14th Data Driven Control and Learning Systems (DDCLS)}, 
  title={Self-Attention Forged Images Detection Network with Three-Branch Multi-Feature Fusion}, 
  year={2025},
  volume={},
  number={},
  pages={2253-2258},
  abstract={Images play a crucial role in modern society with their intuitiveness and authenticity. However, with the rapid development of generative models, forging realistic images has become easier. The forged images not only mislead public perception, but may also pose serious risks to news authenticity or social stability. In this work, we propose an image forgery detection method based on Swin Transformer, which extracts spatial domain (raw image), frequency domain (discrete cosine transform), and time-frequency domain (wavelet transform) features of the image, and introduces multi-head attention mechanism in the feature fusion stage to achieve deep fusion of cross-domain features. The experimental results demonstrate that the proposed method achieves excellent detection performance and offers a novel approach for image forgery detection.},
  keywords={Training;Time-frequency analysis;Attention mechanisms;Wavelet domain;Transforms;Feature extraction;Transformers;Forgery;Stability analysis;Robustness;Forged Image Detection;Swin Transformer;Self-Attention Network;Multi-Feature Fusion},
  doi={10.1109/DDCLS66240.2025.11064938},
  ISSN={2767-9861},
  month={May},}@ARTICLE{11071712,
  author={Wang, Wei and Li, Bo},
  journal={IEEE Access}, 
  title={A Financial Data Analysis Method Based on Time-Series Generative Adversarial Network and Decomposition Learning}, 
  year={2025},
  volume={13},
  number={},
  pages={118354-118368},
  abstract={Stock price series are highly volatile and non-stationary, making it difficult to predict future movements. Deep learning technology has provided new ways to predict stock prices. In existing works, deep learning combined with stock price series decomposition is a common architecture. Inspired by this, we propose a financial data analysis method based on time series generative adversarial network (TimeGAN) and decomposition learning. Specifically, we use multi-view data, including the highest price, lowest price, opening price, closing price, and trading volume, as explanatory variables for predicting the closing price. In parallel, we utilize TimeGAN to generate data with similar distributions to these data to reduce overfitting due to data scarcity. Further, we decompose the closing price series into several subseries with relatively simple patterns by singular spectrum analysis (SSA) and then use self-attention mechanisms to let the subseries perceive each other and learn their dependency relationships, thus providing useful information about stock price change patterns for the prediction model. Experimental results on multiple national stock indices have demonstrated that the proposed method significantly improves prediction accuracy compared to recent advanced methods.},
  keywords={Time series analysis;Predictive models;Data models;Generative adversarial networks;Deep learning;Correlation;Brain modeling;Accuracy;Data augmentation;Analytical models;Financial data analysis;data augmentation;time series correlation;time-series generative adversarial network},
  doi={10.1109/ACCESS.2025.3585968},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11149947,
  author={Lee, Jongmin and Lee, Jaeyoon and Lim, Hyoungjun and Choi, Jongwook and Choi, Jongwon},
  booktitle={2025 IEEE International Conference on Advanced Visual and Signal-Based Systems (AVSS)}, 
  title={Facemover: Unsupervised face frontalization with embedding segments}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Face frontalization, which aims to synthesize a frontal view from an arbitrary facial pose, plays a crucial role in enhancing downstream tasks such as face recognition, expression analysis, and lip reading. However, previous methods often rely on paired or annotated datasets, which are costly and impractical to obtain, particularly in unconstrained real-world scenarios. To overcome this limitation, we propose a novel framework that leverages pretrained generative and encoder-based projection models for efficient frontal face synthesis. Our method extracts identity-aware segmentation embeddings and manipulates the corresponding segmentation masks to generate multiple realistic frontal views, selecting the optimal output based on identity loss. The proposed approach demonstrates strong robustness even in extreme cases with severely limited facial input, while requiring only minimal fine-tuning, thereby offering both effectiveness and computational efficiency.},
  keywords={Image segmentation;Visualization;Face recognition;Lips;Computational modeling;Coherence;Data collection;Robustness;Computational efficiency},
  doi={10.1109/AVSS65446.2025.11149947},
  ISSN={2643-6213},
  month={Aug},}@INPROCEEDINGS{10193162,
  author={Anglada-Rotger, David and Marques, Ferran and Pardas, Montse},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={Color Deconvolution Applied to Domain Adaptation in HER2 Histopathological Images}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Breast cancer early detection is crucial for improving patient outcomes. The Institut Catala de la Salut (ICS) has launched‘ the DigiPatICS project to develop and implement artificial intelligence algorithms to assist with the diagnosis of cancer. In this paper, we propose a new approach for facing the color normalization problem in HER2-stained histopathological images of breast cancer tissue, posed as an style transfer problem. We combine the Color Deconvolution technique with the Pix2Pix GAN network to present a novel approach to correct the color variations between different HER2 stain brands. Our approach focuses on maintaining the HER2 score of the cells in the transformed images, which is crucial for the HER2 analysis. Results demonstrate that our final model outperforms the state-of-the-art image style transfer methods in maintaining the cell classes in the transformed images and is as effective as them in generating realistic images.},
  keywords={Measurement;Integrated circuits;Pathology;Deconvolution;Image color analysis;Signal processing;Generative adversarial networks;Domain Adaptation;Style Transfer;GAN;Deep Learning;Breast Cancer;Digital Pathology;Histopathological Images},
  doi={10.1109/ICASSPW59220.2023.10193162},
  ISSN={},
  month={June},}@INPROCEEDINGS{10189608,
  author={Shen, Zuyu and Piao, Yinzhu and Tan, Cong and Lin, Ruikai and Zhao, Xu and Wan, Xi},
  booktitle={2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)}, 
  title={Project Co-Art: Improving Children’s Imagination Through AI-based Human-Computer Co-creation}, 
  year={2022},
  volume={},
  number={},
  pages={1597-1602},
  abstract={This research proposes a new AI-based human-computer co-creation system, Co-Art, which helps enhance children’s imagination through drawing. In contrast to existing approaches, we use a more efficient and child-friendly form of voice and brush interaction, where the machine understands the sentence described by the child and visualizes it in an abstract style. The machine’s artwork can inspire the child to create a second version. In a comparative experiment, we found that the aesthetic and richness of the images that the Co-Art-trained children drew showed a significant difference compared to the non-trained group $(p\lt 0.01)$, with a significant increase in imagination and creativity $(p\lt 0.05)$.},
  keywords={Human computer interaction;Visualization;Brushes;Pain;Generative adversarial networks;User experience;Artificial intelligence;Children’s imagination;AI collaboration;Drawing;Education},
  doi={10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00231},
  ISSN={},
  month={Dec},}@ARTICLE{10496150,
  author={Chamola, Vinay and Sai, Siva and Sai, Revant and Hussain, Amir and Sikdar, Biplab},
  journal={IEEE Consumer Electronics Magazine}, 
  title={Generative AI for Consumer Electronics: Enhancing User Experience With Cognitive and Semantic Computing}, 
  year={2025},
  volume={14},
  number={2},
  pages={10-19},
  abstract={Generative artificial intelligence (GAI) models such as ChatGPT, DALL-E, and the recently introduced Gemini have attracted considerable interest in both business and academia because of their capacity to produce material in response to human inputs. Cognitive computing is a broader field of machine learning that encompasses GAI, which particularly emphasizes systems capable of creating content, such as images, text, or sound, while semantic computing acts as a fundamental element of GAI, furnishing the comprehension of context and significance essential for GAI systems to generate content akin to human-like standards. GAI is becoming a game-changing technology for consumer electronics industry with a variety of applications that improve user experiences and product development. GAI can revolutionize architectural visualization by facilitating quick prototyping and the investigation of cutting-edge design ideas. By creating unique compositions and graphics for a variety of applications, it also empowers media production and music composition. Our research identifies several applications of GAI in the consumer electronics industry. We analyze how GAI is utilized in augmented reality applications, optimizing user interactions and immersive experiences. Moreover, we explore the integration of GAI in voice assistants and virtual avatars, enhancing images, natural language understanding and delivering more personalized interactions. We present a novel case study on a generative artificial intelligence-based framework for answering consumer electronics queries. We have developed and presented the system using various GAI-based tools and integrations. The article also discusses the challenges in implementing GAI in consumer electronics, such as ethical considerations, data privacy, compatibility with existing systems, and the need for continuous updates and improvements.},
  keywords={Consumer electronics;Speech recognition;Hidden Markov models;Testing;Optimization;Generative AI;Chatbots;Cognitive systems;User experience},
  doi={10.1109/MCE.2024.3387049},
  ISSN={2162-2256},
  month={March},}
