@INBOOK{10766914,
  author={Cady, Field},
  booktitle={The Data Science Handbook}, 
  title={Deep Learning and AI}, 
  year={2025},
  volume={},
  number={},
  pages={309-329},
  abstract={Summary <p>Artificial intelligence has taken the world by storm, and this chapter will give you the foundation to understand and take advantage of this revolution. It shows in detail how to build and train deep neural networks, applying them to classic problems in classification and image analysis. An overview is given of the key concepts underlying modern AI applications, such as diffusion models and transformers, and how they are put together to make generative AI tools like Stable Diffusion. Finally, there is a discussion of Large Language Models (LLMs) and prompt engineering using the LangChain library.</p>},
  keywords={Deep learning;Neurons;Data models;Training;Neural networks;Artificial intelligence;Graphics processing units;Libraries;Codes;Adaptation models},
  doi={10.1002/9781394234523.ch24},
  ISSN={},
  publisher={Wiley},
  isbn={9781394234516},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10766914},}@INPROCEEDINGS{10647543,
  author={Zhang, Weixia and Zhu, Chengguang and Gao, Jingnan and Yan, Yichao and Zhai, Guangtao and Yang, Xiaokang},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={A Comparative Study of Perceptual Quality Metrics For Audio-Driven Talking Head Videos}, 
  year={2024},
  volume={},
  number={},
  pages={1218-1224},
  abstract={The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications. However, performance evaluation research lags behind the development of talking head generation techniques. Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment. To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness. Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures. We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context. Code is available at https://github.com/zwx8981/ADTH-QA.},
  keywords={Performance evaluation;Visualization;Head;Image coding;Gaussian noise;Transform coding;Propulsion;Perceptual quality assessment;AIGC;digital humans;audio-driven talking head generation},
  doi={10.1109/ICIP51287.2024.10647543},
  ISSN={2381-8549},
  month={Oct},}@ARTICLE{11084845,
  author={Saxena, Divya and Cao, Jiannong and Xu, Jiahao and Kulshrestha, Tarun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration}, 
  year={2025},
  volume={},
  number={},
  pages={1-18},
  abstract={The training of Generative Adversarial Networks (GANs) for high-fidelity images has predominantly relied on large-scale datasets. Emerging research, particularly on GANs ‘lottery tickets’, suggests that dense GANs models have sparse sub-networks capable of superior performance with limited data. However, the conventional process to uncover these ‘lottery tickets’ involves a resource-intensive train-prune-retrain cycle. Addressing this, our paper introduces Re-GAN, a novel, dataefficient approach for GANs training that dynamically reconfigures the GANs architecture during training. This method focuses on iterative pruning of non-important connections and regrowing them, thereby preventing premature loss of important features and maintaining the model's representational strength. Re-GAN provides a more stable and efficient solution for GANs models with limited data, offering an alternative to existing progressive growing methods and GANs tickets. While Re-GAN has already demonstrated its potential in image generation across diverse datasets, domains, and resolutions, in this paper, we significantly expand our study. We incorporate new applications, notably Image-to-Image translation, include additional datasets, provide in-depth analyses, and explore compatibility with data augmentation techniques. This expansion not only broadens the scope of Re-GAN but also establishes it as a generic training methodology, demonstrating its effectiveness and adaptability in different GANs scenarios. Code is available at https://github.com/IntellicentAI-lab/Re-GAN},
  keywords={Training;Computer architecture;Data models;Computational modeling;Translation;Generative adversarial networks;Adaptation models;Network architecture;Image synthesis;Generators;Data-efficient GANs training;Lottery tickets;Image generation;Image-to-image translation;GANs training},
  doi={10.1109/TPAMI.2025.3590650},
  ISSN={1939-3539},
  month={},}@INPROCEEDINGS{11064466,
  author={P, Matan and Velvizhy, P},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Sequential Story Illustration Generation with Fine-Tuned Diffusion Model for Children Stories}, 
  year={2025},
  volume={3},
  number={},
  pages={1212-1217},
  abstract={Illustrating children's stories is essential for nurturing creativity, imagination, and emotional engagement in young readers. High-quality visuals enhance the storytelling experience, yet traditional methods for creating illustrations demand significant artistic expertise, time, and financial resources, making them less accessible and scalable. Existing generative AI models, often lack finegrained adaptation to the domain-specific requirements of children's literature, such as stylized imagery and narrative coherence. To address this gap, this study finetuned the ShuttleAI/Shuttle-3-Diffusion model to generate illustrations that align closely with the narrative context of children's stories. The goal was to develop a system capable of producing stylistically consistent, high-resolution images from sequential textual inputs, ensuring continuity across storylines. The methodology involved preprocessing and fine-tuning the Norod78/Cartoon-Blip-Captions dataset to match the unique visual style needed for children's stories. A sequential image generation pipeline was implemented, where individual sentences were fed as prompts to the model, enabling it to maintain thematic and stylistic alignment across multiple illustrations. Training parameters were optimized, and rigorous evaluations were conducted to benchmark performance. The results revealed that the finetuned model achieved significant improvements in Fréchet Inception Distance (FID) and Inception Score (IS), outperforming baseline generative models. Qualitative analyses highlighted the model's ability to generate visually engaging and contextually accurate illustrations. However, challenges persist in handling complex narratives and expanding stylistic flexibility. This work offers substantial implications for the future of automated content creation in children's literature, paving the way for scalable, cost-effective, and personalized storytelling solutions. Future research could focus on leveraging larger, diverse datasets and real-time application scenarios, further advancing the integration of generative AI in creative and educational domains.},
  keywords={Training;Visualization;Adaptation models;Generative AI;Image synthesis;Text to image;Diffusion models;Real-time systems;Security;Context modeling;Text-to-Image Generation;Visual Storytelling;Diffusion Models;Children's Story Illustration;Sequential Image Generation;Generative AI},
  doi={10.1109/ICCSAI64074.2025.11064466},
  ISSN={},
  month={April},}@INPROCEEDINGS{11103313,
  author={Yang, Wangying and Xu, Peng and Zhang, Xiaogong},
  booktitle={2025 6th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={A Two-Stage Intent Recognition Framework in Multi-Agent Environments}, 
  year={2025},
  volume={},
  number={},
  pages={01-05},
  abstract={With the rapid development of generative artificial intelligence and the rise of AI agent, accurately identifying user intentions through natural language has become a key challenge in multi-agent systems. Traditional rule-based and classification models for intent recognition face limitations in scalability, semantic understanding and data requirements. Although large pre-trained language models improve accuracy, they often suffer from high computational costs and latency. To address these issues, this paper proposes a two-stage intent recognition framework in multi-agent environments. The first stage employs a lightweight TF-IDF-based approach for efficient initial matching, while the second stage leverages a large language model for fallback recognition when necessary. This design effectively balances speed and accuracy, offering a practical solution for intent recognition in complex, dynamic multi-agent systems.},
  keywords={Accuracy;Intent recognition;Large language models;Computational modeling;Scalability;Semantics;User experience;Real-time systems;Computational efficiency;Multi-agent systems;intent recognition;multi-agent;large language model;artificial intelligence;natural language processing},
  doi={10.1109/ICCEA65460.2025.11103313},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10985304,
  author={Yaswanth, Yalamuri and Das, Navnil and Priyanka and Rohan, Samala and Bhutani, Agam and Rathore, Rachna},
  booktitle={2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)}, 
  title={Machine-Generated Art: Exploring Neural Visions}, 
  year={2025},
  volume={3},
  number={},
  pages={1-6},
  abstract={Art has long been a reflection of human creativity and the cultures of societies. With the rise of technology, new forms of art have emerged, including digital paintings and drawings created without the use of paper or canvas. More recently, machine-generated images introduce a new evolution in artistic expression. The use of AI in creating art has raised considerable debate. Some people say, because the work of art has been created by something other than human, it cannot be considered true art, whereas others see it as a new and valid form of expression. This paper attempts to elaborate on an overall understanding of machine-generated imagery, the role of AI in art, its ethical use, and its potential effect. It is, therefore, the aim of this paper to add to the continuing discussion of the place of technology in the world of art through the analysis of existing literature, articles, and current trends, as well as hands-on testing of the technology.},
  keywords={Ethics;Technological innovation;Art;Noise;Market research;Regulation;Reflection;Artificial intelligence;Testing;Painting;Image Generation;Neural Networks;Noise;Encoder;Generator;Decoder;Discriminator;Artificial Neural Networks;Digital Art;Deep Learning;Generative Adversarial Networks},
  doi={10.1109/IATMSI64286.2025.10985304},
  ISSN={},
  month={March},}@INBOOK{10951909,
  author={K&#xf6;hler, Daniel},
  booktitle={Diffusions in Architecture: Artificial Intelligence and Image Generators}, 
  title={The Advent of Trees in Architecture or the Reversal of Autonomy with Large&#x2010;Scale Models}, 
  year={2024},
  volume={},
  number={},
  pages={130-139},
  abstract={Summary <p>Generative large&#x2010;scale models draw on billions of images, primarily sourced from social media. They capture millions of living environments, each one a unique snapshot taken by millions of people. Throughout history, architecture has consistently positioned itself in opposition to nature. Architecture shelters, tempers, distincts, and separates &#x2013; at times integrating with the natural surroundings, but always building around, rather than from, or with, nature. Throughout discursive history, architectural narratives have framed nature into a symbolic realm, using it to stage a representation. As technology advanced, particularly with the advent of the camera and other forms of mechanical reproduction, architects and artists began to use photography and other techniques to incorporate more accurate and detailed representations of vegetation and trees into their designs. Artificial intelligence models begin to compress contexts into an information density that interfaces with the design dialogues.</p>},
  keywords={Vegetation;Computer architecture;Urban areas;Buildings;Architecture;Artificial intelligence;History;Fuses;Computational modeling;Casting},
  doi={10.1002/9781394191802.ch15},
  ISSN={},
  publisher={Wiley},
  isbn={9781394191796},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951909},}@INPROCEEDINGS{10219520,
  author={Tseng, Ling-Shen and Wu, Chih-Hung and Chen, Yi Han and Tsai, Chuing-Hui},
  booktitle={2023 Sixth International Symposium on Computer, Consumer and Control (IS3C)}, 
  title={GAN-based Data Augmentation for Metal Surface Defect Detection Using Convolutional Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={80-83},
  abstract={Artificial Intelligence-based Automated Optical Inspection (AI-AOI) using Convolutional Neural Networks (CNNs) is commonly used for defect detection, including metal defect detection, in modern manufacturing. However, in most AOI applications, the occurrence of defects is much less than the normal ones. CNN-based defection models perform poorly due to the imbalanced and less divergent training data. This study presents the performance of CNN-based AOI for metal defect detection with the techniques of generative AI for data augmentation. The Wasserstein Generative Adversarial Network (WGAN) is employed for generating negative training data and increasing the divergence when training AOI models. The similarity of data generated by WGAN to the original ones is evaluated by the Structural Similarity Index Measure (SSIM). The performance of ten CNN models trained with data before and after being augmented by WGAN are compared. Three metal defect datasets are used for evaluating the performance of CNN-based AOI with WGAN. The experimental results show that the performance of defect classification can be improved by 1%-12% with data augmented by WGAN.},
  keywords={Training;Neural networks;Metals;Training data;Optical computing;Optical fiber networks;Data augmentation;Automated optical inspection (AOI);defect detection;artificial intelligence (AI);generative adversarial network (GAN);Convolutional Neural Network (CNN)},
  doi={10.1109/IS3C57901.2023.00029},
  ISSN={2770-0496},
  month={June},}@INPROCEEDINGS{10543604,
  author={Barbadekar, Ashwini and Sole, Swarali and Shekhavat, Akash},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={Enhancing Social Media Security: LSTM-Based Deep Fake Video Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid advancement of generative artificial intelligence (AI) techniques has led to the widespread creation and dissemination of deepfake videos, which convincingly manipulated media, containing fabricated content often indistinguishable from reality. Detecting such deepfake videos is a critical challenge in ensuring the originality and credibility of information in the digital age. In this paper, an approach for detecting deepfake videos using generative AI-based methods has been proposed. For this, this paper introduces a deepfake detection method using Long-short term memory (LSTM) based model. Celeb-DF (v2) has been used for this experiment. From the videos in the dataset, faces were extracted, cropped and were saved in a new video having only face images. By using ResNext, a CNN-based approach, feature extraction was performed and classification was done using the LSTM model. Highest accuracy of 95.33% was achieved using this model.},
  keywords={Training;Deepfakes;Visualization;Social networking (online);Media;Feature extraction;Security;Artificial Intelligence (AI);Generative Adversarial Networks (GAN);Long-short term memory (LSTM)},
  doi={10.1109/I2CT61223.2024.10543604},
  ISSN={},
  month={April},}@ARTICLE{11103479,
  author={Zhang, Weiting and Ren, Jiadong and Zheng, Tao and Guo, Ruibin and Zhang, Han and Mao, Shiwen and Zhang, Hongke},
  journal={IEEE Communications Magazine}, 
  title={GenNet: Computing-Efficient Generative AI for Deterministic Transmission Scheduling in 6G Networks}, 
  year={2025},
  volume={63},
  number={8},
  pages={48-55},
  abstract={With the development of the sixth generation (6G) networks, ubiquitous intelligence, computing, and networking integration, low energy consumption, and low delay have become the key characteristics. These characteristics pose significant challenges for traffic scheduling, particularly in the context of intelligent computing services. In recent years, generative artificial intelligence (AI) has demonstrated remarkable performance not only in natural language processing and image generation but also in network optimization. This article presents a generative AI-endogenous three-layer network architecture, named GenNet, to support more efficient scheduling of intelligent computing services in 6G networks by the dynamic adaptation of heterogeneous computing resources and diversified service requirements. Moreover, we propose a dueling double deep Q-network (D3QN)-based transmission scheduling algorithm that leverages diffusion models to achieve cross-domain end-to-end deterministic transmission. The proposed algorithm facilitates efficient interaction between edge devices and intelligent computing centers while reducing system cost and delay, and utilizes the denoising capability of the diffusion model to adaptively configure networks and optimize resource allocation. Finally, we present a case study, followed by a discussion of open research issues that are essential for generative AI and future networks.},
  keywords={6G mobile communication;Generative AI;Scheduling algorithms;Network architecture;Diffusion models;Market research;Dynamic scheduling;Delays;Resource management;Optimization},
  doi={10.1109/MCOM.001.2400588},
  ISSN={1558-1896},
  month={August},}@INPROCEEDINGS{11002693,
  author={Polowczyk, Agnieszka and Polowczyk, Alicja and Wiltos, Katarzyna and Woźniak, Marcin},
  booktitle={2025 IEEE Symposium on Computational Intelligence in Health and Medicine Companion (CIHM Companion)}, 
  title={Segmentation for Retinal Blood Vessels Using Channel Attention U-Net and Pix2Pix}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In medicine, segmentation of medical images is an extremely important process that allows doctors to locate and extract different elements faster. In order to speed up and automate the analysis of patient data, artificial intelligence models are very often used. To date, U-NET models in the application of binary or multi-class segmentation, with many modifications, are the most popular. The researchers also take on the use of generative neural networks to support model training, using different types of GANs. For this purpose, in this article we have researched the use of the generative network called Pix2Pix, which has been little analyzed so far in the problem of blood vessel segmentation. Our overall results presented that the proposed Channel Attention U-NET model in the Generator in the Pix2Pix architecture outperformed previous research on this type of model. The test data set confirmed the superiority of our architecture by obtaining results for Accuracy: 95.72% and for Dice: 82.23%.},
  keywords={Training;Image segmentation;Filters;Accuracy;Neural networks;Computer architecture;Medical services;Blood vessels;Retina;Medical diagnostic imaging;Retinal Blood Vessels Segmentation;Pix2Pix;Channel Attention U-Net},
  doi={10.1109/CIHMCompanion65205.2025.11002693},
  ISSN={},
  month={March},}@ARTICLE{9770477,
  author={Shamsolmoali, Pourya and Zareapoor, Masoumeh and Das, Swagatam and García, Salvador and Granger, Eric and Yang, Jie},
  journal={IEEE Transactions on Cybernetics}, 
  title={GEN: Generative Equivariant Networks for Diverse Image-to-Image Translation}, 
  year={2023},
  volume={53},
  number={2},
  pages={874-886},
  abstract={Image-to-image (I2I) translation has become a key asset for generative adversarial networks. Convolutional neural networks (CNNs), despite having a significant performance, are not able to capture the spatial relationships among different parts of an object and, thus, do not qualify as the ideal representative model for image translation tasks. As a remedy to this problem, capsule networks have been proposed to represent patterns for a visual object in such a way that preserves hierarchical spatial relationships. The training of capsules is constrained by learning all pairwise relationships between capsules of consecutive layers. This design would be prohibitively expensive both in time and memory. In this article, we present a new framework for capsule networks to provide a full description of the input components at various levels of semantics, which can successfully be applied to the generator-discriminator architectures without incurring computational overhead compared to the CNNs. To successfully apply the proposed capsules in the generative adversarial network, we put forth a novel Gromov–Wasserstein (GW) distance as a differentiable loss function that compares the dissimilarity between two distributions and then guides the learned distribution toward target properties, using optimal transport (OT) discrepancy. The proposed method—which is called generative equivariant network (GEN)—is an alternative architecture for GANs with equivariance capsule layers. The proposed model is evaluated through a comprehensive set of experiments on I2I translation and image generation tasks and compared with several state-of-the-art models. Results indicate that there is a principled connection between generative and capsule models that allows extracting discriminant and invariant information from image data},
  keywords={Semantics;Generators;Task analysis;Faces;Shape;Image synthesis;Generative adversarial networks;Capsule networks;disentangle representation;generative model;image-to-image (I2I) translation},
  doi={10.1109/TCYB.2022.3166761},
  ISSN={2168-2275},
  month={Feb},}@INPROCEEDINGS{11138800,
  author={Wu, Chongyan},
  booktitle={2025 IEEE 3rd International Conference on Image Processing and Computer Applications (ICIPCA)}, 
  title={Research on Generative AI-Based Architectural Design and Style Reconstruction Technology}, 
  year={2025},
  volume={},
  number={},
  pages={2003-2007},
  abstract={This research examines the generative artificial intelligence's (AI) application systems on the analysis and reconstruction of architectural styles through the cultural integrity preservation lenses and design modification encouragement. The study constructs a framework for AI comprising feature extraction of architecture, semantic analysis and deep learning style diffusion reconstruction. The framework integrates historical and cultural contexts to evaluate architectural styles in three basic features domains: Assuming a multi-dimensional representation system of features architectural styles as developed structures assists in comprehension of the interrelations between architectural features. The research proposes and applies a new approach to semantic analysis directed towards understanding the construct description of an architectural style from a historical design preservation with a contemporary vision reinterpretation approach. The effectiveness of the framework is validated through case study investigations, comparative performance analysis, and expert appraisal, which attest to changes in style recognition up to 92.5% and design flexibility. Findings also prove that the AI digital tools design and modernisation integration with traditional architecture knowledge and skills facilitates the architectural heritage preservation while providing innovative design approaches. This work constitutes a remarkable achievement of artificial intelligence in the application of architectural design and provides a foundation to reconcile against the principles of heritage conservation and modern architecture.},
  keywords={Deep learning;Technological innovation;Generative AI;Semantics;Computer architecture;Feature extraction;Performance analysis;Cultural differences;Image reconstruction;Lenses;Generative AI;Architectural Style Reconstruction;Semantic Analysis;Deep Learning;Heritage Preservation;Architectural design},
  doi={10.1109/ICIPCA65645.2025.11138800},
  ISSN={},
  month={June},}@INPROCEEDINGS{10852480,
  author={El Ganadi, Amina and Aftar, Sania and Gagliardelli, Luca and Bergamaschi, Sonia and Ruozzi, Federico},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={The Impact of Generative AI on Islamic Studies: Case Analysis of "Digital Muhammad ibn Ismail Al-Bukhari"}, 
  year={2024},
  volume={},
  number={},
  pages={179-187},
  abstract={The emergence of large language models (LLMs) such as ChatGPT, LLaMA, Gemini, and Claude has transformed natural language processing (NLP) tasks by demonstrating remarkable capabilities in generating fluent and contextually appropriate responses. This paper examines the current state of LLMs, their applications, inherent challenges, and potential future directions necessitating multidisciplinary collaboration. A key focus is the application of generative AI in Islamic studies, particularly in managing sensitive content such as the Ahadith (corpus of sayings, actions, and approvals attributed to the Prophet Muḥammad). We detail the customization and refinement of the AI model, "Digital Muḥammad ibn Ismail Al-Bukhari," designed to provide accurate responses based on the Sahih Al-Bukhari collection. Our methodology includes rigorous dataset curation, preprocessing, model customization, and evaluation to ensure the model’s reliability. Strategies to mitigate hallucinations involve implementing context-aware constraints, regular audits, and continuous feedback loops to maintain adherence to authoritative texts and correct biases. Findings indicate a significant reduction in hallucinations, though challenges such as residual biases and handling ambiguous queries persist. This research underscores the importance of recognizing LLMs’ limitations and highlights the need for collaborative efforts in fine-tuning these models with authoritative texts. It offers a framework for the cautious application of generative AI in Islamic studies, emphasizing continuous improvements to enhance AI reliability.},
  keywords={Analytical models;Accuracy;Text analysis;Large language models;Collaboration;Training data;Chatbots;Reliability engineering;Prompt engineering;Artificial intelligence;Large Language Models;Hallucinations;Hadith Studies;AI in Islamic Studies;Context-aware Constraints;Bias Mitigation;Generative AI Applications;Digital Humanities;Prompt Engineering;Sahih Al-Bukhari;GPT builder;Religious Text Analysis},
  doi={10.1109/FLLM63129.2024.10852480},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9409313,
  author={Yang, Yuxin and Gremillion, Ben and Zhang, Xitong and Lin, Youzuo and Wohlberg, Brendt and Guan, Qiang},
  booktitle={2020 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC) and Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S)}, 
  title={How Good Is Your Scientific Data Generative Model?}, 
  year={2020},
  volume={},
  number={},
  pages={96-102},
  abstract={Nowadays, leveraging data augmentation methods on helping resolving scientific problems becomes prevailing. And many scientific problems benefit from data augmentation methods build with deep generative models. Yet due to the complexity of the scientific data, commonly used evaluation methods of generative models appear not so suitable for generated scientific data. In this paper, we explore how do we effectively evaluate data augmentation methods for scientific data generative models? To answer this question, we use one example of real world scientific problem to show how we evaluate the quality of the generated data from two domain specific deep generative models. We observe that most existing state-of-art evaluation metrics are incompetent. They either show completely contradicting results or provide inaccurate insight from real data.},
  keywords={Measurement;Conferences;High performance computing;Computational modeling;Imaging;Machine learning;Data models},
  doi={10.1109/MLHPCAI4S51975.2020.00018},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9605352,
  author={Stein, Samuel A. and Baheri, Betis and Chen, Daniel and Mao, Ying and Guan, Qiang and Li, Ang and Fang, Bo and Xu, Shuai},
  booktitle={2021 IEEE International Conference on Quantum Computing and Engineering (QCE)}, 
  title={QuGAN: A Quantum State Fidelity based Generative Adversarial Network}, 
  year={2021},
  volume={},
  number={},
  pages={71-81},
  abstract={Tremendous progress has been witnessed in artificial intelligence where neural network backed deep learning systems have been used, with applications in almost every domain. As a representative deep learning framework, Generative Adversarial Network (GAN) has been widely used for generating artificial images, text-to-image or image augmentation across areas of science, arts and video games. However, GANs are computationally expensive, sometimes computationally prohibitive. Furthermore, training GANs may suffer from convergence failure and modal collapse. Aiming at the acceleration of use cases for practical quantum computers, we propose QuGAN, a quantum GAN architecture that provides stable convergence, quantum-states based gradients and significantly reduced parameter sets. The QuGAN architecture runs both the discriminator and the generator purely on quantum state fidelity and utilizes the swap test on qubits to calculate the values of quantum-based loss functions. Built on quantum layers, QuGAN achieves similar performance with a 94.98% reduction on the parameter set when compared to classical GANs. With the same number of parameters, additionally, QuGAN outperforms state-of-the-art quantum based GANs in the literature providing a 48.33% improvement in system performance compared to others attaining less than 0.5% in terms of similarity between generated distributions and original data sets. QuGAN code is released at https://github.com/yingmao/Quantum-Generative-Adversarial-Network},
  keywords={Deep learning;Training;System performance;Qubit;Computer architecture;Games;Quantum state;Quantum State Fidelity;Quantum Generative Adversarial Network;IBM-Q},
  doi={10.1109/QCE52317.2021.00023},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9195289,
  author={Sun, Qiankun and Cai, Lei},
  booktitle={2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)}, 
  title={Multi-AUV Target Recognition Method Based on GAN-meta Learning}, 
  year={2020},
  volume={},
  number={},
  pages={374-379},
  abstract={The presence of adverse factors such as turbid water quality and target occlusion prevented the acquisition of valid target characterization data. Due to the variable shape of the target, the accuracy of recognition for untrained new targets is low. In view of the above problems, this paper proposes a Multi-AUV target recognition method based on GAN-meta learning. VGG-19 network is used for feature extraction of target images, and the WGAN network is used to make up for missing target information. Based on the meta-learning theory, the parameters of the feature extraction process are trained by using stochastic gradient descent to improve the algorithm's ability to recognize new targets. Ensure that the GAN-meta learning model has a strong generalization ability. Simulation experiments are conducted on four underwater targets (underwater reconnaissance equipment, submarine, frogman, and torpedo) in the SUN dataset, and the results demonstrate that the proposed model achieves better performance.},
  keywords={Target recognition;Training;Gallium nitride;Task analysis;Generative adversarial networks;Feature extraction;Data models},
  doi={10.1109/ICARM49381.2020.9195289},
  ISSN={},
  month={Dec},}@ARTICLE{10812703,
  author={Jin, Chengxiang and Zhou, Jiajun and Xie, Chenxuan and Yu, Shanqing and Xuan, Qi and Yang, Xiaoniu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Enhancing Ethereum Fraud Detection via Generative and Contrastive Self-Supervision}, 
  year={2025},
  volume={20},
  number={},
  pages={839-853},
  abstract={The rampant fraudulent activities on Ethereum hinder the healthy development of the blockchain ecosystem, necessitating the reinforcement of regulations. However, multiple imbalances involving account interaction frequencies and interaction types in the Ethereum transaction environment pose significant challenges to data mining-based fraud detection research. To address this, we first propose the concept of meta-interactions to refine interaction behaviors in Ethereum, and based on this, we present a dual self-supervision enhanced Ethereum fraud detection framework, named Meta-IFD. This framework initially introduces a generative self-supervision mechanism to augment the interaction features of accounts, followed by a contrastive self-supervision mechanism to differentiate various behavior patterns, and ultimately characterizes the behavioral representations of accounts and mines potential fraud risks through multi-view interaction feature learning. Extensive experiments on real Ethereum datasets demonstrate the effectiveness and superiority of our framework in detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing scams. Additionally, the generative module can effectively alleviate the interaction distribution imbalance in Ethereum data, while the contrastive module significantly enhances the framework’s ability to distinguish different behavior patterns. The source code will be available in https://github.com/GISec-Team/Meta-IFD.},
  keywords={Blockchains;Feature extraction;Fraud;Phishing;Smart contracts;Codes;Representation learning;Data mining;Manuals;Data models;Ethereum;fraud detection;generative learning;contrastive learning;multi-view learning;self-supervision},
  doi={10.1109/TIFS.2024.3521611},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10581562,
  author={Li, Juan and Zhou, Huabing},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Infrared and Visible Image Fusion Based on Region Semantic Discrimination Optimization}, 
  year={2024},
  volume={},
  number={},
  pages={1190-1194},
  abstract={The fusion of infrared and visible images holds significant application value in military, security, environmental monitoring, and other fields. However, infrared images exhibit distinct target features, while visible images contain clear texture details. The fused image should preserve the corresponding features of the infrared or visible source images based on different semantic regions. In this study, based on the semantic discrimination network module, we propose a generative adversarial network with dual discriminators to improve the fusion effect of infrared and visible images. This method extracts semantic information from images through a semantic segmentation network and combines no-reference image quality assessment and entropy to estimate the information content in each semantic object, thereby guiding the fusion process. Experimental results demonstrate that this method can effectively achieve the fusion of infrared and visible images, achieving significant improvements in feature preservation.},
  keywords={Seminars;Semantic segmentation;Semantics;Feature extraction;Robustness;Security;Task analysis;Image fusion;Generative adversarial network;Generative adversa rial network},
  doi={10.1109/AINIT61980.2024.10581562},
  ISSN={},
  month={March},}@ARTICLE{10680417,
  author={Zhang, Hanwen and Li, Peichun and Dai, Minghui and Wu, Yuan and Qian, Liping},
  journal={IEEE Internet of Things Journal}, 
  title={Efficient Federated Learning With Quality-Aware Generated Models: An Incentive Mechanism}, 
  year={2025},
  volume={12},
  number={2},
  pages={1628-1642},
  abstract={Federated learning (FL) encounters slow convergence due to data heterogeneity issues. Recently, generative artificial intelligence (AI) has showcased remarkable capabilities in synthesizing realistic data. To effectively address the challenges of nonindependent and identically distributed (non-IID) data, this article introduces a collaborative AI training framework that leverages generative AI to enhance the learning performance of FL. In this framework, heterogeneous edge devices (HEDs) identify specific data categories lacking in their local data sets and acquire these data from generative AI providers (GAPs). This strategy aims to improve the convergence rate of FL. However, HEDs and GAPs may be reluctant to contribute their resources to FL training due to self-interest. Therefore, an incentive mechanism is necessary to encourage their participation. We propose a reverse auction model to facilitate data transactions among FL training buyers, GAPs, and HEDs within the FL training buyer’s budget. It focuses on determining winners and devising payment rules to maximize the FL training buyer’s utility. This involves solving a 0-1 programming problem with two sellers (GAPs and HEDs). To tackle this, we use joint bidding and virtual seller pairs for analysis. We demonstrate that our method ensures truthfulness, individual rationality, and computational efficiency. Furthermore, we employ a one-side matching mechanism to approximate the optimal solution. We further investigate a strategy to analyze and allocate data based on variance, aiming to minimize non-IID issues in local data. Simulation results demonstrate that our proposed matching mechanism can effectively improve the computational efficiency, with the test accuracy differing from the theoretical optimum by only about 0.7%, and our mechanism can outperform the other greedy algorithms. Additionally, our data allocation strategy enhances the test accuracy by approximately 7% compared to existing methods.},
  keywords={Training;Data models;Biological system modeling;Diffusion models;Costs;Artificial intelligence;Accuracy;Data compensation;federated learning (FL);generative artificial intelligence (AI);incentive mechanism},
  doi={10.1109/JIOT.2024.3461329},
  ISSN={2327-4662},
  month={Jan},}@INPROCEEDINGS{9276871,
  author={Wang, Yutian and Xie, Yuankun and Wang, Hui and Zhang, Qin},
  booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)}, 
  title={Feature Quantization for Many-to-many Voice Conversion}, 
  year={2020},
  volume={1},
  number={},
  pages={1131-1135},
  abstract={In recent years, Voice Conversion (VC) has made great strides with the application of deep learning. Many works on many-to-many Voice Conversion task require to have parallel training data and multiple generative adversarial networks. One of the successful models for VC is the Star generative adversarial network (StarGAN-VC), which does not require parallel training data and can use a single generator and discriminator for all pairs of speakers domains. However, training a high quality StarGAN is not an easy task. Using a single generator and discriminator with mini-batch statistics to perform feature mapping will result in an insurmountable gap between the converted distribution and the real distribution. In this paper, we proposed a Feature Quantization model plugged into the discriminator of StarGAN-VC2, which can quantize the continuous feature into a discrete embedding space to solve the feature mapping problem and improve the quality of converted speech. Experiments show that our proposed model can improve the MCD scores of baseline methods which confirm the efficiency of the proposed method.},
  keywords={Training;Quantization (signal);Training data;Generative adversarial networks;Data models;Generators;Task analysis;Voice Conversion;StarGAN-VC;Feature Quantization;discrete embedding space},
  doi={10.1109/ICIBA50161.2020.9276871},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10986655,
  author={Kaur, Hardarshan and Manish, M and Ngakpal, Tenzin and Gamango, Aswin Kumar and Paul, Kusuma Surendra},
  booktitle={2025 3rd International Conference on Disruptive Technologies (ICDT)}, 
  title={Conditional GANs for Enhanced Artistic Synthesis: Improving Quality and Speed Through Innovative Colorization Technique}, 
  year={2025},
  volume={},
  number={},
  pages={594-599},
  abstract={Art is a powerful way for people to express their emotions creatively through sketches. These sketches can range from simple black-and-white or grayscale to vibrant, colourful images, with colours often playing a key role in conveying feelings. Even though not everyone may be able to express their emotions through sketches, technology has made it possible for machines to create and enhance these images with colour. One innovative approach, the art model, makes it easier for artists to produce colourful sketches, helping them save time working on projects like magazine illustrations or newspaper graphics. This model involves three main steps: processing the data, creating the artwork, and adding colour, resulting in dynamic and engaging colour sketches. The image quality is significantly enhanced through this approach, as compared to different approaches. Its success is examined using performance measures like popularity and its ability to transform sketches into fully realized images.},
  keywords={Image quality;Training;Adaptation models;Art;Image color analysis;Transforms;Generative adversarial networks;Data models;Real-time systems;Testing;Art creation;Image quality;Comparative analysis;Sketch-to-image conversion;Conditional GANs;Data processing;Deep learning;Colorization Techniques},
  doi={10.1109/ICDT63985.2025.10986655},
  ISSN={},
  month={March},}@INPROCEEDINGS{10793532,
  author={Wang, Hsi Yeh and Rueangsirarak, Worasak and Utama, Surapong},
  booktitle={2024 23rd International Symposium on Communications and Information Technologies (ISCIT)}, 
  title={Evaluation of Generative-AI Fashion Images using CNN Classification and Regression}, 
  year={2024},
  volume={},
  number={},
  pages={146-151},
  abstract={The integration of artificial intelligence (AI) in the fashion industry has the potential to revolutionize design processes, enabling the generation of diverse designs based on user inputs. Despite the promise of AI, the unpredictable quality of outputs and the challenge of meeting human aesthetic and market standards remain significant hurdles. To address these issues, this research aims to (1) create a dataset of AI -generated fashion images labeled with customer rankings, and (2) establish a new evaluation method from the perspectives of the general public and the market. Data were collected from fashion e-commerce websites focusing on customer ratings and product images. Two CNN models were trained: one for regression, predicting continuous aesthetic scores, and another for classification, categorizing images into discrete rating intervals. Performance evaluation revealed that the regression model, constrained by clip techniques, maintained the original distribution of data while the classification model provided a benchmark for design indicators. The study concludes that addressing data imbalance and applying augmentation techniques yielded significant results. Specifically, the regression model achieved an RMSE of 0.866, while the classification model attained an accuracy of 93%.},
  keywords={Performance evaluation;Solid modeling;Accuracy;Predictive models;Benchmark testing;Solids;Data models;Artificial intelligence;Standards;Optimization;AI-Generated Designs;Fashion Design;Convolutional Neural Networks;Image Classification;Regression;Aesthetic Evaluation},
  doi={10.1109/ISCIT63075.2024.10793532},
  ISSN={2643-6175},
  month={Sep.},}@INPROCEEDINGS{10604482,
  author={Mu, Ziyu and Shi, Xiyu and Dogan, Safak},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Information System Security Reinforcement with WGAN-GP for Detection of Zero-Day Attacks}, 
  year={2024},
  volume={},
  number={},
  pages={105-110},
  abstract={Growing sophistication among cyber threats has posed increasing challenges to the security and reliability of information systems, especially in the face of zero-day attacks that exploit unknown vulnerabilities. This paper introduces an innovative application of Artificial Intelligence (AI), specifically the adoption of Wasserstein Generative Adversarial Networks with Gradient Penalty (WGAN-GP), to support Intrusion Detection Systems (IDS) to strengthen defences against such attacks. This research focuses on using the WGAN-GP to generate network traffic data in simulating the unpredictable patterns of zero-day attacks. It utilises the widely used network traffic dataset NSL-KDD to conduct data expansion. This approach leverages data generated by the WGAN-GP to train detection systems, enabling them to learn and identify subtle signatures of zero-day attacks. Experimental evaluation demonstrates that the WGAN-GP model can improve the accuracy of zero-day attack detection. In comparison to other methods, such as Convolutional Neural Networks (CNN), the detection accuracy is increased by 2.3% and 2% for binary and multi-classification, respectively. This work shows that combining IDS with advanced generative AI models, such as WGAN-GP, can significantly enhance the security of information systems in identifying and mitigating risks posed by zero-day attacks.},
  keywords={Training;Accuracy;Uncertainty;Telecommunication traffic;NSL-KDD;Security;Convolutional neural networks;Information System Security;IDS;WGAN-GP;Zeroday Attack},
  doi={10.1109/ICAIBD62003.2024.10604482},
  ISSN={2769-3554},
  month={May},}@ARTICLE{10423378,
  author={Su, Cheng and Peng, Xin and Yang, Dan and Li, Zhi and Wu, Xiaolong and Zhong, Weimin},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={A Two-Stage Multi-Target Domain Adaptation Framework for Prediction of Key Performance Indicators Based on Adversarial Network}, 
  year={2024},
  volume={8},
  number={2},
  pages={1772-1787},
  abstract={In wastewater treatment processes, building performance evaluation models to predict key indicators under uncommon operating conditions is difficult due to the lack of labeled data. Domain adaptation can be used to solve this problem through leveraging the knowledge of common conditions to construct prediction models for uncommon conditions. Considering the costs of labeling data, it is reasonable to assume that only data from the most common condition are labeled. Therefore, all domain adaptation tasks share a source domain. Under this assumption, most domain adaptation methods require mapping the same source data multiple times and training multiple task-specific predictors in different tasks, resulting in additional computational costs. To give a solution, a stepwise domain alignment strategy is proposed, which consists of two steps. First, the latent features of source domain are extracted, and the features are fixed after this step. Second, target domains from different tasks are mapped to the fixed feature space to achieve domain alignment. Based on the strategy, a two-stage multi-target adversarial adaptation network (TS-MAAN) for predicting effluent quality index is proposed, which consists of an autoencoder and a generative adversarial network. Additionally, parameter initialization and multi-kernel maximum mean discrepancy optimization are further proposed to improve the stability and prediction accuracy of the TS-MAAN, respectively. Experiments conducted on datasets generated by the Benchmark Simulation Model No.1 demonstrate that TS-MAAN exhibits excellent prediction accuracy and stability, while enabling efficient multi-target domain adaptation. Moreover, these experiments verify the effectiveness of parameter initialization and MK-MMD optimization.},
  keywords={Feature extraction;Generators;Adaptation models;Training;Generative adversarial networks;Data models;Domain adaptation;autoencoder;generative adversarial network;wastewater treatment process;effluent quality index},
  doi={10.1109/TETCI.2024.3358172},
  ISSN={2471-285X},
  month={April},}@INPROCEEDINGS{10650943,
  author={Ghosh, Sayantani and Konar, Amit and Nagar, Atulya K},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Computational Creativity by Generative Adversial Network with Leaked Information}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Computational creativity is defined as the ability of artificial systems to generate artifacts with substantial novelty and originality, comparable to those crafted by human experts. This study introduces an innovative approach to implementing computational creativity in the scientific domain, exemplified through the automatic generation of trigonometric identities using a novel LeakGAN model, a Generative Adversarial Network with leaked information. The novelty of the proposed LeakGAN lies in its discriminator model, constructed on the foundation of a Convolutional Neural Network (CNN), aimed at capturing the most relevant features from input data. These features are subsequently leaked to the generator model, enabling the production of identities with substantial originality and quality. The introduced novelty in the discriminator’s architecture encompasses the use of a new activation function called Mish, strategically employed to enhance the network's convergence rate and address over-fitting issues during training. Additionally, an attention layer is introduced to highlight the most relevant information within the feature space, thereby improving the network's learning capacity. Furthermore, a unique mixed pooling layer is utilized, combining the advantageous effects of max-pooling and average pooling operations to enhance the network's adaptability to varying feature distributions. Performance analysis, incorporating BiLingual Evaluation Understudy (BLEU) metrics and various comparative studies, substantiates the efficacy of the proposed LeakGAN model in generating novel identities compared to its traditional counterparts. Moreover, human evaluation involving 20 mathematical experts confirms the significant novelty of the generated identities compared to existing standard textbook problems. Consequently, the proposed technique proves valuable for generating diverse trigonometric identities suitable for inclusion as chapter-end exercises in middle school mathematics textbooks.},
  keywords={Geometry;Computational modeling;Computer architecture;Generative adversarial networks;Mathematical models;Generators;Data models;computational creativity;LeakGAN;trigonometric identities;CNN},
  doi={10.1109/IJCNN60899.2024.10650943},
  ISSN={2161-4407},
  month={June},}@ARTICLE{8358814,
  author={Wang, Chaoyue and Xu, Chang and Wang, Chaohui and Tao, Dacheng},
  journal={IEEE Transactions on Image Processing}, 
  title={Perceptual Adversarial Networks for Image-to-Image Transformation}, 
  year={2018},
  volume={27},
  number={8},
  pages={4066-4079},
  abstract={In this paper, we propose perceptual adversarial networks (PANs) for image-to-image transformations. Different from existing application driven algorithms, PAN provides a generic framework of learning to map from input images to desired images (Fig. 1), such as a rainy image to its de-rained counterpart, object edges to photos, and semantic labels to a scenes image. The proposed PAN consists of two feed-forward convolutional neural networks: the image transformation network T and the discriminative network D. Besides the generative adversarial loss widely used in GANs, we propose the perceptual adversarial loss, which undergoes an adversarial training process between the image transformation network T and the hidden layers of the discriminative network D. The hidden layers and the output of the discriminative network D are upgraded to constantly and automatically discover the discrepancy between the transformed image and the corresponding ground truth, while the image transformation network T is trained to minimize the discrepancy explored by the discriminative network D. Through integrating the generative adversarial loss and the perceptual adversarial loss, D and T can be trained alternately to solve image-to-image transformation tasks. Experiments evaluated on several image-to-image transformation tasks (e.g., image deraining and image inpainting) demonstrate the effectiveness of the proposed PAN and its advantages over many existing works.},
  keywords={Gallium nitride;Task analysis;Training;Loss measurement;Feature extraction;Image resolution;Semantics;Generative adversarial networks;image de-raining;image inpainting;image-to-image transformation},
  doi={10.1109/TIP.2018.2836316},
  ISSN={1941-0042},
  month={Aug},}@ARTICLE{9979702,
  author={Yang, Wanting and Liew, Zi Qin and Lim, Wei Yang Bryan and Xiong, Zehui and Niyato, Dusit and Chi, Xuefen and Cao, Xianbin and Letaief, Khaled B.},
  journal={IEEE Wireless Communications}, 
  title={Semantic Communication Meets Edge Intelligence}, 
  year={2022},
  volume={29},
  number={5},
  pages={28-35},
  abstract={The development of emerging applications, such as autonomous transportation systems, is expected to result in an explosive growth in mobile data traffic. As the available spectrum resource becomes more and more scarce, there is a growing need for a paradigm shift from Shannon's Classical Information Theory (CIT) to semantic communication (SemCom). Specifically, the former adopts a “transmit-before-understanding” approach while the latter leverages artificial intelligence (AI) techniques to “understand-before-transmit,” thereby alleviating bandwidth pressure by reducing the amount of data to be exchanged without negating the semantic effectiveness of the transmitted symbols. However, the semantic extraction (SE) procedure incurs costly computation and storage overheads. In this article, we introduce an edge-driven training, maintenance, and execution of SE. We further investigate how edge intelligence can be enhanced with SemCom through improving the generalization capabilities of intelligent agents at lower computation overheads and reducing the communication overhead of information exchange. Finally, we present a case study involving semantic-aware resource optimization for the wireless powered Internet of Things (IoT).},
  keywords={Wireless communication;Training;Semantics;Transportation;Symbols;Tutorials;Internet of Things},
  doi={10.1109/MWC.004.2200050},
  ISSN={1558-0687},
  month={October},}@ARTICLE{10304577,
  author={Li, Congyu and Liu, Xinxin and Li, Shutao},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Transformer Meets GAN: Cloud-Free Multispectral Image Reconstruction via Multisensor Data Fusion in Satellite Images}, 
  year={2023},
  volume={61},
  number={},
  pages={1-13},
  abstract={Cloud-free image reconstruction is of great significance for improving the quality of optical satellite images that are vulnerable to bad weather. When cloud cover makes it impossible to obtain information under the cloud, auxiliary data are indispensable to guide the reconstruction of the cloud-contaminated area. In addition, the areas that require continuous observation are mostly regions with complex features, which puts higher demands on the restoration of texture, color, and other details in data reconstruction. In this article, we propose a transformer-based generative adversarial network for cloud-free multispectral image reconstruction (TransGAN-CFR) via multisensor data fusion in satellite images. Synthetic aperture radar (SAR) images that are not affected by clouds are used as auxiliary data and paired with cloudy optical images into the generative adversarial network (GAN) generator. To take advantage of the deep-shallow features and global–local geographical proximity in remote sensing images, the proposed generator uses a hierarchical encoder–decoder structure, in which the transformer blocks adopt a nonoverlapping window multihead self-attention (WMSA) mechanism and a modified feedforward network (FFN) through depthwise convolutions and the gating mechanism. Besides, we introduce a triplet loss function specifically designed for cloud removal tasks to provide the generated cloud-less image with greater proximity to the ground truth. Compared with seven state-of-the-art deep-learning-based cloud removal models, our network can yield more natural cloud-free images with better visual performance and more accurate results in quantitative evaluation on the SEN12MS-CR dataset.},
  keywords={Clouds;Optical sensors;Optical imaging;Cloud computing;Image reconstruction;Adaptive optics;Synthetic aperture radar;Cloud removal;generative adversarial network (GAN);image fusion;optical image;synthetic aperture radar (SAR) image;transformer},
  doi={10.1109/TGRS.2023.3326545},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10823442,
  author={S, Perera A.P.S. and B, Hettiarachchi A. and P, Gunasekara P.G.A.I. and L, Jayasekara H. and Tissera, Wishalya and Rathnayake, Samadhi},
  booktitle={2024 8th International Conference on Business and Information Management (ICBIM)}, 
  title={Enhancing Fashion Intelligence for Exceptional Customer Experience}, 
  year={2024},
  volume={},
  number={},
  pages={23-28},
  abstract={The evolution of e-commerce has significantly transformed retail, with global sales expected to reach $6.4 trillion by 2024. The fashion industry, benefiting from technological advancements like artificial intelligence (AI), uses AI for product recommendations, inventory management, and personalized customer experiences. Despite these advances, challenges persist in image classification, sentiment analysis, personalized recommendations, and trend prediction. This research develops a comprehensive fashion intelligence system to enhance customer experiences. The system integrates multi-modal data fusion to combine predictions from various models, offering a holistic and accurate forecast of fashion trends. It employs explainable AI (XAI) for better sentiment analysis interpretability and a hybrid recommendation system that combines fashion styles and personality traits for more accurate suggestions. Adversarial learning techniques are used to enhance the robustness and security of image classifiers. Addressing these challenges, the system aims to significantly improve the online shopping experience for consumers and sellers by providing personalized, accurate, and trend-aware fashion recommendations.},
  keywords={Sentiment analysis;Accuracy;Explainable AI;Predictive models;Market research;Robustness;Electronic commerce;Security;Recommender systems;Image classification;Fashion E-commerce;AI Integration;Image Classification;Sentiment Analysis;Personalized Recommendations;Trend Prediction;Multi-modal Data;Explainable AI;Adversarial Robustness;Customer Satisfaction;Recommendation on System;Cold Start Solution},
  doi={10.1109/ICBIM63313.2024.10823442},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8954393,
  author={Khan, Salman H. and Guo, Yulan and Hayat, Munawar and Barnes, Nick},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Primitive Discovery for Improved 3D Generative Modeling}, 
  year={2019},
  volume={},
  number={},
  pages={9731-9740},
  abstract={3D shape generation is a challenging problem due to the high-dimensional output space and complex part configurations of real-world objects. As a result, existing algorithms experience difficulties in accurate generative modeling of 3D shapes. Here, we propose a novel factorized generative model for 3D shape generation that sequentially transitions from coarse to fine scale shape generation. To this end, we introduce an unsupervised primitive discovery algorithm based on a higher-order conditional random field model. Using the primitive parts for shapes as attributes, a parameterized 3D representation is modeled in the first stage. This representation is further refined in the next stage by adding fine scale details to shape. Our results demonstrate improved representation ability of the generative model and better quality samples of newly generated 3D shapes. Further, our primitive generation approach can accurately parse common objects into a simplified representation.},
  keywords={Solid modeling;Computer vision;Three-dimensional displays;Shape;Image synthesis;Pattern recognition;Task analysis;3D from Single Image;Deep Learning;Image and Video Synthesis},
  doi={10.1109/CVPR.2019.00997},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9548749,
  author={Lee, Jung-Hoon and Kim, Kyeongrok and Kim, Jae-Hyun},
  booktitle={2021 IEEE VTS 17th Asia Pacific Wireless Communications Symposium (APWCS)}, 
  title={Design of CycleGAN model for SAR image colorization}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={In deep learning based image processing, the number of dataset is important to train the designed model. However, it is hard to secure SAR images, because satellite-based SAR is limited and high-resolution images are very expensive. Generative adversarial network (GAN) supplements this problem by learning two models, generator and discriminator, in an adversarial process at the same time. In this paper, we take one dataset as input data, and compare its accuracy using GAN models. CycleGAN is used to generate images among GAN models. Optical images are used for dataset and Chinese cities are selected for SAR images. The lack of dataset, a drawback of SAR images, is supplemented using data augmentation. SSIM, MSE, and PSNR of fake and original images are calculated using dataset and show that CycleGAN has the most lower MSE with 639.4379 and highest PSNR with 20.0728. Pix2pix has the most highest SSIM with 0.7842.},
  keywords={Wireless communication;Training;Machine learning algorithms;Image processing;Urban areas;Generative adversarial networks;Optical imaging;machine learning;CycleGAN;SAR;colorization},
  doi={10.1109/APWCS50173.2021.9548749},
  ISSN={},
  month={Aug},}@ARTICLE{10982262,
  author={Zhang, Haiming and Yuan, Zhihao and Zheng, Chaoda and Yan, Xu and Wang, Baoyuan and Li, Guanbin and Wu, Song and Cui, Shuguang and Li, Zhen},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance}, 
  year={2025},
  volume={31},
  number={10},
  pages={8231-8242},
  abstract={Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3D face model, which can synthesize smooth lip dynamics while preserving the speaker’s identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip-synchronization, and visual quality.},
  keywords={Faces;Three-dimensional displays;Lips;Solid modeling;Translation;Neural radiance field;Transformers;Training;Adaptation models;Shape;Deep learning;talking face video generation;transformer;generative models},
  doi={10.1109/TVCG.2025.3566382},
  ISSN={1941-0506},
  month={Oct},}@INPROCEEDINGS{9688418,
  author={Lee, Jung-Hoon and Kim, Kyeongrok and Kim, Jae-Hyun},
  booktitle={2021 7th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)}, 
  title={Flood Identification Model Design with Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={Deep learning has been widely used in various areas, such as detecting materials, or estimating natural disasters. Especially, generative adversarial network (GAN), which is one of the deep learning models, is enhanced to CycleGAN for generation and discrimination of images even with unpaired datasets. In this paper, we design a model to generate real-like fake flood models, and we confirm that we distinguish between real and fake images by mixing them with real images. Based on this metric, a deep learning model is designed, and a dataset is generated using CycleGAN. We further perform data augmentation to assist in the dataset generation process. The program used to design the model is Python, which uses data from Sentinel-l. Input data is a collection of data from floods during 2019 in West Africa, Southeast Africa, Middle East Asia, and Australia. To determine the accuracy of the generated data, we compare the image using several indicators. The used indicators judge the accuracy and similarity of images such as SSIM and MSE, and PSNR. SSIM, MSE, and PSNR averaged 0.7192, 2014.0066, and 15.5745, respectively. Comparing images with these indicators, we confirm that the actual flood image and the generated flood image are similar. And using generated images, we use different deep learning model, to confirm how similar the real flood image is to the flood image produced in this paper.},
  keywords={Deep learning;Training;Measurement;Africa;Generative adversarial networks;Radar polarimetry;Floods;SAR;CycleGAN;flood},
  doi={10.1109/APSAR52370.2021.9688418},
  ISSN={2474-2333},
  month={Nov},}@ARTICLE{10335751,
  author={Fang, Hao and Liu, Ajian and Wan, Jun and Escalera, Sergio and Zhao, Chenxu and Zhang, Xu and Li, Stan Z. and Lei, Zhen},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Surveillance Face Anti-Spoofing}, 
  year={2024},
  volume={19},
  number={},
  pages={1535-1546},
  abstract={Face Anti-spoofing (FAS) is essential to secure face recognition systems from various physical attacks. However, recent research generally focuses on short-distance applications (i.e., phone unlocking) while lacking consideration of long-distance scenes (i.e., surveillance security checks). In order to promote relevant research and fill this gap in the community, we collect a large-scale Su rveillance Hi gh-Fi delity Mask (SuHiFiMask) dataset captured under 40 surveillance scenes, which has 101 subjects from different age groups with  $232~3\text{D}$  attacks (high-fidelity masks),  $200~2\text{D}$  attacks (posters, portraits, and screens), and 2 adversarial attacks. In this scene, low image resolution and noise interference are new challenges faced in surveillance FAS. Together with the SuHiFiMask dataset, we propose a Contrastive Quality-Invariance Learning (CQIL) network to alleviate the performance degradation caused by image quality from three aspects: 1) An Image Quality Variable module (IQV) is introduced to recover image information associated with discrimination by combining the super-resolution network. 2) Using generated sample pairs to simulate quality variance distributions to help contrastive learning strategies obtain robust feature representation under quality variation. 3) A Separate Quality Network (SQN) is designed to learn discriminative features independent of image quality. Finally, a large number of experiments verify the quality of the SuHiFiMask dataset and the superiority of the proposed CQIL.},
  keywords={Surveillance;Face recognition;Three-dimensional displays;Cameras;Resins;Faces;Sensors;Face anti-spoofing;dataset;surveillance scenes},
  doi={10.1109/TIFS.2023.3337970},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10161289,
  author={Zhao, Qianfan and Zhang, Lu and He, Bin and Qiao, Hong and Liu, Zhiyong},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Zero-Shot Object Goal Visual Navigation}, 
  year={2023},
  volume={},
  number={},
  pages={2025-2031},
  abstract={Object goal visual navigation is a challenging task that aims to guide a robot to find the target object based on its visual observation, and the target is limited to the classes pre-defined in the training stage. However, in real households, there may exist numerous target classes that the robot needs to deal with, and it is hard for all of these classes to be contained in the training stage. To address this challenge, we study the zero-shot object goal visual navigation task, which aims at guiding robots to find targets belonging to novel classes without any training samples. To this end, we also propose a novel zero-shot object navigation framework called semantic similarity network (SSNet). Our framework use the detection results and the cosine similarity between semantic word embeddings as input. Such type of input data has a weak correlation with classes and thus our framework has the ability to generalize the policy to novel classes. Extensive experiments on the AI2-THOR platform show that our model outperforms the baseline models in the zero-shot object navigation task, which proves the generalization ability of our model. Our code is available at: https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation.},
  keywords={Training;Visualization;Correlation;Codes;Automation;Navigation;Semantics},
  doi={10.1109/ICRA48891.2023.10161289},
  ISSN={},
  month={May},}@ARTICLE{10354385,
  author={Wu, Junfei and Xu, Weizhi and Liu, Qiang and Wu, Shu and Wang, Liang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Adversarial Contrastive Learning for Evidence-Aware Fake News Detection With Graph Neural Networks}, 
  year={2024},
  volume={36},
  number={11},
  pages={5591-5604},
  abstract={The prevalence and perniciousness of fake news have been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from three weaknesses. First, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Second, they underestimate much redundant information contained in evidences that may be useless or even harmful. Third, insufficient data utilization limits the separability and reliability of representations captured by the model, which are sensitive to local evidence. To solve these problems, we propose a unified Graph-based sEmantic structure mining framework with ConTRAstive Learning, namely GETRAL in short. Specifically, different from the existing work that treats claims and evidences as sequences, we first model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Then the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Finally, the supervised contrastive learning accompanied with adversarial augmented instances is applied to make full use of data and strengthen the representation learning. Comprehensive experiments have demonstrated the superiority of GETRAL over the state-of-the-arts and validated the efficacy of semantic mining with graph structure and contrastive learning.},
  keywords={Semantics;Fake news;Task analysis;Termination of employment;Reliability;Graph neural networks;Data models;Evidence-based fake news detection;graph neural networks;contrastive learning},
  doi={10.1109/TKDE.2023.3341640},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{10153686,
  author={Zheng, Liming and Ma, Wenxuan and Cai, Yinghao and Lu, Tao and Wang, Shuo},
  journal={IEEE Robotics and Automation Letters}, 
  title={GPDAN: Grasp Pose Domain Adaptation Network for Sim-to-Real 6-DoF Object Grasping}, 
  year={2023},
  volume={8},
  number={8},
  pages={4585-4592},
  abstract={In this letter, we propose a novel Grasp Pose Domain Adaptation Network (GPDAN) to achieve sim-to-real domain adaptation for 6-DoF grasp pose detection. The main task of GPDAN is to detect feasible 6-DoF grasp poses in cluttered scenes. A point-wise self-supervised domain classification module with point cloud mixture and feature fusion strategy is proposed as the auxiliary task to promote the feature alignment between the source and target domain through adversarial training. Experimental results on both simulation and real-world environments demonstrate that GPDAN outperforms other approaches in detecting 6-DoF grasps on the target domain, highlighting the effectiveness of GPDAN in improving the performance of 6-DoF grasp pose detectors trained in simulation and deployed in real-world environments without any further laborious labeling.},
  keywords={Feature extraction;Task analysis;Point cloud compression;6-DOF;Training;Detectors;Grasping;Domain adaptation;grasp pose detection;feature alignment;sim-to-real},
  doi={10.1109/LRA.2023.3286816},
  ISSN={2377-3766},
  month={Aug},}@INPROCEEDINGS{9894305,
  author={Zhang, Hao and Min, Yuandong and Liu, Sanya and Tong, Hang and Li, Yaopeng},
  booktitle={2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)}, 
  title={MODLSTM: A Method to Recognize DoS Attacks on Modbus/TCP}, 
  year={2022},
  volume={},
  number={},
  pages={319-324},
  abstract={With the rapid development of technology, the scale of traffics in industrial control networks is increasing day by day. More malicious traffics brought terrible impacts on industrial areas. Modbus plays a momentous role in the communications of Industrial Control Systems (ICS), but it’s vulnerable to Denial of Service attacks(DoS). Traditional methods cannot perform well on fine-grained detection tasks which could contribute to locating targets of DoS and preventing the destruction. Considering the temporal locality and high dimension of malicious traffic, we proposed a Neural Network architecture named MODLSTM, which consists of three parts: input preprocessing, feature recoding, and traffic classification. By virtue of the design, MODLSTM can perform high-precision identification and fine-grained classification of DOS attacks in the Modbus/TCP-based system. To test our model’s performances, we conducted experiments on our traffic dataset collected from the industrial control network, and the models achieved excellent performances in comparison with previous work(accuracy increased by 0.74%). The results show that our proposed method has satisfactory abilities to detect DoS attacks related to Modbus, it could help to build a reliable firewall to address the DoS traffic in industrial environments.},
  keywords={Integrated circuits;Deep learning;Firewalls (computing);Industrial control;Neural networks;Data preprocessing;Computer architecture;Modbus;DoS;Deep Learning;Fine-grained Classification},
  doi={10.1109/IPCCC55026.2022.9894305},
  ISSN={2374-9628},
  month={Nov},}@INPROCEEDINGS{10260440,
  author={Wang, Yihan and Wang, Jiaxing and Wang, Weiqun and Su, Jianqiang and Hou, Zeng-Guang},
  booktitle={2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)}, 
  title={Calibration-Free Transfer Learning for EEG-Based Cross-Subject Motor Imagery Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Motor imagery based brain-computer interfaces (MI-BCIs) have been widely used in intelligent medical applications such as post-stroke rehabilitation and mobile assistant robots. However, the high inter-subject variability and the non-stationarity of EEG characteristics limit the cross-subject applications of MI-BCIs. Long-term calibration can be used to improve EEG-based performance, but which will cause low efficiency and reduce practicality. To overcome the limitation, data from other subjects can be used for transfer learning to reduce calibration time. Therefore, a calibration-free transfer learning method for EEG-based cross-subject MI classification is proposed in this paper. On one hand, Euclidean alignment and Riemannian alignment are introduced to reduce domain differences. On the other hand, the similarity is calculated by Multiple Kernel-Maximum Mean Discrepancy (MK-MMD) to select appropriate source domain samples, which is followed by domain adversarial training of neural network (DANN) for the final model construction. In order to achieve calibration-free, the new subjects' resting-state data was used only. Extensive experiments were conducted on BCI competition IV dataset 2a. The results show that the proposed method can achieve 75.96% classification accuracy without using subjects' labeled data, which demonstrates the feasibility of the proposed method in calibration time reduction and classification accuracy improvement.},
  keywords={Training;Computer aided software engineering;Transfer learning;Medical services;Brain modeling;Electroencephalography;Brain-computer interfaces;Calibration-free;transfer learning;motor imagery;cross-subject;EEG},
  doi={10.1109/CASE56687.2023.10260440},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{11127325,
  author={Yin, Lianhao and Ban, Yutong and Eckhoff, Jennifer and Meireles, Ozanan and Rus, Daniela and Rosman, Guy},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Hypergraph-Transformer (HGT) for Interaction Event Prediction in Laparoscopic and Robotic Surgery}, 
  year={2025},
  volume={},
  number={},
  pages={6846-6853},
  abstract={Understanding and anticipating events and actions is critical for intraoperative assistance and decision-making during minimally invasive surgery. We propose a predictive neural network that is capable of understanding and predicting critical interaction aspects of surgical workflow based on endoscopic, intracorporeal video data, while flexibly leveraging surgical knowledge graphs. The approach incorporates a hypergraph-transformer (HGT) structure that encodes expert knowledge into the network design and predicts the hidden embedding of the graph. We verify our approach on established surgical datasets and applications, including the prediction of action-triplets, and the achievement of the Critical View of Safety (CVS), which is a critical safety measure. Moreover, we address specific, safety-related forecasts of surgical processes, such as predicting the clipping of the cystic duct or artery without prior achievement of the CVS. Our results demonstrate improvement in prediction of interactive event when incorporating with our approach compared to unstructured alternatives.},
  keywords={Laparoscopes;Minimally invasive surgery;Ducts;Neural networks;Prediction methods;Knowledge graphs;Safety;Robots;Arteries;Videos},
  doi={10.1109/ICRA55743.2025.11127325},
  ISSN={},
  month={May},}@INPROCEEDINGS{10718785,
  author={Zhu, Zixun and Zhang, Jie and Wang, Junliang and Zhang, Mingzhi},
  booktitle={2024 29th International Conference on Automation and Computing (ICAC)}, 
  title={Lightweight Detection Method for Plaid Pattern Defects with Rank Self-selection Decomposition}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={To address the challenges posed by background patterns and the stringent real-time requirements in defect detection of plaid fabrics, we propose a lightweight defect detection method based on rank self-selection decomposition. Plaid background extraction algorithm is designed for obtaining the minimum periodic patterns of plaid fabrics. Filtering template parameters are derived through rank self-selection decomposition. Singular value decomposition and sparse representation are employed to eliminate the plaid background while preserving high-frequency defect areas. Subsequently, lightweight defect detection is then achieved through transfer learning, leveraging common defect features and transferring model parameters from grey fabric to plaid fabric. Experimental results show that the proposed method mitigates interference from complex plaid textures while achieving real-time, high-precision defect detection. The detection speed can reach up to 298 frames per second on four types of plaid pattern datasets, and up to 96.79% on mean average precision.},
  keywords={Automation;Sparse approximation;Filtering;Transfer learning;Interference;Feature extraction;Fabrics;Real-time systems;Defect detection;Singular value decomposition;plaid fabric defects;rank decomposition;transfer learning;lightweight detection},
  doi={10.1109/ICAC61394.2024.10718785},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10422625,
  author={Lu, Bo and Miao, Qinghai and Dai, Xingyuan and Lv, Yisheng},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={VCrash: A Closed-Loop Traffic Crash Augmentation with Pose Controllable Pedestrian}, 
  year={2023},
  volume={},
  number={},
  pages={5682-5687},
  abstract={Collecting and annotating traffic scene datasets is time-consuming and laborious. Virtual data can serve as an important supplement to the real-world traffic scene. Therefore, we propose a closed-loop data augmentation method based on car crash sequences in traffic scenes. Through the CARLA simulation platform, we obtain crash data that is difficult to collect in real-world traffic scenes, and this is then taken as the basis for subsequent pedestrian pose augmentation. To generate rare pedestrian poses, we specifically used boundary value sampling and ideas from genetic algorithms, which broadened the variety of pedestrian stances in the scenes. An efficient detection model was also employed to provide feedback signals for the closed-loop architecture. Based on this feedback, we created scenes for the corner case with various weather, city, and time conditions. The augmentation efficiency can be significantly improved by closed-loop construction. This approach enables us to produce a diverse and comprehensive dataset that can enhance the performance of pedestrian detection models in various traffic scenes.},
  keywords={Pedestrians;Urban areas;Data augmentation;Automobiles;Intelligent transportation systems;Accidents;Meteorology},
  doi={10.1109/ITSC57777.2023.10422625},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{10844932,
  author={Fernando, K.J.P. and Lakmal, H.K.I.S. and Dissanayake, M.B. and Kalansooriya, L.P.},
  booktitle={2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)}, 
  title={Enhancing Perception for Autonomous Driving Systems with Dual-Vision Input Cycle GAN for Night-to-Day Image Translation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Autonomous Driving Systems (ADS) and Advanced Driver Assistance Systems (ADAS) heavily rely on vision-based data for critical decision-making. However, these systems face significant performance limitations in low-light conditions where conventional RGB vision inputs struggle to capture information due to low light. Current solutions, including high-gain cameras or LiDAR-based approaches, are expensive and increase system complexity. This study presents a novel approach to address these challenges by employing a night-to-day image translation model with modified CycleGAN. The proposed model translates RGB and infrared (IR) inputs into a daytime representation, enhancing overall perception and object detection in low-light scenarios. Unlike existing methods, the model operates in an unsupervised learning framework, avoiding the need for large, paired datasets, which are often difficult to obtain. The study also introduces architectural modifications to the CycleGAN, including a dualinput framework and refined architecture, to optimize performance across varied conditions. The model’s performance is demonstrated through reference-less evaluation metrics, achieving average scores of FID 8.04, NIQE 4.92, and BRISQUE 36.3. These results illustrate the potential of the model to enhance the safety and operational stability of autonomous driving systems in low-light environments.},
  keywords={Solid modeling;Visualization;Translation;Computer architecture;Virtual reality;Generative adversarial networks;Safety;Advanced driver assistance systems;Autonomous vehicles;Unsupervised learning;Generative Adversarial Networks;Night to Day Image Translation;Unsupervised Learning;Autonomous Driving},
  doi={10.1109/SLAAI-ICAI63667.2024.10844932},
  ISSN={},
  month={Dec},}@ARTICLE{10123415,
  author={Wang, Zihan and Byrnes, Olivia and Wang, Hu and Sun, Ruoxi and Ma, Congbo and Chen, Huaming and Wu, Qi and Xue, Minhui},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Data Hiding With Deep Learning: A Survey Unifying Digital Watermarking and Steganography}, 
  year={2023},
  volume={10},
  number={6},
  pages={2985-2999},
  abstract={The advancement of secure communication and identity verification fields has significantly increased through the use of deep learning techniques for data hiding. By embedding information into a noise-tolerant signal, such as audio, video, or images, digital watermarking and steganography techniques can be used to protect sensitive intellectual property (IP) and enable confidential communication, ensuring that the information embedded is only accessible to authorized parties. This survey provides an overview of recent developments in deep learning techniques deployed for data hiding, categorized systematically according to model architectures and noise injection methods. In addition, potential future research directions that unite digital watermarking and steganography on software engineering to enhance security and mitigate risks are suggested and deliberated. This contribution furthers the creation of a more trustworthy digital world and advances responsible artificial intelligence (AI).},
  keywords={Media;Watermarking;Steganography;Deep learning;Data models;Surveys;Data mining;Artificial intelligence;Software engineering;Artificial intelligence (AI);cybersecurity;software engineering},
  doi={10.1109/TCSS.2023.3268950},
  ISSN={2329-924X},
  month={Dec},}@INPROCEEDINGS{9564347,
  author={Ebadi, Hamid and Moghadam, Mahshid Helali and Borg, Markus and Gay, Gregory and Fontes, Afonso and Socha, Kasper},
  booktitle={2021 IEEE International Conference on Artificial Intelligence Testing (AITest)}, 
  title={Efficient and Effective Generation of Test Cases for Pedestrian Detection - Search-based Software Testing of Baidu Apollo in SVL}, 
  year={2021},
  volume={},
  number={},
  pages={103-110},
  abstract={With the growing capabilities of autonomous vehicles, there is a higher demand for sophisticated and pragmatic quality assurance approaches for machine learning-enabled systems in the automotive AI context. The use of simulation-based prototyping platforms provides the possibility for early-stage testing, enabling inexpensive testing and the ability to capture critical corner-case test scenarios. Simulation-based testing properly complements conventional on-road testing. However, due to the large space of test input parameters in these systems, the efficient generation of effective test scenarios leading to the unveiling of failures is a challenge. This paper presents a study on testing pedestrian detection and emergency braking system of the Baidu Apollo autonomous driving platform within the SVL simulator. We propose an evolutionary automated test generation technique that generates failure-revealing scenarios for Apollo in the SVL environment. Our approach models the input space using a generic and flexible data structure and benefits a multi-criteria safety-based heuristic for the objective function targeted for optimization. This paper presents the results of our proposed test generation technique in the 2021 IEEE Autonomous Driving AI Test Challenge. In order to demonstrate the efficiency and effectiveness of our approach, we also report the results from a baseline random generation technique. Our evaluation shows that the proposed evolutionary test case generator is more effective at generating failure-revealing test cases and provides higher diversity between the generated failures than the random baseline.},
  keywords={Software testing;Quality assurance;Web and internet services;Software algorithms;Test pattern generators;Artificial intelligence;Autonomous vehicles;Search-Based Test Generation;Evolutionary Algorithm;Advanced Driver Assistance Systems;Pedestrian Detection;Automotive Simulators},
  doi={10.1109/AITEST52744.2021.00030},
  ISSN={},
  month={Aug},}@ARTICLE{10337784,
  author={Crum, Colton R. and Tinsley, Patrick and Boyd, Aidan and Piland, Jacob and Sweet, Christopher and Kelley, Timothy and Bowyer, Kevin and Czajka, Adam},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Explain to Me: Salience-Based Explainability for Synthetic Face Detection Models}, 
  year={2024},
  volume={5},
  number={6},
  pages={2766-2778},
  abstract={The performance of convolutional neural networks has continued to improve over the last decade. At the same time, as model complexity grows, it becomes increasingly more difficult to explain model decisions. Such explanations may be of critical importance for reliable operation of human–machine pairing setups, or for model selection when the “best”’ model among many equally accurate models must be established. Saliency maps represent one popular way of explaining model decisions by highlighting image regions models deem important when making a prediction. However, examining salience maps at scale is not practical. In this article, we propose five novel methods of leveraging model salience to explain a model behavior at scale. These methods ask: 1) what is the average entropy for a model's salience maps; 2) how does model salience change when fed out-of-set samples; 3) how closely does model salience follow geometrical transformations; 4) what is the stability of model salience across independent training runs; and 5) how does model salience react to salience-guided image degradations. To assess the proposed measures on a concrete and topical problem, we conducted a series of experiments for the task of synthetic face detection with two types of models: those trained traditionally with cross-entropy loss, and those guided by human salience when training to increase model generalizability. These two types of models are characterized by different, interpretable properties of their salience maps, which allows for the evaluation of the correctness of the proposed measures. We offer source codes for each measure along with this article.},
  keywords={Data models;Artificial intelligence;Entropy;Training;Predictive models;Loss measurement;Behavioral sciences;Biometrics;explainable AI;model salience;synthetic face detection},
  doi={10.1109/TAI.2023.3333310},
  ISSN={2691-4581},
  month={June},}@INPROCEEDINGS{10065162,
  author={Bharti, Komal and Das, Pradip K.},
  booktitle={2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)}, 
  title={A Survey on ASR Systems for Dysarthric Speech}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Recently Automatic Speech Recognition (ASR) has been widely overblown with many applications and assistance but orally challenged people, such as people with disordered speech, can’t get much benefits. Speech technologies are very useful on a daily basis to assist people with speech disorders. Dysarthria is a neurological speech disorder caused by significant injury in the left hemisphere of the brain. Dysarthric people have difficulty in the movement of speech-related muscles. As a result of strain on their speech muscles, individuals with dysarthria are able to generate limited speech data for analysis.In order to recognize speech of dysarthria sufferers, a robust technique is needed that can cope with extreme irregularity and narrow training data. This survey details a brief understanding of dysarthric speech characteristics and behavior. It also presents several attempts that have been made to make robust ASR systems for dysarthric speech.},
  keywords={Training;Training data;Speech recognition;Muscles;Behavioral sciences;Artificial intelligence;Injuries;Speech disorders;Dysarthric speech;Speech recognition system},
  doi={10.1109/AIST55798.2022.10065162},
  ISSN={},
  month={Dec},}@ARTICLE{10330895,
  author={Liu, Yexin and Zhang, Weiming and Zhao, Guoyang and Zhu, Jinjing and Vasilakos, Athanasios V. and Wang, Lin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation}, 
  year={2024},
  volume={5},
  number={10},
  pages={4893-4904},
  abstract={The ability to scene understanding in adverse visual conditions, e.g., nighttime, has sparked active research for color-thermal semantic segmentation. However, it is essentially hampered by two critical problems: 1) the day-night gap of color images is larger than that of thermal images; and 2) the classwise performance of color images at night is not consistently higher or lower than that of thermal images. We propose the first test-time adaptation (TTA) framework, dubbed Night-TTA, to address the problems for nighttime color-thermal semantic segmentation without access to the source (daytime) data during adaptation. Our method enjoys three key technical parts. First, as one modality (e.g., color) suffers from a larger domain gap than that of the other (e.g., thermal), imaging heterogeneity refinement (IHR) employs an interaction branch on the basis of color and thermal branches to prevent cross-modal discrepancy and performance degradation. Then, class aware refinement (CAR) is introduced to obtain reliable ensemble logits based on pixel-level distribution aggregation of the three branches. In addition, we also design a specific learning scheme for our TTA framework, which enables the ensemble logits and three student logits to collaboratively learn to improve the quality of predictions during the testing phase of our Night-TTA. Extensive experiments show that our method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mean intersection over union (mIoU).},
  keywords={Image color analysis;Semantic segmentation;Color;Reliability;Entropy;Cross-modal learning;nighttime segmentation;test-time adaptation (TTA)},
  doi={10.1109/TAI.2023.3336611},
  ISSN={2691-4581},
  month={Oct},}@INPROCEEDINGS{10581609,
  author={Zhao, Kai and Liu, Zhiming and Li, Chunquan and Liu, Jiaqi and Zhou, Jinbiao and Liao, Bihong and Tang, Huifang and Wang, Qiuyu},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Uncertainty-Driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={1561-1566},
  abstract={Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR. Second, an adversarial training strategy is introduced to calibrate the segmentation feature map and considers the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale adversarial loss directly improves the ability to discriminate the similarity between organizations. Experiments on the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and adversarial calibration learning can be used to provide additional information for modeling multi-scale ambiguities.},
  keywords={Training;Image segmentation;Uncertainty;Motion segmentation;Volume measurement;Estimation;Fats;component;Epicardial adipose tissue segmentation;Uncertainty-driven;Adversarial calibration;Bayesian estimation},
  doi={10.1109/AINIT61980.2024.10581609},
  ISSN={},
  month={March},}@INPROCEEDINGS{10880752,
  author={Mallick, A. and R., Rahul L. and Shaiju, A. and Neelapala, S.D. and Giri, L. and Sarkar, R. and Jana, S.},
  booktitle={2024 IEEE International Conference on E-health Networking, Application & Services (HealthCom)}, 
  title={AI-Based 3-Lead to 12-Lead ECG Reconstruction: Towards Smartphone-Based Public Healthcare}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Clinicians generally diagnose cardiovascular diseases (CVDs) using standard 12-Lead electrocardiogram (ECG). However, for smartphone-based public healthcare systems, a reduced 3-lead system may be preferred because of (i) increased portability, and (ii) reduced requirement for power, storage and bandwidth. Subsequently, clinicians require accurate 3-lead to 12-Lead ECG reconstruction, which has so far been studied only in the personalized setting. When each device is dedicated to one individual, artificial intelligence (AI) methods such as temporal long short-term memory (LSTM) and a further improved spatio-temporal LSTM-UNet combine have proven effective. In contrast, in the current smartphone-based public health setting where a common device is shared by many, developing an AI lead-reconstruction model that caters to the extensive ECG signal variability in the general population appears a far greater challenge. In this direction, we take a first step, and observe that the performance improvement achieved by a generative model, specifically, 1D Pix2Pix GAN (generative adversarial network), over LSTM-UNet is encouraging.},
  keywords={Performance evaluation;Bandwidth;Electrocardiography;Lead;Generative adversarial networks;Cardiovascular diseases;Public healthcare;Artificial intelligence;Long short term memory;Standards;Mobile healthcare;ECG lead Reconstruction;AI;LSTM;UNet;GAN},
  doi={10.1109/HealthCom60970.2024.10880752},
  ISSN={},
  month={Nov},}@INBOOK{10955645,
  author={Shah, Neel and Shah, Sneh and Bhanushali, Janvi and Bhatt, Nirav and Bhatt, Nikita and Mewada, Hiren},
  booktitle={Artificial Intelligence-Enabled Digital Twin for Smart Manufacturing}, 
  title={The Future of Manufacturing with AI and Data Analytics}, 
  year={2024},
  volume={},
  number={},
  pages={541-564},
  abstract={Summary <p>This chapter explores the potential of applying AI and data analytics to transform manufacturing. It provides an overview of new research trends in smart manufacturing, including the use of IoT, big data, and advanced AI technologies like machine learning and digital twins. The conceptual background of relevant AI approaches is discussed, including deep learning, reinforcement learning, unsupervised learning, and state&#x2010;of&#x2010;the&#x2010;art models. A key focus is examining the role of AI in predictive maintenance through data&#x2010;driven techniques for remaining useful life estimation, anomaly detection, prognostics, and optimizing maintenance strategies. Challenges and limitations such as noisy data, imbalanced datasets, and high computational requirements are addressed. The opportunities enabled by AI in manufacturing are highlighted, spanning synthetic data generation, real&#x2010;time prediction, and enhancing asset utilization. The chapter concludes that transformative gains in productivity, sustainability, and resilience will arise from thoughtfully leveraging AI and data to inform decision&#x2010;making in industrial settings. Adoption remains in the early stages, and realizing the full potential will require interdisciplinary collaboration and purposeful innovation.</p>},
  keywords={Maintenance;Manufacturing;Costs;Smart manufacturing;Predictive maintenance;Industrial Internet of Things;Digital twins;Data analysis;Big Data;Machine learning},
  doi={10.1002/9781394303601.ch23},
  ISSN={},
  publisher={Wiley},
  isbn={9781394303595},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10955645},}@ARTICLE{11016189,
  author={Ren, Zheyong and Pan, Yuhan and Chen, Jieyan and Zhao, Lin and Liao, Ming and Qian, Xuecheng and Gong, Wei},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Survey on Deep Learning-Based Chinese Font Style Transfer}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={In recent years, deep learning-based Chinese font style transfer has garnered extensive research. This field not only opens new possibilities for artistic creation but also provides powerful tools for generating personalized digital content, especially in the thriving era of AIGC. The complexity and diversity of Chinese characters pose significant challenges for font style transfer, prompting numerous solutions proposed by the research community. In light of these rapid advancements, we aim to provide a comprehensive review of the latest developments in Chinese font style transfer. Specifically, we first outline traditional methods, particularly those that existed before the emergence of deep learning techniques, to establish a theoretical foundation. Subsequently, we delve into the current mainstream deep learning methods, including Autoencoder, Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNNs), and their applications in Chinese font style transfer. While these methods have shown remarkable performance in handling the shapes, structures, and artistic styles of Chinese characters, they still face inherent challenges and limitations. Therefore, we propose an innovative set of solutions aimed at overcoming these obstacles and improving conversion effectiveness and practicality. This is the first comprehensive study on font style transfer methods based on deep learning technology. We hope to provide new ideas and directions for future research and application of Chinese font style transfer. The undertaking of this work enriches theoretical research on style transfer and offers substantial support for practical applications, possessing profound significance and widespread impact.},
  keywords={Brushes;Deep learning;Libraries;Training;Surveys;Solid modeling;Computational modeling;Shape;Painting;Deformation;Font style transfer;deep learning;font generation;GANs;Autoencoder},
  doi={10.1109/TAI.2025.3574300},
  ISSN={2691-4581},
  month={},}@ARTICLE{10979359,
  author={Pintelas, Emmanuel and Livieris, Ioannis E. and Pintelas, Panagiotis},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Quantization-based 3D-CNNs through Circular Gradual Unfreezing for DeepFake detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={In the dynamic domain of synthetic media, deepfakes challenge the trust in digital communication. The identification of manipulated content is essential to ensure the authenticity of shared information. Recent advances in deepfake detection focused on developing sophisticated CNN-based approaches. However, these approaches remain anchored within the continuous feature space, potentially missing manipulative signatures that might be more salient in a discrete domain. For this task, we propose a new strategy which combines insights from both continuous and discrete spaces for enhanced deepfake detection. Our hypothesis is that deepfakes may lie closer to a discrete space, potentially revealing hidden patterns that are not evident in continuous representations. In addition, we propose a new gradual-unfreezing technique, employed into the proposed framework in order to slowly adapt the network parameters to align with the new combined representation. Via comprehensive experimentation, it is highlighted the efficiency of the proposed approach, in comparison to state-of-the-art deepfake detection strategies.},
  keywords={Deepfakes;Quantization (signal);Training;Feature extraction;Artificial intelligence;Accuracy;Adaptation models;Data mining;Convolutional neural networks;Transformers;Convolutional neural networks;deepfake detection;quantization;gradual unfreezing;video classification},
  doi={10.1109/TAI.2025.3564903},
  ISSN={2691-4581},
  month={},}@INBOOK{10788736,
  author={Oshida, Yoshiki},
  booktitle={Artificial Intelligence for Medicine: People, Society, Pharmaceuticals, and Medical Materials}, 
  title={Chapter 13 AI in future}, 
  year={2021},
  volume={},
  number={},
  pages={431-471},
  abstract={},
  keywords={Medical services;Dogs;Artificial intelligence;Training;Lungs;Q-learning;Medical diagnostic imaging;Three-dimensional displays;Temperature measurement;Symbols},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783110717921},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10788736},}@INPROCEEDINGS{11011780,
  author={D, Diana Julie and P, Nidya and K, Pavithra and S, Saranya Devi},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Computer Aided Diagnosis Of Cervical Cancer Cells Using Deep Learning Methods}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper shows that the deep learning method can be applied to providing computer-aided diagnosis (CAD) of cervical cancer cells. This research studies the effect of CervicalNet, a dedicated deep neural network algorithm on Detecting and categorizing cervical cancer cells through the Mendeley Liquid-Based Cytology (LBC) dataset. This disease affects women across the world, ranks as the 4th common malignancy in women; early detection improves patient care outcome. The traditional techniques are always based on manual examination of cell samples by cytopathologists, procedures that are tedious and susceptible to human errors. This work illustrates that deep learning particularly CervicalNet, can assist with automating and improving detection in the diagnosis of cell images, by accurately categorizing them as well as identifying cancerous abnormalities with high precision. The aim is to highlight the efficiency of CAD Systems in reducing diagnostic time and improving accuracy in cervical cancer screening.},
  keywords={Deep learning;Solid modeling;Explainable AI;Manuals;Streaming media;Real-time systems;Reliability;Cervical cancer;Medical diagnostic imaging;Image classification;Cervical cancer;CervicalNet;LBC dataset;medical imaging;image classification},
  doi={10.1109/ICDSAAI65575.2025.11011780},
  ISSN={},
  month={March},}@INPROCEEDINGS{10853556,
  author={R, Nandhashree K. and M, Senthil Kumar and K, Susee S.},
  booktitle={2nd International Conference on Computer Vision and Internet of Things (ICCVIoT 2024)}, 
  title={A survey of machine learning applications in medical imaging for neurodegenerative disease diagnosis}, 
  year={2024},
  volume={2024},
  number={},
  pages={274-281},
  abstract={The increasing prevalence of brain degenerative disorders particularly Alzheimer's and Parkinson's, have incurred a dire need for effective diagnostic methodologies. With advancements in medical imaging techniques and Artificial Intelligence (AI), transformative potential exists to enhance early detection and disease classification. This review examines the recent progress made in the use of diagnostic medical imaging and artificial intelligence (AI) techniques for the early detection of Alzheimer's disease and Parkinson's disease. Despite significant progress, a comprehensive overview is lacking. The review aims to summarize and synthesize existing methods, critically evaluate their strengths and limitations, and identify emerging trends. Multimodal imaging data, combining information from different sources like MRI, PET, and SPECT, has shown progress in improving diagnostic accuracy and providing insights into disease progression. Advanced AI techniques, such as transfer learning and generative adversarial networks, have further enhanced the performance of diagnostic models. Previous studies have predominantly focused on using standalone techniques, often leaving the integration of multi-modal imaging and deep learning methodologies that could lead to more robust diagnostic frameworks. This literature review addresses this gap by synthesizing current research on medical imaging modalities, AI applications, and their implications in diagnosing Alzheimer's and Parkinson's diseases. This review seeks to connect technological innovations with clinical applications to establish more robust and dependable diagnostic methods.},
  keywords={},
  doi={10.1049/icp.2024.4435},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10581105,
  author={Kumar, Varun and Kumar, Dharmender},
  booktitle={2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)}, 
  title={Recent Trends in Text-to-Image GAN}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Generating genuine images from textual description is challenging for both computer vision and linguistic representation in text-to-image synthesis. Generative adversarial networks (GAN) are an emerging generative model that has been producing great results by generating high-quality images with diverse images. The present review provides an overview of GAN with its background like architecture, game theory key ideas, loss functions, performance metrics and challenges. Recent and relevant text-to-image GAN models are discussed, including Gradual Refinement GAN (GR-GAN), Generative Adversarial CLIPS (GALIP), GigaGAN and StyleGAN-T with their architecture, dataset used highlighting limitations, strengths, year, and applications and comparing their performance metrics like Inception Score (IS) and Fréchet Inception Distance (FID) with identifying future directions, such as drawing boundaries for open research challenges. The present review functions as an extensive knowledge base and is highly valuable for researchers and practitioners who are interested in learning more about text-to-image using GANs.},
  keywords={Measurement;Reviews;Knowledge based systems;Text to image;Computer architecture;Linguistics;Generative adversarial networks;Text-To-Image Synthesis;Generative Adversarial Network;GR-GAN;GALIP;GigaGAN;StyleGAN-T},
  doi={10.1109/ISCS61804.2024.10581105},
  ISSN={},
  month={May},}@ARTICLE{11119080,
  author={Li, Xiaoming and Zuo, Wangmeng and Loy, Chen Change},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Enhanced Generative Structure Prior for Chinese Text Image Super-Resolution}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Faithful text image super-resolution (SR) is challenging because each character has a unique structure and usually exhibits diverse font styles and layouts. While existing methods primarily focus on English text, less attention has been paid to more complex scripts like Chinese. In this paper, we introduce a high-quality text image SR framework designed to restore the precise strokes of low-resolution (LR) Chinese characters. Unlike methods that rely on character recognition priors to regularize the SR task, we propose a novel structure prior that offers structure-level guidance to enhance visual quality. Our framework incorporates this structure prior within a StyleGAN model, leveraging its generative capabilities for restoration. To maintain the integrity of character structures while accommodating various font styles and layouts, we implement a codebook-based mechanism that restricts the generative space of StyleGAN. Each code in the codebook represents the structure of a specific character, while the vector $w$ in StyleGAN controls the character's style, including typeface, orientation, and location. Through the collaborative interaction between the codebook and style, we generate a high-resolution structure prior that aligns with LR characters both spatially and structurally. Experiments demonstrate that this structure prior provides robust, character-specific guidance, enabling the accurate restoration of clear strokes in degraded characters, even for real-world LR Chinese text with irregular layouts. Our code and pre-trained models will be available at https://github.com/csxmli2016/MARCONetPlusPlus.},
  keywords={Image restoration;Layout;Degradation;Accuracy;Text recognition;Codes;Training;Superresolution;Transformers;Image recognition;Blind text image restoration;chinese character restoration;generative structure prior},
  doi={10.1109/TPAMI.2025.3596329},
  ISSN={1939-3539},
  month={},}@INPROCEEDINGS{9810762,
  author={Peltomäki, Jarkko and Spencer, Frankie and Porres, Ivan},
  booktitle={2022 IEEE/ACM 15th International Workshop on Search-Based Software Testing (SBST)}, 
  title={WOGAN at the SBST 2022 CPS Tool Competition}, 
  year={2022},
  volume={},
  number={},
  pages={53-54},
  abstract={WOGAN is an online test generation algorithm based on Wasser-stein generative adversarial networks. In this note, we present how WOGAN works and summarize its performance in the SBST 2022 CPS tool competition concerning the AI of a self-driving car.},
  keywords={Software testing;Conferences;Software algorithms;Generative adversarial networks;Autonomous automobiles;Test pattern generators;Artificial intelligence},
  doi={10.1145/3526072.3527535},
  ISSN={},
  month={May},}@ARTICLE{10756676,
  author={Park, Hyerim and Hong, Jinseok and Ko, Heejeong and Woo, Woontack},
  journal={IEEE Consumer Electronics Magazine}, 
  title={Revolutionizing Metaverse Content Creation: GenAI's Role and Future Directions}, 
  year={2025},
  volume={14},
  number={5},
  pages={60-67},
  abstract={For the growth of the metaverse-related consumer electronics market, it is essential that users from diverse backgrounds can easily create and share content on metaverse platforms. Generative AI (GenAI) has the potential to transform the metaverse into a major consumer industry by innovating asset acquisition and editing within authoring tools. This article analyzes the current applications of GenAI in metaverse content creation and proposes strategies for its effective use across various authoring stages, including data integration, content creation, and modification. In addition, it suggests solutions to address the lack of training data and enhance content quality.},
  keywords={Metaverse;Graphical user interfaces;Authoring systems;Three-dimensional displays;Solid modeling;Consumer electronics;Natural language processing;Knowledge graphs;Collaboration;User experience;Generative AI;Content management;Artificial intelligence},
  doi={10.1109/MCE.2024.3501512},
  ISSN={2162-2256},
  month={Sep.},}@INPROCEEDINGS{9298135,
  author={Dutta, Indira Kalyan and Ghosh, Bhaskar and Carlson, Albert and Totaro, Michael and Bayoumi, Magdy},
  booktitle={2020 11th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={Generative Adversarial Networks in Security: A Survey}, 
  year={2020},
  volume={},
  number={},
  pages={0399-0405},
  abstract={In the Information Age, the majority of data stored and transferred is digital; however, current security systems are not powerful enough to secure this data because they do not anticipate unknown attacks. With a growing number of attacks on cybersecurity systems defense mechanisms need to stay updated with the evolving threats. Security and their related attacks are an iterative pair of objects that learn to enhance themselves based upon each others' advances - a cybersecurity "arms race." In this survey, we focus on the various ways in which Generative Adversarial Networks (GANs) have been used to provide both security advances and attack scenarios in order to bypass detection systems. The aim of our survey is to examine works completed in the area of GANs, specifically device and network security. This paper also discusses new challenges for intrusion detection systems that have been generated using GANs. Considering the promising results that have been achieved in different GAN applications, it is very likely that GANs can shape security advances if applied to cybersecurity.},
  keywords={Security;Generative adversarial networks;Gallium nitride;Computer hacking;Training;Ransomware;Intrusion detection;Security;Generative Adversarial Networks;Cybersecurity;Machine Learning;Artificial Intelligence},
  doi={10.1109/UEMCON51285.2020.9298135},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9899769,
  author={Sun, Hanwen and Nie, Yuanping and Li, Xiang and Huang, Minhuan and Tian, Jianwen and Kong, Wei},
  booktitle={2022 7th IEEE International Conference on Data Science in Cyberspace (DSC)}, 
  title={An Automatic Code Generation Method Based on Sequence Generative Adversarial Network}, 
  year={2022},
  volume={},
  number={},
  pages={383-390},
  abstract={Automatic code generation has been around for a long time, and the core technologies used are constantly evolving. Around application development work in the demand for high quality code generated rapidly, this paper proposes a automatic code generation method based on sequence generative adversarial network, the method adopts the adversarial learning thought, using LSTM as a generator, CNN as a discriminator, both through against each other to generate higher quality code samples. Experiment results show that this method can generate code automatically at the second level, and is significantly better than the existing methods in terms of generation rate and accuracy, which explores a new way of automatic code generation.},
  keywords={Codes;Cyberspace;Data science;Generative adversarial networks;Generators;automatic code generation;artificial Intelligence;Generative Adversarial Networks},
  doi={10.1109/DSC55868.2022.00059},
  ISSN={},
  month={July},}@INPROCEEDINGS{9837581,
  author={Ma, Zhansong and Xu, Bingrong and Wang, Lei and Liu, Hanwen and Zeng, Zhigang},
  booktitle={2022 14th International Conference on Advanced Computational Intelligence (ICACI)}, 
  title={A Unified Weighted MMD For Unsupervised Domain Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={388-394},
  abstract={Unsupervised domain adaptation (UDA) recognizes unlabeled domain data by using the classifier learned from another domain. Previous works mainly focus on domain-level alignment that usually ignores the class-level information, resulting in the samples of different classes being too close to be classified correctly. To tackle this challenge, we design a unified weighted maximum mean discrepancy (MMD) metric method, that measures the differences in empirical distributions of two domains by calculating the weights of different sample pairs adaptively. The unified weighted MMD method is proposed which combines the class-level alignment with domain-level alignment, making full use of intra-domain, inter-domain, intra-class, and inter-class information with adaptive weights, and it is easy to implement. Experiment results demonstrate that our method can obtain superior results from two standard UDA datasets Office-31 and ImageCLEF-DA, compared with other UDA approaches.},
  keywords={Weight measurement;Measurement;Benchmark testing;Standards;Computational intelligence;unsupervised domain adaptation;unified weighted maximum mean discrepancy;adaptive weights},
  doi={10.1109/ICACI55529.2022.9837581},
  ISSN={},
  month={July},}@ARTICLE{10716549,
  author={Jiang, Dingde and Wang, Zhihao and Liu, Xinhui and Xu, Qi and Zou, Tao and Zhang, Ruyun and Tan, Lizhuang and Zhang, Peiying},
  journal={IEEE Internet of Things Journal}, 
  title={Toward Synthetic Network Traffic Generating in NTN-Enabled IoT: A Generative AI Approach}, 
  year={2025},
  volume={12},
  number={2},
  pages={2174-2187},
  abstract={Nonterrestrial networks (NTNs) enabled Internet of Things (IoT) extends connectivity to remote and underserved areas, enhances network reliability and coverage, and supports diverse IoT applications in challenging environments, such as rural, maritime, and disaster-stricken regions. As an emerging and fast-evolving IoT scheme, NTN-enabled IoT requires extensive evaluation to ensure effective deployment in real-world scenarios, such as connectivity, performance, and security evaluation. Since conducting testing in remote and diverse environments is logistically challenging and costly, we propose a generative artificial intelligence (GAI)-based synthetic traffic generation framework that facilitates comprehensive traffic analysis and performance evaluation. The proposed framework employs a GAI model to learn the traffic pattern and generate synthetic traffic from historical data. Our approach includes an embedding-based model for representing network flow attributes and a conditional generative adversarial network (CGAN) for generating traffic flows. Considering both source-destination information and statistical features achieves more comprehensive characterization of traffic flows. Finally, the simulation results demonstrate that the proposed approach can generate high quality traffic that conforms to real data distribution and shows obvious difference between multiple applications.},
  keywords={Internet of Things;Testing;Telecommunication traffic;Generative adversarial networks;Vectors;Performance evaluation;Vocabulary;Training;Traffic control;Load modeling;Generative AI (GAI);Internet of Things (IoT);nonterrestrial networks (NTNs);traffic generation;word embedding},
  doi={10.1109/JIOT.2024.3468209},
  ISSN={2327-4662},
  month={Jan},}@INPROCEEDINGS{10193089,
  author={Yamini, B and Jayaprakash, M and Logesswari, S and Ulagamuthalvi., V and Porselvi, R and Uthayakumar, G S},
  booktitle={2023 4th International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Enhanced Expectation-Maximization Algorithm for Smart Traffic IoT Systems using Deep Generative Adversarial Networks to Reduce waiting time}, 
  year={2023},
  volume={},
  number={},
  pages={380-385},
  abstract={The main focus of the work is to improve the smart traffic systems for the Internet of Things (IoT) such as traditional signals and safe driving via tracking the congestion and monitoring traffic slowly. Depending on the circumstances, intersections between roads can sometimes lead to accidents, so this problem is considered part of the research. These limitations can be overcome by using modern sensors that can reduce the traffic signal waiting time and rash driving on the National Highway (NH) road. Urban countries make use of AI-based decisions performing smart control over managing vehicles. In the proposed model the novel combination of collecting road maps coordinating points that can connect the access movements. Also IoT devices such as mobile phones, unusual network failure, and peak time traffic control by preprocessing the collected traffic dataset. Based on the dataset the detection of coverage area can be extracted using Enhanced Expectation Maximization Algorithm for ranking the variables. Initially, the classification algorithms were used and grouped as accident months, non-accident months, hill stations, and highway NH Road delay time are some of categories. Using Deep Generative Adversarial Network (DGAN) data can be balanced and used to reduce the waiting time along with predicting the smart cities’ transportation for reliable customers.},
  keywords={Roads;Transportation;Traffic control;Generative adversarial networks;Prediction algorithms;Classification algorithms;Internet of Things;Artificial Intelligence;Deep Learning;Enhanced Expectation-Maximization Algorithm;Internet of Things;National Highway road;Deep Generative Adversarial Network},
  doi={10.1109/ICESC57686.2023.10193089},
  ISSN={},
  month={July},}@ARTICLE{10845876,
  author={Zhang, Jifa and Sheng, Min and Xing, Chengwen and Liu, Junyu and Zhao, Nan and Karagiannidis, George K.},
  journal={IEEE Internet of Things Journal}, 
  title={Generative-Adversarial-Network-Enhanced DRL for ISAC With Double Active RISs}, 
  year={2025},
  volume={12},
  number={10},
  pages={13487-13499},
  abstract={integrated sensing and communication (ISAC) is a promising paradigm to alleviate spectrum congestion and facilitate a variety of emerging Internet of Things (IoT) applications. However, the direct links from the ISAC base station (BS) to the users may be blocked due to the obstacles. In this article, we investigate the double-active reconfigurable intelligent surfaces (RISs) assisted ISAC, where two active RISs are used to establish virtual line-of-sight (LoS) links from the ISAC BS to the users. In addition, the sum of the minimum sensing signal-to-interference-plus-noise ratios (SINRs) among multiple targets during a series of time slots is maximized, subject to Quality of Service (QoS) and transmit power constraints, through the joint optimization of transmit, reflection and receive beamforming. We first transform this nonconvex optimization problem in the dynamic environment into a Markov decision process (MDP), and then propose a twin delayed deep deterministic policy gradient (TD3)-based algorithm to solve it. Moreover, to enhance the generalization and stability, we integrate the generative adversarial network (GAN) into the TD3 algorithm and propose a GAN-TD3-based algorithm to handle the beamforming optimization problem. Compared with the TD3-based algorithm, the proposed GAN-TD3-based algorithm achieves the better performance and higher stability at the cost of higher computational complexity and slower convergence speed. Simulation results are presented to verify the effectiveness of our proposed algorithms and the superiority of the active RIS over the passive counterpart.},
  keywords={Array signal processing;Sensors;Optimization;Interference;Heuristic algorithms;Signal to noise ratio;Internet of Things;Generative adversarial networks;Vectors;Wireless sensor networks;Active reconfigurable intelligent surface (RIS);beamforming design;deep reinforcement learning (DRL);generative artificial intelligence (GAI);Internet of Things (IoT);integrated sensing and communication (ISAC)},
  doi={10.1109/JIOT.2025.3527441},
  ISSN={2327-4662},
  month={May},}@ARTICLE{9161395,
  author={Li, Huaiyu and Dong, Weiming and Hu, Bao-Gang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Incremental Concept Learning via Online Generative Memory Recall}, 
  year={2021},
  volume={32},
  number={7},
  pages={3206-3216},
  abstract={The ability to learn more concepts from incrementally arriving data over time is essential for the development of a lifelong learning system. However, deep neural networks often suffer from forgetting previously learned concepts when continually learning new concepts, which is known as the catastrophic forgetting problem. The main reason for catastrophic forgetting is that past concept data are not available, and neural weights are changed during incrementally learning new concepts. In this article, we propose an incremental concept learning framework that includes two components, namely, ICLNet and RecallNet. ICLNet, which consists of a trainable feature extractor and a dynamic concept memory matrix, aims to learn new concepts incrementally. We propose a concept-contrastive loss to alleviate the magnitude of neural weight changes and mitigate the catastrophic forgetting problems. RecallNet aims to consolidate old concepts memory and recall pseudo samples, whereas ICLNet learns new concepts. We propose a balanced online memory recall strategy to reduce the information loss of old concept memory. We evaluate the proposed approach on the MNIST, Fashion-MNIST, and SVHN data sets and compare it with other pseudorehearsal-based approaches. Extensive experiments demonstrate the effectiveness of our approach.},
  keywords={Task analysis;Learning systems;Neural networks;Feature extraction;Visualization;Knowledge engineering;Training;Catastrophic forgetting;continual learning;generative adversarial networks (GANs)},
  doi={10.1109/TNNLS.2020.3010581},
  ISSN={2162-2388},
  month={July},}@INPROCEEDINGS{9302595,
  author={Tarekegn, Getaneh Berie and Juang, Rong-Terng and Lin, Hsin-Piao and Munaye, Yirga Yayeh and Belay Adege, Abebe},
  booktitle={2020 International Conference on Pervasive Artificial Intelligence (ICPAI)}, 
  title={Reduce Fingerprint Construction for Positioning IoT Devices Based on Generative Adversarial Nets}, 
  year={2020},
  volume={},
  number={},
  pages={23-28},
  abstract={Fingerprint-based positioning is popular and applicable for Internet of Things (IoT) applications to offer seamless, intelligent and adaptive location-aware services for IoT devices. However, it takes time and cost to build the radio-map. This paper proposed deep convolutional Generative adversarial nets (DCGANs) to minimize the site survey time and cost, and to mitigate signal fluctuations. The radio-map was designed for receiving radio signals from detectable wireless local area network (WLAN) and cellular networks in scalable environments. The proposed fingerprinting-based positioning is a sequential combination of the hybrid support vector machine and long short-term memory algorithms. The experimental results indicate that the proposed method achieves a promising and reasonable positioning performance for IoT devices in scalable wireless environments.},
  keywords={Fingerprint recognition;Wireless communication;Wireless LAN;Internet of Things;Generators;Gallium nitride;Support vector machines;Deep Convolutional Generative Adversarial Network;Linear Discriminant Analysis;Long Short-Term Memory;Positioning;Radio signal;Support Vector Machine},
  doi={10.1109/ICPAI51961.2020.00012},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10058338,
  author={Duan, YiCong and Peng, Yu and Zhou, JianBao and Xue, Muyao},
  booktitle={2022 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Remaining Useful Life Prediction for Complex Electro-Mechanical System Based on Conditional Generative Adversarial Networks}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Remaining Useful Life (RUL) prediction is of significance to provide valuable information for implementing condition-based maintenance and repair. Except for the difficulty on formulating the physical model of the complex electro-mechanical system, another challenge is how to utilize the sparse samples to achieve accurate prediction results. To address this issue, this paper proposes a novel RUL prediction method based on the sample augmentation by the improved Conditional Generative Adversarial Networks (CGAN). The aircraft Auxiliary Power Unit (APU) is taken as a typical complex electro-mechanical object. Two-dimensional condition monitoring samples of the aircraft APU contain the potential degradation information, which bring difficulty for formulating an accurate and stable RUL prediction model. First, its two-dimension condition monitoring samples are augmented by the improved CGAN. Then, the augmented samples and the original samples are both utilized as the input of the RUL prediction method. Through comparison experiments on a practical sample set, the effectiveness of the proposed method is evaluated by different RUL prediction methods and combinations of samples.},
  keywords={Degradation;Condition monitoring;Electric potential;Data analysis;Atmospheric modeling;Estimation;Maintenance engineering;Electro-Mechanical System;Prognostic Health Management;Sample Augmentation;Conditional Generative Adversarial Networks},
  doi={10.1109/ICSMD57530.2022.10058338},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10762049,
  author={Bao, Shenglin and Jiang, Nan and Zhu, Weijie and Zhang, Pei},
  booktitle={2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={Generative Model-Based Test Case Generation and Operational Testing for Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={565-570},
  abstract={As deep learning technologies, particularly those driven by deep neural networks, revolutionize various industries, testing and verifying deep learning models remain challenging, especially regarding their security. Traditional software testing methods like test case selection and generation are often unsuitable for deep learning applications due to their reliance on high-cost, manually labeled data. This paper introduces a test case generation method using deep generative models and a testing framework tailored for operational testing. This approach reduces the need for data annotation and lowers the costs associated with testing deep learning software. The framework focuses on using training data alone to assess quality metrics in operational environments, without additional annotated test cases. Experimental results confirm the method’s efficacy in accurately evaluating the performance of deep learning models and identifying regression issues, offering significant advantages over traditional testing methods.},
  keywords={Deep learning;Software testing;Measurement;Industries;Training data;Production;Software;Security;Testing;Software engineering;deep learning;deep generative models;deep learning testing;test input generation;operational testing},
  doi={10.1109/ICBASE63199.2024.10762049},
  ISSN={},
  month={Sep.},}@ARTICLE{9810175,
  author={Zhu, Junchen and Gao, Lianli and Song, Jingkuan and Li, Yuan-Fang and Zheng, Feng and Li, Xuelong and Shen, Heng Tao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Label-Guided Generative Adversarial Network for Realistic Image Synthesis}, 
  year={2023},
  volume={45},
  number={3},
  pages={3311-3328},
  abstract={Generating photo-realistic images from labels (e.g., semantic labels or sketch labels) is much more challenging than the general image-to-image translation task, mainly due to the large differences between extremely sparse labels and detail rich images. We propose a general framework Lab2Pix to tackle this issue from two aspects: 1) how to extract useful information from the input; and 2) how to efficiently bridge the gap between the labels and images. Specifically, we propose a Double-Guided Normalization (DG-Norm) to use the input label for semantically guiding activations in normalization layers, and use global features with large receptive fields for differentiating the activations within the same semantic region. To efficiently generate the images, we further propose Label Guided Spatial Co-Attention (LSCA) to encourage the learning of incremental visual information using limited model parameters while storing the well-synthesized part in lower-level features. Accordingly, Hierarchical Perceptual Discriminators with Foreground Enhancement Masks are proposed to toughly work against the generator thus encouraging realistic image generation and a sharp enhancement loss is further introduced for high-quality sharp image generation. We instantiate our Lab2Pix for the task of label-to-image in both unpaired (Lab2Pix-V1) and paired settings (Lab2Pix-V2). Extensive experiments conducted on various datasets demonstrate that our method significantly outperforms state-of-the-art methods quantitatively and qualitatively in both settings.},
  keywords={Task analysis;Visualization;Semantics;Generators;Feature extraction;Adaptation models;Training;Generative Adversarial Networks (GANs);label-to-image synthesis;photo-realistic image generation},
  doi={10.1109/TPAMI.2022.3186752},
  ISSN={1939-3539},
  month={March},}@INPROCEEDINGS{10602417,
  author={Sasikala, G.M. and Anand, K.},
  booktitle={2024 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)}, 
  title={Evaluating the Efficiency of GAN-Generated Data in Deep Learning Models for Lung Disease Parameter Detection using CT and PET Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={This research delves into the inclusive evaluation of the effectiveness of engaging Generative Adversarial Network (GAN)-create dossier within deep education foundations for the exact identification of body part affliction limits utilizing a blend of Computed Tomography (CT) and Positron Emission Tomogra- phy (PET) countenances. By controlling the volume of GANs to produce authentic-facing artificial images, our study surrounds the preparation of deep learning architectures on a various dataset including honest and GAN-synthesized figures. Through perfectionist test and meticulous approximate surveillance, educate the potential of integrating GAN-produce dossier to help the efficacy of deep knowledge models in the detailed discovery of lung ailment gravestones. This investigation not only underscores the animation of artificial dossier augmentation but too showcases hopeful paths for advancing demonstrative veracity and refinement in the domain of healing image.},
  keywords={Computed tomography;Computational modeling;Surveillance;Pulmonary diseases;Lung;Positrons;Generative adversarial networks;Generative Adversarial Network (GAN);Deep Learning Models;Lung Disease Parameters;Computed Tomog- raphy (CT);Positron Emission Tomography (PET) Images;Syn- thetic Data Generation;Diagnostic Accuracy;Medical Imaging},
  doi={10.1109/ACCAI61061.2024.10602417},
  ISSN={},
  month={May},}@INPROCEEDINGS{9936741,
  author={Chengda, Ouyang and Abdullah, Noramalina},
  booktitle={2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={Wasserstein Generative Adversarial Networks with Meta Learning for Fault Diagnosis of Few-shot Bearing}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={In practical work situations, the bearing fault diagnosis is a small and imbalanced data challenge. However, the intelligent fault diagnosis model relies on a mass of label data. This research, presents a different method, Wasserstein GAN with Meta Learning, for overcoming the difficulty of few-shot fault diagnosis under imbalanced data constraints. The WGAN module can generate synthetic samples for the data argument, and the first-order model agnostic meta-learning (FOMAML) to initialize and modify the network parameters. Validation of the comparative performance has been made using a benchmark dataset, i.e. CWRU datasets, which show that can achieve excellent diagnostic accuracy with small data. It's successfully overcome that the imbalanced data lead to the sample distribution bias and over-fitting. In addition, it can leverage that can precisely identify the bearing fault health types in a variety of working environments, even with noise interference. It is also found that the proposed model performs better in the testing set after training difficult datasets.},
  keywords={Fault diagnosis;Training;Employee welfare;Interference;Benchmark testing;Generative adversarial networks;Data models;Bearing fault diagnosis;FOMAML;WGAN;few-shot},
  doi={10.1109/IICAIET55139.2022.9936741},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10882183,
  author={Wargantiwar, Tanmay and Chandekar, Yash and Agrawal, Pratik and Badhiye, Sagarkumar and Borkar, Pradnya},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Implementation of Generative AI in Resource Management of Cloud Computing}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Virtual computing however has recently emerged as the new way of accessing computing resources and services through the internet. In this regard scheduling algorithms holds a prominent position which would determine the cumulative use of the resources as well as the efficiency of the entire system. In this paper, the author's aim is to assess and compare various forms of the scheduling algorithms used in cloud computing systems. Some of the basic scheduling techniques include the First-in First-out (FIFO), Shortest Job First (SJF), Round Robin (RR), Priority Scheduling and few more complex scheduling techniques include the Multilevel Feedback Queues and the Shortest Timed Remaining (SRTF). Every one of them is described, compared and contrasted over this criterion:, including characteristics, strengths, weaknesses, and applications. In this section we present how scheduling decisions impact primary performance measures of response time, throughput, and user-to-user sharing criterion. Further, we present the ongoing issues and further research areas in the case of scheduling algorithms for cloud computing environment while elucidating the recent advancements in a cloud services and applications along with. It becomes beneficial for the system designers, cloud service providers as well as for the researchers to lift the efficiency of operation in cloud computing with the help of scheduling.},
  keywords={Cloud computing;Analytical models;Generative AI;Computational modeling;Transformers;Throughput;Data models;Time measurement;Resource management;Time factors;Cloud computing;Scheduling algorithms;Distribution of resource;Task scheduling;CPU scheduling;Virtual machine scheduling;Optimization;Performance;Response time;Throughput;Fairness;First Come First Serve (FCFS);Shortest Job First (SJF);Round Robin (RR);Multilevel Feedback Queue;Shortest Time Remaining First (SRTF);Preemptive scheduling;non-preemptive scheduling;Scalability},
  doi={10.1109/ICAIQSA64000.2024.10882183},
  ISSN={},
  month={Dec},}@ARTICLE{10352174,
  author={Wang, Xiao and Wang, Yutong and Netto, Mariana and Stapleton, Larry and Wan, Zhe and Wang, Fei-Yue},
  journal={IEEE Intelligent Systems}, 
  title={Smart Decentralized Autonomous Organizations and Operations for Smart Societies: Human–Autonomous Organizations for Industry 5.0 and Society 5.0}, 
  year={2023},
  volume={38},
  number={6},
  pages={70-74},
  abstract={This article explores the concept of human–autonomous organizations (HAOs) based on decentralized autonomous organizations (DAOs) and operations as well as human, artificial, natural, and organizational intelligence and their roles in shaping smart societies in the context of Industry 5.0 and Society 5.0. It discusses the potential of AI-generated content and prompt engineering in specific goal-guided manufacture and governance. Additionally, the article introduces the concept of the HAO as a framework for integrating human intelligence to achieve fair, transparent, and accountable decision making within DAOs. The proposed HAO reduces the risk of instability and unreliability in “human-in-the-loop” copilot systems and human–machine hybrid systems, leading to more reliable, secure, and flexible systems. It provides insights into the future management of smart societies and the symbiotic relationship between human ingenuity and the suite of emerging new AI technologies.},
  keywords={Industries;Symbiosis;Ethics;Decentralized autonomous organization;Human intelligence;Decision making;Market research;Smart devices;Smart cities;Human factors;Human computer interaction},
  doi={10.1109/MIS.2023.3324471},
  ISSN={1941-1294},
  month={Nov},}@ARTICLE{9189823,
  author={Ding, Han and Gao, Robert X. and Isaksson, Alf J. and Landers, Robert G. and Parisini, Thomas and Yuan, Ye},
  journal={IEEE/ASME Transactions on Mechatronics}, 
  title={State of AI-Based Monitoring in Smart Manufacturing and Introduction to Focused Section}, 
  year={2020},
  volume={25},
  number={5},
  pages={2143-2154},
  abstract={Over the past few decades, intelligentization, supported by artificial intelligence (AI) technologies, has become an important trend for industrial manufacturing, accelerating the development of smart manufacturing. In modern industries, standard AI has been endowed with additional attributes, yielding the so-called industrial artificial intelligence (IAI) that has become the technical core of smart manufacturing. AI-powered manufacturing brings remarkable improvements in many aspects of closed-loop production chains from manufacturing processes to end product logistics. In particular, IAI incorporating domain knowledge has benefited the area of production monitoring considerably. Advanced AI methods such as deep neural networks, adversarial training, and transfer learning have been widely used to support both diagnostics and predictive maintenance of the entire production process. It is generally believed that IAI is the critical technologies needed to drive the future evolution of industrial manufacturing. This article offers a comprehensive overview of AI-powered manufacturing and its applications in monitoring. More specifically, it summarizes the key technologies of IAI and discusses their typical application scenarios with respect to three major aspects of production monitoring: fault diagnosis, remaining useful life prediction, and quality inspection. In addition, the existing problems and future research directions of IAI are also discussed. This article further introduces the papers in this focused section on AI-based monitoring in smart manufacturing by weaving them into the overview, highlighting how they contribute to and extend the body of literature in this area.},
  keywords={Production;Monitoring;Smart manufacturing;Fault diagnosis;Machine learning;Artificial intelligence (AI);deep learning;fault diagnosis (FD);machine learning;quality inspection (QI);remaining useful life prediction (RULP);smart manufacturing},
  doi={10.1109/TMECH.2020.3022983},
  ISSN={1941-014X},
  month={Oct},}@ARTICLE{11015722,
  author={Li, Dawei and Ren, Yangkun and Liu, Di and Bian, Song and Guan, Zhenyu and Susilo, Willy and Liu, Jianwei and Wu, Qianhong},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={LOGO-Based Intellectual Property Right Protection Scheme for GANs on FPGA}, 
  year={2025},
  volume={22},
  number={5},
  pages={5701-5712},
  abstract={In recent years, Generative Adversarial Networks (GANs) have become essential tools in artificial intelligence research. Field Programmable Gate Arrays (FPGAs) offer remarkable flexibility, high performance, and energy efficiency for deploying GANs. However, the open and reprogrammable architecture of FPGAs, despite its advantages, introduces risks of unauthorized access and reverse engineering. To address this challenge, this article presents a novel approach integrating Physical Unclonable Functions (PUFs) and logos to protect the Intellectual Property Rights (IPR) of GANs. Our method establishes a closed-loop conversion process where logos are transformed into PUF responses, generating unique identities fed into the GAN to reproduce the original logo. By embedding PUF response information into latent vectors, the generator produces images with embedded logos. Thanks to the uniqueness of PUF, a robust binding of the logo, FPGA, and GANs’ IPR is implemented, allowing verification of the IPR with the assistance of a unique FPGA fingerprint, even when a publicly available logo is used. Experimental results show that embedding the logo does not change the performance of the original GANs, and the logo detection rate exceeds 90%. At the same time, the scheme can effectively resist brute force, fine-tuning and pruning attacks.},
  keywords={Watermarking;Field programmable gate arrays;Intellectual property;Computational modeling;Generators;Physical unclonable function;Training;Protection;Artificial neural networks;Generative adversarial networks;Generative adversarial network;physical unclonable function;intellectual property right;FPGA},
  doi={10.1109/TDSC.2025.3571715},
  ISSN={1941-0018},
  month={Sep.},}@INPROCEEDINGS{11095910,
  author={Figueroa-Mora, Karina M. and Ortiz-Herrera, Ángel L. and Casillas-Farfán, Christian E. and Rodriguez, Kattia},
  booktitle={2024 13th International Conference On Software Process Improvement (CIMPS)}, 
  title={LiveQuiz as Intelligent Tutor Using GenAI}, 
  year={2024},
  volume={},
  number={},
  pages={298-303},
  abstract={Nowadays, the use of technology in all life processes is inevitable. Education and teaching processes are precisely those systems that have been revolutionized rapidly, not only with the modernization of information communication technology tools but also with the use of Artificial Intelligence (AI) and now with Generative Artificial Intelligence (GenAI). This article presents the methodology for transforming Live- Quiz [1] (an online gaming platform) into an intelligent tutor by leveraging GenAI. LiveQuiz is an online game in which questions are posed, and students must type in the answers. Students could make minor mistakes during the game, do not understand the question, or just the do not know the answer. In theses scenarios, GenAI will provide encouraging messages and hints to the players and give feedback during the game.},
  keywords={Games;Chatbots;Education;Artificial intelligence;Proposals;Hands;User interfaces;Tutorials;Software;Training;Intelligent Tutor Systems;Learning environments},
  doi={10.1109/CIMPS65195.2024.11095910},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10705275,
  author={Ince, Volkan and Bader-El-Den, Mohamed and Sari, Omer Faruk},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Enhanced Dataset Synthesis Using CTGAN for Metagenomic Dataset}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The examination of bacterial communities has increasingly relied on machine learning methods and metagenomic analysis, providing novel solutions across various domains. However, the restricted size of metagenomic datasets presents challenges for robust model training. Consequently, data augmentation techniques, such as Conditional Tabular Generative Adversarial Networks (CTGAN), have obtained attention. This study seeks to utilize machine learning algorithms, incorporating CTGAN, to assess the influence of microbial community composition on the growth patterns of Clostridium bacteria in metagenomic dataset. Additionally, the study employs SHAP analysis to explain feature importance and contrast model performance pre- and post-data augmentation. The findings demonstrate notable enhancements in classification metrics subsequent to data augmentation, particularly evident when excluding the “Day” feature. Moreover, SHAP analysis identifies pivotal features, notably the absence of the “Day” variable post-CTGAN synthesis, emphasizing the significance of specific bacterial genera like Clostridium in bacterial growth dynamics. Overall, this study underscores the efficacy of data augmentation techniques, specifically CTGAN, in enhancing machine learning model performance for metagenomic data classification tasks, with implications for refining food safety and healthcare protocols. Further research could explore advanced data augmentation methodologies and validate outcomes on more expansive datasets for practical implementation.},
  keywords={Training;Measurement;Metagenomics;Protocols;Machine learning algorithms;Refining;Machine learning;Medical services;Data augmentation;Safety;Supervised machine learning;Explainable AI;Generative AI;Metagenomic data},
  doi={10.1109/IS61756.2024.10705275},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{9418045,
  author={Bushra, S. Nikkath and Ali, L. Javid},
  booktitle={2021 5th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={A Review on Fuzzy Face Recognition (FFR) using DCGAN}, 
  year={2021},
  volume={},
  number={},
  pages={1299-1305},
  abstract={AI based Facial recognition system has a number of applications in the area of biometric for authenticating a person, smart card identification, AI driven surveillance camera to identify criminals, health sectors, public security system to recognize suspected persons and as well as criminals in society and security mechanism in industry is also based on facial recognition system to identify their employees. Images used in Facial recognition system especially in real scenario have low resolution pictures taken under dark background with inadequate illumination, blurred images taken from low end cameras which has poor pixel quality. It is a cumbersome process to detect the face of a person exactly from inadequate information provided. To obtain precise output, huge amount of input data is required for training the neural network. This paper presents a comprehensive review on facial recognition system for improving improper, low quality and low light images by applying Artificial Intelligence based image and video analysis technique called Deep Convolutional Generative Adversarial Neural Network (DCGAN). DCGAN is commonly applied when the datasets are insufficient for training the neural network. The images to be detected under environment with poor illumination, due to climatic changes or due to capturing low resolution images using low quality cameras. Description regarding training and testing various datasets extracted from several freely available sources has been provided. The enactment of each model is considered and compared with other state-of-art models with respect to accuracy, complexity and execution time.},
  keywords={Training;Image resolution;Face recognition;Neural networks;Lighting;Cameras;Generative adversarial networks;Deep Convolutional Generative Adversarial Network (DCGAN);Illumination;Face recognition;Low Resolution;Artificial Intelligence},
  doi={10.1109/ICCMC51019.2021.9418045},
  ISSN={},
  month={April},}@ARTICLE{9716798,
  author={Wang, Haishuai and Tao, Guangyu and Ma, Jiali and Jia, Shangru and Chi, Lianhua and Yang, Hong and Zhao, Ziping and Tao, Jianhua},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Predicting the Epidemics Trend of COVID-19 Using Epidemiological-Based Generative Adversarial Networks}, 
  year={2022},
  volume={16},
  number={2},
  pages={276-288},
  abstract={The Coronavirus disease 2019 (COVID-19) is a respiratory illness that can spread from person to person. Since the COVID-19 pandemic is spreading rapidly over the world and its outbreak has affected different people in different ways, it is significant to study or predict the evolution of its epidemic trend. However, most of the studies focused solely on either classical epidemiological models or machine learning models for COVID-19 pandemic forecasting, which either suffer from the limitation of the generalization ability and scalability or the lack of surveillance data. In this work, we propose T-SIRGAN that integrates the strengths of the epidemiological theories and deep learning models to be able to represent complex epidemic processes and model the non-linear relationship for more accurate prediction of the growth of COVID-19. T-SIRGAN first adopts the Susceptible-Infectious-Recovered (SIR) model to generate epidemiological-based simulation data, which are then fed into a generative adversarial network (GAN) as adversarial examples for data augmentation. Then, Transformers are used to predict the future trends of COVID-19 based on the generated synthetic data. Extensive experiments on real-world datasets demonstrate the superiority of our method. We also discuss the effectiveness of vaccine based on the difference between the predicted and the reported number of COVID-19 cases.},
  keywords={COVID-19;Predictive models;Mathematical models;Data models;Pandemics;Market research;Biological system modeling;COVID-19;generative adversarial networks;time series prediction;SIR simulation},
  doi={10.1109/JSTSP.2022.3152375},
  ISSN={1941-0484},
  month={Feb},}@ARTICLE{8667687,
  author={Ong, Desmond C. and Soh, Harold and Zaki, Jamil and Goodman, Noah D.},
  journal={IEEE Transactions on Affective Computing}, 
  title={Applying Probabilistic Programming to Affective Computing}, 
  year={2021},
  volume={12},
  number={2},
  pages={306-317},
  abstract={Affective Computing is a rapidly growing field spurred by advancements in artificial intelligence, but often, held back by the inability to translate psychological theories of emotion into tractable computational models. To address this, we propose a probabilistic programming approach to affective computing, which models psychological-grounded theories as generative models of emotion, and implements them as stochastic, executable computer programs. We first review probabilistic approaches that integrate reasoning about emotions with reasoning about other latent mental states (e.g., beliefs, desires) in context. Recently-developed probabilistic programming languages offer several key desidarata over previous approaches, such as: (i) flexibility in representing emotions and emotional processes; (ii) modularity and compositionality; (iii) integration with deep learning libraries that facilitate efficient inference and learning from large, naturalistic data; and (iv) ease of adoption. Furthermore, using a probabilistic programming framework allows a standardized platform for theory-building and experimentation: Competing theories (e.g., of appraisal or other emotional processes) can be easily compared via modular substitution of code followed by model comparison. To jumpstart adoption, we illustrate our points with executable code that researchers can easily modify for their own models. We end with a discussion of applications and future directions of the probabilistic programming approach.},
  keywords={Computational modeling;Probabilistic logic;Programming;Object oriented modeling;Cognition;Psychology;Affective computing;Emotion recognition;Artificial intelligence;Affective computing;artificial intelligence;emotion theory;modeling human emotion},
  doi={10.1109/TAFFC.2019.2905211},
  ISSN={1949-3045},
  month={April},}@INPROCEEDINGS{9515093,
  author={Ramwala, Ojas A. and Dhakecha, Smeet A. and Ganjoo, Antriksh and Visiya, Divyanshu and Sarvaiya, Jignesh N.},
  booktitle={2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, 
  title={Leveraging Adversarial Training for Efficient Retinal Vessel Segmentation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Detection and diagnosis of retinal-related diseases is a crucial problem in the field of ophthalmology. Segmentation of blood vessels forms the basis for the identification of several ailments. Manual inspection of the retinal blood vessels necessitates utmost expertise and focus. Moreover, it is a strenuous and cumbersome process. It becomes imperative to develop a reliable automatic retinal vessel segmentation algorithm that can reduce the burden on the medical fraternity. This paper leverages adversarial training techniques to deploy an accurate deep learning architecture for efficacious retinal vessel segmentation. A Conditional Generative Adversarial Network with U-Net architecture as the Generative model and Patch-GAN architecture as the Discriminative model has been developed. Furthermore, Depth-Wise Separable Convolutions have been utilized to ameliorate the efficiency of the architecture. The incorporation of Dice Loss with L1 Loss in the GAN objective has enhanced the proposed architecture's precision in identifying even thin blood vessels in the retina. To allow rapid convergence of the loss function, the NADAM optimizer has been incorporated. Quantitative and qualitative comparison with several state-of-the-art methods indicate the improved performance of the proposed method and establishes the developed architecture's clinical reliability.},
  keywords={Training;Computer network reliability;Computer architecture;Blood vessels;Generative adversarial networks;Retinal vessels;Reliability;Conditional Generative Adversarial Network;Depth-Wise Separable Convolution;Dice Loss;NADAM;Patch-GAN;U-NET},
  doi={10.1109/ECAI52376.2021.9515093},
  ISSN={},
  month={July},}@ARTICLE{10631666,
  author={Jonnalagedda, Padmaja and Weinberg, Brent and Min, Taejin L. and Bhanu, Shiv and Bhanu, Bir},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Comprehensive Radiogenomic Feature Characterization of 19/20 Co-gain in Glioblastoma}, 
  year={2024},
  volume={5},
  number={12},
  pages={6442-6456},
  abstract={The prognosis and treatment planning of glioblastoma multiforme (GBM) involves a holistic analysis of imaging, clinical, and molecular data. The correlation of imaging and molecular features has garnered much interest due to its potential to reduce the number of invasive procedures on a patient and resource utilization of the overall prognostic and treatment planning process. This article detects and characterizes the impact of tumor biomarkers (such as shape, texture, location, and the tissue surrounding the tumor) in detecting a prognostic mutation – the concurrent gain of 19 and 20 chromosomes, and proposes two novel ideas for this analysis. First, to address the challenges associated with the limited, diverse, and complex nature of medical data, this article proposes a novel generative model – the realistic radiogenomic design using disentanglement in generative adversarial networks (R2D2-GAN), designed to recreate highly subtle, unapparent manifestations of mutations in magnetic resonance imaging. It generates high-resolution, diverse data that captures the discriminatory visual features of the molecular markers while tackling the high diversity, unbalanced, and limited GBM data with rare mutations correlating with patient survival such as 19/20 co-gain. Second, this study proposes a quantitative metric called the synthetic image fidelity (SIF) score to evaluate the performance of GANs in learning visually unapparent prognostic features through the use of gradient-based model explanations. Results are compared with current methods.},
  keywords={Tumors;Generative adversarial networks;Task analysis;Shape;Data models;Visualization;Magnetic resonance imaging;GAN evaluation score;generative adversarial networks (GANs);glioblastoma;limited patient data;MRI},
  doi={10.1109/TAI.2024.3440219},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{11042317,
  author={Muralidharan, Adarsh and Aji, Minsa and Jacob, Rohan Jacob and Cherukat, Shasna and C V, Priya},
  booktitle={2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)}, 
  title={TransLip: AI-Driven Video Translation and Synchronization}, 
  year={2025},
  volume={1},
  number={},
  pages={1-6},
  abstract={The increasing need for region-specific digital content has spurred progress in technologies like video translation and lip synchronization. This work introduces an innovative system for converting video and audio from English into Malayalam, a regional language while achieving precise lip synchronization. The process involves extracting audio and converting speech to text, translating the text and generating speech in the target language. The synchronized audio is then aligned with the speaker’s lip movements using the Generative Adversarial Network(GAN) powered Wav2Lip model. The result is a cohesive video where the translated audio and lip movement are in harmony, delivering a lifelike and engaging experience for viewers. This approach offers substantial benefits in sectors like entertainment, education, and media localization, broadening access to content while upholding the integrity of the original material.},
  keywords={Location awareness;Translation;Lips;Education;Entertainment industry;Streaming media;Media;Real-time systems;Synchronization;Speech to text;Video translation;Lip synchronization;Malayalam;Generative Adversarial Network;Wav2Lip;Speech to text;Translated audio;Media localization;Lip movements},
  doi={10.1109/ICTEST64710.2025.11042317},
  ISSN={},
  month={April},}@ARTICLE{9769950,
  author={Duan, Huiyu and Shen, Wei and Min, Xiongkuo and Tian, Yuan and Jung, Jae-Hyun and Yang, Xiaokang and Zhai, Guangtao},
  journal={IEEE Transactions on Multimedia}, 
  title={Develop Then Rival: A Human Vision-Inspired Framework for Superimposed Image Decomposition}, 
  year={2023},
  volume={25},
  number={},
  pages={4267-4281},
  abstract={A single superimposed image containing two image views causes visual confusion for both human vision and computer vision. Human vision needs a “develop-then-rival” process to decompose the superimposed image into two individual images, which effectively suppresses visual confusion. However, separating individual image views from a single superimposed image has been an important but challenging task in computer vision area for a long time. In this paper, we propose a human vision-inspired framework for single superimposed image decomposition. We first propose a network to simulate the development stage, which tries to understand and distinguish the semantic information of the two layers of a single superimposed image. To further simulate the rivalry activation/suppression process in human brains, we carefully design a rivalry stage, which incorporates the original mixed input (superimposed image), the activated visual information (outputs of the development stage) together, and then rivals to get images without ambiguity. Experimental results show that our novel framework effectively separates the superimposed images and significantly improves the performance with better output quality compared with state-of-the-art methods. The proposed method also achieves state-of-the-art results on related applications including single image reflection removal, single image rain removal, single image shadow removal, and illumination correction, etc., which validates the generalization of the framework.},
  keywords={Visualization;Task analysis;Rain;Image decomposition;Lighting;Generative adversarial networks;Computer vision;Develop then rival;illumination correction;rain removal;reflection removal;shadow removal;superimposed image decomposition},
  doi={10.1109/TMM.2022.3172882},
  ISSN={1941-0077},
  month={},}@ARTICLE{9309095,
  author={Chen, Xingyu and Li, Jin and Lan, Xuguang and Zheng, Nanning},
  journal={IEEE Transactions on Multimedia}, 
  title={Generalized Zero-Shot Learning Via Multi-Modal Aggregated Posterior Aligning Neural Network}, 
  year={2022},
  volume={24},
  number={},
  pages={177-187},
  abstract={The visual-semantic gap between the visual space (visual features) and semantic space (semantic attributes) is one of the main problems in the Generalized Zero-Shot Learning (GZSL) task. The essence of this problem is that the structure of manifolds in these two spaces is inconsistent, which makes it difficult to learn embeddings that unify visual features and semantic attributes for similarity measurement. In this work, we tackle this problem by proposing a multi-modal aggregated posterior aligning neural network based on Wasserstein Auto-encoders (WAE) which learns a shared latent space for visual features and semantic attributes. The key to our approach is that the aggregated posterior distribution of the latent representations encoded from visual features of each class is encouraged to be aligned with a Gaussian distribution predicted by the corresponding semantic attribute in the latent space. On one hand, requiring the latent manifolds of visual features and semantic attributes to be consistent preserves the inter-class association between seen and unseen classes. On the other hand, the aggregated posterior of each class is directly defined as a Gaussian in the latent space, which provides a reliable way to synthesize latent features for training classification models. Using the AWA1, AWA2, CUB, aPY, FLO, and SUN benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over state-of-the-art approaches.},
  keywords={Semantics;Visualization;Generative adversarial networks;Gallium nitride;Training;Task analysis;Gaussian distribution;Aggregated posterior distribution alignment;generalized zero-shot learning;multi-modal neural network},
  doi={10.1109/TMM.2020.3047546},
  ISSN={1941-0077},
  month={},}@ARTICLE{9669049,
  author={Wu, Jipeng and Ji, Rongrong and Wang, Qiang and Zhang, Shengchuan and Sun, Xiaoshuai and Wang, Yan and Xu, Mingliang and Huang, Feiyue},
  journal={IEEE Transactions on Multimedia}, 
  title={Fast Monocular Depth Estimation via Side Prediction Aggregation with Continuous Spatial Refinement}, 
  year={2023},
  volume={25},
  number={},
  pages={1204-1216},
  abstract={Recent works have validated the benefit of integrating spatial information into deep networks to improve pixel-level prediction tasks such as monocular depth estimation. However, how to efficiently and robustly integrate spatial cues retains as an open problem. In this paper, we introduce the Side Prediction Aggregation (termed SPA) method to enhance the embedding of scene structural information from low-level to high-level layers. To improve the estimation accuracy, the proposed method is further equipped with continuous Spatial Refinement Loss (termed SRL) at multiple resolutions with negligible extra computation. Besides, the proposed sequential network can further perform adversarial learning at multiple resolutions. Such an adversarial refinement strategy greatly improves the accuracy of estimated depth with a little extra computation. Without using any pre-trained models, our network achieves the the-state-of-art accuracy on KITTI, NYUD V2, and Cityscapes datasets, which has achieved real-time depth estimation online.},
  keywords={Estimation;Predictive models;Task analysis;Spatial resolution;Real-time systems;Generators;Generative adversarial networks;Adversarial network;depth estimation;side output;spatial refinement constraint},
  doi={10.1109/TMM.2021.3140001},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{11016298,
  author={Naeem, Usman and Styve, Arne and Virkki, Outi T.},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Stimulating Critical Thinking in a Web Programming Module with Generative AI Tools}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Web frameworks have significantly changed how developers create web applications for the Internet. Thanks to pre-defined libraries, these frameworks not only accelerate development time but also reduce the amount of code developers need to write. However, to get the most out of the frameworks and libraries, developers need to have a deep understanding of core web programming languages. This allows them to write efficient code, troubleshoot effectively, and push the boundaries of what the frameworks can achieve. The same principle applies to Generative Artificial Intelligence (AI) tools, as they have the potential to enhance a developer's toolkit. However, they will only be useful if the developer has sound fundamental knowledge to verify the output from these tools. Educators in higher education face a similar predicament with the widespread use of Generative AI tools by learners. Many learners rely on these tools as a go-to solution without being able to verify or fully comprehend the output, leading to shallow understanding. The work in this paper outlines an approach used in a first-year web programming module within the School of Electronic Engineering and Computer Science at Queen Mary University of London, where learners were encouraged to use Generative AI tools to stimulate critical thinking when conducting assessments. Specifically, GitHub CoPilot was used as a pair programmer, and ChatGPT served as a peer reviewer. In this context, the peer reviewer's role was to help the learner reflect on the tool's output. The aim of this study was to explore the design of active learning activities that incorporate Generative AI tools for web programming to foster critical thinking practices among learners. To evaluate our approach, we employed a critical thinking self-evaluation questionnaire instrument, where learners' opinions and customs were surveyed before and after both of the assignments.},
  keywords={Codes;Generative AI;Instruments;Active learning;Chatbots;Libraries;Internet;Programming profession;Faces;Software development management;Generative AI;critical thinking;ChatGPT;GitHub CoPilot;web programming},
  doi={10.1109/EDUCON62633.2025.11016298},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11053088,
  author={A, Abinaya and T, Veeramakali},
  booktitle={2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)}, 
  title={AI-Powered 3D Printing Error Detection and Optimization System}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Recent advancements and rising popularity of 3d printing technology have increased innovation and advancements in manufacturing vertical. But this has also brought in challenges in waste reduction particularly for materials like PLA, ABS, and PETG. Waste reduction in 3D printing is a crucial challenge due to the huge dependency on thermoplastics like PLA, ABS, and PETG. Each of these materials has distinct characteristics that influence their recyclability, disposal methods, and potential environmental impact. In another parallel, Generative AI is gaining momentum due to its ability in generate complex data based on the provided prompt and model training. This project aims to utilize the benefits and advantages of Generative AI in addressing the challenges of increasing waste in Additive manufacturing processes. Manual inspection of printing defects is inefficient and prone to human errors. AI can automate and improve this process. This paper presents an AI-powered 3D printing error detection and optimization system that leverages computer vision, deep learning, and generative AI to enhance print quality and reduce material waste. The system employs a convolutional neural network (CNN) to classify common 3D printing defects from real-time images and logistic regression model to identify the material type used for 3d printing. This is followed by a Generative AI Model that provides useful insights for the users on the recommendations to use manufacturing processes in more reliable way with a motto to reduce waste reduction during 3D printing processes. For generative AI model, Prompts are provided in such a way that it contains the printing error type and classified material type. Prompt optimization techniques like gradient-descent algorithm and genetic algorithm are considered and both are being used to create different prompt variations and most reliable prompt is sent to Generative AI model. This is to refine the defect descriptions for an LLM-powered recommendation system (Mistral 7B).},
  keywords={Waste reduction;Deep learning;Solid modeling;Computational modeling;Three-dimensional printing;Real-time systems;Prompt engineering;Convolutional neural networks;Optimization;Genetic algorithms;3D printing;defect detection;AI optimization;computer vision;deep learning;prompt engineering;generative AI;hybrid optimization;genetic algorithms;mistral 7b},
  doi={10.1109/ICCRTEE64519.2025.11053088},
  ISSN={},
  month={May},}@ARTICLE{9043519,
  author={Wang, Lei and Chen, Wei and Yang, Wenjia and Bi, Fangming and Yu, Fei Richard},
  journal={IEEE Access}, 
  title={A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={63514-63537},
  abstract={Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.},
  keywords={Image synthesis;Generative adversarial networks;Training;Face;Task analysis;Generators;Generative adversarial networks;image synthesis;image-to-image translation;image editing;cartoon generation},
  doi={10.1109/ACCESS.2020.2982224},
  ISSN={2169-3536},
  month={},}@ARTICLE{9328280,
  author={Fu, Chaoyou and Wu, Xiang and Hu, Yibo and Huang, Huaibo and He, Ran},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition}, 
  year={2022},
  volume={44},
  number={6},
  pages={2938-2952},
  abstract={Heterogeneous face recognition (HFR) refers to matching cross-domain faces and plays a crucial role in public security. Nevertheless, HFR is confronted with challenges from large domain discrepancy and insufficient heterogeneous data. In this paper, we formulate HFR as a dual generation problem, and tackle it via a novel dual variational generation (DVG-Face) framework. Specifically, a dual variational generator is elaborately designed to learn the joint distribution of paired heterogeneous images. However, the small-scale paired heterogeneous training data may limit the identity diversity of sampling. In order to break through the limitation, we propose to integrate abundant identity information of large-scale visible data into the joint distribution. Furthermore, a pairwise identity preserving loss is imposed on the generated paired heterogeneous images to ensure their identity consistency. As a consequence, massive new diverse paired heterogeneous images with the same identity can be generated from noises. The identity consistency and identity diversity properties allow us to employ these generated images to train the HFR network via a contrastive learning mechanism, yielding both domain-invariant and discriminative embedding features. Concretely, the generated paired heterogeneous images are regarded as positive pairs, and the images obtained from different samplings are considered as negative pairs. Our method achieves superior performances over state-of-the-art methods on seven challenging databases belonging to five HFR tasks, including NIR-VIS, Sketch-Photo, Profile-Frontal Photo, Thermal-VIS, and ID-Camera.},
  keywords={Face recognition;Learning systems;Databases;Generators;Gallium nitride;Image recognition;Training;Heterogeneous face recognition;cross-domain;dual generation;contrastive learning},
  doi={10.1109/TPAMI.2021.3052549},
  ISSN={1939-3539},
  month={June},}@ARTICLE{8654640,
  author={Yang, Jinfeng and Zhao, Zihao and Zhang, Haigang and Shi, Yihua},
  journal={IEEE Access}, 
  title={Data Augmentation for X-Ray Prohibited Item Images Using Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={28894-28902},
  abstract={Recognizing prohibited items automatically is of great significance for intelligent X-ray baggage security screening. Convolutional neural networks (CNNs), with the support of big training data, have been verified as the powerful models capable of reliably detecting the expected objects in images. Therefore, building a specific CNN model working reliably on prohibited item detection also requires large amounts of labeled item image data. Unfortunately, the current X-ray baggage image database is not big enough in count and diversity for CNN model training. In this paper, we propose a novel method for X-ray prohibited item data augmentation using generative adversarial networks (GANs). The prohibited items are first extracted from X-ray baggage images using a K-nearest neighbor matting scheme. Then, the poses of the obtained item images are estimated using a space rectangular coordinate system and categorized into four or eight classes for constructing a training database. For generating the realistic samples reliably, different GAN models are evaluated using Frechet Inception Distance scores, and some important tips of handling GAN training in X-ray prohibited item image generation are reported. Finally, to verify whether the generated images belong to its corresponding class or not, a cross-validation scheme based on a CNN model is implemented. The experimental results show that most of the generated images can be classified correctly by the CNN model. This implies that the generated prohibited item images can be used as the extended samples to augment an X-ray image database.},
  keywords={X-ray imaging;Gallium nitride;Training;Generative adversarial networks;Image databases;Reliability;Data augmentation;generative adversarial networks;image generation;prohibited item;X-ray image},
  doi={10.1109/ACCESS.2019.2902121},
  ISSN={2169-3536},
  month={},}@ARTICLE{8920019,
  author={Ahmed, Imran and Zabit, Usman and Salman, Ahmad},
  journal={IEEE Access}, 
  title={Self-Mixing Interferometric Signal Enhancement Using Generative Adversarial Network for Laser Metric Sensing Applications}, 
  year={2019},
  volume={7},
  number={},
  pages={174641-174650},
  abstract={Measurement performance of self-mixing interferometric (SMI) laser sensor can be significantly affected due to the presence of noise. In this case, conventional signal enhancement techniques yield compromised performance due to several limitations which include processing signals in frequency domains only, relying mainly on first order statistics, loss of important information present in higher frequency band and handling limited number of noise types. To address these issues, we propose a solution based on using generative adversarial network, a popular deep learning scheme, to enhance SMI signal corrupted with different noise types. Thus, taking advantage of the deep networks that can learn arbitrary noise distribution from large example set, our proposed method trains the deep network model end-to-end, able to process raw waveforms directly, learn 51 different noise conditions including white noise and amplitude modulation noise for 1,140 different types of SMI waveforms made up of 285 different optical feedback coupling factor ( $C$ ) values and 4 different line-width enhancement factor  $\alpha $  values. The results show that the proposed method is able to significantly improve the SNR of noisy SM signals on average of 19.49, 16.29, 10.34 dB for weak-, moderate-, and strong-optical feedback regime signals, respectively. For amplitude modulated SMI signals, the proposed method has corrected the amplitude modulation with maximum error (using area-under-the-curve based quantitative analysis) of 0.73% for SMI signals belonging to all optical feedback regimes. Thus, our proposed method can effectively reduce the noise without distorting the original signal. We believe that such a unified and precise method leads to enhancement of performance of SMI laser sensors operating under real-world, noisy conditions.},
  keywords={Optical feedback;Laser feedback;Gallium nitride;Noise measurement;White noise;Distortion;Generative adversarial networks;Interferometry laser sensors;self-mixing signal enhancement;vibration measuring laser sensors;waveform enhancement;generative adversarial network (GAN);signal noise removal;neural network for signal enhancement},
  doi={10.1109/ACCESS.2019.2957272},
  ISSN={2169-3536},
  month={},}@ARTICLE{9893100,
  author={Jimale, Ali Olow and Mohd Noor, Mohd Halim},
  journal={IEEE Access}, 
  title={Fully Connected Generative Adversarial Network for Human Activity Recognition}, 
  year={2022},
  volume={10},
  number={},
  pages={100257-100266},
  abstract={Conditional Generative Adversarial Networks (CGAN) have shown great promise in generating synthetic data for sensor-based activity recognition. However, one key issue concerning existing CGAN is the design of the network architecture that affects sample quality. This study proposes an effective CGAN architecture that synthesizes higher quality samples than state-of-the-art CGAN architectures. This is achieved by combining convolutional layers with multiple fully connected networks in the generator’s input and discriminator’s output of the CGAN. We show the effectiveness of the proposed approach using elderly data for sensor-based activity recognition. Visual evaluation, similarity measure, and usability evaluation are used to assess the quality of generated samples by the proposed approach and validate its performance in activity recognition. In comparison to the state-of-the-art CGAN, the visual evaluation and similarity measure demonstrate that the proposed models’ synthetic data more accurately represents actual data and creates more variations in each synthetic data than the state-of-the-art approach respectively. The experimental stages of the usability evaluation, on the other hand, show a performance gain of 2.5%, 2.5%, 3.1%, and 4.4% over the state-of-the-art CGAN when using synthetic samples by the proposed architecture.},
  keywords={Activity recognition;Sensors;Deep learning;Feature extraction;Data models;Training data;Generators;Generative adversarial networks;Activity recognition;deep learning;generative adversarial network},
  doi={10.1109/ACCESS.2022.3206952},
  ISSN={2169-3536},
  month={},}@ARTICLE{9340185,
  author={Du, Kangning and Liu, Changtong and Cao, Lin and Guo, Yanan and Zhang, Fan and Wang, Tao},
  journal={IEEE Access}, 
  title={Double-Channel Guided Generative Adversarial Network for Image Colorization}, 
  year={2021},
  volume={9},
  number={},
  pages={21604-21617},
  abstract={Image colorization has a widespread application in video and image restoration in the past few years. Recently, automatic colorization methods based on deep learning have shown impressive performance. However, these methods map grayscale image input into multi-channel output directly. In the process, it usually loses detailed information during feature extraction, resulting in abnormal colors in local areas of the colorization image. To overcome abnormal colors and improve colorization quality, we propose a novel Double-Channel Guided Generative Adversarial Network (DCGGAN). It includes two modules: a reference component matching module and a double-channel guided colorization module. The reference component matching module is introduced to select suitable reference color components as auxiliary information of the input. The double-channel guided colorization module is designed to learn the mapping relationship from the grayscale to each color channel with the assistance of reference color components. Experimental results show that the proposed DCGGAN outperforms existing methods on different quality metrics and achieves state-of-the-art performance.},
  keywords={Image color analysis;Gray-scale;Generative adversarial networks;Feature extraction;Gallium nitride;Deep learning;Color;Grayscale image colorization;generative adversarial network;double-channel guided colorization;reference component},
  doi={10.1109/ACCESS.2021.3055575},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10578989,
  author={Marimekala, Sanjeev Kumar and Lamb, John and Epstein, Robert and Bhupathi, Vasundhara},
  booktitle={2024 IEEE World AI IoT Congress (AIIoT)}, 
  title={Using AI and Big Data in the HealthCare Sector to help build a Smarter and more Intelligent HealthCare System}, 
  year={2024},
  volume={},
  number={},
  pages={356-362},
  abstract={The purpose of this paper is to demonstrate the use of AI and Big Data in HealthCare Sector to help build a smarter and more intelligent HealthCare System. Researchers in HealthCare Sectors are relying heavily on big data and compute power to build correlations by using statistical methods and artificial intelligence (AI) models. These models enable Healthcare Sector participants to manage HealthCare for a core set of the population. They also help providers to analyze the impact of decisions on their most vulnerable patients. There are many factors that are considered in performing big data analysis, some of them are: the patient’s medical history, genetic information, eating habits and fitness regimen. The data that is analyzed includes several key decision-making processes. Some of the challenges with the data used include data quality, data validation, data knowledge, domain expertise, and data integration challenges with various end points. While performing data analysis, the HealthCare Sectors must take security and data governance (HIPPA regulations etc.) into consideration. Big data analysis follows the (4P) approach [1], preference, prediction, personalization, and promotion. The question that arises most often is the type of data that is the most reliable for analysis in the HealthCare Sector. Most HealthCare organizations use demographic information, diagnosis, treatment, prescription drugs, laboratory tests, physiologic monitoring data, hospitalization, and patient insurance for their analysis. Since the data comes from multiple sources [2], there is a big challenge to perform data integration, extraction, and transformation as it consumes large amounts of resources and compute power, coupled with the additional challenges of data aggregation, data enrichment and format inconsistencies. To address this challenge and to analyze the process completely requires data scientists who have domain knowledge and expertise to extract, enrich and transform data. and group them into a meaningful format for proper data analysis and research. The HealthCare Sector faces this as a major challenge. Another key component in big data analysis is the lack of data visualization tools that can take structured and unstructured data and build customized dashboards for data correlation. There are also challenges with big data management in the HealthCare Sector. This data needs to be highly secured, with proper guardrails with tightened security measures and controls, and with proper data governance in place. There is also a need for a robust infrastructure that can handle large amounts of medical data that can allow researchers to analyze, build data correlation models and generate meaningful insights by using AI models through prompt engineering techniques [3] to build data correlation. In our paper, our focus is to understand, through several use cases the key challenges in gathering and grouping HealthCare data (both structured and unstructured data) from various sources. We also focus on understanding the impact of technical advancements in emerging AI technologies and how it plays a vital role in defining and deriving meaningful data insights for research and for learning HealthCare data patterns with a primary focus on data authenticity, ethics, privacy, governance, integrity, and security.},
  keywords={Correlation;Medical services;Big Data;Chatbots;Data models;Regulation;Security;Generative AI;ChatGPT;Guardrails;Big Data;AI Models;Large Language Models;ML;Prompt Engineering and Prompt Tuning},
  doi={10.1109/AIIoT61789.2024.10578989},
  ISSN={},
  month={May},}@ARTICLE{10011419,
  author={Kang, Sanghoon and Gao, Yunfei and Jeong, Jaeho and Park, Seong-Joon and Kim, Jae-Won and No, Jong-Seon and Jeon, Hahyeon and Lee, Jeong Wook and Kim, Sunghwan and Park, Hosung and No, Albert},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks for DNA Storage Channel Simulator}, 
  year={2023},
  volume={11},
  number={},
  pages={3781-3793},
  abstract={DNA data storage systems have rapidly developed with novel error-correcting techniques, random access algorithms, and query systems. However, designing an algorithm for DNA storage systems is challenging, mainly due to the unpredictable nature of errors and the extremely high price of experiments. Thus, a simulator is of interest that can imitate the error statistics of a DNA storage system and replace the experiments in developing processes. We introduce novel generative adversarial networks that learn DNA storage channel statistics. Our simulator takes oligos (DNA sequences to write) as an input and generates a FASTQ file that includes output DNA reads and quality scores as if the oligos are synthesized and sequenced. We trained the proposed simulator with data from a single experiment consisting of 14,400 input oligo strands and 12,108,573 output reads. The error statistics between the input and the output of the trained generator match the actual error statistics, including the error rate at each position, the number of errors for each nucleotide, and high-order statistics. The code is available at https://github.com/gyfbianhuanyun/DNA_storage_simulator_GAN.},
  keywords={DNA;Generators;Sequential analysis;Generative adversarial networks;Transformers;Hidden Markov models;Error analysis;Recurrent neural networks;Channel simulator;DNA storage;generative adversarial networks;recurrent neural networks;transformer},
  doi={10.1109/ACCESS.2023.3235201},
  ISSN={2169-3536},
  month={},}@ARTICLE{9146829,
  author={Niu, Zhewen and Reformat, Marek Z. and Tang, Wenhu and Zhao, Baining},
  journal={IEEE Access}, 
  title={Electrical Equipment Identification Method With Synthetic Data Using Edge-Oriented Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={136487-136497},
  abstract={The fourth industrial revolution - Industry 4.0 - puts emphasis on the application of intelligent technologies in the area of monitoring and identification of electrical equipment. High precision and non-contact qualities make the infrared thermography one of the most suitable technologies for intelligent inspection of high-voltage apparatus. Yet, due to imperfect data acquisition methods and difficulties in collecting data, electrical equipment images are limited in quantities and imbalanced in representing different types of devices. Additionally, it is not easy to extract representative features of infrared images due to their low-intensity contrast and uneven distribution. In this paper, a data-driven framework is proposed for the identification of electrical equipment based on infrared images. The main technique of this proposed system is a novel process of generating synthetic infrared images. For this purpose, an Edge-Oriented Generative Adversarial Network (EOGAN) is developed. The EOGAN is designed to create realistic infrared images that can be used as augmented data for developing data-driven identification methods. Extracted edge features of electrical equipment are utilized as prior information to guide the process of generating realistic infrared images. Finally, comparative experiments are carried out to show the effectiveness of the proposed EOGAN-based framework for equipment identification in the presence of limited and imbalanced image datasets.},
  keywords={Image edge detection;Generative adversarial networks;Gallium nitride;Feature extraction;Image generation;Morphology;Inspection;Edge prior knowledge;electrical equipment identification;generative adversarial network;infrared image},
  doi={10.1109/ACCESS.2020.3011689},
  ISSN={2169-3536},
  month={},}
