@INPROCEEDINGS{9288255,
  author={Qiu, Jingjun and Gao, Yan},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Position and Channel Attention for Image Inpainting by Semantic Structure}, 
  year={2020},
  volume={},
  number={},
  pages={1290-1295},
  abstract={Image inpainting has made great progress with the emergence of deep learning. However, there are also problems of structural loss and blurry textures, which lead to the generated image has artifacts and incomplete object. When a heavily structured object is partially masked, most methods cannot complete the repair work while keeping the semantic structure intact. To solve these problems, we propose a two-stage adversarial model: introduce unsupervised semantic structure guidance. Compared with other methods, the semantic information obtained by the introduction of semantic segmentation is more accurate. Compared with the structure information of supervised semantic segmentation of manual labels, the unsupervised semantic structure information is more flexible, and the labels are more diverse. The attention model with location information and channel information strengthens the model's long-range contextual information and multi-scale context information fusion capabilities. We evaluate our model over the publicly available datasets CelebA, Places2, and Paris StreetView, our method has a higher repair quality than the existing state-of-the-art approaches, especially when repairing the image with the large missing area.},
  keywords={Deep learning;Semantics;Maintenance engineering;Tools;Image restoration;Image reconstruction;Context modeling;Inpainting;Generative model;Deep learning},
  doi={10.1109/ICTAI50040.2020.00194},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{9836461,
  author={Yang, Qingyu and Chen, Wei and Cai, Yichao and Liu, XinYing and Liu, Taian and Wang, Ge},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={LWComicGAN: A Lightweight Method for Realizing Scene Animation}, 
  year={2022},
  volume={10},
  number={},
  pages={2285-2289},
  abstract={The style transfer algorithm was originally proposed to solve the generation problem of art paintings. In recent years, the generation of animation style images has gradually become a hot research direction. The content presented in many animation film and television works is fascinating. In order to satisfy people's desire to turn real scenes into animation scenes and reduce the workload of animation producers, a Light Weight Animation Generated Adversarial Network (LWComicGAN) is proposed, which can reduce the amount of parameters and enable low-memory devices to complete network training. An optional instance layer normalization function is designed to adapt the input of each layer, and an optional instance layer residual block is proposed. The LWComicGAN algorithm uses the objective function of WGAN-GP and other various loss functions as the total loss, and also considers the gradient penalty mechanism in discriminator. The former guarantees the generation quality of all aspects of the image, and the latter guarantees the stability of the training process. The effectiveness of the proposed algorithm is verified after animation transfer experiments of realistic landscapes and characters, and we have produced an ink painting dataset and completed ink animation style transfer.},
  keywords={Training;TV;Image synthesis;Conferences;Ink;Animation;Linear programming;deep learning;image style transfer;generative adversarial networks;photo animation;LWComicGAN},
  doi={10.1109/ITAIC54216.2022.9836461},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{9936860,
  author={Matsuda, Yusuke and Miyazaki, Tomo and Omachi, Shinichiro},
  booktitle={2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={GAN-based Privacy-Conscious Data Augmentation with Finger-Vein Images}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={The lack of sufficient data for evaluation and development is a major problem in biometrics. A novel GAN-based data-augmentation method for finger-vein authentication is proposed and evaluated in this study. Based on the GAN model structure, a subnetwork is added that lowers the similarity between the real data used for training and the fake data from the generator; the fake data looks remarkably similar to the real data, and the correlation between the real and fake data is lowered. Because the real data and fake data are different individuals, the privacy of a particular person is not considered when examining authentication technologies using only generated fake data. Moreover, the possibility of improving the authentication accuracy is confirmed by using both real data and generated fake data for training. The effectiveness of the proposed method is proved experimentally.},
  keywords={Training;Data privacy;Correlation;Biometrics (access control);Biological system modeling;Fingers;Authentication;Finger vein;Data augmentation;Generative adversarial network},
  doi={10.1109/IICAIET55139.2022.9936860},
  ISSN={},
  month={Sep.},}@ARTICLE{11000300,
  author={Liang, Yin and Jia, Yingchen},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Multiscale Spatial–Temporal Graph Attention Network for fMRI Brain Disease Classification}, 
  year={2025},
  volume={74},
  number={},
  pages={1-15},
  abstract={Brain disease classification based on functional magnetic resonance imaging (fMRI) has become a hotspot in artificial intelligence research. Considering the graph structure properties of the brain network, graph learning methods have attracted increasing attention in recent work. However, these studies mainly construct a spatial graph on a single scale to represent the brain network, ignoring the complex functional interactions of brain network at multiple scales, as well as the potential of graph learning in temporal feature extraction. To address these issues, this study proposes a novel multiscale spatial–temporal graph attention network (MSTGAT) for fMRI brain disease classification. We design multiscale spatial graph attention learning (MS-GAT) and temporal graph attention learning (T-GAT) modules, in which the former one constructs multiscale topological brain networks to learn and combine high-level spatial functional interactions among brain regions, and the latter one engages time encoding to learn temporal hypercorrelations among different time points. The learned spatial and temporal features are adaptively integrated, and the fused spatiotemporal features are incorporated into multilayer perceptron for brain disease classification. Systematical experiments on three fMRI datasets indicate robust classification performance of our MSTGAT for different classification tasks and brain parcellations, outperforming several state-of-the-art classification methods. Our model demonstrates a better classification accuracy and computational efficiency tradeoff. We also identify important brain regions and connections associated with brain disease classification. Together, this study provides a promising model to effectively learn and integrate complementary features of multiscale topological brain networks and spatiotemporal dynamics from the fMRI data to further promote brain disease classification.},
  keywords={Feature extraction;Diseases;Functional magnetic resonance imaging;Brain modeling;Deep learning;Data mining;Machine learning;Generative adversarial networks;Diffusion tensor imaging;Data models;Artificial intelligence;brain disease classification;functional magnetic resonance imaging (fMRI);graph attention learning;spatial–temporal feature integration},
  doi={10.1109/TIM.2025.3568941},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10799251,
  author={Poopradit, Warit and Saebe, Pongpom and Chinnasri, Veravich and Phienthrakul, Tanasanee},
  booktitle={2024 19th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)}, 
  title={Deepfake Voice System for Spoofing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper presents a deep fake voice spoofing system. To develop this system, the concept of a voice modulation system for altering and replicating human voices are explored. Tacotron2, Glow-TTS, and XTTS-V2 are studied, and they demonstrate the significant advancements in voice replication technology. The methodology involves a comprehensive process of audio data collection, and model implementation, followed by integration into a user-friendly website. The evaluation phase employs user acceptance testing with the Mean Opinion Score (MOS) as the primary metric, where participants rate the quality and resemblance of the synthesized voice compared to the original. The results highlight the voice modulator's effectiveness in achieving high-fidelity voice replication, with positive user feedback.},
  keywords={Training;Deepfakes;Modulation;Data models;Reproducibility of results;Human voice;Reliability;Robots;Testing;Speech to text;Voice Manipulation;Deep Learning;Generative Adversarial Networks (GANs);Speech Synthesis;Text-to-Speech},
  doi={10.1109/iSAI-NLP64410.2024.10799251},
  ISSN={2831-4565},
  month={Nov},}@INPROCEEDINGS{10581615,
  author={Zhang, Zishi and Zhang, Renbin and Cui, Yuhang},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Adversarial Federated Learning Algorithms for Non-IID Data}, 
  year={2024},
  volume={},
  number={},
  pages={740-745},
  abstract={Aiming at the problems of poor performance and high communication cost of federated learning under the condition of data heterogeneity (non-IID), this paper proposes a federated learning algorithm based on adversarial learning: the local model uses a discriminator to extract and amplify the difference between the local model and the global model in order to improve the model bias problem under non-IID. The discriminator output is also used to guide the fusion of local features with global features to construct a dynamic feature fusion mechanism to improve the accuracy of the model. In addition, restricting the discriminative model to the local to reduce the communication cost. Experimental results based on CIFAR-10 and CIFAR-100 data show that compared with the state-of-the-art federated learning algorithms, the algorithm in this paper improves its accuracy by about 2%-3% with almost no additional communication resources, indicating that the algorithm in this paper is able to effectively mitigate the problems caused by data heterogeneity.},
  keywords={Seminars;Costs;Accuracy;Limiting;Federated learning;Heuristic algorithms;Feature extraction;component;Federated Learning;Non-IID;Generative adversarial network},
  doi={10.1109/AINIT61980.2024.10581615},
  ISSN={},
  month={March},}@INPROCEEDINGS{9705011,
  author={Zhu, Zheng-An and Chen, Chien-Hao and Chiang, Chen-Kuo},
  booktitle={2021 IEEE/ACIS 22nd International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
  title={Conditional Data Augmentation For Sky Segmentation}, 
  year={2021},
  volume={},
  number={},
  pages={177-182},
  abstract={Outdoor scene parsing is a very popular topic which algorithms seek to labels or identify objects in images. Sky segmentation is one of the popular outdoor scene parsing task. Sky segmentation models are usually trained on ideal datasets and produce high quality results. However, the performance of sky segmentation model decreases because of varying weather conditions, different time and scene changes due to seasonal weather or other issues in reality. This paper focuses on applying data augmentation methods to generate diversified images. A conditional data augmentation method based on BicycleGAN is proposed in this paper. The model considers mask loss and content loss for improving the quality and details of the generated images. The experimental results demonstrate that the quality of the generated image is better than the existing methods.},
  keywords={Training;Image segmentation;Codes;Computational modeling;Data models;Object recognition;Task analysis;Conditional data augmentation;conditional generative adversarial networks;sky segmentation},
  doi={10.1109/SNPD51163.2021.9705011},
  ISSN={2693-8421},
  month={Nov},}@INPROCEEDINGS{11029995,
  author={Sivasothy, Shangeetha and Barnett, Scott and Kurniawan, Stefanus and Rasool, Zafaryab and Vasa, Rajesh},
  booktitle={2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN)}, 
  title={RAGProbe: Breaking RAG Pipelines with Evaluation Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={60-71},
  abstract={Retrieval Augmented Generation (RAG) is increasingly employed in building Generative AI applications, yet their evaluation often relies on manual, trial-and-error processes. Automating this evaluation process involves generating test data to trigger failures involving context comprehension, data formatting, specificity, and content completeness. Random question-answer generation is insufficient. However, prior works rely on standard QA datasets, benchmarks and tactics that are not tailored to the specific domain requirements. Hence, current approaches and datasets do not trigger sufficiently broad and context-specific failures. In this paper, we introduce evaluation scenarios that describe the process of generating question-answer pairs from content indexed by RAG pipelines, and they are designed to trigger a wider range of failures and to simplify automation. This enables developers to identify and address weaknesses more effectively. We validate our approach on five open-source RAG pipelines using three datasets. Our approach triggers high failure rates, by generating prompts that combine multiple questions (up to 91% failure rate) highlighting the need for developers to prioritize handling such queries. We generated failure rates of 60% in an academic domain dataset and 53% and 64% in open-domain datasets. Compared to existing state-of-the-art methods, our approach triggers 77% more failures on average per RAG pipeline and 53% more failures on average per dataset, offering a mechanism to support developers to improve the RAG pipeline quality.},
  keywords={Generative AI;Large language models;Retrieval augmented generation;Pipelines;Buildings;Manuals;Benchmark testing;Software;Standards;Software engineering;Retrieval Augmented Generation;Large Language Models;Software Evaluation},
  doi={10.1109/CAIN66642.2025.00015},
  ISSN={},
  month={April},}@INPROCEEDINGS{9836590,
  author={Chen, Errui and Feng, Xubin},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Single Space Object Image Denoising and Super Resolution Reconstructing based on Unpaired images}, 
  year={2022},
  volume={10},
  number={},
  pages={1572-1576},
  abstract={High quality space target image is of great significance for space attack and defense because space exploration missions are becoming more and more important. But high quality images of space object are difficult to obtain due to the large number of cosmic rays in the space environment, as well as the limitations of optical lenses, detectors and transmission links on satellites. Image denoising and super-resolution reconstruction are the most economical and effective methods to solve this problem. This paper presents an unpaired denoising and super-resolution reconstruction method for optical remote sensing images which could obtain the images has higher quality than dataset itself. In order to further improve the quality of optical remote sensing images, high quality natural images are added into the training set, and unpaired image data sets (natural images and optical remote sensing images belong to different fields and cannot correspond one to one) are adopted to complete the training of the whole network by using the idea of unsupervised learning. Through the verification tests of three optical remote sensing image data sets, it can be seen that the method in this paper has reconstructed high quality optical remote sensing images with higher resolution than the dataset itself.},
  keywords={Training;Superresolution;Noise reduction;Optical computing;Optical fiber networks;Optical imaging;Remote sensing;component;dual cycle;generative adversiral network;space object image;unpaired},
  doi={10.1109/ITAIC54216.2022.9836590},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{10800887,
  author={Vimaladevi, M and Thangamani, R and Suganth Krishna, E and Naveen, B and Srihari Prrasath, A},
  booktitle={2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA)}, 
  title={Deep Learning Approaches for Dog Breed Identification and Behaviour Analysis in Conversational AI Systems}, 
  year={2024},
  volume={},
  number={},
  pages={727-733},
  abstract={This research study aims to develop an innovative system for dog breed and behavior identification using deep learning algorithms, integrated with ChatGPT API and Google Text-to-Speech (TTS) for enhanced user interaction. Leveraging advanced convolutional neural networks (CNNs) such as ResNet, Inception, and VGG16, the system accurately identifies dog breeds based on visual input. These models are trained on a comprehensive dataset of dog images to ensure high accuracy and robust performance in breed classification. This multi-faceted approach not only streamlines the process of identifying dog breeds and behaviors but also enhances user experience through seamless interaction and accessibility features. The CNN models ResNet, Inception, and VGG16 demonstrated their effectiveness in identifying dog breeds, with accuracies of $87 \%, 85 \%$, and $83 \%$ respectively. The project showcases the potential of combining state-of-the-art deep learning models with advanced AI and TTS technologies to create an intelligent, user-friendly system for dog enthusiasts, trainers, and veterinarians.},
  keywords={Deep learning;Visualization;Accuracy;Dogs;Streaming media;Chatbots;Transformers;User experience;Text to speech;Convolutional neural networks;Deep learning;Convolutional neural networks;Chat generative pre-trained transformer;Google text to speech},
  doi={10.1109/ICECA63461.2024.10800887},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9498250,
  author={Li, Jiajing and Qu, Zhaowei and Wang, Xiaoru and Dan, Jiawang and Ma, Bing},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Multi-Scale Semantic Transfer for Person Image Generation}, 
  year={2021},
  volume={},
  number={},
  pages={888-893},
  abstract={Pose transfer is a task of transferring the given person pose image to the target pose image. There are mainly two problems of inaccurate generated pose and the texture dissimilarity between generated pose and target pose in the previous work. We propose the multi-scale semantic transfer network(MSTN) to generate the target pose image. It contains the semantic parsing generation module(SPGM) and multi-scale semantic transfer module(MSTM). The semantic parsing generation module(SPGM) generates target human mask with semantic information to improve structural similarity. The multi-scale semantic transfer module(MSTM) transfers pose both in high dimension and low dimension. Then it merges the information of different scales to improve texture similarity. Compared with the previous work, the posture of person and the texture such as face, clothes and hair in the images generated by our networks are more similar with the target pose image},
  keywords={Hair;Image synthesis;Conferences;Semantics;Computer applications;Generators;Task analysis;pose transfer;generative adversarial network;multi-scale;semantic},
  doi={10.1109/ICAICA52286.2021.9498250},
  ISSN={},
  month={June},}@INPROCEEDINGS{10829310,
  author={Mishra, Ajit Kumar and Verma, Vikash and Manivannan, R. and Kumar, N. Krishna and Srividya, Mudunuru and Sharma, Sanskriti},
  booktitle={2024 International Conference on Intelligent Systems and Advanced Applications (ICISAA)}, 
  title={The Frontier of Geospatial AI Deep Learning Applications in Data Mining and Spatial Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Modern artificial intelligence uses geographical data mining and regional analysis to analyze large datasets. Deep learning is a new geographic AI method. Example: Geographic Deep Mining Network (GDMN). The Spatial Feature Extractor (SFE), Temporal Geospatial Attention Mechanism (TGAM), and Geospatial Data Fusion Module (GDFM) comprise this technique. We examined six popular approaches and found GDMN performed best. It worked successfully for several reasons. GDMN always outperforms its competitors in memory, F1 score, IoU, training time, and inference time. This is always true. These findings suggest that GDMN might boost geospatial AI’s data mining and location analysis abilities. After all the effort, we have developed a solid basis for location intelligence, making it useful in various subject areas. Because it’s versatile, mapping specialists may use GDMN for better environmental monitoring or city development. Deep learning helps GDMN make better space choices and uncover insights in large, challenging data sets that no one knew about. GDMN’s cutting-edge approaches make space and time research and prediction simpler. This wide perspective offers a more full and all-encompassing picture of today’s challenges than previous techniques. This makes it impossible to overstate how significant this work is to location intelligence. Its unmatched research and insights usher in a new era.},
  keywords={Deep learning;Training;Urban areas;Transforms;Feature extraction;Solids;Spatial databases;Geospatial analysis;Data mining;Artificial intelligence;Data Mining;Geospatial Analysis;Geospatial Intelligence;Machine Learning;Spatial Data;Spatial Feature Extraction;Temporal Attention Mechanism;Deep Learning;Data Fusion;Urban Planning},
  doi={10.1109/ICISAA62385.2024.10829310},
  ISSN={},
  month={Oct},}@ARTICLE{11152676,
  author={Cui, Haixia and Xie, Bo and Wang, Hongjiang and Leung, Victor C. M.},
  journal={IEEE Communications Magazine}, 
  title={Generative-Artificial-Intelligence-Based Wireless Channel Modeling: Challenges and Opportunities}, 
  year={2025},
  volume={63},
  number={9},
  pages={20-26},
  abstract={Wireless channel modeling is critical to understanding and optimizing signal transmission. Wireless channels are influenced by many factors, including path loss, reflection, fading, and interference, making them complex and difficult to model and predict. Although some traditional wireless channel modeling methods are effective, they have limitations in handling complex multipath effects and nonlinear characteristics. Generative artificial intelligence (GAI) has the ability to generate robust data and therefore can be potentially used to model realistic wireless channel characteristics. However, there exist some challenges in GAI for wireless channel modeling, including physical layer network security issues, limited generalization ability, and the bandwidth consumption of centralized training. This article introduces a GAI-based wireless channel modeling framework, which leverages blockchain and federated learning to address efficiency, network security, and data privacy concerns in GAI channel modeling. Blockchain technology, through distributed ledgers and smart contracts, enables resource sharing and efficient utilization, enhancing computational efficiency and reducing network security risks. Federated learning allocates training tasks to different devices and nodes, thus allowing model training without centralizing data, protecting user privacy, and reducing network bandwidth usage. This article demonstrates the performance and effectiveness of the proposed framework through numerical results.},
  keywords={Wireless communication;Training;Data privacy;Federated learning;Computational modeling;Network security;Data models;Blockchains;Numerical models;Communication system security},
  doi={10.1109/MCOM.001.2400699},
  ISSN={1558-1896},
  month={Sep.},}@INPROCEEDINGS{6493015,
  author={Huang Wan-Fu},
  booktitle={International Conference on Automatic Control and Artificial Intelligence (ACAI 2012)}, 
  title={The design of an eight-digit Fibonacci sequence generator}, 
  year={2012},
  volume={},
  number={},
  pages={2082-2087},
  abstract={This paper presents a field programmable gate array (FPGA) prototype of an 8-digit Fibonacci sequence generator. The design is based on the Verilog hardware description language, synthesized by the Xilinx Synthesis Technology (XST) synthesizer, and implemented with the ISE design flow. The circuit was tested to run on the Spartan-3E Starter Kit Board. The resource utilization and the test results are reported. The design itself can be an independent tool or a small device in a bigger system.},
  keywords={Fibonacci Sequence;FPGA;Verilog;ISE;XST},
  doi={10.1049/cp.2012.1408},
  ISSN={},
  month={March},}@INPROCEEDINGS{10214752,
  author={Bran, Emanuela and Rughiniş, Cosima and Nadoleanu, Gheorghe and Flaherty, Michael G.},
  booktitle={2023 24th International Conference on Control Systems and Computer Science (CSCS)}, 
  title={The Emerging Social Status of Generative AI: Vocabularies of AI Competence in Public Discourse}, 
  year={2023},
  volume={},
  number={},
  pages={391-398},
  abstract={The emerging social status of generative AI as a social actor and interactant shapes the legitimacy of AI use. We study how the social status of generative AI takes shape in public discourse, through conflicting vocabularies of competence that justify AI’s alleged creativity or lack thereof. We analyze four sources of news, and we identify several discursive themes regarding the competence and worth of generative AI. There is a wide and nuanced spectrum of discursive positions, from AI as a competent and creative partner in work, to AI as a recombinant yet dumb tool, or quasi-creative magician. This debate reshapes public understanding of what it means to be creative and, ultimately, what is distinctive about humanity.},
  keywords={Computer science;Vocabulary;Art;Shape;Organizations;Control systems;Cognition;generative AI;public perception;social status;vocabularies of motive;ChatGPT;Lensa},
  doi={10.1109/CSCS59211.2023.00068},
  ISSN={2379-0482},
  month={May},}@ARTICLE{10904141,
  author={Haldar, Susmita and Pierce, Mary and Fernando Capretz, Luiz},
  journal={IEEE Access}, 
  title={Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning}, 
  year={2025},
  volume={13},
  number={},
  pages={46070-46090},
  abstract={Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.},
  keywords={Software testing;Education;Generative AI;Industries;Chatbots;Software engineering;Sentiment analysis;Large language models;Accuracy;Systematic literature review;Capstone project;ChatGPT;generative AI;software testing education;Microsoft Copilot;sentiment analysis},
  doi={10.1109/ACCESS.2025.3545882},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11016449,
  author={Shu, Chao and Yao, Na and Chen, Yue and Wijeratne, Vindya and Ma, Ling and Loo, Jonathan and Chai, Kok Keong and Alam, Atm and Abuelmaatti, Aisha},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Ai-Assisted Multiple-Choice Questions Generation with Multimodal Large Language Models in Engineering Higher Education}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={This paper presents an AI-assisted approach that leverages Multimodal Large Language Models (MLLMs) to automate the generation of Multiple-Choice Questions (MCQs) for modules in engineering education. The system introduces a LOs extraction to MCQs generation pipeline, which extracts Learning Outcomes (LOs) from provided lecture notes and generates relevant MCQs with solutions and explanations based on the extracted LOs. By harnessing MLLMs' capabilities in vision and text comprehension, coupled with carefully crafted prompts from human educators, the tool efficiently produces context-relevant MCQs that can streamline teaching material development. The effectiveness of this AI-powered MCQ generation pipeline is investigated through experiments across a number of engineering modules with evaluations on the quality of the generated MCQs by human educators. The analysis of the evaluation results shows the AI tool's ability to generate MCQs that are well-aligned with LOs and exhibit strong contextual relevance, demonstrating the potential of AI-assisted approaches to enhance the efficiency of creating high-quality MCQs in engineering education. However, the variability in quality ratings across different aspects underscores the continued need for human expertise and oversight in the assessment design process. The findings provide useful insights into the capabilities and limitations of state-of-the-art multimodal language models in supporting assessment development in engineering education.},
  keywords={Generative AI;Large language models;Pipelines;Learning (artificial intelligence);Question generation;Engineering education;Multiple-choice Question design;Learning Outcome authoring;Generative Artificial Intelligence (GenAI);Multimodal Large Language Models;Contextual generation;Engineering Education},
  doi={10.1109/EDUCON62633.2025.11016449},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{8613637,
  author={Zhao, Zhenjie and Ma, Xiaojuan},
  booktitle={2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={A Compensation Method of Two-Stage Image Generation for Human-AI Collaborated In-Situ Fashion Design in Augmented Reality Environment}, 
  year={2018},
  volume={},
  number={},
  pages={76-83},
  abstract={In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design.},
  keywords={Image generation;Training;Task analysis;Clothing;Artificial intelligence;Collaboration;Neural networks;Augmented reality;Human-AI collaboration;In situ design;Generative adversarial network;Authoring tool},
  doi={10.1109/AIVR.2018.00018},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9904267,
  author={Jiang, Senlin and Shi, Yunan and Cheng, Keyang},
  booktitle={2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Text-to-Face Generation via Multi-Modal Attention Memory Network with Fine-Grained Feedback}, 
  year={2022},
  volume={},
  number={},
  pages={940-947},
  abstract={Over the past few years, a lot of works have explored on Text-to-Image (TTI), which aims to generate images from text descriptions. Compared with Text-to-Image (TTI), Text-to-Face (TTF) is a subtopic with greater challenges but meaningful, and less of the related studies. The text description of the face is more abstract and complex, which leads to poor quality generated face images and a low level of semantic consistency between text description and face images. As a solution to this problem, we propose a Multi-Modal Attention Memory Network with Fine-Grained Feedback. The proposed method uses a Multi-Modal Attention Memory Network to refine the face image features continuously and to improve the quality of the resulting face image. And the word-level discriminator is designed to establish the correlation between words and facial attributes, providing the generator with fine-grained training feedback. In order to verify the effectiveness of our method, we have performed a verification on the Multi-Modal CelebA-HQ dataset, and the experimental results confirm its effectiveness.},
  keywords={Training;Correlation;Face recognition;Semantics;Generators;Artificial intelligence;Facial features;generative adversarial networks;Text-to-Image;Text-to-Face;Multi-Modal;attention memory network;word-level discriminator},
  doi={10.1109/PRAI55851.2022.9904267},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10987412,
  author={D R, Praveen and G M, Harshitha and Borkar, Pruthvi Surender and D’Souza, Reehan and Shetty, Keerthi and Murthy, Anantha},
  booktitle={2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)}, 
  title={Innovative AI Solutions for Multi-Modal Content Generation}, 
  year={2025},
  volume={},
  number={},
  pages={678-685},
  abstract={To overcome contemporary constraints in the field of AI-driven art generation, the AI ImageGen project offers a robust, intuitive framework for creating images utilizing different AI models. Utilizing a variety of cutting-edge models integrated through the Hugging Face API, this web application enables users to generate distinctive, high-quality photos by entering bespoke text prompts. AI ImageGen, which is built with React for the frontend and Node.js for the backend, provides a smooth user experience with a simplified UI thanks to Ant Design. In addition to creating photographs, users may efficiently manage their outputs by viewing, downloading, and deleting their creations. AI methods for creating images have historically advanced quickly, but many platforms still only provide a limited amount of customisation and model selection flexibility.By letting users select from a variety of models, AI ImageGen closes this gap and promotes output diversity and creativity. In order to facilitate the creation of AI-based art by a wider audience, this project also attempts to enhance the general usability of AI tools for nontechnical users. With its adaptable, multi-model methodology and emphasis on a seamless user experience, AI ImageGen is a cutting-edge solution in the rapidly expanding field of AI art generating.},
  keywords={Art;Text to image;User interfaces;Transformers;User experience;Data models;Multilingual;Artificial intelligence;Usability;Faces;AI Image Creation;CLIP,DALL-E;Diffusion Models;Generative Adversarial Networks (GANs);Hugging Face Models;React with Node.js;Stable Diffusion;Text-To-Image Creation;Transformer Models;User Interface Design},
  doi={10.1109/AIDE64228.2025.10987412},
  ISSN={},
  month={Feb},}@ARTICLE{9195834,
  author={Mishra, Bhabani Shankar Prasad and Pandey, Om and Dehuri, Satchidananda and Cho, Sung-Bae},
  journal={IEEE Access}, 
  title={Unsupervised Functional Link Artificial Neural Networks for Cluster Analysis}, 
  year={2020},
  volume={8},
  number={},
  pages={169215-169228},
  abstract={In this paper, we propose a novel method of cluster analysis called unsupervised functional link artificial neural networks (UFLANNs), which inherit the best characteristics of functional link artificial neural networks and self-organizing feature maps (SOFMs). UFLANNs adopt three types of basis functions such as Chebyshev, Legendre orthogonal polynomials, and power series for mapping the input data into a new feature space with higher dimensions, where the objects are clustered based on the principle of competitive learning of SOFMs. The effectiveness of this algorithm has been tested with various artificial and real-life datasets including remote sensing images. A thorough comparison with other popular clustering algorithms shows that the proposed method is promising in revealing clusters from many complex datasets.},
  keywords={Neurons;Unsupervised learning;Clustering algorithms;Artificial neural networks;Computer architecture;Feature extraction;Data mining;Cluster analysis;competitive learning;FLANN;SOFM},
  doi={10.1109/ACCESS.2020.3024111},
  ISSN={2169-3536},
  month={},}@ARTICLE{10185973,
  author={Yaacob, Hamwira and Hossain, Farhad and Shari, Sharunizam and Khare, Smith K. and Ooi, Chui Ping and Acharya, U. Rajendra},
  journal={IEEE Access}, 
  title={Application of Artificial Intelligence Techniques for Brain–Computer Interface in Mental Fatigue Detection: A Systematic Review (2011–2022)}, 
  year={2023},
  volume={11},
  number={},
  pages={74736-74758},
  abstract={Mental fatigue is a psychophysical condition with a significant adverse effect on daily life, compromising both physical and mental wellness. We are experiencing challenges in this fast-changing environment, and mental fatigue problems are becoming more prominent. This demands an urgent need to explore an effective and accurate automated system for timely mental fatigue detection. Therefore, we present a systematic review of brain-computer interface (BCI) studies for mental fatigue detection using artificial intelligent (AI) techniques published in Scopus, IEEE Explore, PubMed and Web of Science (WOS) between 2011 and 2022. The Boolean search expression that comprised (((ELECTROENCEPHALOGRAM) AND (BCI)) AND (FATIGUE CLASSIFICATION)) AND (BRAIN-COMPUTER INTERFACE) has been used to select the articles. Through the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) methodology, we selected 39 out of 562 articles. Our review identified the research gap in employing BCI for mental fatigue intervention through automated neurofeedback. The AI techniques employed to develop EEG-based mental fatigue detection are also discussed. We have presented comprehensive challenges and future recommendations from the gaps identified in discussions. The future direction includes data fusion, hybrid classification models, availability of public datasets, uncertainty, explainability, and hardware implementation strategies.},
  keywords={Fatigue;Electroencephalography;Electrodes;Sleep;Brain-computer interfaces;Systematics;Hardware;Brain-computer interfaces;Mental disorders;Brain-computer interface (BCI);electroencephalogram (EEG);mental fatigue detection;PRISMA},
  doi={10.1109/ACCESS.2023.3296382},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9142886,
  author={Kounte, Manjunath R. and Tripathy, Pratyush Kumar and P., Pramod and Bajpai, Harshit},
  booktitle={2020 4th International Conference on Trends in Electronics and Informatics (ICOEI)(48184)}, 
  title={Analysis of Intelligent Machines using Deep learning and Natural Language Processing}, 
  year={2020},
  volume={},
  number={},
  pages={956-960},
  abstract={Intelligent Machines are playing an important role in performing various activities in the industry thereby reducing the human efforts, error rate and increase the efficiency and accuracy. Artificial Intelligence is the backbone for development of intelligent machines which use natural language processing and deep learning as fundamental tools. In this paper, we present the new perspective of intelligent machines in everyday usage, along with the importance of understanding the natural language and generating machine level natural language for intelligent machines. Further, we summarize an overview of deep learning concurrent neural network and recurrent neural network models. Also, we review the significance of sentimental analysis using natural language processing and the ability of machines to take decisions and solve problems.},
  keywords={Natural language processing;Machine learning;Neurons;Feature extraction;Data mining;Market research;Natural Language Understanding(NLU);Natural Language Processing(NLP);Natural Language Generation(NLG);Deep Learning(DL);Intelligent Machine(IM);Convolution Neural Network(CNN);Recurrent Neural Network(RNN) and Machine learning(ML)},
  doi={10.1109/ICOEI48184.2020.9142886},
  ISSN={},
  month={June},}@INPROCEEDINGS{9882355,
  author={Zhen, Rui and Song, Wenchao and Cao, Juan},
  booktitle={2022 IEEE/ACIS 22nd International Conference on Computer and Information Science (ICIS)}, 
  title={Research on the Application of Virtual Human Synthesis Technology in Human-Computer Interaction}, 
  year={2022},
  volume={},
  number={},
  pages={199-204},
  abstract={With the rapid development of 5g, artificial intelligence, chip technology (GPU computing power), and capital investment, human body synthesis technology is entering a growth stage. According to people’s comprehensive image, we divide them into idol school, academic school, and practical school; According to the driving methods of virtual human synthesis, we divide it into avatar type, voice or text content driving type, and human-computer interaction type. We recently introduced the relevant model of the synthesis method and the implementation process of virtual human lip synthesis and explained the synthesis process. We discussed voice2face, the latest scheme for real-time synthesis of virtual human, but this method still can not synthesize high-quality video in milliseconds.},
  keywords={Human computer interaction;Information science;5G mobile communication;Lips;Avatars;Graphics processing units;Streaming media;virtual human;virtual human synthesis;deep learning;human-computer interaction},
  doi={10.1109/ICIS54925.2022.9882355},
  ISSN={},
  month={June},}@INPROCEEDINGS{9356648,
  author={Wang, Haomiao and Ma, Hongliang and He, Huan and Meng, Zhiruo and Liu, Xuesong and Cai, Siyuan},
  booktitle={2020 7th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={Application Resolution of Deep Learning in Electric Power System}, 
  year={2020},
  volume={},
  number={},
  pages={348-352},
  abstract={The construction of the electric power system has entered the era of big data, and all links in the system have accumulated massive amounts of electric power big data. Doing a better analysis of these valuable data resources will bring considerable benefits to electric power companies. The deep learning technology in the field of artificial intelligence has obvious advantages when dealing with massive data tasks. Benefit from the advancement of deep learning technology, power big data research has developed rapidly. This paper introduces the current development of machine learning algorithms, and focuses on the application of deep learning in the power industry.},
  keywords={Deep learning;Machine learning algorithms;Big Data;Market research;Power industry;Power systems;Task analysis;Power system;power big data;machine learning;deep learning},
  doi={10.1109/IFEEA51475.2020.00079},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10424123,
  author={Agus Surya Darma, I Wayan and Mahendra Putra, Putu Riky and Sugiartawan, Putu and Waas, Valentino and Sutramiani, Ni Putu},
  booktitle={2023 International Conference on Smart-Green Technology in Electrical and Information Systems (ICSGTEIS)}, 
  title={A Fine-tuned BERT-based Approach for Sentiment Analysis of Indonesian Public Towards ChatGPT}, 
  year={2023},
  volume={},
  number={},
  pages={88-93},
  abstract={The rapid development of artificial intelligence technology, one of which is ChatGPT, not also invites several opinions in Indonesian society, so it is necessary to do further sentiment analysis to find out the views of the Indonesian people towards ChatGPT on popular social media for opinionated communities such as Twitter. By using the sentiment analysis method and utilizing BERT (Bidirectional Encoder Representations from Transformers) technology, this study aims to analyze the sentiments of the Indonesian people towards ChatGPT and measure the performance of IndoBERT-Base and IndoBERT-Large to obtain sentiment analysis results. Based on the experiments, the IndoBERT-Base yielded an accuracy of 78%, and IndoBERT-Large yielded an accuracy of 79% based on hyperparameter tuning.},
  keywords={Sentiment analysis;Social networking (online);Blogs;Chatbots;Transformers;Tuning;Information systems;BERT;ChatGPT;fine-tuning;sentiment;social media;twitter},
  doi={10.1109/ICSGTEIS60500.2023.10424123},
  ISSN={2831-400X},
  month={Nov},}@INBOOK{10951267,
  author={Baker, Pam},
  booktitle={ChatGPT For Dummies}, 
  title={Warnings, Ethics, and Responsible AI}, 
  year={2023},
  volume={},
  number={},
  pages={71-85},
  abstract={Summary <p>Each model improvement aims to increase stability and overall performance in terms of reliability, accuracy, and ethics. This chapter helps the readers to learn what that means and why the effort is critical. Many artificial intelligence (AI) providers and communities embrace the baseline principals promoted by the Responsible AI movement, which include the following: accountability, bias evaluation, reliability and safety, fairness and accessibility, transparency and explainability, and privacy and security. OpenAI has repeatedly stated a commitment to Responsible AI principles. Several countries are working to contain consequences from the proliferation of AI models. OpenAI released each ChatGPT version with clearly posted public warnings. The US Copyright Office ruled that any works containing AI&#x2010;generated content can be copyrighted only to the extent of human authorship. AI can be objectively judged on predictability. The four categories used to determine the level of confidence an AI has in its response are repeatability, believability, sufficiency, and adaptability.</p>},
  keywords={Wireless sensor networks;Optimization;Agriculture;Reliability;Machine learning;Surveillance;Machine learning algorithms;Genetic algorithms;Energy efficiency;Routing protocols},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394204649},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951267},}@INPROCEEDINGS{10936699,
  author={Badal, Kushal and Zhu, Binhai and Liu, Xiaowen and Qingge, Letu},
  booktitle={2025 27th International Conference on Advanced Communications Technology (ICACT)}, 
  title={PSF: A Web Application Tool for Protein Scaffold Filling}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The protein scaffold filling problem remains a significant challenge in computational proteomics, which is critical for accurate protein function prediction and drug design. Despite recent advancements, current sequencing methods often yield incomplete protein sequences, referred to as scaffolds, which require precise filling for further analysis. This paper presents a web-based application, implemented using the Django framework, adopting our previously developed machine learning and deep learning techniques for protein scaffold filling. The platform allows users to try our pre-trained models or train models on their datasets for new scaffolds. This system provides a versatile tool for researchers in computational proteomics, enhancing the efficiency of protein sequence prediction. The developed web application can be accessed through https://psf.ncat.edu/.},
  keywords={Proteins;Deep learning;Drugs;Sequential analysis;Generative AI;Computational modeling;Proteomics;Filling;Protein sequence;Computational efficiency;Protein Scaffold Filling;Deep Learning;Generative AI;Django;Web Application},
  doi={10.23919/ICACT63878.2025.10936699},
  ISSN={1738-9445},
  month={Feb},}@INPROCEEDINGS{11108679,
  author={K, Sneha and Badrinath, Priya},
  booktitle={2025 International Conference on Emerging Technologies in Computing and Communication (ETCC)}, 
  title={FITS: Fashion Innovation Through Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={FITS: Fashion Innovation Through Synthesis represents an advanced AI-driven fashion recommendation system that utilizes deep learning, computer vision, and artificial intelligence to offer highly personalized styling guidance. By employing YOLOv11 for facial detection and MediaPipe Pose for body analysis, the system delivers real-time outfit suggestions tailored to users' body type, skin tone, and current weather conditions. A distinctive feature of FITS is its weather-aware fashion recommendations, wherein the system incorporates real-time meteorological data to suggest outfits optimized for both comfort and style. Additionally, the system supports image-based product recognition, enabling users to upload clothing images and receive AI-generated suggestions for visually similar fashion items. Designed with a privacy-conscious, real-time architecture, the system guarantees low-latency performance and secure user interactions. Future enhancements include the expansion of the fashion dataset and the integration of e-commerce APIs, facilitating a seamless AI-driven shopping experience. By integrating state-of-the-art deep learning models with interactive recommendations, FITS provides a next-generation fashion advisory platform tailored to individual user preferences.},
  keywords={Deep learning;Technological innovation;Pose estimation;Object detection;Real-time systems;Skin;Systems support;Recommender systems;Next generation networking;Residual neural networks;YOLOv11;ResNet50;MediaPipe Pose Estimation;OpenCv;Image-Based Recommendations;Fashion Chatbot;Product Recommendation Engine;Real-Time Object Detection},
  doi={10.1109/ETCC65847.2025.11108679},
  ISSN={},
  month={June},}@INPROCEEDINGS{10543384,
  author={Gunda, Manisha and Manda, Vinay and Naradasu, Pranay and Mekala, Sanjaykumar and Bhattacharya, Sandip},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={ChatGPT in Cyber Onslaught and Fortification: Past, Present, and Future}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Contemporary conversational models of language like ChatGPT have progressed tremendously in recent years. The comprehensive review includes ChatGPT's history, uses, challenges, and possible future advancements. The paper review also comprises the role of ChatGPT in cyber onslaught and cyber fortification. ChatGPT has numerous benefits, but it also has certain drawbacks. Depending on the data it is trained on, one of its challenges may be bias. Lastly, because ChatGPT is still in its early stages of development, it occasionally struggles to comprehend complex or complex commands. This paper review tackle ChatGPT's and other generative AI models' futures in their concluding remarks. They forecast that ChatGPT will become continually integrated with other technologies, like augmented reality and virtual assistants. Furthermore, they predict that ChatGPT will become more and more important in domains like scientific research and others that need sophisticated natural language processing.},
  keywords={Codes;Reviews;Generative AI;Virtual assistants;Chatbots;Turning;History;GPT;AI;Regenerate},
  doi={10.1109/I2CT61223.2024.10543384},
  ISSN={},
  month={April},}@INPROCEEDINGS{11135945,
  author={Surya, S. and K, Navaprakash and A, Bhavya and A, Aarthi},
  booktitle={2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)}, 
  title={Real-Time Face Recognition and Monitoring System for High-Risk Individuals}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={The development of computer networks and artificial intelligence has played an important role in contemporary surveillance systems. In this paper, an adaptive and intelligent face surveillance and recognition system in real-time that was developed with the aim of object detection and tracking of hazardous objects is discussed. The system operates based on high-end facial recognition algorithms using GPS location tracking in addition to GSM modules for communications to provide adequate location tracking and fast data transmission. Its architecture is built to support continuous observation, proactive alerts, and immediate response mechanisms from remote locations, thereby enhancing both operational security and decision-making efficiency. By automating recognition and alerting processes, the system minimizes errors that result from fatigue or complacency on the part of human beings, thus enabling staff to concentrate on high-priority strategic tasks. The solution is extremely potential for deployment in public safety, law enforcement, and security-sensitive environments.},
  keywords={GSM;Face recognition;Surveillance;Object detection;Real-time systems;Public security;Sensor systems;Sensors;Security;Global Positioning System;Face recognition;high-risk individuals;surveillance;GSM module;GPS tracking;public safety},
  doi={10.1109/SENNET64220.2025.11135945},
  ISSN={},
  month={July},}@INPROCEEDINGS{10931974,
  author={Jha, Saurabh and Sanghi, Akash and Upadhyay, Pragati and Agarwal, Gaurav},
  booktitle={2024 International Conference on Communication, Control, and Intelligent Systems (CCIS)}, 
  title={Analysing the Identification Approaches of Deep Fake Images and Videos Encapsulated in Fake Contents Available on Social Platforms}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The 21st century is characterized by the emergence of AI tools, which have become essential in today's diverse application landscape. These tools find utility across various sectors such as research, customer service, medical sciences, education, software industries, and factories. AI has the capability to generate images and videos that closely resemble reality, often making it challenging to distinguish between authentic and altered content. In contemporary usage, artificial intelligence software is frequently employed in creating WhatsApp videos and Instagram reels, blurring the line between reality and fabrication. Consequently, there is a tendency for such manipulated content to circulate unchecked, masquerading as genuine news. In this research paper, we aim to examine instances of ostensibly authentic news that have been tampered with using image editing tools. Accurate identification of such manipulated content is crucial, particularly as it may serve as essential legal evidence.},
  keywords={Industries;Deepfakes;Freeware;Law;Production facilities;Web sites;Multimedia communication;Internet telephony;Intelligent systems;Faces;Deep fakes;face2face;face swap;face forensics++;Neural Textures;Exception;MobileNet},
  doi={10.1109/CCIS63231.2024.10931974},
  ISSN={},
  month={Dec},}@INBOOK{10955666,
  author={Siva Kumar, Ram Shankar and Anderson, Hyrum and Schneier, Bruce},
  booktitle={Not with a Bug, But with a Sticker: Attacks on Machine Learning Systems and What To Do About Them}, 
  title={Sailing for Adventure on the Deep Blue Sea}, 
  year={2023},
  volume={},
  number={},
  pages={133-158},
  abstract={Summary <p>Named after the intrepid traveler, the JASON advisory group is a star&#x2010;studded team of Nobel laureates, scientists, and professors that advises the U.S. Department of Defense (DoD) on strategic topics. The DoD tasks this distinguished group of experts to investigate thorny issues in national security&#x2014;everything from hypersonic missiles to quantum mechanics to the U.S. census&#x2014;and JASON comes back recommendations. The DoD has always been interested in artificial intelligence (AI). Even from its early days, AI was destined to become a tool for geopolitical, technological, and military advantage. The &#x201c;winner takes all&#x201d; dynamic also applies to AI development. This can be attributed to three reasons Anderson identified in the early days of commercial software and the Internet, which we depict in the context of AI systems. They are: Up&#x2010;front investment; High switching costs; and Mutual benefits.</p>},
  keywords={Artificial intelligence;US Department of Defense;Security;Automobiles;Testing;Computer bugs;Standards organizations;Snow;Random access memory;Organizations},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119884903},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10955666},}@INBOOK{10955667,
  author={Siva Kumar, Ram Shankar and Anderson, Hyrum and Schneier, Bruce},
  booktitle={Not with a Bug, But with a Sticker: Attacks on Machine Learning Systems and What To Do About Them}, 
  title={The Big One}, 
  year={2023},
  volume={},
  number={},
  pages={159-180},
  abstract={Summary <p>Experts like the authors, who deal with securing artificial intelligence (AI) systems daily, might make for the worst prognosticators of what will happen in the future. So, taking take a page from Superforcasting, a book authored by Wharton professor Philip Tetlock and journalist Dan Gardner, the authors spoke to hundreds of stakeholders in security and AI. This ranged from upper management to front&#x2010;line professionals, from medical doctors to U.S. career diplomats to data scientists, from researchers to agriculturists employing AI, from policy wonks to venture capitalists. In 2018, at the height of adversarial machine learning interest in academia, the authors wanted to know how organizations were thinking about securing their AI systems. They saw contemporary movement in the standards and regulations space addressing safety and security. After all, AI systems are more likely to fail due to common errors than because of a malicious adversary actively trying to subvert them.</p>},
  keywords={Artificial intelligence;Standards organizations;Internet;Industries;Government;Auditory system;Random access memory;Computer hacking;Social networking (online);Safety},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119884903},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10955667},}@ARTICLE{9763358,
  author={Zhang, Zhi and Liu, Yan and Zhong, Sheng-hua},
  journal={IEEE Transactions on Affective Computing}, 
  title={GANSER: A Self-Supervised Data Augmentation Framework for EEG-Based Emotion Recognition}, 
  year={2023},
  volume={14},
  number={3},
  pages={2048-2063},
  abstract={Electroencephalography (EEG)-based affective computing has a scarcity problem. As a result, it is difficult to build effective, highly accurate and stable models using machine learning algorithms, especially deep learning models. Data augmentation has recently shown performance improvements in deep learning models with increased accuracy, stability and reduced overfitting. In this paper, we propose a novel data augmentation framework, named the generative adversarial network-based self-supervised data augmentation (GANSER). As the first to combine adversarial training with self-supervised learning for EEG-based emotion recognition, the proposed framework generates high-quality and high-diversity simulated EEG samples. In particular, we utilize adversarial training to learn an EEG generator and force the generated EEG signals to approximate the distribution of real samples, ensuring the quality of the augmented samples. A transformation operation is employed to mask parts of the EEG signals and force the generator to synthesize potential EEG signals based on the unmasked parts to produce a wide variety of samples. A masking possibility during transformation is introduced as prior knowledge to generalize the classifier for the augmented sample space. Finally, numerous experiments demonstrate that our proposed method can improve emotion recognition with an increase in performance and achieve state-of-the-art results.},
  keywords={Electroencephalography;Emotion recognition;Brain modeling;Training;Generative adversarial networks;Deep learning;Electrodes;EEG-based emotion recognition;data augmentation;generative adversarial networks},
  doi={10.1109/TAFFC.2022.3170369},
  ISSN={1949-3045},
  month={July},}@INPROCEEDINGS{10193232,
  author={Jain, Mr Ashish and Rao, B. Srinivasa and Chattopadhyay, Saumitra and Kumar, Aniruddh and Muthuraman, M. S. and Manjula, A.},
  booktitle={2023 4th International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={An Artificial Intelligence Network based-Host Intrusion Detection System for Internet of Things Devices}, 
  year={2023},
  volume={},
  number={},
  pages={656-661},
  abstract={Internet of Things (IoT) is currently employed in almost all the areas, including applications in smart cities, smart homes, e-Wellbeing, and others. Due to its wider utilization, IoT security has become a serious concern. A secure Intrusion Detection System (IDS) for the Internet of Things is often built using artificial intelligence (AI) and its subsets, deep learning (DL), and machine learning (ML). Industrial IoT devices, which are readily available, are regularly used by researchers and industry experts. This research study investigates the possibility of deploying a DL-Based Host-IDS (DL-HIDS) on specific commercial IoT devices. In this study, an optimized Convolutional Neural Network (O-CNN) based on DL is used. The proposed model’s efficiency is evaluated by utilizing performance metrics like recall, precision, accuracy, and f1score. The proposed model’s effectiveness is verified by analyzing the promising results obtained from the implementation of the proposed DL-HIDS on various existing models.},
  keywords={Measurement;Industries;Analytical models;Smart cities;Neural networks;Intrusion detection;Smart homes;Deep Learning;Internet of Things;Intrusion Detection System;Artificial Intelligence;Convolutional Neural Network},
  doi={10.1109/ICESC57686.2023.10193232},
  ISSN={},
  month={July},}@INPROCEEDINGS{9381319,
  author={Zhao, Ying and Mata, Gabe E.},
  booktitle={2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
  title={Leverage Artificial Intelligence to Learn, Optimize, and Win (LAILOW) for the Marine Maintenance and Supply Complex System}, 
  year={2020},
  volume={},
  number={},
  pages={678-684},
  abstract={A complex enterprise includes multiple subsystems and organizations. The U.S. Marine Corps (USMC) maintenance and supply chain is a complex enterprise and exemplifies a socio-technological infrastructures. It is imperative for the USMC to adopt more advanced data sciences including ML/AI techniques to the entire spectrum or end-to-end (E2E) logistic planning as a complex enterprise including maintenance, supply, transportation, health services, general engineering, and finance. In this paper, we first review an overall framework of leveraging artificial Intelligence to learn, optimize, and win (LAILOW) for a complex enterprise, and then show how a LAILOW framework is applied to the USMC maintenance and supply chain data as a use case. We also compare various machine learning (ML) algorithms such as supervised machine learning/predictive models and unsupervised machine learning algorithms such as lexical link analysis (LLA). The contribution of the paper is that LLA computes stable and sensitive components of a complex system with respective to a perturbation. LLA allows to discover and search for associations, predict probability of demand and fail rates, prepare spare parts, and improve operational availability and readiness.},
  keywords={Machine learning algorithms;Databases;Perturbation methods;Supply chains;Transportation;Maintenance engineering;Complex systems;supervised machine learning;predictive models;unsupervised machine learning;lexical link analysis;LLA;association patterns;data mining;maintenance and supply chain;complex system;leverage artificial intelligence to learn;optimize;win;LAILOW},
  doi={10.1109/ASONAM49781.2020.9381319},
  ISSN={2473-991X},
  month={Dec},}@INPROCEEDINGS{10961869,
  author={Krishnamurthy, Dasaprakash and Neelanath, Vinod},
  booktitle={2025 Emerging Technologies for Intelligent Systems (ETIS)}, 
  title={Establishing a Robust LLMOps Framework for Intelligent Automation: Strategies and Best Practices}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper delves into the establishment of a robust LLMOps framework, crucial for intelligent automation in modern enterprises. LLMOps for intelligent automation platforms require composability and capability to use different architectures, embeddings, and Generative AI models seamlessly with domain focus. In this paper we discuss an architecture we designed for UST SmartOps, UST's Intelligent Automation platform, keeping automation at its core. Large Language Models (LLMs) have significantly advanced AI capabilities in natural language processing and understanding, but their deployment and management, integration and using them effectively for automation use cases pose unique challenges. LLMOps, similar to DevOps and MLOps, is dedicated to the operationalization of these models, addressing infrastructure, performance optimization, security, and continuous improvement. The paper outlines the strategies and best practices essential for LLMOps, emphasizing data management, model monitoring, and the integration of AI with automation to improve operational efficiency, reduce costs, and improve decision-making processes. Through systematic orches-tration of tools, processes, and infrastructure, the document aims to provide a comprehensive guide to effectively leverage LLMs within intelligent automation frameworks.},
  keywords={Intelligent automation;Costs;Systematics;Large language models;Organizations;Security;Reliability;Monitoring;Optimization;Best practices;LLMOps;LLMOps framework;AI infrastructure;AI security;Model monitoring;LLM Cost Optimization},
  doi={10.1109/ETIS64005.2025.10961869},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11073654,
  author={Tóthfalusi, Tamás and Csiszár, Zoltán and Varga, Pál},
  booktitle={NOMS 2025-2025 IEEE Network Operations and Management Symposium}, 
  title={TACO - A Generative AI Copilot for Intent-Based Telecommunication Core Network Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper presents the methodology for using LLMs to ease core network signaling analysis. This is done by applying RAG techniques to process standards that describe protocol data formats - and then asking natural language questions about actual capture traces. Analyzing 5G networks is very challenging due to the complex and dynamic nature of signaling protocols. Unlike previous generations, protocol fields and values are described in a human-readable format, enabling textual post-processing, and the direct application of LLM models. Intent-based network management involves natural language-based human interaction with the networking equipment so the desired outcome is achieved without step-by-step instructions and settings by the human. This paper proposes a novel approach that uses the combination of Retrieval Augmented Generation (RAG) and Langchain to automatically answer human questions regarding signalling data. This toolchain makes the analysis part of network fault management intent-based. Moreover, by training LLMs on a vast corpus of standardized signaling data, we demonstrate the model's ability to generate realistic test data. This approach improves the efficiency of automated test environments, ensuring the reliability and performance of networks in real-world conditions.},
  keywords={Training;Protocols;Generative AI;Retrieval augmented generation;Natural languages;Network analyzers;Data models;Communications technology;Telecommunication network reliability;Standards},
  doi={10.1109/NOMS57970.2025.11073654},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{10100131,
  author={Zhou, Chunrong and Jiang, Zhenghong},
  booktitle={2023 IEEE International Conference on Integrated Circuits and Communication Systems (ICICACS)}, 
  title={Artificial Intelligence-Based Super-Resolution Reconstruction Algorithm for Pulsed Multi-Frame Images}, 
  year={2023},
  volume={},
  number={},
  pages={01-07},
  abstract={Pulsed SRR is a research hotspot that has emerged in recent years, and it has certain practicality, but there are still some limitations in practical applications. Pulsed SRR is a method that can reconstruct images according to different image requirements. The image SRR can be achieved by using the principle of multi-frame imaging. The main objective of this paper is to investigate the pulse multiframe image SRR algorithm based on artificial intelligence. In this paper, a MFI dataset is used to evaluate the effectiveness of the system reconstruction model. Firstly, a method with strong similarity is applied to generate matching images to improve the reconstruction effect and quality. The method is optimized on Data Market Alpha version (ML) and is able to retain image files with higher frame rate under the same conditions. The results obtained after several experimental trainings are a significant improvement compared to the classical algorithm. There are also significant differences in the predicted resolution and contrast obtained by the two different versions of the algorithm on the test dataset. In addition, the algorithm is optimized to improve efficiency and avoid local network performance degradation due to inter-frame interference, since a larger bandwidth needs to be used for transmission. This paper focuses on using this approach to reduce the memory footprint during compression, effectively improving the processing speed performance of the system. In practical applications, this method can be used for image SRR and other applications to improve the resolution.},
  keywords={Degradation;Training;System performance;Energy resolution;Superresolution;Memory;Reconstruction algorithms;artificial intelligence;impulse function;multi-frame images;super-resolution reconstruction},
  doi={10.1109/ICICACS57338.2023.10100131},
  ISSN={},
  month={Feb},}@ARTICLE{10274865,
  author={Liu, Zhe and Li, Yun and Yao, Lina and Chang, Xiaojun and Fang, Wei and Wu, Xiaojun and Saddik, Abdulmotaleb El},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Simple Primitives With Feasibility- and Contextuality-Dependence for Open-World Compositional Zero-Shot Learning}, 
  year={2024},
  volume={46},
  number={1},
  pages={543-560},
  abstract={The task of Open-World Compositional Zero-Shot Learning (OW-CZSL) is to recognize novel state-object compositions in images from all possible compositions, where the novel compositions are absent during the training stage. The performance of conventional methods degrades significantly due to the large cardinality of possible compositions. Some recent works consider simple primitives (i.e., states and objects) independent and separately predict them to reduce cardinality. However, it ignores the heavy dependence between states, objects, and compositions. In this paper, we model the dependence via feasibility and contextuality. Feasibility-dependence refers to the unequal feasibility of compositions, e.g., hairy is more feasible with cat than with building in the real world. Contextuality-dependence represents the contextual variance in images, e.g., cat shows diverse appearances when it is dry or wet. We design Semantic Attention (SA) to capture the feasibility semantics to alleviate impossible predictions, driven by the visual similarity between simple primitives. We also propose a generative Knowledge Disentanglement (KD) to disentangle images into unbiased representations, easing the contextual bias. Moreover, we complement the independent compositional probability model with the learned feasibility and contextuality compatibly. In the experiments, we demonstrate our superior or competitive performance, SA-and-kD-guided Simple Primitives (SAD-SP), on three benchmark datasets.},
  keywords={Visualization;Semantics;Probability distribution;Footwear;Pattern recognition;Training;Context modeling;Attention network;compositional zero-shot learning;generative network;knowledge disentanglement;open world},
  doi={10.1109/TPAMI.2023.3323012},
  ISSN={1939-3539},
  month={Jan},}@INPROCEEDINGS{11131784,
  author={Žufić, J. and Debeljuh, A. and Kukuljan, D.},
  booktitle={2025 MIPRO 48th ICT and Electronics Convention}, 
  title={Frequency of Use and User Feelings When Using ChatGPT}, 
  year={2025},
  volume={},
  number={},
  pages={430-436},
  abstract={ChatGPT is an advancement in natural language processing and neural network design. It is a variant of generative pre-trained transformer models, specifically adapted for conversational tasks. This study explores the frequency and purpose of use, and feelings of users when using ChatGPT among high school and university students. Respondents were primarily from Croatia and Slovenia, with approximately 10 % coming from other countries. The study included participants. Findings indicate that curiosity was the main reason for respondents' first use of ChatGPT. They were positively surprised by the speed of responses, did not feel frustrated by inaccuracies, were not concerned about potential misuse of ChatGPT, and did not express privacy concerns. Furthermore, when first using ChatGPT for writing assignments, essays, or seminar papers, they felt satisfied, happy, and curious. They did not feel confused, disappointed, or uncomfortable - they did not experience shame. Regarding their current usage, respondents believe that ChatGPT provides adequate responses. They most frequently use it to generate ideas and gain alternative perspectives, while the least common use is copying responses verbatim. In terms of perceived benefits, respondents believe they can continue their education without relying on ChatGPT and do not consider its use to be ultimately harmful.},
  keywords={Seminars;Privacy;Adaptation models;Education;Neural networks;Writing;Chatbots;Transformers;Artificial intelligence;ChatGPT;frequency of ChatGPT usage among students;user feelings when using ChatGPT},
  doi={10.1109/MIPRO65660.2025.11131784},
  ISSN={1847-3938},
  month={June},}@ARTICLE{11175542,
  author={Wan, Zheng and Zhao, Shenglu and Dong, Xiaogang and Liu, Xuelin and Tan, Yifeng and Fang, Yuming},
  journal={IEEE Transactions on Multimedia}, 
  title={Distributed Two-Tier Cache Optimization in Metaverse Scenarios Combining MADDPG and GCN}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={The rapid emergence of the Metaverse requires higher network throughput and lower latency to deliver immersive and responsive virtual experiences. Traditional centralized data processing approaches are constrained by limited computational and bandwidth resources when handling large-scale user data. A Cloud-Edge-End transmission architecture is proposed in this study, tailored for Metaverse scenarios to optimize resource allocation, minimize latency, and enhance rendering efficiency. A real-time trajectory segment prediction scheme (FDK) was developed, which combines FastDTW with K-means by leveraging user behavior trajectories to determine subscene popularity and store them on GPU servers, thereby reducing user wait time. A two-tier cache optimization scheme (MAE2C) is also proposed, incorporating GCN for subscene feature identification. GPU servers employ the MADDPG strategy to cache popular subscenes, while edge servers utilize DDPG to cache missed scenes. This approach effectively reduces cloud access and cache replacement frequency. Simulation results demonstrate that the subscene cache hit rate of the MAE2C scheme significantly outperforms existing methods across various cache capacities, with a 6.9% reduction in cache replacement frequency. This research provides effective technical support for Metaverse scene rendering and offers insights into the development of generative Metaverse systems.},
  keywords={Metaverse;Trajectory;Servers;Real-time systems;Rendering (computer graphics);Optimization;Graphics processing units;Three-dimensional displays;Collaboration;Cloud computing;Metaverse;Edge Caching;MADDPG;Graph Neural Network;FastDTW},
  doi={10.1109/TMM.2025.3613168},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10261631,
  author={Yu, Hao and Liu, Zhaoyang and Guo, Yunyun},
  booktitle={2023 5th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Application Status, Problems and Future Prospects of Generative AI in Education}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The emergence of Chat GPT signifies another revolutionary wave of information technology brought about by generative AI. This article introduces the development and technical support of generative AI, revealing that there are four main issues regarding its application in the education field: opacity and inexplicability, data privacy and security, individualization and fairness, and effectiveness and reliability. The article then looks forward to the future trends of generative AI in the education field from four aspects: personalized education, intelligent teaching, joint education, and virtual teaching, aiming to provide important reference value for research and practice in this field.},
  keywords={Training;Learning systems;Ethics;Education;Virtual reality;Market research;Security;generative artificial intelligence;educational application;countermeasure research;development prospects},
  doi={10.1109/CSTE59648.2023.00065},
  ISSN={},
  month={April},}@ARTICLE{10224242,
  author={Tan, Yong Xuan and Lee, Chin Poo and Neo, Mai and Lim, Kian Ming and Lim, Jit Yan and Alqahtani, Ali},
  journal={IEEE Access}, 
  title={Recent Advances in Text-to-Image Synthesis: Approaches, Datasets and Future Research Prospects}, 
  year={2023},
  volume={11},
  number={},
  pages={88099-88115},
  abstract={Text-to-image synthesis is a fascinating area of research that aims to generate images based on textual descriptions. The main goal of this field is to generate images that match the given textual description in terms of both semantic consistency and image realism. While text-to-image synthesis has shown remarkable progress in recent years, it still faces several challenges, mainly related to the level of image realism and semantic consistency. To address these challenges, various approaches have been proposed, which mainly rely on Generative Adversarial Networks (GANs) for optimal performance. This paper provides a review of the existing text-to-image synthesis approaches, which are categorized into four groups: image realism, multiple scene, semantic enhancement, and style transfer. In addition to discussing the existing approaches, this paper also reviews the widely used datasets for text-to-image synthesis, including Oxford-102, CUB-200-2011, and COCO. The evaluation metrics used in this field are also discussed, including Inception Score, Fréchet Inception Distance, Structural Similarity Index, R-precision, Visual-Semantic Similarity, and Semantic Object Accuracy. The paper also offers a compilation of the performance of existing works in the field.},
  keywords={Generators;Semantics;Computer architecture;Generative adversarial networks;Visualization;Convolutional neural networks;Task analysis;Text categorization;Image classification;Surveys;Text-to-image synthesis;generative model;GAN;generative adversarial networks;review;survey},
  doi={10.1109/ACCESS.2023.3306422},
  ISSN={2169-3536},
  month={},}@ARTICLE{10410841,
  author={Mubin, Omar and Alnajjar, Fady and Trabelsi, Zouheir and Ali, Luqman and Parambil, Medha Mohan Ambali and Zou, Zhao},
  journal={IEEE Access}, 
  title={Tracking ChatGPT Research: Insights From the Literature and the Web}, 
  year={2024},
  volume={12},
  number={},
  pages={30518-30532},
  abstract={This article presents a scientometric and literature analysis of current research on ChatGPT, a conversational AI technology developed by OpenAI. Using various databases, 103 relevant articles were retrieved and analyzed through scientometric, quantitative, and application-based approaches. A Google trend analysis and comparison with other generative AI and chatbot technologies were also carried out. The study provides insights into the distribution of ChatGPT publications across different countries and regions, the network of co-occurring keywords, authorship analysis, article typology, and publishing entities. The findings offer a comprehensive overview of the current state of ChatGPT research, highlighting key directions for future research. The study finds that ChatGPT has gained significant attention and interest in online platforms, particularly in technology, education, and healthcare, and highlights potential ethical and legal concerns related to its use. Its applications extend to several literary and text generation areas. We do note that the sample of extracted publications is lower than anticipated due to the niche area of investigation. The article is relevant to researchers, practitioners, and policymakers interested in the field of AI-powered language models, especially ChatGPT.},
  keywords={Chatbots;Bibliometrics;Education;Artificial intelligence;Task analysis;Market research;Search engines;Natural language processing;Open Access;ChatGPT;artificial intelligence;natural language processing;NLM},
  doi={10.1109/ACCESS.2024.3356584},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9902599,
  author={Ye, Zhuoxun and Liu, Meiqin and Zhang, Senlin and Wei, Ping},
  booktitle={2022 41st Chinese Control Conference (CCC)}, 
  title={Dual-Path GAN: A Method for Enhancing Small-scale Defect Detection on Metal Images}, 
  year={2022},
  volume={},
  number={},
  pages={6292-6297},
  abstract={Automated surface inspection (ASI) is an important research content in computer vision. In recent years, with the application of deep learning models represented by convolutional neural networks (CNN) in computer vision, surface defect detection based on computer vision has made impressive progress. However, compared with a few or dozens of pictures in real industrial scenes, traditional deep learning methods require a large amount of annotation data for training, so it is quite difficult to adapt to the complex industrial scenarios with varying surface profiles, lighting conditions, imaging angles and environments. Despite these difficulties, there is still a wide gap in the performance between the detection of small-scale and large-scale objects. Traditional Generative Adversarial Nets (GANs) based augmentation method can only be used for classification networks or unsupervised learning, and if applied to detection networks, they need to be labeled, which requires much labor and time. In order to solve these problems, a metal surface small-scale defect images augmentation method is proposed. Our method contains two parts: generation part and augmentation part. In the generation part, two pairs of generators and discriminators are used to generate the defect areas and the background areas of the image, which can not only generate more realistic defect images than others but also save the training process for surface images. In the augmentation part, the real defect images and the generated defect images are concatenated before copy-pasting them on background images to generate annotated dataset for training. We conduct experiments on our metal surface dataset. The experimental results show that our method can generate high-quality defect samples and background samples, which greatly enriches the original dataset. We evaluate different augmentation strategies, and ultimately, we achieve 6.2% improvement on baseline and 4.4% on copy-pasting method.},
  keywords={Training;Deep learning;Computer vision;Metals;Lighting;Object detection;Inspection;Generative adversarial nets;automated surface inspection;data augmentation;few-shot learning;small-scale object detection},
  doi={10.23919/CCC55666.2022.9902599},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{9710608,
  author={Saseendran, Amrutha and Skubch, Kathrin and Keuper, Margret},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Multi-Class Multi-Instance Count Conditioned Adversarial Image Generation}, 
  year={2021},
  volume={},
  number={},
  pages={6742-6751},
  abstract={Image generation has rapidly evolved in recent years. Modern architectures for adversarial training allow to generate even high resolution images with remarkable quality. At the same time, more and more effort is dedicated towards controlling the content of generated images. In this paper, we take one further step in this direction and propose a conditional generative adversarial network (GAN) that generates images with a defined number of objects from given classes. This entails two fundamental abilities (1) being able to generate high-quality images given a complex constraint and (2) being able to count object instances per class in a given image. Our proposed model modularly extends the successful StyleGAN2 architecture with a count-based conditioning as well as with a regression subnetwork to count the number of generated objects per class during training. In experiments on three different datasets, we show that the proposed model learns to generate images according to the given multiple-class count condition even in the presence of complex backgrounds. In particular, we propose a new dataset, CityCount, which is derived from the Cityscapes street scenes dataset, to evaluate our approach in a challenging and practically relevant scenario. An implementation is available at https://github.com/boschresearch/MCCGAN.},
  keywords={Training;Representation learning;Deep learning;Image resolution;Image synthesis;Layout;Training data;Neural generative models;Adversarial learning},
  doi={10.1109/ICCV48922.2021.00669},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9382077,
  author={Yang, Ziduo and Zhao, Lu and Wu, Shuyu and Chen, Calvin Yu-Chian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Lung Lesion Localization of COVID-19 From Chest CT Image: A Novel Weakly Supervised Learning Method}, 
  year={2021},
  volume={25},
  number={6},
  pages={1864-1872},
  abstract={Chest computed tomography (CT) image data is necessary for early diagnosis, treatment, and prognosis of Coronavirus Disease 2019 (COVID-19). Artificial intelligence has been tried to help clinicians in improving the diagnostic accuracy and working efficiency of CT. Whereas, existing supervised approaches on CT image of COVID-19 pneumonia require voxel-based annotations for training, which take a lot of time and effort. This paper proposed a weakly-supervised method for COVID-19 lesion localization based on generative adversarial network (GAN) with image-level labels only. We first introduced a GAN-based framework to generate normal-looking CT slices from CT slices with COVID-19 lesions. We then developed a novel feature match strategy to improve the reality of generated images by guiding the generator to capture the complex texture of chest CT images. Finally, the localization map of lesions can be easily obtained by subtracting the output image from its corresponding input image. By adding a classifier branch to the GAN-based framework to classify localization maps, we can further develop a diagnosis system with improved classification accuracy. Three CT datasets from hospitals of Sao Paulo, Italian Society of Medical and Interventional Radiology, and China Medical University about COVID-19 were collected in this article for evaluation. Our weakly supervised learning method obtained AUC of 0.883, dice coefficient of 0.575, accuracy of 0.884, sensitivity of 0.647, specificity of 0.929, and F1-score of 0.640, which exceeded other widely used weakly supervised object localization methods by a significant margin. We also compared the proposed method with fully supervised learning methods in COVID-19 lesion segmentation task, the proposed weakly supervised method still leads to a competitive result with dice coefficient of 0.575. Furthermore, we also analyzed the association between illness severity and visual score, we found that the common severity cohort had the largest sample size as well as the highest visual score which suggests our method can help rapid diagnosis of COVID-19 patients, especially in massive common severity cohort. In conclusion, we proposed this novel method can serve as an accurate and efficient tool to alleviate the bottleneck of expert annotation cost and advance the progress of computer-aided COVID-19 diagnosis.},
  keywords={Computed tomography;COVID-19;Lesions;Location awareness;Generators;Medical diagnostic imaging;Lung;Coronavirus disease 2019;weakly supervised learning;generative adversarial network;lesion localization;lesion segmentation},
  doi={10.1109/JBHI.2021.3067465},
  ISSN={2168-2208},
  month={June},}@ARTICLE{10614634,
  author={Huang, Yudong and Du, Hongyang and Zhang, Xinyuan and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Wang, Shuo and Huang, Tao},
  journal={IEEE Network}, 
  title={Large Language Models for Networking: Applications, Enabling Techniques, and Challenges}, 
  year={2025},
  volume={39},
  number={1},
  pages={235-242},
  abstract={The rapid evolution of network technologies and the growing complexity of network tasks necessitate a paradigm shift in how networks are designed, configured, and managed. With a wealth of knowledge and expertise, large language models (LLMs) are one of the most promising candidates. This paper aims to pave the way for constructing domain-adapted LLMs for networking. Firstly, we present potential LLM applications for vertical network fields and showcase the mapping from natural language to network language. Then, several enabling technologies are investigated, including parameter-efficient finetuning and prompt engineering. The insight is that language understanding and tool usage are both required for network LLMs. Driven by the idea of embodied intelligence, we propose the ChatNet, a domain-adapted network LLM framework with access to various external network tools. ChatNet can reduce the time required for burdensome network planning tasks significantly, leading to a substantial improvement in processing efficiency. Finally, key challenges and future research directions are highlighted.},
  keywords={Natural languages;Task analysis;Manuals;Protocols;Knowledge engineering;Artificial intelligence;Planning;Large Language Models;Generative AI;Intentdriven Networking;Network Intelligence},
  doi={10.1109/MNET.2024.3435752},
  ISSN={1558-156X},
  month={Jan},}@ARTICLE{10123949,
  author={Wang, Chuang and Wang, Zidong and Ma, Lifeng and Dong, Hongli and Sheng, Weiguo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Subdomain-Alignment Data Augmentation for Pipeline Fault Diagnosis: An Adversarial Self-Attention Network}, 
  year={2024},
  volume={20},
  number={2},
  pages={1374-1384},
  abstract={Data augmentation (DA) has the potential to address the issue of imbalanced and insufficient datasets (I&ID) in pipeline fault diagnosis. However, the majority of existing DA methods for time series are inspired by computer vision techniques, ignoring the temporal dynamic properties and fine-grained fault features, which leads to limited performance of the augmentation. To tackle this problem, we introduce a novel DA approach called the subdomain-alignment adversarial self-attention network (SA-ASN), which takes into account both temporal association and semantic correlation. Our approach features a novel temporal association learning (TAL) mechanism, which transfers temporal information from the discriminator to the generator via a customized knowledge-sharing structure, improving the reliability of synthetic long-range associations. Additionally, we introduce a prototype-assisted subdomain alignment (PASA) strategy that forms a hierarchical structure in the synthetic dataset by incorporating local semantic correlation into the model training. With the support of TAL and PASA, our SA-ASN algorithm enhances the authenticity of temporal structure at the instance level and improves the discriminability of fault features at the category level. Our experimental results show that the SA-ASN algorithm provides a more diverse and accurate augmentation of pipeline data. The effectiveness of our SA-ASN algorithm encourages the use of data-driven diagnostic models in complex real-world oilfield pipeline networks.},
  keywords={Fault diagnosis;Pipeline processing;Generative adversarial networks;Generators;Training;Heuristic algorithms;Time series analysis;Adversarial learning;data augmentation;multi-head self-attention mechanism;pipeline fault diagnosis;subdomain alignment},
  doi={10.1109/TII.2023.3275701},
  ISSN={1941-0050},
  month={Feb},}@ARTICLE{9954208,
  author={Wang, Jiaxiang and Li, Chenglong and Zheng, Aihua and Tang, Jin and Luo, Bin},
  journal={IEEE Transactions on Multimedia}, 
  title={Looking and Hearing Into Details: Dual-Enhanced Siamese Adversarial Network for Audio-Visual Matching}, 
  year={2023},
  volume={25},
  number={},
  pages={7505-7516},
  abstract={Audio-visual cross-modal matching aims to explore the intrinsic correspondence between face images and audio clips. Existing methods usually focus on the salient features of identities between visual images and voice clips, while neglecting their subtle differences, which are crucial to distinguishing cross-modal samples. To deal with this problem, we propose a novel Dual-enhanced Siamese Adversarial Network (DSANet), which pursues the adversarial dual enhancement to highlight both salient and subtle features for robust audio-visual cross-modal matching. First, we designed a dual enhancement mechanism to enhance potential subtle features by randomly selecting a region feature for salient feature suppression, while enhancing salient features in the corresponding region to ensure the global discriminative ability. Second, to establish the correlation of subtle features in the process of eliminating cross-modal heterogeneity, we design a siamese adversarial structure to perform modal heterogeneity elimination for both enhanced salient and subtle features in a parallel manner. Moreover, we propose an adaptive masked cross-entropy loss to force the network to focus on the feature differences among hard classes. Experiments on public benchmark datasets validate the effectiveness of the proposed algorithm.},
  keywords={Feature extraction;Task analysis;Generative adversarial networks;Synthetic aperture sonar;Representation learning;Visualization;Optimization;Adaptive masked cross-entropy;audio-visual cross-modal matching;dual enhancement mechanism;siamese adversarial network},
  doi={10.1109/TMM.2022.3222936},
  ISSN={1941-0077},
  month={},}@ARTICLE{10847932,
  author={Feng, Shuang and Zhang, Zhirui and Zheng, Yuhang and Lei, Jiaxing and Tang, Yi},
  journal={Journal of Modern Power Systems and Clean Energy}, 
  title={Transfer-Learning-Based BiLSTM-WGAN Approach for Synthetic Data Generation of Sub-Synchronous Oscillations in Wind Farms}, 
  year={2025},
  volume={13},
  number={4},
  pages={1199-1210},
  abstract={The phenomenon of sub-synchronous oscillation (SSO) poses significant threats to the stability of power systems. The advent of artificial intelligence (AI) has revolutionized SSO research through data-driven methodologies, which necessitates a substantial collection of data for effective training, a requirement frequently unfulfilled in practical power systems due to limited data availability. To address the critical issue of data scarcity in training AI models, this paper proposes a novel transfer-learning-based (TL-based) Wasserstein generative adversarial network (WGAN) approach for synthetic data generation of SSO in wind farms. To improve the capability of WGAN to capture the bidirectional temporal features inherent in oscillation data, a bidirectional long short-term memory (BiLSTM) layer is introduced. Additionally, to address the training instability caused by few-shot learning scenarios, the discriminator is augmented with mini-batch discrimination (MBD) layers and gradient penalty (GP) terms. Finally, TL is leveraged to fine-tune the model, effectively bridging the gap between the training data and real-world system data. To evaluate the quality of the synthetic data, two indexes are proposed based on dynamic time warping (DTW) and frequency domain analysis, followed by a classification task. Case studies demonstrate the effectiveness of the proposed approach in swiftly generating a large volume of synthetic SSO data, thereby significantly mitigating the issue of data scarcity prevalent in SSO research.},
  keywords={Oscillators;Generative adversarial networks;Data models;Power system stability;Training;Synthetic data;Feature extraction;Generators;Wind farms;Data mining;Sub-synchronous oscillation;wind farm;data scarcity;synthetic data generation;artifical intelligence;Wasserstein generative adversarial network;transfer learning},
  doi={10.35833/MPCE.2024.000550},
  ISSN={2196-5420},
  month={July},}@ARTICLE{9472967,
  author={Cai, Xingjuan and Lan, Yang and Zhang, Zhixia and Wen, Jie and Cui, Zhihua and Zhang, Wensheng},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Many-Objective Optimization Based Federal Deep Generation Model for Enhancing Data Processing Capability in IoT}, 
  year={2023},
  volume={19},
  number={1},
  pages={561-569},
  abstract={The rapid progress of artificial intelligence expands its wide applicability in Internet of Things (IoT). Meanwhile, data insufficient and data source privacy are key supply chain challenges facing IoT especially in the healthcare industry. To address this problem in healthcare IoT, in this article, we propose a skin cancer detection model based on federated learning integrated with deep generation model. First, we employ dual generative adversarial networks to address the problem of insufficient data. In addition, to improve the quality of generated images, we synchronously optimize the sharpness of images, Frechet inception distance, image diversity, and loss using knee point-driven evolutionary algorithm (KnEA). Then, we protect patient information privacy by training federated skin cancer framework. Finally, we employ the ISIC 2018 dataset to test the performance of the proposed training model under different situations, including using identically distributed data, nonidentically distributed data, a sparse convolutional neural network, and a fully connected convolutional neural network. The experiment results demonstrate that the accuracy and area under the curve reach 91% and 88%, respectively. This model can help resolve problems of insufficient data in smart medicine of IoT and protect the privacy of user data while also providing an excellent detection rate.},
  keywords={Skin cancer;Data models;Servers;Generators;Privacy;Generative adversarial networks;Evolutionary computation;Deep generative models;federated learning (FL);Internet of Thing (IoT);knee point-driven evolutionary algorithm (KnEA);skin cancer},
  doi={10.1109/TII.2021.3093715},
  ISSN={1941-0050},
  month={Jan},}@ARTICLE{10107629,
  author={Behera, Sourajit and Misra, Rajiv and Sillitti, Alberto},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={GAN-Based Multi-Task Learning Approach for Prognostics and Health Management of IIoT}, 
  year={2024},
  volume={21},
  number={3},
  pages={2742-2762},
  abstract={Health assessment (HA) & remaining useful life (RUL) estimation, the two essential pillars of the prognostics and health management (PHM) paradigm, help improve industrial equipment reliability while reducing maintenance costs. However, reported works treat HA and RUL estimation as disjoint problems though there exist unexploited similarities among these related issues. Additionally, in practical industrial working scenarios, equipment(s) stay in a normal condition for most of its lifespan, leading to a disproportionate training dataset, hampering the prediction accuracy. To overcome the above problems, we propose a data-driven multi-task learning framework aided by a novel least squares recurrent auxiliary classifier generative adversarial network (LS-R-ACGAN). LS-R-ACGAN employs recurrent neural networks (RNNs) in its generator & discriminator networks for multi-variate fault data generation while overcoming the vanishing gradient problem of ACGANs. Post-data-augmentation, a balanced training dataset, trains a multi-task learning model based on a deep gated RNN (DGRU) for joint HA and RUL estimation. Our simulations use the C-MAPSS dataset for testing the proposed approach’s accuracy. The final results showcase improvements by at least 354%238% Note to Practitioners—This study attacks multiple presumptions concerning the maintenance of complex systems including, ample availability of fault samples alongside normal samples and solving only one of the problems, i.e., health assessment (HA) or remaining useful life (RUL) estimations rather than both. Popular artificial intelligence (AI) based solutions rely heavily upon the quantity of training data for desired performance outcomes. However, in real-life industrial scenarios, fault instances are rare, inherently resulting in an imbalanced dataset. Additionally, it is both labor-intensive and costly to repeat multiple experiments for gathering required data as the equipment(s) pass through short-lived fault states. Thus conventional AI-based solutions may end up producing biased predictions. This work proposes to solve the imbalanced training dataset problem along with joint HA and RUL estimations, in one solution pipeline, currently unexplored in majority of the existing works in the literature. LS-R-ACGAN generates ample diverse fault samples for reducing the degree of imbalance between normal and fault classes. Subsequently, after data augmentation, the balanced training dataset helps train a multi-task learning model for joint HA and RUL estimations. The proposed framework has been implemented and tested on a benchmark dataset proving its superiority over multiple existing methods in the literature.},
  keywords={Prognostics and health management;Estimation;Industrial Internet of Things;Training;Computational modeling;Multitasking;Generative adversarial networks;Life estimation;Generative adversarial networks (GAN);health assessment (HA);remaining useful life (RUL);prognostics and health management (PHM);Industrial Internet of Things (IIoT)},
  doi={10.1109/TASE.2023.3267860},
  ISSN={1558-3783},
  month={July},}@INPROCEEDINGS{9445243,
  author={Usama, Muhammad and Arif, Aqib and Haris, Farhan and Khan, Shahroz and Afaq, S. Kamran and Rashid, Shahrukh},
  booktitle={2021 International Conference on Artificial Intelligence (ICAI)}, 
  title={A Data-Driven Interactive System for Aerodynamic and User-centred Generative Vehicle Design}, 
  year={2021},
  volume={},
  number={},
  pages={119-127},
  abstract={In this work, we propose a data-driven design pipeline for quick design exploration of performance and appearance guided alternatives for vehicle design. At the heart of our system is a machine learning-based generative design method to provide users with a set of diverse optimal design alternatives and an interactive design technique to induce users' preference into the design exploration. The generative design method is the structure on two search process, qualitative and quantitative. To avoid the curse of dimensionality, the qualitative search process first builds up a lower-dimensional representation of a given design space, which is then explored using the unsupervised k-means clustering to synthesise a representative set of user-preferred designs. The quantitative search process explores the design space to find an optimal design in terms of performance criterion such as drag coefficient. To reduce the computational complexity, instead of evaluating drag via Computational Fluid Dynamics simulations, a surrogate model is developed to predict the drag coefficients. The designs generated after the generative design step are presented to the user at the interactive step, where potential regions of the design space are identified around the user-selected designs. Afterwards, a new design space is generated by removing the non-preferred regions, which helps to focus the computational efforts on the exploration of the user preferred regions of the design space for a design tailored to the user's requirements. We demonstrated the performance of the proposed approach on a two-dimensional side silhouette of a sport-utility vehicle.},
  keywords={Heart;Fluids;Drag;Design methodology;Computational modeling;Interactive systems;Computational fluid dynamics;Generative Design;Interactive Design;Machine Learning;Computational Fluid Dynamics;Drag Coefficient},
  doi={10.1109/ICAI52203.2021.9445243},
  ISSN={},
  month={April},}@ARTICLE{10250864,
  author={Li, Wei and Gu, Chengchun and Chen, Jinlin and Ma, Chao and Zhang, Xiaowu and Chen, Bin and Chen, Ping},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={DW-GAN: Toward High-Fidelity Color-Tones of GAN-Generated Images With Dynamic Weights}, 
  year={2024},
  volume={35},
  number={12},
  pages={18090-18104},
  abstract={Color-tone represents the prominent color of an image, and training generative adversarial nets (GAN) to change color-tones of generated images is desirable in many applications. Advances such as HistoGAN can manipulate color-tones of generated images with a target image. Yet, there are challenges. Kullback–Leibler (KL) divergence adopted by HistoGAN might bring the color-tone mismatching, because it is possible to provide infinite score to a generator. Moreover, only relying on distribution estimation also produces images with lower fidelity in HistoGAN. To address these issues, we propose a new approach, named dynamic weights GAN (DW-GAN). We use two discriminators to estimate the distribution matching degree and details’ similarity, with Laplacian operator and Hinge loss. Laplacian operator can help capture more image details, while Hinge loss is deduced from mean difference (MD) that could avoid the case of infinite score. To synthesize desired images, we combine the loss of the two discriminators with generator loss and set the weights of the two estimated scores to be dynamic through the previous discriminators’ outputs, given that the training signal of a generator is from a discriminator. Besides, we innovatively integrate the dynamic weights into other GAN variants (e.g., HistoGAN and StyleGAN) to show the improved performance. Finally, we conduct extensive experiments on one industrial Fabric and seven public datasets to demonstrate the significant performance of DW-GAN in producing higher fidelity images and achieving the lowest Frechet inception distance (FID) scores over SOTA baselines.},
  keywords={Image color analysis;Training;Laplace equations;Generative adversarial networks;Image edge detection;Histograms;Generators;Color-tone changing;dynamic weights;generative adversarial net (GAN);Laplacian},
  doi={10.1109/TNNLS.2023.3311545},
  ISSN={2162-2388},
  month={Dec},}@INPROCEEDINGS{10692949,
  author={Yang, Shengcheng and Song, Chengshun and Bao, Liang and Zhao, Guangde},
  booktitle={2024 6th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)}, 
  title={An Exploration of the Impact of Training Datasets on Deep Learning-Based Building Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={541-545},
  abstract={Building extraction is important in several applications such as urban planning, disaster assessment and navigation systems, and helps to improve the accuracy and application efficiency of spatial data. In recent years, deep learning techniques have progressed significantly in the field of building extraction. However, the quality and diversity of the training dataset are crucial to the model performance. This study aims to investigate the impact of training datasets on deep learning-based building extraction. By comparing datasets of different sizes, quality, and diversity, we evaluate their performance on a deep learning model (UNet) in a building extraction task. The experimental results show that a high-quality and diverse training dataset significantly improves the extraction accuracy and robustness of the model. The deep learning model trained with the richer dataset achieves an overall accuracy of 96.5% for building extraction, which is a 13% improvement in extraction accuracy and a 10% improvement in IoU compared to the model trained on the other dataset. Meanwhile, we found that data enhancement techniques (e.g., image rotation, flipping, and numerical stretching) help improve the generalization ability of the model. The results in this paper not only provide valuable insights into the building extraction task, but also provide methodological guidance for constructing datasets for future deep learning-based remote sensing image processing research.},
  keywords={Training;Deep learning;Accuracy;Image processing;Buildings;Urban planning;Robustness;Data models;Numerical models;Optimization;Building extraction;Deep learning;Convolutional neural networks},
  doi={10.1109/IoTAAI62601.2024.10692949},
  ISSN={},
  month={July},}@INPROCEEDINGS{10581594,
  author={Pen, Haibo and Wang, Shuangshuang and Cai, Shaotang and Zhang, Ye and Hu, Jun},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Mural Image Shedding Diseases Automatic Marking and Inpainting Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={785-789},
  abstract={Murals are witnesses of Chinese history and culture, and have important historical research value. The research object selected in this article is the widely existing shedding disease. The traditional methods for repairing shed diseases usually include manual annotation and simultaneous repair of structure and texture, but this method has little effect on repairing shed diseases through contour lines. In addition, it is more difficult to repair diseases caused by the detachment of contour lines. To tackle these challenges, it is necessary to explore more suitable repair methods. Considering the particularity of shedding diseases caused by contour lines, this paper proposes an automatic marking and restoration algorithm for mural shedding diseases. We present a novel approach that incorporates enhancements across multiple stages, including multiscale morphological edge gradient detection, separate shedding edges and internal filling, relevant structural information restoration and textural information restoration. By calculating the image quality assessment (IQA) index to form a comprehensive evaluation system, our validation results demonstrate the effectiveness of the proposed method across multiple types of shedding disease. Notably, enhancements in edge detection and structural priority restoration optimization modeling significantly boost the whole method's performance. These improvements underscore the method's completeness, accuracy, and performance efficacy.},
  keywords={Seminars;Accuracy;Image edge detection;Manuals;Maintenance engineering;Image restoration;Indexes;mural image inpainting;shedding diseases;automatic marking;optimization theory;intelligent algorithm},
  doi={10.1109/AINIT61980.2024.10581594},
  ISSN={},
  month={March},}@INPROCEEDINGS{10528650,
  author={Zhong, Zhaoyi and Sun, Le},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={PerDetect: A Personalized Arrhythmia Detection System Based on Unsupervised Autoencoder}, 
  year={2023},
  volume={},
  number={},
  pages={914-919},
  abstract={Cardiovascular disease has become a common cause of death. Arrhythmia is a common cardiovascular disease. Cardiovascular disease has become a common cause of death. Arrhythmia is a common cardiovascular disease. Deep learning has been widely used in arrhythmia detection. However, the application of unsupervised learning to arrhythmia detection is not extensive. This paper proposes a personalized arrhythmia detection system PerDetect based on an unsupervised autoencoder. The system trains a separate BiLSTM-based autoencoder BiAE for each patient for arrhythmia detection. BiAE only needs to use the patient’s normal heartbeat for training, which effectively avoids the problem of data imbalance. We carried out experiments on MIT-BIH Arrhythmia Database. Experiments show that the system only needs a small amount of ECG training data (within five minutes) to achieve good performance. The ACC of our method on MIT-BIH Arrhythmia Database is 97%.},
  keywords={Training;Heart beat;Databases;Arrhythmia;Training data;Electrocardiography;Real-time systems;Arrhythmia detection;unsupervised Learning;autoencoder;ECG signal},
  doi={10.1109/ACAIT60137.2023.10528650},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11034997,
  author={Fu, QinYu and Wang, Yu and Zhao, Yining},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Research on Mural Style Generation and Parameter Adjustment Based on Stable Diffusion}, 
  year={2025},
  volume={},
  number={},
  pages={1799-1802},
  abstract={In this paper, we propose a method for the restoration and generation of mural art based on the Stable Diffusion Inpainting model, which aims to explore the influence of different parameters on the stylization and detail preservation of murals. The system architecture includes an input module, a processing module, and an output module, allowing users to upload concept sketches and masks, and generate murals with three adjustable parameters: style strength, modern blend scale, and historical detail accuracy. By building an interactive system, users can upload concept sketches and custom masks to automatically generate mural images that conform to historical textures and styles. The methods mainly include image and mask preprocessing, model parameter adjustment, intelligent image scaling, style fusion and online interactive interface design. The results show that the appropriate parameter setting can enhance the details and stylization effect while retaining the artistic characteristics of the mural, and provide technical support for the restoration and innovation of the mural. Finally, the advantages and limitations of the system are discussed, and the future optimization direction is discussed. Limitations and future improvement directions.},
  keywords={Seminars;Visualization;Technological innovation;Art;Systems architecture;Manuals;Robustness;Image restoration;Cultural differences;Optimization;stable diffusion;fresco style;parameter adjustment;cultural heritage;deep learning},
  doi={10.1109/AINIT65432.2025.11034997},
  ISSN={},
  month={April},}@INBOOK{10951594,
  author={},
  booktitle={The New Advanced Society: Artificial Intelligence and Industrial Internet of Things Paradigm}, 
  title={Index}, 
  year={2022},
  volume={},
  number={},
  pages={473-480},
  abstract={},
  keywords={},
  doi={10.1002/9781119884392.index},
  ISSN={},
  publisher={Wiley},
  isbn={9781119884385},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951594},}@INPROCEEDINGS{11047909,
  author={Liang, Yingbin and Dai, Lu and Shi, Shuo and Dai, Minghao and Du, Junliang and Wang, Haige},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={Contrastive and Variational Approaches in Self-Supervised Learning for Complex Data Mining}, 
  year={2025},
  volume={},
  number={},
  pages={525-529},
  abstract={Complex data mining has wide application value in many fields, especially in the feature extraction and classification tasks of unlabeled data. This paper proposes an algorithm based on self-supervised learning and verifies its effectiveness through experiments. The study found that in terms of the selection of optimizer and learning rate, the combination of AdamW optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks. In addition, the ablation experiment further analyzed the contribution of each module. The results show that contrastive learning, variational modules, and data augmentation strategies play a key role in the generalization ability and robustness of the model. Through the convergence curve analysis of the loss function, the experiment verifies that the method can converge stably during the training process and effectively avoid serious overfitting. Further experimental results show that the model has strong adaptability on different data sets, can effectively extract high-quality features from unlabeled data, and improves classification accuracy. At the same time, under different data distribution conditions, the method can still maintain high detection accuracy, proving its applicability in complex data environments. This study analyzed the role of self-supervised learning methods in complex data mining through systematic experiments and verified its advantages in improving feature extraction quality, optimizing classification performance, and enhancing model stability.},
  keywords={Training;Adaptation models;Accuracy;Optimization methods;Contrastive learning;Feature extraction;Data models;Robustness;Data mining;Overfitting;Self-supervised learning;complex data mining;contrastive learning;variational inference},
  doi={10.1109/AIITA65135.2025.11047909},
  ISSN={},
  month={March},}@INBOOK{10710473,
  author={Togelius, Julian},
  booktitle={Artificial General Intelligence}, 
  title={GLOSSARY}, 
  year={2024},
  volume={},
  number={},
  pages={205-208},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262380157},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10710473},}@INPROCEEDINGS{9943260,
  author={Sun, Haijia},
  booktitle={2022 International Conference on Artificial Intelligence and Autonomous Robot Systems (AIARS)}, 
  title={Research on Intelligent Decision Method of Image Segmentation Based on Deep Learning Technology}, 
  year={2022},
  volume={},
  number={},
  pages={417-420},
  abstract={Image segmentation is the first step in image analysis and one of the most important links. Because image segmentation can process images into simpler and more characteristic forms for analysis. With the rapid development of deep learning technology and its wide application in the field of semantic segmentation, the effect of semantic segmentation has been significantly improved. Image semantic segmentation is one of the core tasks of computer vision, and its goal is to efficiently classify each pixel of an input image. In recent years, deep learning is the technology that has the most profound impact on the computer field. With the help of deep learning, image semantic segmentation tasks have achieved many results in the fields of autonomous driving, biomedicine, and augmented reality. Compared with image classification and object detection, semantic segmentation can provide richer image semantic information. However, there are many problems in semantic segmentation based on deep learning. Firstly, it is difficult to make semantic segmentation data sets, which has the problems of difficult training and high production cost; Secondly, the computation and network parameters of most algorithms are huge, which makes them unable to be applied to mobile devices with limited computing resources, which limits the development of semantic segmentation; Moreover, many algorithms do not make full use of the hardware resources of the computing platform to accelerate the running speed of the program. This paper analyzes and summarizes the image semantic segmentation methods based on deep neural network. According to different network training methods, the existing image semantic segmentation is divided into fully supervised learning image semantic segmentation and weakly supervised learning image semantic segmentation. The effects, advantages and disadvantages of representative algorithms in each method are compared and analyzed, it also expounds the contribution of deep neural network to the field of semantic segmentation.},
  keywords={Deep learning;Training;Convolution;Semantics;Neural networks;Supervised learning;Rough sets;Deep learning technology;Image semantic segmentation;Intelligent decision making},
  doi={10.1109/AIARS57204.2022.00100},
  ISSN={},
  month={July},}@INPROCEEDINGS{10899618,
  author={Chen, Weitong and Li, Lei},
  booktitle={2024 2nd International Conference on Artificial Intelligence and Automation Control (AIAC)}, 
  title={A Study of PPO Algorithms Combining Curiosity and Imitation Learning in Maze Environments}, 
  year={2024},
  volume={},
  number={},
  pages={411-416},
  abstract={This study implements a maze navigation robot on the Unity platform, aiming to solve the problems of sparse rewards and slow model convergence, which are common in maze environments. To address the problem of sparse rewards, the study adds an intrinsic curiosity mechanism to the baseline algorithm PPO, which compensates for the lack of external rewards by incentivizing the agent to explore unknown regions. Meanwhile, in order to accelerate the convergence process of the model and improve its stability, the study further combines deep reinforcement learning techniques with imitation learning to optimize the model. Experimental results show that the proposed method not only significantly improves the learning efficiency of the model in the maze environment, but also exhibits high robustness and adaptability.},
  keywords={Training;Adaptation models;Navigation;Imitation learning;Focusing;Stability analysis;Robustness;Robots;Convergence;Overfitting;component;Deep reinforcement learning;maze navigation;intrinsic curiosity;imitation learning;Unity},
  doi={10.1109/AIAC63745.2024.10899618},
  ISSN={},
  month={Dec},}@ARTICLE{9794622,
  author={Ali, Mansoor and Naeem, Faisal and Tariq, Muhammad and Kaddoum, Georges},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Federated Learning for Privacy Preservation in Smart Healthcare Systems: A Comprehensive Survey}, 
  year={2023},
  volume={27},
  number={2},
  pages={778-789},
  abstract={Recent advances in electronic devices and communication infrastructure have revolutionized the traditional healthcare system into a smart healthcare system by using internet of medical things (IoMT) devices. However, due to the centralized training approach of artificial intelligence (AI), mobile and wearable IoMT devices raise privacy issues concerning the information communicated between hospitals and end-users. The information conveyed by the IoMT devices is highly confidential and can be exposed to adversaries. In this regard, federated learning (FL), a distributive AI paradigm, has opened up new opportunities for privacy preservation in IoMT without accessing the confidential data of the participants. Further, FL provides privacy to end-users as only gradients are shared during training. For these specific properties of FL, in this paper, we present privacy-related issues in IoMT. Afterwards, we present the role of FL in IoMT networks for privacy preservation and introduce some advanced FL architectures by incorporating deep reinforcement learning (DRL), digital twin, and generative adversarial networks (GANs) for detecting privacy threats. Moreover, we present some practical opportunities for FL in IoMT. In the end, we conclude this survey by discussing open research issues and challenges while using FL in future smart healthcare systems.},
  keywords={Medical services;Privacy;Security;Artificial intelligence;Data privacy;Hospitals;Electronic healthcare;Digital twin;federated learning;internet of medical things;privacy preservation;smart health care system},
  doi={10.1109/JBHI.2022.3181823},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{24530,
  author={Rickel, J.W.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Intelligent computer-aided instruction: a survey organized around system components}, 
  year={1989},
  volume={19},
  number={1},
  pages={40-57},
  abstract={The issues and previous research in intelligent computer-aided instruction (ICAI) are surveyed, concentrating on the contribution of each effort to understanding the various components of ICAI systems. Comparisons are made between ICAI and CAI. Various learning scenarios are discussed including computer coaches, gaming environments, mixed initiative dialog, Socratic tutors, articulate experts, interactive simulation, and discovery learning. Various forms of knowledge representation are discussed along with relevant issues and examples. Several techniques for student modeling and diagnosis are presented, as are their respective advantages and disadvantages. Pedagogical knowledge, its role in ICAI and several examples are highlighted. The evolution of discourse-management techniques for ICAI is outlined. Techniques for the automatic generation of problems from a general base of domain knowledge are presented. The design of user interfaces for ICAI systems is briefly discussed.<>},
  keywords={Computer aided instruction;Artificial intelligence;Computational modeling;Computer simulation;Knowledge representation;User interfaces;Education;Instruments;Laboratories;Computer science},
  doi={10.1109/21.24530},
  ISSN={2168-2909},
  month={Jan},}@ARTICLE{9910574,
  author={Peng, Shaoming and Xiong, Gang and Ren, Yanfang and Shen, Zhen and Liu, Sheng and Han, Yunjun},
  journal={IEEE Journal of Radio Frequency Identification}, 
  title={A Parallel Learning Approach for the Flexible Job Shop Scheduling Problem}, 
  year={2022},
  volume={6},
  number={},
  pages={851-856},
  abstract={Reinforcement learning is emerging to achieve real-time response and near-optimization for solving the flexible job shop scheduling problem (FJSP), an important and NP-hard problem for intelligent manufacturing systems. Although some methods based on reinforcement learning have been proposed to solve the FJSP, there’s still room for improvement. In this paper, we propose a new approach called reinforcement learning with the generative adversarial network (RLGAN) based on the parallel learning framework. A simulation-based artificial workshop system is established to generate a large number of sample plans as a training set for RLGAN to develop a near-optimal scheduling model. The case study shows that an implementation of our proposed method, QTRAN-GAN, can generate near-optimal plans and outperforms the corresponding pure reinforcement learning method, QTRAN.},
  keywords={Generators;Reinforcement learning;Radiofrequency identification;Workstations;Generative adversarial networks;Training data;Intelligent systems;Flexible job shop scheduling;parallel learning;generative adversarial network;reinforcement learning},
  doi={10.1109/JRFID.2022.3211555},
  ISSN={2469-7281},
  month={},}@ARTICLE{10226197,
  author={Chen, Yenming J. and Lin, Lung-Chang and Yang, Shu-Ting and Hwang, Kao-Shing and Liao, Chia-Te and Ho, Wen-Hsien},
  journal={IEEE Access}, 
  title={High-Reliability Non-Contact Photoplethysmography Imaging for Newborn Care by a Generative Artificial Intelligence}, 
  year={2023},
  volume={11},
  number={},
  pages={90801-90810},
  abstract={Long-term wiring on a newborn patient could be a disguise scene for parents. Unobtrusive and reliable monitoring without wiring can be a euphoric alternative for newborns and parents in obstetrics and gynecology (OB/GYN) incubation rooms. However, reliable and continuous non-contact surveillance in an incubation room is challenging. Therefore, a novel photoplethysmography imaging (PPGi) is developed specifically for baby skins through predictive adversarial adaptation and risk-sensitive generative synchronizer. Our artificial intelligence approach does not take blind guesses from input-output pairs. We apply an intelligent step to decouple the influence of fluctuated illumination through a generative algorithm of artificial intelligence. To boost skin detection performance, we capture those pixels with periodic variations and maximize the coherence of the extraction algorithm by the generative synchronizer. The periodic variations are matched by a synthesized pulse from the output PPGi signals through the control of a risk-sensitive filter to not over-compensate the illuminate variation. Based on the sensed pulsation, we synthesize the corresponding pulsation signals on the flight to identify the living skin in a spatiotemporal image sequence. We find that our skin classifier in risk-sensitive generative synchronizer effectively improves the quality of the resulting non-contact PPGi signal. Our algorithm produces substantial accuracy in the performance of PPGi reconstruction in the critical environment of newborn care. In the limited illustration of the incubation room, our non-contact PPGi can still achieve an average accuracy of 96.62%.},
  keywords={Skin;Pediatrics;Biomedical imaging;Image color analysis;Lighting;Synchronization;Filtering algorithms;Photoplethysmography;Surveillance;Prediction methods;Risk management;Thresholding (Imaging);Photoplethysmogram imaging (PPGi);non-contact surveillance;predictive adversarial adaptation;homographic filter;risk-sensitive generation;adaptive thresholding},
  doi={10.1109/ACCESS.2023.3307637},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11034476,
  author={Jin, Hengyu and Cheng, Chenggang and Luo, Zhaohua and Gan, Yi and Cao, Yucheng},
  booktitle={2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)}, 
  title={Image Rotation Correction via Diffusion and Consistency Models}, 
  year={2025},
  volume={},
  number={},
  pages={340-344},
  abstract={Tilted images are common in everyday photography and pose challenges for traditional correction methods that rely on geometric transformations or prior knowledge of rotation angles. Existing approaches, such as mesh warping or optical flow prediction, either require accurate structural cues or suffer from instability in complex scenes. Recent advances in generative modeling, particularly diffusion models, offer strong priors but remain computationally intensive. In this paper, we propose a novel one-step image rotation correction framework based on Consistency Models, a new class of generative models that enable efficient, angle-free, and semantically consistent transformation. Our method learns a direct mapping from a tilted image to its upright version using self-supervised consistency constraints, bypassing the need for explicit angle estimation or flow computation. Experimental results demonstrate that our approach achieves superior visual quality and inference speed compared to state-of-the-art methods, especially in large-tilt and structure-free settings.},
  keywords={Wavelet transforms;Photography;Visualization;Computational modeling;Biological system modeling;Noise reduction;Predictive models;Diffusion models;Internet;Optical flow;Image rotation correction;consistency models;Optical flow;Latent Diffusion Models},
  doi={10.1109/ICAID65275.2025.11034476},
  ISSN={},
  month={April},}@INPROCEEDINGS{10552567,
  author={Wu, Duo and Hao, Lingguang and Wei, Bing and Hao, Kuangrong and Han, Tao and He, Langping},
  booktitle={2024 7th International Symposium on Autonomous Systems (ISAS)}, 
  title={Backdoor Attack Based on Privacy Inference against Federated Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={As a pioneering distributed learning framework, federated learning (FL) has gained widespread adoption. It operates collaboratively among participants, with communication limited to sharing model parameters between the server and participants. However, FL is also more susceptible to active attacks from malicious insiders. Poisoned updates submitted by attackers can degrade the performance of the global model. Previous research only considered using the naive data of malicious clients for backdoor poisoning, and therefore achieved limited backdoor attack success rates. In this paper, we propose a novel Federated Backdoor Attack based on Privacy Inference (FBA-PI). Combining privacy inference based on generative adversarial networks (GAN), the attacker first infers sensitive information from victim participants and then injects backdoor triggers into the naive and generated data. Finally, malicious clients can embed backdoor semantics into the global model by participating in regular federated aggregation. Extensive experiments on the MNIST dataset verify the effectiveness of our proposed method.},
  keywords={Training;Privacy;Data privacy;Computer aided instruction;Federated learning;Distance learning;Semantics;Federated learning;privacy inference;generative adversarial networks;backdoor attacks},
  doi={10.1109/ISAS61044.2024.10552567},
  ISSN={},
  month={May},}@INPROCEEDINGS{10914647,
  author={Yu, Junwei and Xia, Yuhe},
  booktitle={2024 8th International Symposium on Computer Science and Intelligent Control (ISCSIC)}, 
  title={From Segmentation to Generation: the Emerging Trends in Skin Lesion Detection}, 
  year={2024},
  volume={},
  number={},
  pages={142-147},
  abstract={Skin disease detection has undergone significant advancements with the advent of deep learning-based image segmentation techniques. In this paper, we provide a comprehensive overview of the evolution of skin disease detection, from traditional methods to modernapproaches. We discuss the role of U-Net and its variants in medical image segmentation, highlighting their improved performance and robustness through multi-scale features, multi-task learning, and attention mechanisms. Additionally, we explore the recent trend of generation-based methods, including generative adversarial networks and diffusion models, which have shown promising results in generating high-quality skin disease segmentation masks. We also review popular skin disease segmentation datasets and evaluation metrics,and present experimental results comparing the performance of various models. As deep learning continues to advance and skin disease datasets expand, we anticipate the emergence of innovative network architectures and technologies that will further improve the diagnosis and treatment of skin diseases. This paper aims to inspire future research in this direction, exploring the potential of generation-based methods for skin disease detection and paving the way for more accurate and efficient diagnosis and treatment.},
  keywords={Training;Image segmentation;Accuracy;Diffusion models;Skin;Robustness;Lesions;Medical diagnostic imaging;Optimization;Diseases;kin lesion segmentation;U-Net;generative adversarial networks;diffusion model},
  doi={10.1109/ISCSIC64297.2024.00038},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9630590,
  author={Xue, Song and Bohn, Karl Peter and Guo, Rui and Sari, Hasan and Viscione, Marco and Rominger, Axel and Li, Biao and Shi, Kuangyu},
  booktitle={2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner}, 
  year={2021},
  volume={},
  number={},
  pages={4120-4122},
  abstract={Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.},
  keywords={Deep learning;Training;Nuclear medicine;PSNR;Biographies;Computed tomography;Transfer learning;total-body PET;CT-free;scatter correction;attenuation correction;deep learning},
  doi={10.1109/EMBC46164.2021.9630590},
  ISSN={2694-0604},
  month={Nov},}@INPROCEEDINGS{11042093,
  author={J, Aarav Kannan and Rajalakshmi, R. and Rajasekharachari, Garugu and Vikas, Garapati and Amshavalli, R. S.},
  booktitle={2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={Enhancing Heart Disease Diagnosis Through Advanced Deep CNNs for Multi-Class ECG Image Classification}, 
  year={2025},
  volume={},
  number={},
  pages={590-597},
  abstract={Heart disease remains a leading cause of mortality worldwide, necessitating rapid and accurate diagnostic systems to enhance patient outcomes. Traditional electrocardiogram (ECG) analysis methods are prone to errors, particularly when dealing with complex patterns or large datasets. This paper proposes an advanced deep learning approach using Convolutional Neural Networks (CNNs) for multi-class ECG image classification, aimed at improving diagnostic accuracy and efficiency. The methodology incorporates techniques such as transfer learning, data augmentation, and Generative Adversarial Networks (GANs) to enhance model robustness and performance. A user-friendly interface is developed for real-time diagnostic application, promoting clinical integration. Comparative analysis reveals that the proposed model achieves an overall accuracy of 94.7%, significantly outperforming traditional methods such as Support Vector Machine (SVM), Random Forest, K-Nearest Neighbors (KNN), and Decision Tree. These findings demonstrate the potential of deep learning-based systems to revolutionize cardiac diagnosis, providing scalable, accurate, and efficient diagnostic solutions applicable to diverse clinical environments.},
  keywords={Heart;Support vector machines;Accuracy;Transfer learning;Electrocardiography;Nearest neighbor methods;Robustness;Real-time systems;Artificial intelligence;Diseases;Heart Disease;ECG;Deep CNN;Multi-Class Classification;Artificial Intelligence;Data Augmentation;Transfer Learning},
  doi={10.1109/ICAISS61471.2025.11042093},
  ISSN={},
  month={May},}@INPROCEEDINGS{10333364,
  author={Li, Chen and Qi, Weijie and Jin, Bailu and Demestichas, Panagiotis and Tsagkaris, Kostas and Kritikou, Yiouli and Guo, Weisi},
  booktitle={2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)}, 
  title={Quality-of-Trust in 6G: Combining Emotional and Physical Trust through Explainable AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Wireless networks like many multi-user services have to balance limited resources in real-time. In 6G, increased network automation makes consumer trust crucial. Trust is reflect in both a personal emotional sentiment as well as a physical understanding of the transparency of AI decision making. Whilst there has been isolated studies of consumer sentiment to wireless services, this is not well linked to the decision making engineering. Likewise, limited recent research in explainable AI (XAI) has not established a link to consumer perception.Here, we develop a Quality-of-Trust (QoT) KPI that balances personal perception with the quality of decision explanation. That is to say, the QoT varies with both the time-varying sentiment of the consumer as well as the accuracy of XAI outcomes. We demonstrate this idea with an example in Neural Water-Filling (N-WF) power allocation, where the channel capacity is perceived by artificial consumers that communicate through Large Language Model (LLM) generated text feedback. Natural Language Processing (NLP) analysis of emotional feedback is combined with a physical understanding of N-WF decisions via meta-symbolic XAI. Combined they form the basis for QoT. Our results show that whilst the XAI interface can explain up to 98.9% of the neural network decisions, a small proportion of explanations can have large errors causing drops in QoT. These drops have immediate transient effects in the physical mistrust, but emotional perception of consumers are more persistent. As such, QoT tends to combine both instant physical mistrust and long-term emotional trends.},
  keywords={6G mobile communication;Vehicular and wireless technologies;Wireless networks;Decision making;Neural networks;Natural language processing;Real-time systems;machine learning;deep learning;XAI;wireless;trust;sentiment;NLP;LLM},
  doi={10.1109/VTC2023-Fall60731.2023.10333364},
  ISSN={2577-2465},
  month={Oct},}@INPROCEEDINGS{10223798,
  author={Monkam, Galamo and Xu, Weifeng and Yan, Jie},
  booktitle={2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)}, 
  title={A GAN-based Approach to Detect AI-Generated Images}, 
  year={2023},
  volume={},
  number={},
  pages={229-232},
  abstract={The ease with which deep learning can generate fake images has created a pressing need for a robust platform to distinguish between real and fake imagery. However, existing methods in image forensics rely on complex deep learning architectures that are expensive to train and have limited usability due to their large model size. This study examines the difficulty of detecting state-of-the-art image manipulations, both manually and automatically. We introduce G-JOB GAN, a machine learning model based on Generative Adversarial Networks (GAN), which generates highly realistic images and achieves a 95.7% accuracy in detecting realistic generated images. The same architecture of G-JOB Gan can also detect fake images with a similar probability. To verify the results, we compare our results to several similar GAN architectures, including Style GAN, Pro GAN, and the Original GAN. Our model outperforms other GAN models in term of detection accuracy.},
  keywords={Deep learning;Training;Image forensics;Costs;Computational modeling;Computer architecture;Pressing;GAN;Generated Image;Detection},
  doi={10.1109/SNPD-Winter57765.2023.10223798},
  ISSN={},
  month={July},}@INPROCEEDINGS{10620975,
  author={Crabb, Erin Smith and Jones, Matthew T.},
  booktitle={2024 19th Annual System of Systems Engineering Conference (SoSE)}, 
  title={Accelerating Model-Based Systems Engineering by Harnessing Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={110-115},
  abstract={With the rise of artificial intelligence (AI) tools to support the work of numerous disciplines, we describe a preliminary investigation into the benefits and drawbacks of large language model (LLM) use as part of a traditional systems engineering and design workflow. To explore this, we tasked a group of systems engineers to each create a list of requirements and use case diagram to satisfy a systems of systems user scenario presented in a proposal document. Participants created models of a healthcare setting in which clinicians resolved discrepancies with patient care by consulting additional sources of record, demonstrating the importance of integrating new systems within the larger healthcare system of systems. The first group were provided open access to an LLM, the second group were provided draft materials generated by an LLM, and the third followed their normal workflow. A subject matter expert (SME) evaluator then scored each model according to its completeness, consistency, correctness, simplicity, and traceability. Through this, we show that although LLMs are not a replacement for a trained systems engineer, they can contribute in two primary ways to the modeling process: first, they can generate a significant portion of the information necessary to create a minimum viable product (MVP) model within a fraction of the time, offering a promising way to accelerate the overall model development process. Second, they can answer detailed, domain-specific questions and reduce the time spent on external research.},
  keywords={Generative AI;Open Access;Large language models;Subject matter experts;Medical services;Proposals;Modeling;model-based systems engineering;generative artificial intelligence;large language models;modeling},
  doi={10.1109/SOSE62659.2024.10620975},
  ISSN={2835-3161},
  month={June},}@INPROCEEDINGS{9564675,
  author={Solano-Carrillo, Edgardo and Carrillo-Perez, Borja and Flenker, Tino and Steiniger, Yannik and Stoppe, Jannis},
  booktitle={2021 IEEE International Intelligent Transportation Systems Conference (ITSC)}, 
  title={Detection and Geovisualization of Abnormal Vessel Behavior from Video}, 
  year={2021},
  volume={},
  number={},
  pages={2193-2199},
  abstract={Intelligent maritime situational awareness pursues an effective understanding of the majority of the activities related to the maritime domain (impacting the safety, security, economy, or environment), with the aid of artificial intelligence systems. Such an understanding requires the development of automated processes capable of not only detecting abnormal behavior but also of visually-representing and interpreting it. Although much progress has been made in anomaly detection and visualization using vessel self-reporting positioning data, there have been no corresponding advances using video data, despite the increasing use of cameras for maritime surveillance. In this work, we introduce a framework which goes beyond vessel tracking for anomaly detection in video, and is therefore applicable to scenes with a high density of vessels. The proposed framework detects abnormal behavior using a Generative Adversarial Network (GAN) and interprets this knowledge using metrics derived from clustering the positions and courses provided by an independent vessel/motion detector. These detections are geovisualized using an advanced displaying tool where detected abnormal behavior may be localized on the globe, providing an infrastructure for intelligent maritime situational awareness.},
  keywords={Measurement;Knowledge engineering;Detectors;Tools;Generative adversarial networks;Real-time systems;Safety},
  doi={10.1109/ITSC48978.2021.9564675},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9643153,
  author={Dionelis, Nikolaos and Yaghoobi, Mehrdad and Tsaftaris, Sotirios A.},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Few-Shot Adaptive Detection of Objects of Concern Using Generative Models with Negative Retraining}, 
  year={2021},
  volume={},
  number={},
  pages={528-535},
  abstract={Detecting objects which we are interested in, Objects of Concern (OoC), is nowadays attracting attention. In aviation and transport, it is important to robustly detect OoC in security images. OoC are rare, differ from typical samples, and may be unknown during training. Most OoC detection methods need to be trained on large datasets to achieve good performance and have limited real-world generalization ability. A large variety of samples is needed, and it is expensive to collect and label large datasets due to the rarity of OoC. To address such limitations, we propose the negative REtraining with Few-shots Generative Adversarial Network (REFGAN) for detecting OoC. REFGAN aims at automatically identifying OoC by learning from Objects of No Concern (OoNC) and OoC. Our methodology comprises learning a prior using OoNC, and few-shot adaptation using the Few-Shot OoC (FSOoC). We propose a methodology to robustly perform few-shot adaptive detection of OoC using GANs and learned distribution boundaries. The evaluation of REFGAN on the Baggage SIXray dataset shows that when FSOoC are used, our model outperforms the prior improving OoC detection, and outperforms recent benchmarks by approximately 6.3% in mean values. REFGAN using few-shots of 80 samples shows a robust comparable performance compared to REFGAN using all the samples for retraining and model adaptation. REFGAN can detect unknown OoC and its evaluation on SIXray and CIFAR-10 shows robustness against the number of few-shot samples of OoC. REFGAN on CIFAR-10 outperforms benchmarks by approximately 16% using few-shots of 80 and of 10 samples.},
  keywords={Training;Adaptation models;Sensitivity analysis;Conferences;Benchmark testing;Generative adversarial networks;Robustness;Generative Adversarial Network;Anomaly Detection;Detection of Objects of Concern;Baggage X-Ray},
  doi={10.1109/ICTAI52525.2021.00086},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10673438,
  author={Sarada, B. and Sudha, TVS. Laxmi and Domakonda, Meghana and Vasantha, B.},
  booktitle={2024 Asia Pacific Conference on Innovation in Technology (APCIT)}, 
  title={Audio Deepfake Detection and Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={AI Voice Cloning, also called audio deepfakes is a highly advanced process that utilizes Artificial Intelligence to create a replica of a human voice. There is no doubt that this technology has revolutionized the way we interact with machines and has immense potential for various industries. This technology is used to create new identities or to steal the identities of the original voice owner and spread misinformation using cloned audio. This paper aims to differentiate between cloned voice and original voice using GAN and random forest. A generative adversarial network (GAN) is a deep learning architecture where two neural networks engage in a competitive dynamic within a zero-sum framework, striving to enhance the precision of these predictions. The Synthesized Data contains a lot of disturbances in the background which are generally referred to as Noise. To decrease this noise from the speech signals Spectral Subtraction is used. Feature extraction is done through zero crossing and a Random Forest classifier is used. By this classification, 100% accuracy has been acquired and other metrics such as precision, recall, and F1 score are also approximately equal to 100%. For analysis, a folder of 88 audio is considered.},
  keywords={Deepfakes;Technological innovation;Accuracy;Noise;Neural networks;Generative adversarial networks;Feature extraction;Deepfakes;GANs;Spectral Subtraction method;Zero Crossing method},
  doi={10.1109/APCIT62007.2024.10673438},
  ISSN={},
  month={July},}@INPROCEEDINGS{10827773,
  author={Wang, Qiaoyun and Feng, Siling and Huang, Mengxing},
  booktitle={2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Design of a Distributed Fusion Network Based on Knowledge Distillation}, 
  year={2024},
  volume={},
  number={},
  pages={360-365},
  abstract={Given the outstanding performance of knowledge distillation networks in optimizing traditional deep neural networks, this paper proposes a distributed fusion network based on knowledge distillation. It leverages knowledge distillation techniques to partition the different branches of the distributed fusion network into teacher and student networks. Specifically, the branch modules responsible for extracting feature information from source images are designated as the teacher network, while those extracting feature information from fused images are designated as the student network. Additionally, the designed distillation loss facilitates the transfer of feature information from the teacher network to the student network, enabling the learning of feature information from source images. Extensive experimental results on public datasets demonstrate the model's promising performance in enhancing fusion quality.},
  keywords={Knowledge engineering;Representation learning;Artificial neural networks;Feature extraction;Data mining;Image fusion;Knowledge distillation;Distributed fusion network;Teacher network;Student network},
  doi={10.1109/PRAI62207.2024.10827773},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11166348,
  author={Belkadi, Widad Hassina and Drias, Yassine and Drias, Habiba and Bouchelkia, Hichem and Hamdous, Samira},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Exploring Generative AI and Unsupervised Learning for Digital Soil Classification: A Case Study of Algeria}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Accurate soil type mapping is vital for sustainable agriculture and land management. Yet, Algeria remains under-represented in global soil databases. To address this, we propose a pipeline combining Generative AI for data extraction with unsupervised learning for soil classification. After harmonizing Algerian soil data, we evaluate four clustering algorithms—K-means, DBSCAN, HDBSCAN, and Self-Organizing Maps—under various preprocessing settings. Internal and external metrics guide model selection. K-means and DBSCAN produced the most coherent clusters, while SOM best aligned with FAO soil types. RuleFit was then used to extract interpretable rules defining each cluster. This work highlights the potential of AI-based, interpretable clustering for digital soil mapping in data-scarce regions like Algeria.},
  keywords={Self-organizing feature maps;Satellites;Generative AI;Databases;Pipelines;Soil properties;Clustering algorithms;Soil;Classification algorithms;Unsupervised learning;Digital Soil Mapping;Generative AI;Clustering;K-means;DBSCAN;HDBSCAN;Self-Organizing Maps;RuleFit},
  doi={10.1109/ACDSA65407.2025.11166348},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11080941,
  author={Sharma, Tanishque and Singh, Anmol and Singh, Sanjay and Gupta, Ganesh},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={AI-Powered Mock Interview Platform using Computer Vision, Natural Language Processing and Generative AI}, 
  year={2025},
  volume={},
  number={},
  pages={1258-1263},
  abstract={In this paper, we propose an AI-based mock interview platform addressing effective interviewing techniques with real-time results and intelligent mock interviews for candidates. It is a combination of generative AI, computer vision, and natural language processing (NLP) that provides a simulated experience of real-time interviews. It still continues to use methods such as CNNs for Facial expressions analysis, emotion detection from facial expressions and Voice recognition and NLP for tone, fluency, and confidence detection of a candidate. The platform compares responses with relevant industry standards using keyword-based semantic analysis in order to evaluate domain knowledge. The main highlight of this work includes emotion and answer-based feedback, where it provides the candidate with a performance report and suggestions for improvement. This ensures that users receive dynamic, multimodal insights that are tailored to the user profile (job role and skill set) and go beyond the scope of traditional mock interviews, where all interview questions feel the same. Overall, the platform is designed to help minimize stress, enhance communication capabilities, and empower career progression, while contributing to the evolution of AI in talent development and acquisition.},
  keywords={Industries;Computer vision;Emotion recognition;Generative AI;Semantics;Speech recognition;Natural language processing;Real-time systems;Interviews;Standards;AI-driven Mock Interviews;Generative AI;Computer Vision;Natural Language Processing (NLP);Emotion Detection;Speech Recognition;Convolutional Neural Networks (CNNs);Interview Simulation;Personalized Feedback;Career Development},
  doi={10.1109/ICSSAS66150.2025.11080941},
  ISSN={},
  month={June},}@INPROCEEDINGS{11081353,
  author={Manigandan, K. and Anand, M.},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Deep Learning Approach to Diagnose Diabetic Retinopathy and Glaucoma in Human Fundus Images}, 
  year={2025},
  volume={},
  number={},
  pages={1139-1145},
  abstract={Diseases significantly impact individuals' lives and health. Currently, there is a proposal to create an effective method for identifying potential disorders within the human body through the analysis of human fundus photos. Developing automated methods is essential for the successful identification of multifaceted illnesses. This system might implement a novel identification algorithm capable of identifying several ailments, including Diabetic Retinopathy, Glaucoma, cardiac conditions, tumours, and others. In this system, pictures underwent pre-processing using filtering and modification to eliminate noise and extraneous background. This system employed a grey-level co-occurrence matrix (GLCM) technique to segment illness pictures, enabling precise extraction of texture and colour information from various disease images. Ultimately, several illness categories are discovered utilising the deep neural network (DNN) classification approach.},
  keywords={Glaucoma;Deep learning;Image segmentation;Diabetic retinopathy;Accuracy;Image color analysis;Artificial neural networks;Proposals;Diseases;Tumors;Disease identification;Fundus images;GLCM;Preprocessing;Texture features;DNN;Multitype diseases},
  doi={10.1109/ICSSAS66150.2025.11081353},
  ISSN={},
  month={June},}@INPROCEEDINGS{10920732,
  author={Tayeb, Adnan Md and Nakayiza, Hope Leticia and Shin, Heejae and Lee, Seungmin and Lee, Jae-Min and Kim, Dong-Seong},
  booktitle={2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={DefectDiffusion: A Generative Diffusion Model for Robust Data Augmentation in Industrial Defect Detection}, 
  year={2025},
  volume={},
  number={},
  pages={0066-0071},
  abstract={The accurate detection of industrial defects is critical for ensuring product quality and minimizing operational inefficiencies. However, deep learning models for defect detection often require large, balanced datasets, which are challenging to obtain in industrial settings due to the rarity and variability of defects. In this study, we propose DefectDiffusion, a novel generative diffusion model designed for robust data augmentation in industrial defect detection tasks. By leveraging the progressive noise reduction process inherent to diffusion models, DefectDiffusion synthesizes high-quality, diverse defect images that closely mimic real-world conditions. Unlike traditional augmentation techniques, our approach selectively augments defective regions while preserving the structural integrity of defect-free areas, ensuring realistic and meaningful data augmentation. Experimental results demonstrate that integrating DefectDiffusion-generated images significantly enhances the performance of state-of-the-art defect detection models, improving both precision and recall.},
  keywords={Accuracy;Generative AI;Noise reduction;Diffusion models;Data augmentation;Product design;Quality assessment;Steel;Few shot learning;Defect detection;Generative AI;Stable Diffusion;Few-Shot Learning;Defect generation;Defect detection},
  doi={10.1109/ICAIIC64266.2025.10920732},
  ISSN={2831-6983},
  month={Feb},}@ARTICLE{10855559,
  author={Ma, Mengru and Ma, Wenping and Jiao, Licheng and Li, Lingling and Liu, Xu and Liu, Fang and Yang, Shuyuan and Guo, Yuwei},
  journal={IEEE Transactions on Multimedia}, 
  title={A 3D Self-Awareness Diffusion Network for Multimodal Classification}, 
  year={2025},
  volume={27},
  number={},
  pages={3462-3475},
  abstract={As imaging sensor technology in remote sensing has advanced quickly, multimodal fusion classification has become an important research direction in land cover and urban planning classification tasks. While generative models and image classification have greatly benefited from diffusion models, the present ones primarily concentrate on single-modality-driven diffusion processes. Therefore, this paper presents a 3D self-awareness diffusion network (3DSA-DiffNet) for multispectral (MS) and panchromatic (PAN) image fusion classification, which would make it easier to classify heterogeneous data from various sensors. First, in order to model the relationship between multi-channel spectra and multi-pixel spatial distributions as well as samples, respectively, a spatial-spectral joint denoising network (S$^{2}$JD-Net) is proposed. It can incorporate the diffusion process into the neural network to enhance the quality of diffusion features. Secondly, to imitate the brain's spatial-spectral coexistence learning mechanism, this work offers a 3D self-awareness module (3DSA-Module) that can learn the weight of each pixel in 3D space, resulting in extraordinarily high feature representation capabilities. Finally, experimental verification demonstrates that the 3D self-awareness diffusion fusion network driven by brain inspiration outperforms more sophisticated approaches on the Xi'an, Huhhot, and Muufl datasets.},
  keywords={Three-dimensional displays;Noise reduction;Feature extraction;Diffusion models;Solid modeling;Remote sensing;Brain modeling;Image fusion;Accuracy;Satellites;Diffusion model;3D self-attention mechanism;multimodal;fusion classification;remote sensing images},
  doi={10.1109/TMM.2025.3535295},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10574692,
  author={Visumathi, J. and Kalaivani, N. and Hemanth, S V and Balasubramanian, K. and Karthikeyan, S. and Amirthayogam, G.},
  booktitle={2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)}, 
  title={Optimized Deep Neural Network for Accurate Detection of Malignant and Benign Brain Tumors}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid development of medical technology has ushered in a new age of crucial medical records. A brain tumor is an aberrant mass of rapidly expanding brain cells, some of which have the potential to metastasize. Brain tumors are often diagnosed in clinical practice using magnetic resonance imaging (MRI) scans. These tumors appear as clusters of irregular cells that could endanger patients by invading nearby tissues. However, manually spotting and segmenting brain tumors via MRI scans present complex challenges and are prone to mistakes. Thus, analysis of such data has influenced the prognosis, monitoring, diagnosis, and treatment of tumor-related conditions. To overcome these issues, we propose an Optimized Deep Neural Network (ODNN) method to accurately predict benign or malignant brain tumor diseases. Moreover, pre-processing uses an Adaptive Histogram Contrast Normalization (AHCN) technique to enhance the distribution and contrast of MRI image density. Afterward, features can be extracted using a Texture-Based Gray Level Co-Occurrence Matrix (TGLCM) method to separate the dark and bright parts of the image. Finally, a Machine Learning (ML) based ODNN method is employed to classify benign or malignant diseases. Utilizing sensitivity, specificity, recall, and accuracy metrics, the simulation results of the recommended ODNN approach are contrasted with those of preexisting approaches.},
  keywords={Accuracy;Sensitivity;Magnetic resonance imaging;Simulation;Artificial neural networks;Feature extraction;Prognostics and health management;Image Processing;brain tumor;MRI image;machine learning;Optimized Deep Neural Network},
  doi={10.1109/AIIoT58432.2024.10574692},
  ISSN={},
  month={May},}@INPROCEEDINGS{10823034,
  author={Shang, Wenwen},
  booktitle={2024 6th International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Performance Optimization and Model Research of Machine Learning in Speech Recognition System}, 
  year={2024},
  volume={},
  number={},
  pages={18-23},
  abstract={This paper proposes an improved method based on machine learning, which combines the deep neural network (DNN) architecture and speech enhancement technology to significantly improve the recognition accuracy and robustness of the system. In traditional speech recognition models, complex noise environments often lead to inaccurate feature extraction and poor model training effects, thereby reducing the practical application value of the system. To solve the above problems, this paper introduces adaptive noise reduction algorithms and time-frequency domain enhancement technologies in speech preprocessing to optimize the quality of input signals. In addition, through the improved DNN model structure, the ability to model the spatiotemporal features of speech signals is enhanced. At the same time, the dynamic learning rate strategy and gradient optimization algorithm are adopted to improve the convergence efficiency and generalization performance of the model. Experiments verify the superior performance of the improved model in multi-noise scenarios. Compared with traditional methods, the speech recognition accuracy of the improved model is improved by 12.5%, indicating that it has stronger robustness under noise interference. The processing delay of the system is reduced by an average of 15.3%, which greatly improves the real-time performance.},
  keywords={Training;Adaptation models;Accuracy;Heuristic algorithms;Noise;Speech recognition;Machine learning;Artificial neural networks;Speech enhancement;Robustness;Speech recognition;machine learning;performance optimization;model research;speech enhancement;human-computer interaction},
  doi={10.1109/ICAICA63239.2024.10823034},
  ISSN={2833-8413},
  month={Nov},}@ARTICLE{8716718,
  author={Weng, Yu and Zhou, Haiwen},
  journal={IEEE Access}, 
  title={Data Augmentation Computing Model Based on Generative Adversarial Network}, 
  year={2019},
  volume={7},
  number={},
  pages={64223-64233},
  abstract={The edge intelligent computing technology can reduce the delay and energy consumption of deep learning model reasoning through the collaborative terminal acquisition equipment and edge server. We apply the neural network to the edge computing and build a data augmentation computing model based on the sparse data volume. Then, we get an intelligent generative image after the network training to achieve the effect of enhancement computing. In this paper, we choose a relatively small number of national elements and the generative adversarial network (GAN) as the experimental data calculation set and network model. First, we normalize the preprocessing of the collected data to form the initial sample data set. Second, the model extracts the feature vector by input image to the convolution neural network (CNN) layer. After that, we use a random noise vector z which follows a Gaussian distribution as the initial input of the conditional generative adversarial network (CGAN). The feature vector extracted from the image is taken as a label and a condition constraint of the CGAN to train the parameters of the CGAN. Finally, the trained CGAN model is used to complete the data augmentation computing. A total of 350 samples were collected, and 97 sample images were actually applied for data augmentation. The enhanced data set of this model is as many as 1,700 samples, and it is found that the generated image is of good quality by using this data set for the peak signal-to-noise ratio (PSNR) detection, which is of innovative value in the standard of real samples.},
  keywords={Data models;Computational modeling;Solid modeling;Gallium nitride;Deep learning;Neural networks;Edge computing;Edge computing;data augmentation computing;feature rules;generative adversarial network},
  doi={10.1109/ACCESS.2019.2917207},
  ISSN={2169-3536},
  month={},}@ARTICLE{10057085,
  author={Francisco, Manuel and Castro, Juan Luis},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Methodology to Quickly Perform Opinion Mining and Build Supervised Datasets Using Social Networks Mechanics}, 
  year={2023},
  volume={35},
  number={9},
  pages={9797-9808},
  abstract={Social Networking Sites (SNS) offer a full set of possibilities to perform opinion studies such as polling or market analysis. Normally, artificial intelligence techniques are applied, and they often require supervised datasets. The process of building these is complex, time-consuming and expensive. In this paper, we propose to assist the labelling task by taking advantage of social network mechanics. In order to do that, we introduce the co-retweet relation to build a graph that allows us to propagate user labels to their similarity neighbourhood. Therefore, it is possible to iteratively build supervised datasets with significant less human effort and with higher accuracy than other weak-supervision techniques. We tested our proposal with 3 datasets labelled by an expert committee, and results shows that it outperforms other weak-supervision techniques. This methodology may be adapted to other social networks and topics, and it is relevant for applications like informed decision-making (e.g., content moderation), specially when interpretability is required.},
  keywords={Labeling;Task analysis;Social networking (online);Sentiment analysis;Data models;Training;Proposals;Human-in-the-loop labelling;opinion mining;social network analysis;supervised learning;user profiling},
  doi={10.1109/TKDE.2023.3250822},
  ISSN={1558-2191},
  month={Sep.},}@INPROCEEDINGS{11093664,
  author={Wang, Sen and Wang, Le and Zhou, Sanping and Tian, Jingyi and Li, Jiayi and Sun, Haowen and Tang, Wei},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation}, 
  year={2025},
  volume={},
  number={},
  pages={12176-12186},
  abstract={Robotic manipulation in high-precision tasks is essential for numerous industrial and real-world applications where accuracy and speed are required. Yet current diffusion-based policy learning methods generally suffer from low computational efficiency due to the iterative denoising process during inference. Moreover, these methods do not fully explore the potential of generative models for enhancing information exploration in 3D environments. In response, we propose FlowRAM, a novel framework that leverages generative models to achieve region-aware perception, enabling efficient multimodal information processing. Specifically, we devise a Dynamic Radius Schedule, which allows adaptive perception, facilitating transitions from global scene comprehension to fine-grained geometric details. Furthermore, we integrate state space models to integrate multimodal information, while preserving linear computational complexity. In addition, we employ conditional flow matching to learn action poses by regressing deterministic vector fields, simplifying the learning process while maintaining performance. We verify the effectiveness of the FlowRAM in the RLBench, an established manipulation benchmark, and achieve state-of-the-art performance. The results demonstrate that FlowRAM achieves a remarkable improvement, particularly in high-precision tasks, where it outperforms previous methods by 12.0% in average success rate. Additionally, FlowRAM is able to generate physically plausible actions for a variety of real-world tasks in less than 4 time steps, significantly increasing inference speed.},
  keywords={Adaptation models;Schedules;Solid modeling;Three-dimensional displays;Service robots;Computational modeling;Noise reduction;Dynamic scheduling;Vectors;Pattern recognition;flow matching;region-aware;mamba},
  doi={10.1109/CVPR52734.2025.01137},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9349211,
  author={Chen, Xin and Luo, Xizhao and Weng, Jian and Luo, Weiqi and Li, Huiting and Tian, Qi},
  journal={IEEE Transactions on Image Processing}, 
  title={Multi-View Gait Image Generation for Cross-View Gait Recognition}, 
  year={2021},
  volume={30},
  number={},
  pages={3041-3055},
  abstract={Gait recognition aims to recognize persons' identities by walking styles. Gait recognition has unique advantages due to its characteristics of non-contact and long-distance compared with face and fingerprint recognition. Cross-view gait recognition is a challenge task because view variance may produce large impact on gait silhouettes. The development of deep learning has promoted cross-view gait recognition performances to a higher level. However, performances of existing deep learning-based cross-view gait recognition methods are limited by lack of gait samples under different views. In this paper, we take a Multi-view Gait Generative Adversarial Network (MvGGAN) to generate fake gait samples to extend existing gait datasets, which provides adequate gait samples for deep learning-based cross-view gait recognition methods. The proposed MvGGAN method trains a single generator for all view pairs involved in single or multiple datasets. Moreover, we perform domain alignment based on projected maximum mean discrepancy to reduce the influence of distribution divergence caused by sample generation. The experimental results on CASIA-B and OUMVLP dataset demonstrate that fake gait samples generated by the proposed MvGGAN method can improve performances of existing state-of-the-art cross-view gait recognition methods obviously on both single-dataset and cross-dataset evaluation settings.},
  keywords={Gait recognition;Generative adversarial networks;Feature extraction;Gallium nitride;Generators;Training;Task analysis;Cross-view gait recognition;gait image generation;multi-domain generative adversarial networks;domain alignment;convolutional neural networks},
  doi={10.1109/TIP.2021.3055936},
  ISSN={1941-0042},
  month={},}@ARTICLE{9732175,
  author={Liu, Deyin and Wu, Lin and Zheng, Feng and Liu, Lingqiao and Wang, Meng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Verbal-Person Nets: Pose-Guided Multi-Granularity Language-to-Person Generation}, 
  year={2023},
  volume={34},
  number={11},
  pages={8589-8601},
  abstract={Person image generation conditioned on natural language allows us to personalize image editing in a user-friendly manner. This fashion, however, involves different granularities of semantic relevance between texts and visual content. Given a sentence describing an unknown person, we propose a novel pose-guided multi-granularity attention architecture to synthesize the person image in an end-to-end manner. To determine what content to draw at a global outline, the sentence-level description and pose feature maps are incorporated into a U-Net architecture to generate a coarse person image. To further enhance the fine-grained details, we propose to draw the human body parts with highly correlated textual nouns and determine the spatial positions with respect to target pose points. Our model is premised on a conditional generative adversarial network (GAN) that translates language description into a realistic person image. The proposed model is coupled with two-stream discriminators: 1) text-relevant local discriminators to improve the fine-grained appearance by identifying the region–text correspondences at the finer manipulation and 2) a global full-body discriminator to regulate the generation via a pose-weighting feature selection. Extensive experiments conducted on benchmarks validate the superiority of our method for person image generation.},
  keywords={Visualization;Image synthesis;Generators;Generative adversarial networks;Electronic mail;Semantics;Computer science;Fine-grained generation;generative adversarial networks (GANs);human poses;person image generation;text-to-image translation},
  doi={10.1109/TNNLS.2022.3151631},
  ISSN={2162-2388},
  month={Nov},}@ARTICLE{1597112,
  author={Bouchard, G. and Celeux, G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Selection of generative models in classification}, 
  year={2006},
  volume={28},
  number={4},
  pages={544-554},
  abstract={This paper is concerned with the selection of a generative model for supervised classification. Classical criteria for model selection assess the fit of a model rather than its ability to produce a low classification error rate. A new criterion, the Bayesian entropy criterion (BEC), is proposed. This criterion takes into account the decisional purpose of a model by minimizing the integrated classification entropy. It provides an interesting alternative to the cross-validated error rate which is computationally expensive. The asymptotic behavior of the BEC criterion is presented. Numerical experiments on both simulated and real data sets show that BEC performs better than the BIC criterion to select a model minimizing the classification error rate and provides analogous performance to the cross-validated error rate.},
  keywords={Error analysis;Entropy;Testing;Bayesian methods;Pattern recognition;Maximum likelihood estimation;Computational modeling;Approximation error;Estimation error;Q measurement;Generative classification;integrated likelihood;integrated conditional likelihood;classification entropy;cross-validated error rate;AIC and BIC criteria.},
  doi={10.1109/TPAMI.2006.82},
  ISSN={1939-3539},
  month={April},}@ARTICLE{8528843,
  author={Kim, Geonmin and Lee, Hwaran and Kim, Bo-Kyeong and Oh, Sang-Hoon and Lee, Soo-Young},
  journal={IEEE Signal Processing Letters}, 
  title={Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition}, 
  year={2019},
  volume={26},
  number={1},
  pages={159-163},
  abstract={Many speech enhancement methods try to learn the relationship between noisy and clean speechs, obtained using an acoustic room simulator. We point out several limitations of enhancement methods relying on clean speech targets; the goal of this letter is to propose an alternative learning algorithm, called acoustic and adversarial supervision (AAS). AAS makes the enhanced output both maximizing the likelihood of transcription on the pre-trained acoustic model and having general characteristics of clean speech, which improve generalization on unseen noisy speeches. We employ the connectionist temporal classification and the unpaired conditional boundary equilibrium generative adversarial network as the loss function of AAS. AAS is tested on two datasets including additive noise without and with reverberation, Librispeech + DEMAND, and CHiME-4. By visualizing the enhanced speech with different loss combinations, we demonstrate the role of each supervision. AAS achieves a lower word error rate than other state-of-the-art methods using the clean speech target in both datasets.},
  keywords={Speech enhancement;Acoustics;Noise measurement;Speech recognition;Training;Generative adversarial networks;Data models;Speech enhancement;room simulator;connectionist temporal classification;generative adversarial network},
  doi={10.1109/LSP.2018.2880285},
  ISSN={1558-2361},
  month={Jan},}@ARTICLE{10470461,
  author={Li, Leyan and Yang, Rennong and Lv, Maolong and Wu, Ao and Zhao, Zilong},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={From Behavior to Natural Language: Generative Approach for Unmanned Aerial Vehicle Intent Recognition}, 
  year={2024},
  volume={5},
  number={12},
  pages={6196-6209},
  abstract={This article introduces a novel cross-modal neural network model that aims to convert long-term temporal behavior data into natural language to achieve unmanned aerial vehicle (UAV) intent recognition. Our generative intent recognition model effectively utilizes the inherent redundancy present in long temporal behavior data by incorporating a sequence compression module, which enables the cross-modal generation and alignment of intents while preserving the integrity of the standard Transformer architecture. Importantly, we observe that this approach mitigates the negative impact of imbalanced database distribution by mapping intent categories onto the modality of natural language. Furthermore, we propose three comprehensive pretraining tasks specifically designed for time series data, thoroughly examining their interconnections and analyzing the impact of a hybrid pretraining framework on the accuracy of intent recognition. Our experimental results demonstrate the superiority of our proposed generative UAV intent recognition model, along with the hybrid pretraining initialization method, compared to conventional classification models. Simultaneously, the intent recognition method exhibits heightened temporal sensitivity and robust resilience, enabling it to deal with complex UAV confrontation and interference environment.},
  keywords={Autonomous aerial vehicles;Intent recognition;Transformers;Data models;Artificial intelligence;Crossmodal Integration;hybrid pretraining;improving situation awareness;intent recognition;text generation},
  doi={10.1109/TAI.2024.3376510},
  ISSN={2691-4581},
  month={Dec},}@ARTICLE{10764738,
  author={Zhu, Yonghuai and Cheng, Jiangfeng and Liu, Zhifeng and Zou, Xiaofu and Cheng, Qiang and Xu, Hui and Wang, Yong and Tao, Fei},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Data Generation Approach Based on Data Model Fusion: An Application for Rolling Bearings Fault Diagnosis With Small Samples}, 
  year={2025},
  volume={74},
  number={},
  pages={1-16},
  abstract={Utilizing fake data (simulated based on mechanism models or generated through data-driven models) for data enhancement is a popular approach to solve the problem of fault diagnosis with small samples. Consequently, the quality of such fake data impacts fault diagnosis accuracy. This article proposes a data model fusion (DMF)-driven framework for small sample fault diagnosis. This framework integrates the digital twin model (DTM) and the conditional deep convolutional generative adversarial network (C-DCGAN). Digital twin data (DTD) under various fault conditions is first obtained in the data generation stage based on DTM simulation. Then, a data generation method based on DTM-C-DCGAN is proposed. The method adopts DTD as the soft-physics constraint input to the generator of C-DCGAN. Hence, the generator is induced to generate data that is more consistent with the failure mechanism and closer to the real data. During the fault diagnosis stage, the generated data (GD) are used to enhance the training process of the fault diagnosis model, improving its generalization ability. Finally, the effectiveness of the proposed method is comprehensively verified via two publicly rolling bearing datasets. Compared with the existing single data-driven and physics-based methods, the experimental results demonstrate that the proposed DMF method can significantly enhance the quality of the GD and improve the accuracy of fault identification, achieving an average accuracy of 97.31%.},
  keywords={Fault diagnosis;Data models;Generative adversarial networks;Data collection;Accuracy;Generators;Training;Digital twins;Feature extraction;Rolling bearings;Conditional deep convolutional generative adversarial network (C-DCGAN);data model fusion (DMF);digital twin model (DTM);generated data (GD);small sample fault diagnosis},
  doi={10.1109/TIM.2024.3504567},
  ISSN={1557-9662},
  month={},}@ARTICLE{10497125,
  author={Xie, Jie and Fang, Leyuan and Wu, Cheng and Xie, Feng and Chanussot, Jocelyn},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Blind Spectral Super-Resolution by Estimating Spectral Degradation Between Unpaired Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={The spectral super-resolution (SpeSR) from multispectral images (MSIs) to hyperspectral images (HSIs) can bring rich spectral information. The deep learning-based methods have demonstrated their powerful ability for the SpeSR task, which requires the paired HSI/MSI to train the model. However, HSIs and MSIs are always obtained at different times and under different imaging conditions, covering different areas. To address this issue, in this article, a framework named BliEstGAN based on the generative adversarial network (GAN) is proposed to estimate the spectral resolution degradation between unpaired HSIs and MSIs that can be used for the blind SpeSR. Specifically, each MSI imaging sensor has its own unique spectral sampling process, which can be modeled as a spectral degradation from its paired HSI. Different spectral degradations can be discriminated by the deep model. Therefore, the generator of the GAN is used to estimate the spectral degradation from HSIs to MSIs, and the discriminator of the GAN is adopted to distinguish whether the estimated and real spectral degradation is similar. The MSIs and HSIs are easy to discriminate against by the discriminator because their spatial resolutions are different. Furthermore, according to the imaging sensor mechanism, some special regularization terms are designed for the generator to guarantee its correct convergence. Finally, the estimated spectral resolution degradation can be adopted to generate HSI/MSI pairs for the supervised learning-based SpeSR methods. Experimental results demonstrate the effectiveness of the proposed method.},
  keywords={Degradation;Imaging;Spatial resolution;Dictionaries;Superresolution;Generators;Generative adversarial networks;Blind spectral super-resolution (SpeSR);generative adversarial network (GAN);spectral degradation estimation},
  doi={10.1109/TGRS.2024.3387857},
  ISSN={1558-0644},
  month={},}@ARTICLE{10707182,
  author={Zhang, Yan and Fan, Rongbo and Duan, PeiPei and Dong, Jinfang and Lei, Zhiyong},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={DCDGAN-STF: A Multiscale Deformable Convolution Distillation GAN for Remote Sensing Image Spatiotemporal Fusion}, 
  year={2024},
  volume={17},
  number={},
  pages={19436-19450},
  abstract={Remote sensing image spatiotemporal fusion (STF) aims to generate composite images with high-temporal and spatial resolutions by combining remote sensing images captured at different times and with different spatial resolutions (DTDS). Among the existing fusion algorithms, deep learning-based fusion models have demonstrated outstanding performance. These models treat STF as an image super-resolution problem based on multiple reference images. However, compared to traditional image super-resolution tasks, remote sensing image STF involves merging a larger amount of multitemporal data with greater resolution difference. To enhance the robust matching performance of spatiotemporal transformations between multiple sets of remote sensing images captured at DTDS and to generate super-resolution composite images, we propose a feature fusion network called the multiscale deformable convolution distillation generative adversarial network (DCDGAN-STF). Specifically, to address the differences in multitemporal data, we introduce a pyramid cascading deformable encoder to identify disparities in multitemporal images. In addition, to address the differences in spatial resolution, we propose a teacher–student correlation distillation method. This method uses the texture details' disparities between high-resolution multitemporal images to guide the extraction of disparities in blurred low-resolution multitemporal images. We comprehensively compared the proposed DCDGAN-STF with some state-of-the-art algorithms on two landsat and moderate-resolution imaging spectroradiometer datasets. Ablation experiments were also conducted to test the effectiveness of different submodules within DCDGAN-STF. The experimental results and ablation analysis demonstrate that our algorithm achieves superior performance compared to other algorithms.},
  keywords={Spatial resolution;Spatiotemporal phenomena;Remote sensing;Feature extraction;Convolution;Image reconstruction;Generative adversarial networks;Superresolution;Earth;Correlation;Deformable convolution;generative adversarial network (GAN);remote sensing image spatiotemporal fusion (STF);teacher–student correlation distillation},
  doi={10.1109/JSTARS.2024.3476153},
  ISSN={2151-1535},
  month={},}
