@ARTICLE{9126270,
  author={Kang, Qi and Yao, Siya and Zhou, MengChu and Zhang, Kai and Abusorrah, Abdullah},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Enhanced Subspace Distribution Matching for Fast Visual Domain Adaptation}, 
  year={2020},
  volume={7},
  number={4},
  pages={1047-1057},
  abstract={In computer vision, when labeled images of the target domain are highly insufficient, it is challenging to build an accurate classifier. Domain adaptation stands for an effective solution to address it by utilizing available and related source domain which has sufficient labeled images, even when there is a substantial difference in properties and distributions of these two domains. Yet, most prior approaches merely reduce subspace conditional or marginal distribution differences between domains but entirely ignoring label dependence (LD) information of source data in subspace. This article proposes a novel approach of domain adaptation, called enhanced subspace distribution matching (ESDM), which makes good use of label information to enhance the distribution matching between the source and target domains in a shared subspace. It reduces both conditional and marginal distributions in a shared subspace during a procedure of kernel principal dimensionality reduction and also preserves source data LD information to the maximum extent, thereby significantly improving cross domain subspace distribution matching. We also provide a learning algorithm with highly affordable computation, which solves the ESDM optimization problem without using time-consuming iterations. Results confirm that it can well outperform several recent domain adaptation methods on image classification tasks in terms of classification accuracy and running time. The results can be used in social cognition, person reidentification, and humanâ€“machine interactions.},
  keywords={Task analysis;Kernel;Visualization;Probability distribution;Generative adversarial networks;Optimization;Computer vision;Distribution matching;domain adaptation;image classification;social intelligence and cognition;visual domain adaptation},
  doi={10.1109/TCSS.2020.3001517},
  ISSN={2329-924X},
  month={Aug},}@INPROCEEDINGS{5365338,
  author={Codognet, Philippe and Pasquet, Olivier},
  booktitle={2009 11th IEEE International Symposium on Multimedia}, 
  title={Swarm Intelligence for Generative Music}, 
  year={2009},
  volume={},
  number={},
  pages={1-8},
  abstract={Sound Agents is an art installation relating real space and virtual sound space. It follows previous work on virtual agents and autonomous characters in 3D immersive spaces. However in Sound Agents, each virtual entity will not be a visual character but an invisible sound agent producing music, which will have its own autonomous behavior. Therefore the ambient music is dynamically spatialized in the actual installation space (through several loudspeakers (24 loudspeakers + 1 subwoofer) and modified by the movement of the virtual agents, providing thus an ever-changing music soundscape. Each sound agent is autonomous and his behavior will be described by simple local rules, in the tradition of Artificial Life and Self-Organizing Systems. Therefore we are interested in multi-agent simulation of Complex Systems who exhibit some emergent behavior, in particular what has been called swarm intelligence, such as ant foraging, or termite nest construction.},
  keywords={Particle swarm optimization;Music;Computational modeling;Loudspeakers;Art;Space technology;Real time systems;Computer simulation;Engines;Information technology;multimedia installations;multi-agent systems;simulation;computer-based music;real-time sound processing;media art},
  doi={10.1109/ISM.2009.38},
  ISSN={},
  month={Dec},}@ARTICLE{9447958,
  author={Zhai, Zhiqun and Jiang, Hexun and Fu, Mengfan and Liu, Lei and Yi, Lilin and Hu, Weisheng and Zhuge, Qunbi},
  journal={Journal of Lightwave Technology}, 
  title={An Interpretable Mapping From a Communication System to a Neural Network for Optimal Transceiver-Joint Equalization}, 
  year={2021},
  volume={39},
  number={17},
  pages={5449-5458},
  abstract={In this paper, we propose a scheme that utilizes the optimization ability of artificial intelligence (AI) for optimal transceiver-joint equalization in compensating for the optical filtering impairments caused by wavelength selective switches (WSS). In contrast to adding or replacing a certain module of existing digital signal processing (DSP), we exploit the similarity between a communication system and a neural network (NN). By mapping a communication system to an NN, in which the equalization modules correspond to the convolutional layers and other modules can been regarded as static layers, the optimal transceiver-joint equalization coefficients can be obtained. In particular, the DSP structure of the communication system is not changed. Extensive numerical simulations are performed to validate the performance of the proposed method. For a 65 GBaud 16QAM signal, it can achieve a 0.76 dB gain when the number of WSSs is 16 with a -6 dB bandwidth of 73 GHz.},
  keywords={Convolution;Finite impulse response filters;Artificial neural networks;Optimization;Communication systems;Adaptive filters;Optical transmitters;Coherent transceiver;digital signal processing;neural network;optical filtering impairments;transceiver-joint equalization},
  doi={10.1109/JLT.2021.3086301},
  ISSN={1558-2213},
  month={Sep.},}@INBOOK{10948965,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={48 Exploring the Realm of Perfect AI: Myth or Eventuality}, 
  year={2025},
  volume={},
  number={},
  pages={317-324},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948965},}@INBOOK{10948934,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={Appendix: Communicating with The White House}, 
  year={2025},
  volume={},
  number={},
  pages={325-328},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948934},}@INBOOK{10948940,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={10 Large Language Models as Data Compression Engines}, 
  year={2025},
  volume={},
  number={},
  pages={61-68},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948940},}@INBOOK{10948962,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={4 AI Hallucinations}, 
  year={2025},
  volume={},
  number={},
  pages={19-24},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948962},}@INBOOK{10948927,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={42 Machines with Sight, Sound, Taste, Touch, and Speech}, 
  year={2025},
  volume={},
  number={},
  pages={277-282},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948927},}@INBOOK{10948929,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={Index}, 
  year={2025},
  volume={},
  number={},
  pages={329-330},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948929},}@INBOOK{10948972,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={29 The Power of Multimodal AI}, 
  year={2025},
  volume={},
  number={},
  pages={185-190},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948972},}@INPROCEEDINGS{10397753,
  author={Tyagi, Deepanshu and Tanwar, Sarvesh and Mittal, Neetu and Badotra, Sumit},
  booktitle={2023 6th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={Analyse and Evaluate Quixbugs with Open AI Codex and Powering Next Generation Application}, 
  year={2023},
  volume={6},
  number={},
  pages={165-170},
  abstract={The paper investigates the ordinary language age limits of massive language models, with applications to the development of two types of learning tools commonly found in programming courses. We construct programming workouts and code explanations, reviewing these abstractly and statistically, using the Open AI Codex as the massive language model. Our findings indicate that the majority of the content so given is both unique and sensible, and that it is occasionally ready to use without any guarantees. While rehearsing, we discovered that it is astonishingly simple to affect both the programming ideas and the practical subjects they include simply by offering expressions of commitment to the model. Findings indicate massively generative computer intelligence models have fundamental value.},
  keywords={Training;Codes;Smoothing methods;Computational modeling;Programming;Artificial intelligence;Robots;GPT-3;Open AI;Codex;Java;Security;Intelligence;Application},
  doi={10.1109/IC3I59117.2023.10397753},
  ISSN={},
  month={Sep.},}@INBOOK{10948939,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={9 Creating AI Models: From Data to Deployment}, 
  year={2025},
  volume={},
  number={},
  pages={53-60},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948939},}@INBOOK{10948924,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={27 Dawn of the Machine: A Chronicle of the First Day Under Super AI}, 
  year={2025},
  volume={},
  number={},
  pages={175-178},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948924},}@ARTICLE{10943150,
  author={Aleixandre, Manuel and Prasetyawan, Dani and Nakamoto, Takamichi},
  journal={IEEE Access}, 
  title={Generative Diffusion Network for Creating Scents}, 
  year={2025},
  volume={13},
  number={},
  pages={57311-57321},
  abstract={This paper introduces a novel application of generative diffusion networks for creating scents. The research presents a generative diffusion network designed to create new aromas with specified, required odor descriptors using essential oils as the basic components. The model uses mass spectrometry data of essential oils as latent embedding space of the essential oils. The generative network outputs mass spectrometry data as the primary output. These generated mass spectrometry profiles are then processed by non-negative least squares to create essential oils recipes that have the required odor descriptors. The results demonstrate the modelâ€™s ability to produce diverse and new aroma profiles, which are validated by sensory tests. The method can create new scents by mixing essential oils, making automated aroma design possible. This approach shows major progress in aroma design. These results suggest many uses in industries like perfumery and food and beverage, improving efficiency and creativity in making many different fragrances.},
  keywords={Oils;Noise;Mass spectroscopy;Olfactory;Noise measurement;Machine learning;Noise reduction;Temperature measurement;Chemicals;Industries;Essential oils;fragrance synthesis;generative diffusion networks;mass spectrometry},
  doi={10.1109/ACCESS.2025.3555273},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10677925,
  author={Peng, Fei and Fu, Huiyuan and Ming, Anlong and Wang, Chuanming and Ma, Huadong and He, Shuai and Dou, Zifei and Chen, Shu},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={AIGC Image Quality Assessment via Image-Prompt Correspondence}, 
  year={2024},
  volume={},
  number={},
  pages={6432-6441},
  abstract={In the rapidly evolving landscape of deep learning, generative models such as Generative Adversarial Networks (GANs) and diffusion models have significantly advanced the capabilities of Artificial Intelligence Generated Content (AIGC). These technologies have streamlined the creative process, enabling AI to autonomously produce a diverse range of content with minimal human input. Despite the remarkable progress in AI-generated images (AIGIs), evaluating the quality of AIGIs remains a complex challenge. Traditional image quality assessment (IQA), focusing on aspects like distortion and blurriness, are insufficient for capturing the correspondence between AIGIs and their prompts. To address this, we propose a novel AIGC image quality assessment (AIGCIQA) framework that emphasizes the correspondence between images and prompts. Utilizing the CLIP modelâ€™s pre-trained image and text encoders, our method effectively measures the correspondence between visual and textual inputs. By transforming the assessment into classification probabilities and subsequently into a precise regression task, our method enhances the CLIP modelâ€™s performance in AIGCIQA. Our methodâ€™s effectiveness is confirmed by its first place in the image track of the NTIRE 2024 Quality Assessment for AI-Generated Content challenge and its state-of-the-art (SOTA) performance on benchmark datasets AGIQA-1K, AGIQA-3K, and AIG-CIQA2023. This research represents a significant advancement in the field, offering an efficient and versatile tool for the evaluation of AIGIs and contributing to the ongoing development of AIGC technologies. Our codes are available at https://github.com/pf0607/IPCE.},
  keywords={Image quality;Visualization;Refining;Focusing;Benchmark testing;Streaming media;Generative adversarial networks},
  doi={10.1109/CVPRW63382.2024.00644},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{9377878,
  author={Menon, Sumeet and Galita, Joshua and Chapman, David and Gangopadhyay, Aryya and Mangalagiri, Jayalakshmi and Nguyen, Phuong and Yesha, Yaacov and Yesha, Yelena and Saboury, Babak and Morris, Michael},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={Generating Realistic COVID-19 x-rays with a Mean Teacher + Transfer Learning GAN}, 
  year={2020},
  volume={},
  number={},
  pages={1216-1225},
  abstract={COVID-19 is a novel infectious disease responsible for over 1.2 million deaths worldwide as of November 2020. The need for rapid testing is a high priority and alternative testing strategies including x-ray image classification are a promising area of research. However, at present, public datasets for COVID-19 x-ray images have low data volumes, making it challenging to develop accurate image classifiers. Several recent papers have made use of Generative Adversarial Networks (GANs) in order to increase the training data volumes. But realistic synthetic COVID-19 x-rays remain challenging to generate. We present a novel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID-19 chest x-ray images of high quality. In order to create a more accurate GAN, we employ transfer learning from the Kaggle pneumonia x-ray dataset, a highly relevant data source orders of magnitude larger than public COVID-19 datasets. Furthermore, we employ the Mean Teacher algorithm as a constraint to improve stability of training. Our qualitative analysis shows that the MTT-GAN generates x-ray images that are greatly superior to a baseline GAN and visually comparable to real x-rays. Although board-certified radiologists can distinguish MTT-GAN fakes from real COVID-19 x-rays, quantitative analysis shows that MTT-GAN greatly improves the accuracy of both a binary COVID-19 classifier as well as a multi-class pneumonia classifier as compared to a baseline GAN. Our classification accuracy is favorable as compared to recently reported results in the literature for similar binary and multi-class COVID-19 screening tasks.},
  keywords={COVID-19;Transfer learning;Lung;Generative adversarial networks;Task analysis;X-ray imaging;Testing;Coronavirus;deep transfer learning;mean teacher;artificial intelligence;diagnostic radiology;x-ray},
  doi={10.1109/BigData50022.2020.9377878},
  ISSN={},
  month={Dec},}@ARTICLE{11130579,
  author={Qin, Shaojie and Fang, Yong and Li, Youbang and Li, Zongyang},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Generation of Infrastructure Crack Images for Self-Supervision Training Based on Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-17},
  abstract={Data scarcity often hinders the application of artificial intelligence techniques for detecting defects in infrastructure. To address this issue, a pure crack image generation method is proposed based on the diffusion algorithm. This method can produce images with various crack features in real backgrounds and improve the accuracy of the detection model by enhancing the training data. Compared to generative adversarial networks (GAN), the proposed method does not require a discriminator or labeled samples for training, thus generating more diverse results. To improve the sensitivity of the generative model to crack features, a specialized denoising model is developed that simultaneously employs convolution and transformer for feature extraction branching. These two types of feature maps are effectively fused by proposing a deformable feature extraction block (DFEB) and self-selecting feature fusion block (SFFB) so that the cracks and the backgrounds receive sufficient attention at the same time. A staged training strategy is used to merge the crack and background styles of multiple datasets, which further increases the diversity of the model. The experimental results show that the FrÃ©chet inception distance (FID), generation precision (GP), generation recall (GR) and generation F1 score (GF1) of the proposed crack generation method are 3.10, 0.72, 0.75 and 0.73, respectively, which is more advantageous than the existing diffusion models and GAN models. In addition, an existing defect segmentation model is tested using the generated crack images based on self-supervised transfer learning. This model improves the accuracy (Acc), F1-score (F1), and intersection over union (IoU) of the target dataset by 9.18 percentage points (pp), 6.99 pp, and 7.83 pp, respectively. The experimental results demonstrate the significant effect of the proposed method on improving the detection accuracy of small-scale datasets. The trained model has been made into a crack image generation tool that is open-sourced for validation and use by other scholars.},
  keywords={Training;Generative adversarial networks;Noise;Feature extraction;Data models;Transformers;Noise reduction;Accuracy;Image synthesis;Diffusion models;Infrastructure cracks;crack image generation;diffusion model;data enhancement;self-supervised learning},
  doi={10.1109/TITS.2025.3597284},
  ISSN={1558-0016},
  month={},}@INPROCEEDINGS{11086263,
  author={Anandhakumar, Dharmalingam and Kishore, K. V. Krishna},
  booktitle={2025 Second International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS)}, 
  title={Improved Adaptive Instance Normalization for StarGANv2-VC}, 
  year={2025},
  volume={},
  number={},
  pages={229-236},
  abstract={Voice Conversion (VC) systems modify vocal style while preserving linguistic information. Their relevance has grown as communication technologies have been more widely used. Deep Learning (DL) models based on Generative Adversarial Networks (GANs) have recently made great strides in VC. Among the numerous GAN models, the StarGANv2-VC includes adversarial source classifier loss and perceptual loss for VC. It uses single set of generators and discriminators to execute VCs across multiple speakers. It incorporates feature statistics with Adaptive Instance Normalization (AdaIN), which effectively modifies the content input's mean and variance to align with the style input. On the other hand, AdaIN applies normalization to the mean and variance of all feature maps separately, which could potentially remove information regarding the relative sizes of features. In view of the above, this work proposes an Improved StarGANv2-VC (IStarGANv2-VC) model, which modifies the AdaIN to perform normalization and modulation to remove artifacts and enhance speech quality. These two operations are only performed on the standard variance alone, whereas the mean is not required. This MAdaIN improves data efficiency in models by normalizing and modifying convolutional weights rather than feature maps, hence reducing information loss. MAdaIN uses speech embedding collected from target speaker data to generate affine variables directly controlling convolutional weights. Thus, this technique helps maintain essential elements while improving VC performance, specifically converted speech quality and similarity. Finally, test results using various benchmark corpora prove that the IStarGANv2-VC achieves higher efficiency and quality of converted speech, especially based on Root Mean Square Error (RMSE), Mel Cepstral Distortion (MCD), Mean Opinion Score (MOS), Word Error Rate (WER), Character Error Rate (CER), and voice similarity score.},
  keywords={Hands;Adaptation models;Convolution;Error analysis;Speech enhancement;Linguistics;Generative adversarial networks;Root mean square;Intelligent systems;Standards;Voice conversion;GAN;StarGANv2-VC;Adaptive instance normalization;Standard variance;signal processing;Machine Learning and Artificial Intelligence},
  doi={10.1109/ICC-ROBINS64345.2025.11086263},
  ISSN={},
  month={June},}@INPROCEEDINGS{11140024,
  author={Rai, Abhijeet and Vaishnav, Vaidehi and Bopche, Akshay and Natraj, Amarjit and Solanke, Tanaya and Sakhare, Apeksha V.},
  booktitle={2025 6th International Conference for Emerging Technology (INCET)}, 
  title={Deep Learning Techniques for Detecting Lunar Craters and Surface Features}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Lunar surface analysis plays a pivotal role in unraveling the Moon's geological history and ensuring the safety of future lunar missions. Accurate detection and classification of features such as craters and boulders are essential for understanding impact processes and assessing potential hazards for spacecraft landings. In this project, we introduce a deep learning-based approach for automated lunar surface analysis, focusing on the detection and classification of craters and boulders using segmentation techniques. Moving beyond the YOLO based models, our methodology employs advanced models such as U-Net and Mask R-CNN for precise segmentation, as well as Generative Adversarial Networks (GANs) for data augmentation. The dataset is sourced from highresolution lunar images, including data from NASA's Lunar Reconnaissance Orbiter mission, with carefully annotated labels for both craters and boulders.Key stages of the process include image preprocessing using Python-based tools such as Canny Edge Detection and noise reduction techniques. The segmentation models then extract features, followed by the application of morphological operations for post-processing to refine object boundaries. This comprehensive approach ensures high precision in object detection, with clear labeling and bounding box generation for improved interpretability. Model performance is evaluated rigorously using metrics such as Intersection over Union (IoU),Dice Coefficient, Precision, and Recall. A webbased interface, developed using Flask, enables real-time analysis, allowing users to upload lunar images for immediate crater and boulder detection. This framework enhances the efficiency of lunar surface feature detection and segmentation, with broader implications for planetary exploration and mission planning.},
  keywords={Image segmentation;Accuracy;Moon;Surface morphology;Feature extraction;Generative adversarial networks;Data models;Real-time systems;Planning;Surface treatment;Lunar object detection;Deep learning;GAN;U-Net;Image Processing;Artificial Intelligence},
  doi={10.1109/INCET64471.2025.11140024},
  ISSN={2996-4490},
  month={May},}@ARTICLE{10856224,
  author={Chen, Zhuangzhuang and Xu, Chengqi and Hu, Tao and Wang, Li and Chen, Jie and Li, Jianqiang},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Decompose-Compose Feature Augmentation for Imbalanced Crack Recognition in Industrial Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Automated crack recognition has achieved remarkable progress in the past decades as a critical task in structure health monitoring, to ensure safety and durability in many industrial scenarios. However, imbalanced crack recognition remains challenging due to the scarcity of crack samples and the consequential limited diversity. To resolve this, Artificial Intelligence Generated Content (AIGC) has been gradually adopted to generate synthetic data and reduce reliance on large amounts of labeled crack samples. This paper assumes that a crack sample in the feature space can be regarded as a combination of crack and background semantics. Then, the decompose-compose feature augmentation framework (DeCo) is proposed to perform crack data synthesis in the feature space by randomly composing crack and background semantic-relevant features. Specifically, the contrastive learning-based decomposing loss is proposed to enforce two encoders to separately learn crack and background semantics from crack samples with the theoretical guarantee. After that, an effective cross-instance feature union strategy is proposed to synthesize diverse crack samples by composing the crack-relevant features from a crack sample and background-relevant features across other training samples. To address the limited availability of related benchmarks, we collect INPP2022 and IRC2022 datasets from real-world applications in nuclear power plants and road pavement. Experimental results show that DeCo performs favorably against state-of-the-art competitors in imbalanced crack recognition tasks.},
  keywords={Training;Semantics;Power generation;Roads;Feature extraction;Mutual information;Costs;Automation;Visualization;Training data;Automated crack recognition;Data scarcity;Artificial Intelligence Generated Content;Nuclear power plants},
  doi={10.1109/TASE.2025.3535764},
  ISSN={1558-3783},
  month={},}@ARTICLE{10759659,
  author={Lee, Hyunmin and Kang, Donggoo and Park, Hasil and Park, Sangwoo and Jeong, Dasol and Paik, Joonki},
  journal={IEEE Access}, 
  title={Real-Time Human Group Detection and Clustering in Crowded Environments Using Enhanced Multi-Object Tracking}, 
  year={2024},
  volume={12},
  number={},
  pages={184028-184039},
  abstract={Group detection is a critical yet challenging task in video-based applications such as surveillance analysis, especially in crowded and dynamic environments where complex pedestrian interactions occur. Traditional trajectory-based methods often struggle with occlusions and overlapping behaviors, leading to inaccurate group identification. To address these limitations, we propose a novel algorithm that integrates an optimized YOLOv8 model with DeepSORT tracking, enhancing both detection accuracy and real time performance. Our approach uniquely combines high-precision object detection with stable multi-object tracking, ensuring consistent identification of individuals and groups over time, even in high-density scenarios. Additionally, we introduce an innovative method of constructing an adjacency matrix by integrating Euclidean distances and bounding box diagonal ratios, which is transformed into a graph to intricately analyze and predict complex group dynamics in real time. Experimental results on real-world airport CCTV footage demonstrate that our method significantly outperforms existing approaches, achieving higher precision and recall rates. Furthermore, the algorithm operates efficiently on standard hardware, indicating strong practical feasibility for real-time applications in public spaces. While challenges such as misclassification due to incomplete data annotations and occlusions remain, our study showcases the potential of integrating spatial and temporal data to advance real-time group detection and tracking, aiming to improve crowd management systems in public spaces.},
  keywords={Pedestrians;Real-time systems;Accuracy;Heuristic algorithms;Tracking;Object recognition;Faces;Deep learning;Airports;Visualization;Multi-object tracking;visual surveillance;group detection},
  doi={10.1109/ACCESS.2024.3503661},
  ISSN={2169-3536},
  month={},}@ARTICLE{11106921,
  author={Xue, Yilei and Zhong, Ruikang and Li, Jianhua and Chen, Yue and Liu, Yuanwei},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Privacy-Preserving Mobile Edge Generation: Analog and Digital Transmission}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={To address the communication overhead of generative artificial intelligence (GAI), mobile edge generation (MEG), which deploys GAI on edge servers, has emerged. However, MEG is vulnerable to eavesdropping threats in wireless environments. In this paper, we propose a latent space-based privacy-preserving MEG framework (LPM) to mitigate such threats. The proposed LPM scheme incorporates two sub-schemes: analog LPM (A-LPM) and digital LPM (D-LPM), both of which transmit latent representations instead of raw data and incorporate differential privacy to safeguard sensitive information. Specifically, we consider two levels of potential adversaries: L1 adversaries, who focus solely on eavesdropping, and L2 adversaries, who further employ non-local means denoising algorithms to enhance reconstruction quality. Through theoretical analysis, we demonstrate that both A-LPM and D-LPM enhance privacy protection against L1 and L2 adversaries. Experimental results further validate the LPM framework, showcasing its robust privacy protection, superior image quality, and lower symbol transmission demand for legitimate users. Under the LPM scheme, L1 adversaries experience a PSNR degradation of up to 9Â dB, while L2 adversaries are unable to reconstruct high-quality images even after applying denoising techniques.},
  keywords={Wireless communication;Wireless sensor networks;Communication system security;Protection;Vectors;Symbols;Image reconstruction;Image edge detection;Decoding;Noise reduction;Generative artificial intelligence;mobile edge generation;privacy preservation;wireless communication},
  doi={10.1109/TVT.2025.3594710},
  ISSN={1939-9359},
  month={},}@INPROCEEDINGS{10178711,
  author={Xing, Xiaodan and Nan, Yang and Felder, Federico and Walsh, Simon and Yang, Guang},
  booktitle={2023 IEEE 36th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={The Beauty or the Beast: Which Aspect of Synthetic Medical Images Deserves Our Focus?}, 
  year={2023},
  volume={},
  number={},
  pages={523-528},
  abstract={Training medical AI algorithms requires large volumes of accurately labeled datasets, which are difficult to obtain in the real world. Synthetic images generated from deep generative models can help alleviate the data scarcity problem, but their effectiveness relies on their fidelity to real-world images. Typically, researchers select synthesis models based on image quality measurements, prioritizing synthetic images that appear realistic. However, our empirical analysis shows that high-fidelity and visually appealing synthetic images are not necessarily superior. In fact, we present a case where low-fidelity synthetic images outperformed their high-fidelity counterparts in downstream tasks. Our findings highlight the importance of comprehensive analysis before incorporating synthetic data into real-world applications. We hope our results will raise awareness among the research community of the value of low-fidelity synthetic images in medical AI algorithm training.},
  keywords={Training;Image quality;Image synthesis;Computational modeling;Data models;Task analysis;Artificial intelligence;Data augmentation;Generative models;Medical image synthesis},
  doi={10.1109/CBMS58004.2023.00273},
  ISSN={2372-9198},
  month={June},}@INPROCEEDINGS{8628691,
  author={Zonta, Alessandro and Smit, S. K. and Haasdijk, Evert and Eiben, A. E.},
  booktitle={2018 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Modelling Human Movements With Turing Learning}, 
  year={2018},
  volume={},
  number={},
  pages={2254-2261},
  abstract={Modelling human behaviour is still an ongoing challenge that spaces between several fields like social science, artificial intelligence, and philosophy. Since the research of a metric able to define all the aspect of the human nature is still an ambitious task, most current studies use concepts like social forces or handwritten rules for modelling. Following the growing trend behind a new branch of Artificial Intelligence called Generative AI, this paper presents the application of Turing Learning on the problem of modelling human movements. Turing Learning is a generative model that uses evolutionary algorithms as a way to learn behaviours without the need for predefined metrics and, using deep learning models, it is able to produce human-like trajectories. We show how the system is able to infer the behaviours of the trajectories in the ETH dataset, forecasting the next points with the truthfulness of being a possible human movement.},
  keywords={Trajectory;Computational modeling;Sociology;Statistics;Hidden Markov models;Data models;Measurement;Human movements;Collective movements;Generative models;Co-evolution;Machine learning},
  doi={10.1109/SSCI.2018.8628691},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10748521,
  author={Dharwadkar, Vineet and R, Veena and S, Manohar and M G, Jayanthi and Kannadaguli, Prashanth},
  booktitle={2024 5th International Conference on Circuits, Control, Communication and Computing (I4C)}, 
  title={Smart Cart: Revolutionizing E-Commerce in India with AI-Powered Personalized Product Recommendations Overview}, 
  year={2024},
  volume={},
  number={},
  pages={87-92},
  abstract={The rapid evolution of e-commerce has intensified the need for effective personalization to enhance user experience. This paper introduces SmartCart, an innovative website that integrates cutting-edge AI technologies to provide personalized product recommendations based on image uploads. Users can interact with the platform by uploading images of products they like and selecting from various AI models designed for different recommendation tasks. These models include convolutional neural networks (CNNs) for image similarity analysis and generative adversarial networks (GANs) for contextual product suggestions. SmartCart leverages these technologies to offer highly relevant and visually similar product recommendations, aiming to improve user satisfaction and engagement. Despite its advancements, SmartCart faces limitations inherent to AI in e-commerce. Challenges such as the variability in image quality, the need for extensive training data, and potential biases in model predictions can impact the accuracy and relevance of recommendations. Additionally, the system's effectiveness is constrained by the diversity of user preferences and the capability of AI models to generalize across different product categories. This paper explores how SmartCart addresses these challenges and discusses future directions for improving AI-driven recommendation systems in the context of e-commerce.},
  keywords={Support vector machines;Training data;Transforms;Feature extraction;User experience;Data models;Electronic commerce;Artificial intelligence;Recommender systems;Testing;E-commerce;personalized recommendations;convolutional neural networks;generative adversarial networks;SmartCart},
  doi={10.1109/I4C62240.2024.10748521},
  ISSN={2473-7690},
  month={Oct},}@ARTICLE{10883946,
  author={Alam, Lamia and Kehtarnavaz, Nasser},
  journal={IEEE Access}, 
  title={Improving Accuracy of IC Surface Defects Detection via Enhanced-CycleGAN Data Augmentation}, 
  year={2025},
  volume={13},
  number={},
  pages={29932-29943},
  abstract={An important step in integrated circuit (IC) manufacturing is inspection of the chip surface for defects. In practice, there exists a data imbalance problem associated with IC surface defect images which affects the detection performance of a deep learning-based detection model. In this paper, this data imbalance problem is addressed via generating synthetic IC surface defect images by the generative network of Enhanced-CycleGAN. The main contribution of this work involves exhibiting that such a data augmentation enables defective IC surfaces to be detected at higher detection accuracies compared with the situation when no data augmentation is used. First, the generated synthetic images by the Enhanced-CycleGAN generative network are compared with the synthetic generative networks of GAN, VAE, Diffusion, and CycleGAN as well as with the traditional image processing data augmentation. The closeness of synthetic images to real images is assessed based on five similarity metrics of Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), Visual Information Fidelity (VIF), FrÃ©chet Inception Distance (FID), and Kernel Inception Distance (KID). The assessment conducted based on these metrics indicates that the synthetic IC surface defect images generated by Enhanced-CycleGAN resemble closer to the real IC surface defect images compared to the other generative networks. Second, the impact of the data augmentation by the Enhanced-CycleGAN generative network on the detection of IC surface defects is examined by considering three typical and commonly used detection models of VGG16, ResNet50, and YOLO11. It is shown that our Enhanced-CycleGAN data augmentation improves the detection performance across all the three detection models.},
  keywords={Surface treatment;Data augmentation;Surface morphology;Surface cracks;Integrated circuit modeling;Training;Surface contamination;Accuracy;Generative adversarial networks;Data models;Data augmentation;data imbalance;generative AI;enhanced-CycleGAN;surface defects of integrated circuits},
  doi={10.1109/ACCESS.2025.3541614},
  ISSN={2169-3536},
  month={},}@INBOOK{10811687,
  author={Salucci, Marco and Li, Maokun and Massa, Andrea},
  booktitle={Artificial Intelligence for Future Networks}, 
  title={AI&#x2010;Driven Approaches for Solving Electromagnetic Inverse Problems}, 
  year={2025},
  volume={},
  number={},
  pages={257-281},
  abstract={Abstract <p>This chapter provides an overview of artificial intelligence&#x2010;driven methods for solving electromagnetic (EM) inverse problems (IPs) with high reliability, robustness, and computational efficiency. Several methodologies are detailed and discussed, including the recent developments within the so&#x2010;called (i) three&#x2010;step learning&#x2010;by&#x2010;examples, (ii) system&#x2010;by&#x2010;design, and (iii) deep learning frameworks. Afterward, a survey on their customization to several EM&#x2013;IPs, such as those arising in microwave imaging of free&#x2010;space and buried targets, biomedical imaging, nondestructive testing and evaluation, as well as the detection, localization, and tracking of non&#x2010;cooperative targets in wireless networks, is given.</p>},
  keywords={Imaging;Vectors;Permittivity;Training;Shape;Q measurement;Object recognition;Maxwell equations;Mathematical models;Location awareness},
  doi={10.1002/9781394227952.ch8},
  ISSN={},
  publisher={IEEE},
  isbn={9781394227945},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10811687},}@INPROCEEDINGS{9641383,
  author={Bhadra, Ananta Bijoy and Islam, Sheikh Shafaet and Hasan, Kazi and Sarker, Niloy and Uddin, Jalal and Bhuiyan, Erphan A.},
  booktitle={2021 International Conference on Electronics, Communications and Information Technology (ICECIT)}, 
  title={A Convolutional Generative Model for short circuit fault protection of a microgrid system}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper aims to develop an intelligent protection scheme for microgrids with a number of distributed generation units considering different modes of operation. The conventional computational intelligence-based shunt fault detection and classification approaches have shallow architecture and involve a huge number of trainable parameters that restrains the effective feature extraction. In this work, a hierarchical generative model is developed that fuses the benefit of the convolutional operation and the weight sharing mechanism which improves the feature extraction process as well as reduces the trainable parameters. Also, the fault data in transmission line domain is limited. The proposed method can able to dig out the most efficient feature from the limited training dataset. The results presented in this study confirm the high performance of the proposed framework.},
  keywords={Training;Transmission line matrix methods;Power transmission lines;Systems operation;Microgrids;Voltage;Transforms;Microgrid;feature extraction;time-frequency energy image;wavelet transform Voltage and current waveform},
  doi={10.1109/ICECIT54077.2021.9641383},
  ISSN={},
  month={Sep.},}@ARTICLE{9165162,
  author={Firdaus, Mauajama and Chauhan, Hardik and Ekbal, Asif and Bhattacharyya, Pushpak},
  journal={IEEE Transactions on Affective Computing}, 
  title={EmoSen: Generating Sentiment and Emotion Controlled Responses in a Multimodal Dialogue System}, 
  year={2022},
  volume={13},
  number={3},
  pages={1555-1566},
  abstract={An essential skill for effective communication is the ability to express specific sentiment and emotion in a conversation. Any robust dialogue system should handle the combined effect of both sentiment and emotion while generating responses. This is expected to provide a better experience and concurrently increase usersâ€™ satisfaction. Previously, research on either emotion or sentiment controlled dialogue generation has shown great promise in developing the next generation conversational agents, but the simultaneous effect of both is still unexplored. The existing dialogue systems are majorly based on unimodal sources, predominantly the text, and thereby cannot utilize the information present in the other sources, such as video, audio, image, etc. In this article, we present at first a large scale benchmark Sentiment Emotion aware Multimodal Dialogue (SEMD) dataset for the task of sentiment and emotion controlled dialogue generation. The SEMD dataset consists of 55k conversations from 10 TV shows having text, audio, and video information. To utilize multimodal information, we propose multimodal attention based conditional variational autoencoder (M-CVAE) that outperforms several baselines. Quantitative and qualitative analyses show that multimodality, along with contextual information, plays an essential role in generating coherent and diverse responses for any given emotion and sentiment.},
  keywords={Visualization;Task analysis;Artificial intelligence;TV;Buildings;History;Generative adversarial networks;Conversational AI;natural language generation;sentiment-aware NLG;emotion-aware NLG;multimodality},
  doi={10.1109/TAFFC.2020.3015491},
  ISSN={1949-3045},
  month={July},}@INPROCEEDINGS{9185538,
  author={Shu, Tianye and Wang, Ziqi and Liu, Jialin and Yao, Xin},
  booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Novel CNet-assisted Evolutionary Level Repairer and Its Applications to Super Mario Bros}, 
  year={2020},
  volume={},
  number={},
  pages={1-10},
  abstract={Applying latent variable evolution to game level design has become more and more popular as little human expert knowledge is required. However, defective levels with illegal patterns may be generated due to the violation of constraints for level design. A traditional way of repairing the defective levels is programming specific rule-based repairers to patch the flaw. However, programming these constraints is sometimes complex and not straightforward. An autonomous level repairer which is capable of learning the constraints is needed. In this paper, we propose a novel approach, CNet, to learn the probability distribution of tiles giving its surrounding tiles on a set of real levels, and then detect the illegal tiles in generated new levels. Then, an evolutionary repairer is designed to search for optimal replacement schemes equipped with a novel search space being constructed with the help of CNet and a novel heuristic function. The proposed approaches are proved to be effective in our case study of repairing GAN-generated and artificially destroyed levels of Super Mario Bros. game. Our CNet-assisted evolutionary repairer can also be easily applied to other games of which the levels can be represented by a matrix of objects or tiles.},
  keywords={Games;Gallium nitride;Generative adversarial networks;Genetic algorithms;Maintenance engineering;Artificial intelligence;Generators;Procedural content generation;level repair;latent vector evolution;evolutionary algorithms;video games},
  doi={10.1109/CEC48606.2020.9185538},
  ISSN={},
  month={July},}@INPROCEEDINGS{10571127,
  author={Zhang, Xinyuan and Liu, Jiang and Xiong, Zehui and Huang, Yudong and Xie, Gaochang and Zhang, Ran},
  booktitle={2024 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={Edge Intelligence Optimization for Large Language Model Inference with Batching and Quantization}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Generative Artificial Intelligence (GAI) is taking the world by storm with its unparalleled content creation ability. Large Language Models (LLMs) are at the forefront of this movement. However, the significant resource demands of LLMs often require cloud hosting, which raises issues regarding privacy, latency, and usage limitations. Although edge intelligence has long been utilized to solve these challenges by enabling real-time AI computation on ubiquitous edge resources close to data sources, most research has focused on traditional AI models and has left a gap in addressing the unique characteristics of LLM inference, such as considerable model size, auto-regressive processes, and self-attention mechanisms. In this paper, we present an edge intelligence optimization problem tailored for LLM inference. Specifically, with the deployment of the batching technique and model quantization on resource-limited edge devices, we formulate an inference model for transformer decoder-based LLMs. Furthermore, our approach aims to maximize the inference throughput via batch scheduling and joint allocation of communication and computation resources, while also considering edge resource constraints and varying user requirements of latency and accuracy. To address this NP-hard problem, we develop an optimal Depth-First Tree-Searching algorithm with online tree-Pruning (DFTSP) that operates within a feasible time complexity. Simulation results indicate that DFTSP surpasses other batching benchmarks in throughput across diverse user settings and quantization techniques, and it reduces time complexity by over 45% compared to the brute-force searching method.},
  keywords={Quantization (signal);Accuracy;Computational modeling;Simulation;Throughput;Inference algorithms;Data models;Generative AI;large language model;edge intelligence;wireless networks},
  doi={10.1109/WCNC57260.2024.10571127},
  ISSN={1558-2612},
  month={April},}@ARTICLE{10973284,
  author={Chen, Zhiwei and Cai, Kai and Ye, Junliang and Li, Qiang and Ge, Xiaohu},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Resource Block-Granularity Precoding Optimization and Compression for Cell-Free Mobile Networks}, 
  year={2025},
  volume={12},
  number={5},
  pages={3641-3655},
  abstract={The forward-precoding scheme could significantly reduce downlink fronthaul traffic in cell-free mobile networks by compressing resource-block (RB)-granularity precoding matrices. Nevertheless, this scheme would result in a sum-rate degradation due to the compression distortion and the significant frequency-selective fading. To address this issue, the compressed RB-granularity precoding optimization is first formulated as a non-convex stochastic problem and then its lower bound is derived. By maximizing the lower bound, the stochastic non-convex problem is transformed into a deterministic RB-granularity precoding optimization problem and a matrix compression problem. For the deterministic RB-granularity precoding optimization problem, its stationary point is proved to be a linear combination of frequency channel matrices. Based on this property, an RB-granularity weighted minimum mean-square-error (RB-WMMSE) precoding algorithm is designed. For the matrix compression problem, a transformer-based vector-quantized variational autoencoder (TVQ-VAE) algorithm is designed to achieve a high ratio compression. Simulation results show that the proposed RB-WMMSE algorithm could improve the sum-rate by a maximum of 101% compared to the traditional RB-granularity precoding algorithm. When the compression distortion of precoding matrix is considered, the proposed TVQ-VAE algorithm could improve the sum-rate by a maximum of 106% compared to the traditional autoencoder algorithm.},
  keywords={Precoding;Optimization;Symbols;Vectors;Downlink;Generative adversarial networks;Artificial intelligence;Training;Distributed databases;Computer architecture;Cell-free mobile network;fronthoul compression;matrix compression;RB-granularity precoding;sum-rate optimization},
  doi={10.1109/TNSE.2025.3563370},
  ISSN={2327-4697},
  month={Sep.},}@ARTICLE{10912725,
  author={Yang, Zhixiong and Zhen, Ziyi and Xu, Hui and Feng, Xinlong},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={K-AirWrite: Real-Time Continuous Air Handwriting Recognition Using Kinect}, 
  year={2025},
  volume={74},
  number={},
  pages={1-12},
  abstract={Camera-based air handwriting recognition can enable a new form of user-computer interface. For a practically viable air handwriting recognition system, real-time responsiveness, robustness against environmental and human-induced interferences, and coherent handwriting recognition are essential. However, existing systems fall short of fulfilling these requirements simultaneously. To this end, we propose K-AirWrite, a real-time, robust system designed for continuous air handwriting recognition. The key enabler is to simplify complex video stream recognition into a single-frame trajectory image recognition problem by mapping hand motion in the video stream to hand trajectory coordinate images and enhancing real-time performance through a lightweight detection network. Furthermore, we develop a segmentation method that leverages coordinate changes induced by the inherent pattern during the character-switching process to segment continuous characters. These segmented characters are then sequentially input into the M-RepVGG network for recognition. Extensive experiments demonstrate that K-AirWrite is remarkably efficient and exhibits robust generalization capabilities, achieving an accuracy rate of 98.2% for individual characters and 96.4% for continuous characters.},
  keywords={Real-time systems;Hands;Feature extraction;Character recognition;Training;Streaming media;Data mining;Artificial intelligence;Handwriting recognition;Trajectory;Air handwriting recognition;CA attention;Kinect;RepVGG;WIoUv3 loss function;YOLOv8},
  doi={10.1109/TIM.2025.3548230},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10656730,
  author={Liu, Xiangyue and Xue, Han and Luo, Kunming and Tan, Ping and Yi, Li},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GenN2N: Generative NeRF2NeRF Translation}, 
  year={2024},
  volume={},
  number={},
  pages={5105-5114},
  abstract={We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, in-painting, etc. Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs. To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs. The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings. To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme. Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power. More results on our project page: https://xiangyueliu.github.io/GenN2N/.},
  keywords={Solid modeling;Three-dimensional displays;Codes;Superresolution;Gaussian distribution;Neural radiance field;Rendering (computer graphics);NeRF Editing;Generative Editing;NeRF-to-NeRF Translation},
  doi={10.1109/CVPR52733.2024.00488},
  ISSN={2575-7075},
  month={June},}@ARTICLE{11177170,
  author={Ghosh, Saswath and Maurya, Rahul and Roy, Sitikantha},
  journal={IEEE Robotics and Automation Letters}, 
  title={PaGDeSA: An Automated Generative Design of Multimodal Soft Pneumatic Actuators}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The recent advancements in machine learning techniques have fostered a growing emphasis on data-driven design. In soft robotics, this often involves optimizing for specific tasks rather than learning the design space. Motivated by this, the present study proposes a Performance-augmented Generative Design of Soft Actuators (PaGDeSA), an automated data-driven framework that explores the design space to generate new soft actuator designs. As a first step, a synthetic dataset of soft Pneumatic Network (Pneu-net) actuators has been created using data augmentation to train the generative model. Following on, the parametric design of two different unimodal (namely, pure bending and twisting type) actuators is used to train the Gaussian mixture model that generates novel actuator designs to perform in-plane and out-of-plane deformations. A distance-based metric is used to describe the novelty and diversity of the generated designs. Later, the feasibility of these designs is ensured using an automated finite element simulation. Further, the feasible generated design is passed on for digital fabrication to assure the manufacturability of the actuator design. In addition, a comparison of the bending angle (quality metric) between the simulated and real design strengthens the proposed approach. The framework could accelerate the design of new soft robots by enabling the selection of appropriate actuators from the diverse set of generated multimodal actuators.},
  keywords={Actuators;Soft robotics;Training;Bending;Synthetic data;Solid modeling;Pneumatic actuators;Optimization;Finite element analysis;Data models;Soft Sensors and Actuators;Soft Robot Materials and Design;Soft Robot Applications;Generative Design},
  doi={10.1109/LRA.2025.3614019},
  ISSN={2377-3766},
  month={},}@ARTICLE{10473032,
  author={Tan, Juan and Wu, Baochen and Ma, Yunlong},
  journal={IEEE Access}, 
  title={A Visual Characteristic Land-Scape Design for EEG Signal Based on LSTM-GAN}, 
  year={2024},
  volume={12},
  number={},
  pages={41896-41907},
  abstract={The ultimate goal of artificial intelligence is to endow machines with human intelligence. Studying and simulating electroencephalogram signals is a way to achieve this goal. The human brain has similar representation abilities for similar visual stimuli. By utilizing this feature, a visual stimulus electroencephalogram signal decoding model based on Long Short-Term Memory Network Bagging was proposed to decode and classify human brain signals. And based on this extracted classification model, a generative adversarial network based on a bi-directional short-term memory network was proposed. It could generate similar visual stimulus images similar to the human brain and represent the visual signals of the human brain. These experiments confirmed that the classification accuracy of the research method in the decoding of electroencephalogram signals reached 91.17%. In terms of extracting visual characteristics and land-scape features from the electroencephalogram, this research model had the highest classification accuracy and recall rates, with 98.38% and 97.94%, respectively. This stimulation image generation model studied had the best actual image generation performance, with an Inception score of 7.27. The study not only improves the accuracy of electroencephalogram signal classification, but also completes the re-construction of brain signals into images. It improves the collaborative representation ability of human-machine collaborative visual cognitive systems and has important significance in brain computer interaction.},
  keywords={Visualization;Electroencephalography;Mathematical models;Brain modeling;Feature extraction;Decoding;Reliability;Visual analytics;Encoding;Generative adversarial networks;Brain-computer interfaces;Electroencephalogram visual decoding;Bi-LSTM;LSTM-B-GAN;generate adversarial re-construction;brain computer interaction},
  doi={10.1109/ACCESS.2024.3377689},
  ISSN={2169-3536},
  month={},}@ARTICLE{9565928,
  author={Tazrin, Tahrat and Rahman, Quazi Abidur and Fouda, Mostafa M. and Fadlullah, Zubair Md.},
  journal={IEEE Access}, 
  title={LiHEA: Migrating EEG Analytics to Ultra-Edge IoT Devices With Logic-in-Headbands}, 
  year={2021},
  volume={9},
  number={},
  pages={138834-138848},
  abstract={Traditional cloud computing of raw Electroencephalogram (EEG) data, particularly for continuous monitoring use-cases, consumes precious network resources and contributes to delay. Motivated by the paradigm shift of edge computing and Internet of Things (IoT) for continuous monitoring, we focus on this paper on the first step to carry out EEG edge analytics at the last frontier (i.e., the ultra-edge) of our considered cyber-physical system for ensuring usersâ€™ convenience and privacy. To overcome challenges due to computational and energy resource constraints of IoT devices (e.g., EEG headbands/headsets), in this paper, we envision a smart, lightweight model, referred to as Logic-in-Headbands based Edge Analytics (LiHEA), which can be seamlessly incorporated with the consumer-grade EEG headsets to reduce delay and bandwidth consumption. By systematically investigating various traditional machine and deep learning models, we identify and select the best model for our envisioned LiHEA. We consider a use-case for detecting confusion, representing levels of distraction, during online course delivery which has become pervasive during the novel coronavirus (COVID-19) pandemic. We apply a unique feature selection technique to find out which features are triggered with confusion where delta waves, attention, and theta waves were announced as the three most important features. Among various traditional machine and deep learning models, our customized random forest model demonstrated the highest accuracy of 90%. Since the dataset size might have impacted the performance of deep learning-based approaches, we further apply the deep convolutional generative adversarial network (DCGAN) to generate synthetic traces with representative samples of the original EEG data, and thereby enhance the variation in the data. While the performances of the deep learning models significantly increase after the data augmentation, they still cannot outperform the random forest model. Furthermore, computational complexity analysis is performed for the three best-performing algorithms, and random forest emerges as the most viable model for our envisioned LiHEA.},
  keywords={Electroencephalography;Brain modeling;Computational modeling;Monitoring;Deep learning;Random forests;Feature extraction;Electroencephalogram (EEG);random forest;edge analytics;Internet of Things (IoT);feature selection;deep convolutional generative adversarial network (DCGAN);classification;confusion},
  doi={10.1109/ACCESS.2021.3118971},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10350482,
  author={Rajapaksa, Sajith and Uwabeza Vianney, Jean Marie and Castro, Renell and Khalvati, Farzad and Aich, Shubhra},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Using Large Text To Image Models with Structured Prompts for Skin Disease Identification: A Case Study}, 
  year={2023},
  volume={},
  number={},
  pages={2686-2693},
  abstract={This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a severe lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains to mitigate the data scarcity issue, and debias automated diagnostics from the all-pervasive racial biases.},
  keywords={Linear systems;Location awareness;Computer vision;Conferences;Skin;Data models;Medical diagnostic imaging;LTI;generative images;skin conditions},
  doi={10.1109/ICCVW60793.2023.00284},
  ISSN={2473-9944},
  month={Oct},}@ARTICLE{6079205,
  author={Bernard, S. Maiyo and Wang, Xianku and Liu, Chengying},
  journal={Tsinghua Science and Technology}, 
  title={An integrated application of neural network, fuzzy and expert systems for machining operation sequencing}, 
  year={1999},
  volume={4},
  number={4},
  pages={1632-1637},
  abstract={A part is described using features. A reuro fuzzy system then determines the machining sequerce for each feature. Previous process plans were utilized to build, test, and validate the Neuro Fuzzy Network (NFN). Parts having simlar manufacturing sequences are grouped into families, also using an NFN. A standard manufacturing sequence is obtained for each family comprising all the operations applicable to the features of the parts in the family. An expert system then adapts this standard sequence for the particular part being planned. The optimal operation sequence is inherited by the new part. The procedure is demonstrated by an example industrial part.},
  keywords={Process planning;Shafts;Machining;Expert systems;Surface finishing;machining sequence;neuro fuzzy;expert system;semi generative;group technology},
  doi={},
  ISSN={1007-0214},
  month={Dec},}@INPROCEEDINGS{11112367,
  author={Mirza, Muhammad Usama and Arda AydÄ±n, Mustafa and Ã‡ukur, Tolga},
  booktitle={2025 33rd Signal Processing and Communications Applications Conference (SIU)}, 
  title={Frequency-Based Soft Diffusion Model for Accelerated MRI Reconstruction}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Magnetic Resonance Imaging (MRI) is a powerful diagnostic tool, but its clinical utility is often hindered by long scan times. MRI reconstruction techniques aim to reduce scan times by enabling recovery of high quality MRI images from undersampled k-space acquisitions. This work introduces a novel diffusion model for MRI reconstruction that incorporates two key strategies: k-space spatial frequency removal and noise addition. During the forward diffusion process, k-space data points are progressively removed, simulating the undersampling process encountered in accelerated MRI scans. Simultaneously, images are corrupted via Gauss noise addition to enhance robustness against noise. This dual approach enables the diffusion model to learn the underlying image features while explicitly accounting for the acquisition process for accelerated MRI scans. Our results demonstrate that the proposed techniques offer superior image quality compared to conventional diffusion models in various undersampling rates. This work highlights that injecting prior knowledge on accelerated data acquisitions processes in MRI into diffusion models can help enhance reconstruction performance.},
  keywords={Image quality;Magnetic resonance imaging;Noise;Data acquisition;Diffusion processes;Diffusion models;Robustness;Image reconstruction;MRI;reconstruction;generative;diffusion;undersampling},
  doi={10.1109/SIU66497.2025.11112367},
  ISSN={2165-0608},
  month={June},}@INPROCEEDINGS{6225813,
  author={Son, Changho and Park, Yongtae},
  booktitle={2012 IEEE International Conference on Management of Innovation & Technology (ICMIT)}, 
  title={A quantitative approach to evaluation of services: GTM-based visualization and fuzzy AHP approach}, 
  year={2012},
  volume={},
  number={},
  pages={250-254},
  abstract={With the increasing importance of the service sector to both developed and developing countries, the evaluation of services has been considered as one of critical themes of study. Accordingly, a number of studies for evaluating services have been carried out. However, most of them have been performed in qualitative way due to the paucity of quantitative service data. As a remedy, this study proposes a quantitative and systematic approach for evaluating services using information visualization technique and the concept of service value that calculated by fuzzy AHP and similarity index. This study can contribute to the research themes for service engineering in terms of employing and utilizing the service information data to evaluate services.},
  keywords={Data visualization;Indexes;Vectors;Fuzzy sets;Data models;Systematics;Fuzzy AHP;Generative topographic mapping;Service value;Similarity index},
  doi={10.1109/ICMIT.2012.6225813},
  ISSN={},
  month={June},}@INPROCEEDINGS{10973990,
  author={Martin-Ozimek, Antonio and Jayarathne, Isuru and Mon, Su Larb and Chew, Jouh Yeong},
  booktitle={2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Diffusion-Based Imitation Learning for Social Pose Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1488-1492},
  abstract={Intelligent agents, such as robots and virtual agents, must understand the dynamics of complex social interactions to interact with humans. Effectively representing social dynamics is challenging because we require multi-modal, synchronized observations to understand a scene. We explore how using a single modality, the pose behavior, of multiple individuals in a social interaction can be used to generate nonverbal social cues for the facilitator of that interaction. The facilitator acts to make a social interaction proceed smoothly and is an essential role for intelligent agents to replicate in human-robot interactions. In this paper, we adapt an existing diffusion behavior cloning model to learn and replicate facilitator behaviors. Furthermore, we evaluate two representations of pose observations from a scene, one representation has pre-processing applied and one does not. The purpose of this paper is to introduce a new use for diffusion behavior cloning for pose generation in social interactions. The second is to understand the relationship between performance and computational load for generating social pose behavior using two different techniques for collecting scene observations. As such, we are essentially testing the effectiveness of two different types of conditioning for a diffusion model. We then evaluate the resulting generated behavior from each technique using quantitative measures such as mean per-joint position error (MPJPE), training time, and inference time. Additionally, we plot training and inference time against MPJPE to examine the trade-offs between efficiency and performance. Our results suggest that the further pre-processed data can successfully condition diffusion models to generate realistic social behavior, with reasonable trade-offs in accuracy and processing time. Future work will focus on extending this approach to generate multiple nonverbal social cues, on generalizing this method to multiple types of social activity, and on evaluating the results using human evaluators using a virtual agent or robot as a facilitator.},
  keywords={Training;Imitation learning;Human-robot interaction;Cloning;Position measurement;Diffusion models;Time measurement;Intelligent agents;Synchronization;Testing;Imitation learning;Machine learning;Generative AI;Autonomous robots},
  doi={10.1109/HRI61500.2025.10973990},
  ISSN={},
  month={March},}@INPROCEEDINGS{7361240,
  author={Tabatabai, Shahla and Shamsfard, Mehrnoush},
  booktitle={2015 SAI Intelligent Systems Conference (IntelliSys)}, 
  title={Cognitive modeling of a general-purpose creative system (Creagene model)}, 
  year={2015},
  volume={},
  number={},
  pages={842-846},
  abstract={Creagene is a multi-purpose, multi-agent creative model that has been designed after investigation of creative humans in science, engineering, management, art and scientific discovery. This model is able to solve problems creatively and generates and interprets figures and statements. In the proposed model, there are six intelligent agents (namely, preprocessor generator, explorer, domain processor, analyzer-examiner and postprocessor) as well as a memory and two libraries. First, the inputs from different domains are obtained, abstracted, and then processed iteratively by the generator and explorative agents. The obtained pre-inventive structures are sent to the domain processor to become particular structures in the desired domain. Next, the analyzer-examiner investigates the structures for novelty and value based on some criteria. If any structure is determined to be creative, it is output to the environment and placed into the library of creative products. Otherwise, it is retained in the usual library for future processing by the postprocessor. In this paper, various aspect of the Creagene model is discussed and three examples are presented.},
  keywords={Computational modeling;Generators;Libraries;Diamonds;Intelligent systems;Feature extraction;cognitive modeling;creative systems;computational creativity;pre-inventive structures;generative agent;explorative agent},
  doi={10.1109/IntelliSys.2015.7361240},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10622480,
  author={Raja, Gunasekaran and Essaky, Selvam and Rajendran, Deepak Suresh and Senthivel, Sai Ganesh and Knorr, Sebastian and Dev, Kapal},
  booktitle={ICC 2024 - IEEE International Conference on Communications}, 
  title={Adaptive Model Predictive Control-Driven Approach for Visual Detection of Micro- UAVs}, 
  year={2024},
  volume={},
  number={},
  pages={4530-4535},
  abstract={Unmanned Aerial Vehicles (UAVs) provide a good base platform for dynamic vision-based target detection. The detection process can be enhanced by utilizing multiple UAVs. In such scenarios, precise trajectory planning is essential to achieve objectives while avoiding collisions with obstacles. Furthermore, in most cases, Air-to-Air (A2A) detection of micro-UAVs in large-scale environments is challenging due to their dynamic movements and other complex parameters, such as poor light, motion blur, and occlusion. To solve these challenges, developing an algorithm that can adapt the control strategy based on changing system and environmental parameters is essential. This paper proposes an Adaptive Model Predictive Control (AMPC)-driven hybrid GANYOLOX framework to mitigate these navigation and detection challenges. The proposed AMPC allows model updates during multi-UAV operations, which enables estimation techniques to predict changes in the system model and helps to compute the target UAV position. Alternatively, the framework constructs a hybrid GAN-YOLOX model to overcome various A2A detection challenges. Extensive comparative analysis of the hybrid GANYOLOX framework achieved 94% detection accuracy and out-performed existing DL frameworks by 8%.},
  keywords={Adaptation models;Accuracy;Trajectory planning;Computational modeling;Neural networks;Object detection;Predictive models;Unmanned Aerial Vehicles (UAV);Adaptive Model Predictive Control (AMPC);Trajectory Control;Target Detection;Generative Adversarial Network;Convolutional Neural Networks},
  doi={10.1109/ICC51166.2024.10622480},
  ISSN={1938-1883},
  month={June},}@INPROCEEDINGS{10674429,
  author={Tai, Lori and Wang, Kevin Y.},
  booktitle={2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={Supplementary AI-Enabled Website to Bilingual Picture Book to Learn English with English}, 
  year={2024},
  volume={},
  number={},
  pages={665-666},
  abstract={Bilingual storybooks much like the Happy Meal Readerâ€“â€“a miniature English book that comes with every McDonaldâ€™s Happy Mealâ€“â€“have become an incentive for young children to learn English as a second language. Yet, in the name of production efficiency, translations are often appended at the end of the Reader. Because these pages only include translational text, it becomes difficult for children to locate the translation for a given page of the story. This inconvenience hinders children from learning English as they cannot determine the meaning of specific sentences, leaving them to understand the story only as a whole. Moreover, the level of English is standardized and cannot be tailored to the English level of every child. This study presents an interactive website that stimulates children with their first and second language alternatively. Multiple levels of English will be made available to accommodate young readers of various ages and advance their language abilities.},
  keywords={Production;Consumer electronics;ESL;bilingual picture book;generative AI;ChatGPT;computer-mediated instruction},
  doi={10.1109/ICCE-Taiwan62264.2024.10674429},
  ISSN={2575-8284},
  month={July},}@BOOK{11079732,
  author={LeMaire, Chrissy and Abshire, Brandon},
  booktitle={AI for Everyday IT: Accelerate workplace productivity},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Automate and accelerate your everyday IT tasks with instant solutions! What if you never had to write another after-incident report, piece of boilerplate code, or a performance review from scratch ever again? Use AI tools like ChatGPT, Claude, Gemini, and Copilot right, and youâ€™ll take back hours of your timeâ€”and more! AI for Everyday IT reveals how you can automate dozens of your daily IT tasks with generative AI.  In AI for Everyday IT youâ€™ll learn how to:  Write effective prompts for common IT tasks Optimize report generation, document handling, and workplace communication Resolve IT conflicts and crises Acquire new skills and upgrade your resume AI for help desk, database administration and systems administration Incorporate AI into DevOps processes and create AI-powered applications Simplify time-consuming people management tasks  In this hands-on guide, automation experts Chrissy LeMaire and Brandon Abshire show you how AI tools like ChatGPT have made their lives a million times easier, and how they can do the same for you. Youâ€™ll find proven strategies for using AI to improve help desk support, automate sysadmin and database tasks, aid with DevOps engineering, handle managing IT teams, and dozens more time-saving and quality-improving hacks.},
  keywords={AI for IT;AI automation;workplace productivity;generative AI;ChatGPT for IT;IT help desk AI;automate IT tasks;prompt engineering;IT conflict resolution;AI for sysadmins;database administration AI;DevOps automation;AI-powered applications},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633436428},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11079732},}@INPROCEEDINGS{11094615,
  author={Xu, Jingyi and Tu, Siwei and Yang, Weidong and Fei, Ben and Li, Shuhao and Liu, Keyi and Luo, Yeqi and Ma, Lipeng and Bai, Lei},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior}, 
  year={2025},
  volume={},
  number={},
  pages={10567-10576},
  abstract={Variation of Arctic sea ice has significant impacts on polar ecosystems, transporting routes, coastal communities, and global climate. Tracing the change of sea ice at a finer scale is paramount for both operational applications and scientific studies. Recent pan-Arctic sea ice forecasting methods that leverage advances in artificial intelligence have made promising progress over numerical models. However, forecasting sea ice at higher resolutions is still under-explored. To bridge the gap, we propose a two-module cooperative deep learning framework, IceDiff, to forecast sea ice concentration at finer scales. IceDiff first leverages a vision transformer to generate coarse yet superior forecasting results over previous methods at a regular 25 km grid. This high-quality sea ice forecasting can be utilized as reliable guidance for the next module. Subsequently, an unconditional diffusion model pre-trained on low-resolution sea ice concentration maps is utilized for sampling down-scaled sea ice forecasting via a zero-shot guided sampling strategy and a patch-based method. For the first time, IceDiff demonstrates sea ice forecasting with a 6.25 km resolution. IceDiff extends the boundary of existing sea ice forecasting models and more importantly, its capability to generate high-resolution sea ice concentration data is vital for pragmatic usages and research. Code is available at https://github.com/EtronTech/IceDiff.},
  keywords={Computer vision;Sea measurements;Predictive models;Transformers;Pattern recognition;Arctic;Reliability;Forecasting;Sea ice;Pragmatics},
  doi={10.1109/CVPR52734.2025.00988},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10137850,
  author={Bahrini, Aram and Khamoshifar, Mohammadsadra and Abbasimehr, Hossein and Riggs, Robert J. and Esmaeili, Maryam and Majdabadkohne, Rastin Mastali and Pasehvar, Morteza},
  booktitle={2023 Systems and Information Engineering Design Symposium (SIEDS)}, 
  title={ChatGPT: Applications, Opportunities, and Threats}, 
  year={2023},
  volume={},
  number={},
  pages={274-279},
  abstract={Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer) is an artificial intelligence technology that is fine-tuned using supervised machine learning and reinforcement learning techniques, allowing a computer to generate natural language conversation fully autonomously. ChatGPT is built on the transformer architecture and trained on millions of conversations from various sources. The system combines the power of pre-trained deep learning models with a programmability layer to provide a strong base for generating natural language conversations. In this study, after reviewing the existing literature, we examine the applications, opportunities, and threats of ChatGPT in 10 main domains, providing detailed examples for the business and industry as well as education. We also conducted an experimental study, checking the effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found that the latter performs significantly better. Despite its exceptional ability to generate natural-sounding responses, the authors believe that ChatGPT does not possess the same level of understanding, empathy, and creativity as a human and cannot fully replace them in most situations.},
  keywords={Industries;Ethics;Education;Oral communication;Reinforcement learning;Chatbots;Transformers;Artificial Intelligence;ChatGPT;Chatbots;Natural language generation;Education},
  doi={10.1109/SIEDS58326.2023.10137850},
  ISSN={},
  month={April},}@ARTICLE{10508087,
  author={Liao, Jian and Zhong, Linrong and Zhe, Longting and Xu, Handan and Liu, Ming and Xie, Tao},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Scaffolding Computational Thinking With ChatGPT}, 
  year={2024},
  volume={17},
  number={},
  pages={1628-1642},
  abstract={ChatGPT has received considerable attention in education, particularly in programming education because of its capabilities in automated code generation and program repairing and scoring. However, few empirical studies have investigated the use of ChatGPT to customize a learning system for scaffolding studentsâ€™ computational thinking. Therefore, this article proposes an intelligent programming scaffolding system using ChatGPT following the theoretical framework of computational thinking and scaffolding. A mixed-method study was conducted to investigate the affordance of the scaffolding system using ChatGPT, and the findings show that most students had positive attitudes about the proposed system, and it was effective in improving their computational thinking generally but not their problem-solving skills. Therefore, more scaffolding strategies are discussed with the aim of improving student computational thinking, especially regarding problem-solving skills. The findings of this study are expected to guide future designs of generative artificial intelligence tools embedded in intelligent learning systems to foster studentsâ€™ computational thinking and programming learning.},
  keywords={Chatbots;Education;Programming profession;Codes;Task analysis;Problem-solving;Encoding;Artificial-intelligence-generated content (AIGC);ChatGPT;computational thinking (CT);scaffolding},
  doi={10.1109/TLT.2024.3392896},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{10763589,
  author={Zhu, Zhui and Qi, Guangpeng and Shang, Guangyong and He, Qingfeng and Zhang, Weichen and Li, Ningbo and Chen, Yunzhi and Hu, Lijun and Zhang, Wenqiang and Dang, Fan},
  booktitle={2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Enhancing Large Language Models with Knowledge Graphs for Robust Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={262-269},
  abstract={In recent years, large language models (LLMs) have shown rapid development, becoming one of the most popular topics in the field of artificial intelligence. LLMs have demonstrated powerful generalization and learning capabilities, and their performance on various language tasks has been remarkable. Despite their successes, LLMs face significant challenges, particularly in domain-specific tasks that require structured knowledge, often leading to issues such as hallucinations. To mitigate these challenges, we propose a novel system, SynaptiQA, which integrates LLMs with Knowledge Graphs (KGs) to answer more questions about knowledge. Our approach leverages the generative capabilities of LLMs to create and optimize KG queries, thereby improving the accuracy and contextual relevance of responses. Experimental results in an industrial data set demonstrate that SynaptiQA outperforms baseline models and naive retrieval-augmented generation (RAG) systems, demonstrating improved accuracy and reduced hallucinations. This integration of KGs with LLMs paves the way for more reliable and interpretable domain-specific question answering systems.},
  keywords={Accuracy;Large language models;Distributed databases;Knowledge graphs;Question answering (information retrieval);Vectors;Cognition;Data models;Reliability;Faces;Artificial Intelligence;Knowledge Graph;Large Language Model},
  doi={10.1109/ICPADS63350.2024.00042},
  ISSN={2690-5965},
  month={Oct},}@ARTICLE{10955494,
  author={Wang, Xin and Lyu, Jianhui and Peter, J. Dinesh and Kim, Byung-Gyu and Parameshachari, B.D. and Li, Keqin and Wei, Wei},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Explaining Sentiments: Improving Explainability in Sentiment Analysis Using Local Interpretable Model-Agnostic Explanations and Counterfactual Explanations}, 
  year={2025},
  volume={12},
  number={3},
  pages={1390-1403},
  abstract={Sentiment analysis of social media platforms is crucial for extracting actionable insights from unstructured textual data. However, modern sentiment analysis models using deep learning lack explainability, acting as black box and limiting trust. This study focuses on improving the explainability of sentiment analysis models of social media platforms by leveraging explainable artificial intelligence (XAI). We propose a novel explainable sentiment analysis (XSA) framework incorporating intrinsic and posthoc XAI methods, i.e., local interpretable model-agnostic explanations (LIME) and counterfactual explanations. Specifically, to solve the problem of lack of local fidelity and stability in interpretations caused by the LIME random perturbation sampling method, a new model-independent interpretation method is proposed, which uses the isometric mapping virtual sample generation method based on manifold learning instead of LIMEs random perturbation sampling method to generate samples. Additionally, a generative link tree is presented to create counterfactual explanations that maintain strong data fidelity, which constructs counterfactual narratives by leveraging examples from the training data, employing a divide-and-conquer strategy combined with local greedy. Experiments conducted on social media datasets from Twitter, YouTube comments, Yelp, and Amazon demonstrate XSAs ability to provide local aspect-level explanations while maintaining sentiment analysis performance. Analyses reveal improved model explainability and enhanced user trust, demonstrating XAIs potential in sentiment analysis of social media platforms. The proposed XSA framework provides a valuable direction for developing transparent and trustworthy sentiment analysis models for social media platforms.},
  keywords={Sentiment analysis;Social networking (online);Analytical models;Predictive models;Explainable AI;Lexicon;Electronic mail;Deep learning;Blogs;Transformers;Explainable artificial intelligence (XAI);explainability;local interpretable model-agnostic explanations (LIME);sentiment analysis (SA)},
  doi={10.1109/TCSS.2025.3531718},
  ISSN={2329-924X},
  month={June},}@INPROCEEDINGS{10809976,
  author={Haryono, Kholid and Hidayatullah, Ahmad Fathan},
  booktitle={2024 9th International Conference on Information Technology and Digital Applications (ICITDA)}, 
  title={Large Language Model: Design Mobile Platform for Problem Solving Ideation}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This study focuses on creating a platform design using the TRIZ method to solve problems in business and management. TRIZ is known for its structured approach to problem-solving and has been effective in promoting innovation and improvement in products, services, and systems. However, while TRIZ is generally user-friendly, it is challenging to learn and apply it well. This often requires the help of TRIZ experts working with people who understand the specific area to find the best solutions. Since it was first developed, TRIZ has become much more important in business and management, helping with key areas like human resources and how organizations work. TRIZ remains vital in improving how problems are solved by serving different types of organizations, including social enterprises and non-profits. To overcome the challenges of using TRIZ, IT-based platforms have been very helpful, but the guidance of TRIZ experts is still crucial. Additionally, the rise of artificial intelligence (AI), especially Generative AI models, offers an exciting way to enhance TRIZ methods. Generative AI allows algorithms to analyze various data sources and create content such as text, images, audio, and video. Using Generative AI, interactions between users and systems become more human-like, allowing users to explain the specifics of their problems and what they want to achieve. AI then provides solutions based on TRIZ principles. The method used in this study includes Function Analysis (FA) modeling, which has three main stages: component analysis, interaction analysis, and function modeling. The result of this study is a user-friendly Generative AI function design, shown as a prompt interface that works with TRIZ knowledge. This interface provides users solution suggestions that match their goals, thus making the problem-solving process in business and management easier.},
  keywords={Analytical models;Technological innovation;Adaptation models;Generative AI;Large language models;Soft sensors;Standards organizations;Organizations;Problem-solving;Iterative methods;Large Language Model;Platform TRIZ;TRIZ Business Management;Problem Solving;Function Analysis},
  doi={10.1109/ICITDA64560.2024.10809976},
  ISSN={},
  month={Nov},}@ARTICLE{10445174,
  author={Hasan, Md. Zahid and Montaha, Sidratul and Khan, Inam Ullah and Hassan, Md. Mehedi and Mahmud, Abdullah Al and Rafid, A. K. M. Rakibul Haque and Azam, Sami and Karim, Asif and Prountzos, Spyridon and Alexopoulou, Efthymia and Ashraf, Umama Binta and Islam, Sheikh Mohammed Shariful},
  journal={IEEE Access}, 
  title={Fast and Efficient Lung Abnormality Identification With Explainable AI: A Comprehensive Framework for Chest CT Scan and X-Ray Images}, 
  year={2024},
  volume={12},
  number={},
  pages={31117-31135},
  abstract={A novel automated multi-classification approach is proposed for the anticipation of lung abnormalities using chest X-ray and CT images. The study leverages a publicly accessible dataset with an insufficient and unbalanced number of images, addressing this issue by employing the data augmentation approach DCGAN to balance the dataset. Various preprocessing procedures are applied to improve features and reduce noise in lung pictures. As the base for the model, the vision trans-former and convolution-based compact convolutional transformer (CCT) model is utilized. To determine the best model configuration, an ablation study is performed on the original CCT model using a CT scan dataset with image dimensions of  $32\times32$ . Following that, this model is trained on the X-ray dataset to evaluate performance on an entirely other modality. The performances are compared to six pre-trained models with  $32\times 32$  images. While traditional models achieved modest performance, with test accuracies ranging from 43% to 77% and 49% to 73% requiring lengthy training times, the suggested model performed exceptionally well, obtaining test accuracies of 99.77% and 95.37% for CT and X-ray, respectively with a short training duration of 10â€“12 and 40â€“42 seconds/epoch. Robustness is demonstrated through the progressive reduction of the number of training images, with findings indicating that the model maintains good performance even on a reduced dataset. An explainable AI technique Grad-CAM is used to explain the modelâ€™s judgment. Grad-CAM-based color visualization is shown to explain model assessments and help health specialists make quick, confident decisions. This study used image preprocessing and deep learning techniques to detect lung anomalies, and it addressed the challenges of training time and computational complexity.},
  keywords={X-ray imaging;Lung;Computed tomography;COVID-19;Biomedical imaging;Computational modeling;Solid modeling;Pulmonary diseases;Convolutional neural networks;Generative adversarial networks;Explainable AI;Lung disease;chest x-ray;CT scan;image preprocessing;compact convolutional transformer;deep convolutional GAN;explainable AI},
  doi={10.1109/ACCESS.2024.3369900},
  ISSN={2169-3536},
  month={},}@ARTICLE{8869751,
  author={Han, Changhee and Rundo, Leonardo and Araki, Ryosuke and Nagano, Yudai and Furukawa, Yujiro and Mauri, Giancarlo and Nakayama, Hideki and Hayashi, Hideaki},
  journal={IEEE Access}, 
  title={Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image Augmentation for Tumor Detection}, 
  year={2019},
  volume={7},
  number={},
  pages={156966-156977},
  abstract={Convolutional Neural Networks (CNNs) achieve excellent computer-assisted diagnosis with sufficient annotated training data. However, most medical imaging datasets are small and fragmented. In this context, Generative Adversarial Networks (GANs) can synthesize realistic/diverse additional training images to fill the data lack in the real image distribution; researchers have improved classification by augmenting data with noise-to-image (e.g., random noise samples to diverse pathological images) or image-to-image GANs (e.g., a benign image to a malignant one). Yet, no research has reported results combining noise-to-image and image-to-image GANs for further performance boost. Therefore, to maximize the DA effect with the GAN combinations, we propose a two-step GAN-based DA that generates and refines brain Magnetic Resonance (MR) images with/without tumors separately: (i) Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for high-resolution MR image generation, first generates realistic/diverse 256Ã—256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT) that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused GAN loss, further refines the texture/shape of the PGGAN-generated images similarly to the real ones. We thoroughly investigate CNN-based tumor classification results, also considering the influence of pre-training on ImageNet and discarding weird-looking GAN-generated images. The results show that, when combined with classic DA, our two-step GAN-based DA can significantly outperform the classic DA alone, in tumor detection (i.e., boosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.},
  keywords={Gallium nitride;Training;Tumors;Generative adversarial networks;Medical diagnostic imaging;Image synthesis;Data augmentation;synthetic image generation;GANs;brain MRI;tumor detection},
  doi={10.1109/ACCESS.2019.2947606},
  ISSN={2169-3536},
  month={},}@ARTICLE{9216160,
  author={Wang, Yutian and Yu, Guochen and Wang, Jingling and Wang, Hui and Zhang, Qin},
  journal={IEEE Access}, 
  title={Improved Relativistic Cycle-Consistent GAN With Dilated Residual Network and Multi-Attention for Speech Enhancement}, 
  year={2020},
  volume={8},
  number={},
  pages={183272-183285},
  abstract={Generative adversarial networks (GANs) have been increasingly used as feature mapping functions in speech enhancement, in which the noisy speech features are transformed to the clean ones through the generators. This article proposes a novel speech enhancement model based on a cycle-consistent relativistic GAN with Dilated Residual Networks and a Multi-attention mechanism. Using the adversarial loss, improved cycle-consistency losses, and an identity-mapping loss, a noisy-to-clean generator G and an inverse clean-to-noisy generator F simultaneously learn the forward and backward mappings between the source and target domains. To guarantee the stability of the training process, we replace vanilla GAN loss with relativistic average GAN loss and use spectral normalization in discriminators so that they conform to Lipschitz continuity. Furthermore, we employ two attention-based components as multi-attention mechanism to reduce importing signal distortion: attention U-net gates and dilated residual self-attention blocks. By employing these components, our proposed generators can capture long-term inner dependencies between elements of speech features and further preserve linguistic information. Experimental results on a public dataset indicate that the proposed model achieves state-of-the-art speech enhancement performance, especially in reducing speech distortion and improving signal overall quality. Compared with the representative GAN-based approaches, the proposed method significantly achieves the best performance in terms of STOI, CSIG, COVL, and CBAK objective metrics. Moreover, we demonstrate the contribution of each proposed component including relativistic average loss, attention U-net gate, self-attention layers, spectral normalization, and dilation operation by ten comparison systems.},
  keywords={Speech enhancement;Noise measurement;Generators;Gallium nitride;Generative adversarial networks;Logic gates;Training;Speech enhancement;cycle-consistent GAN;relativistic average loss;multi-attention;dilated residual network;U-net},
  doi={10.1109/ACCESS.2020.3029417},
  ISSN={2169-3536},
  month={},}@ARTICLE{8643911,
  author={Lu, Jie and Yang, Yang and Liu, Ruiyang and Kang, Sing Bing and Yu, Jingyi},
  journal={IEEE Access}, 
  title={2D-to-Stereo Panorama Conversion Using GAN and Concentric Mosaics}, 
  year={2019},
  volume={7},
  number={},
  pages={23187-23196},
  abstract={We describe a learning-based technique to automatically convert a 2-D panorama to its stereoscopic version. In particular, we train a generative adversarial network using perspective stereo pairs as inputs. Given a 2-D panorama, we partition it into overlapping local perspective views. To satisfy the panoramic stereo condition, we generate a sequence of left and right stereo view pairs and stitch them to produce concentric mosaics. We also describe experiments on synthetic and real datasets as well as comparisons with competing state-of-the-art techniques, which validate our technique.},
  keywords={Gallium nitride;Cameras;Generative adversarial networks;Two dimensional displays;Stereo image processing;Generators;Visualization;2D-to-stereo panorama;GAN;concentric mosaics;depth peeling loss;selector;stereo panorama synthesis},
  doi={10.1109/ACCESS.2019.2899221},
  ISSN={2169-3536},
  month={},}@ARTICLE{9296209,
  author={Kim, Yanghoon and Won, Seungpil and Yoon, Seunghyun and Jung, Kyomin},
  journal={IEEE Access}, 
  title={Collaborative Training of Gans in Continuous and Discrete Spaces for Text Generation}, 
  year={2020},
  volume={8},
  number={},
  pages={226515-226523},
  abstract={Applying generative adversarial networks (GANs) to text-related tasks is challenging due to the discrete nature of language. One line of research resolves this issue by employing reinforcement learning (RL) and optimizing the next-word sampling policy directly in a discrete action space. Such methods compute the rewards from complete sentences and avoid error accumulation due to exposure bias. Other approaches employ approximation techniques that map the text to continuous representation in order to circumvent the non-differentiable discrete process. Particularly, autoencoder-based methods effectively produce robust representations that can model complex discrete structures. In this article, we propose a novel text GAN architecture that promotes the collaborative training of the continuous-space and discrete-space methods. Our method employs an autoencoder to learn an implicit data manifold, providing a learning objective for adversarial training in a continuous space. Furthermore, the complete textual output is directly evaluated and updated via RL in a discrete space. The collaborative interplay between the two adversarial trainings effectively regularize the text representations in different spaces. The experimental results on three standard benchmark datasets show that our model substantially outperforms state-of-the-art text GANs with respect to quality, diversity, and global consistency.},
  keywords={Training;Gallium nitride;Generators;Generative adversarial networks;Maximum likelihood estimation;Computer architecture;Collaboration;Adversarial training;collaborative training;text GAN},
  doi={10.1109/ACCESS.2020.3045166},
  ISSN={2169-3536},
  month={},}@ARTICLE{9766329,
  author={Oh, Junyoung and Min, Kyungha and Yang, Heekyung},
  journal={IEEE Access}, 
  title={A Pose and Style-Invariant Reenactment Technique for Artistic Portraits Using GAN}, 
  year={2022},
  volume={10},
  number={},
  pages={50351-50362},
  abstract={Face reenactment techniques change the facial expression of a character in a photograph. This technique is very attractive in that it helps in the creation of new content from existing content. Many researchers have been developing face reenactment techniques, including generative adversarial network-based techniques using action unit vectors. However, face reenactment techniques for artwork are still insufficient. Unlike photographs, artwork includes a variety of poses and styles. To expand the existing techniques into the artwork domain, we propose the following technique. First, we use a rotation module to produce robustly qualitive results even in various poses. This rotation makes source portraits with excessively rotated poses frontal, creating a state in which face reenactment techniques are easy to apply. In addition, we use style loss and attention map to maintain the style of the artwork. To evaluate the proposed technique, we objectively and subjectively compare the results of existing techniques with those of our technique. Our metrics include preservation of identity and facial expression, suppression of artifacts, and conservation of artistic style.},
  keywords={Faces;Generative adversarial networks;Gold;Generators;Three-dimensional displays;Image reconstruction;Solid modeling;GAN;reenactment;portrait artwork;pose;style},
  doi={10.1109/ACCESS.2022.3172297},
  ISSN={2169-3536},
  month={},}@ARTICLE{10718301,
  author={Corbara, Silvia and Moreo, Alejandro},
  journal={IEEE Access}, 
  title={Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation}, 
  year={2024},
  volume={12},
  number={},
  pages={171911-171925},
  abstract={Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author (A) or by someone else ( $\overline {A}$ ). Itehas been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, oreby imitating the style of another author. Inethis paper, weeinvestigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of A. Weeanalyze the improvements in the classifier predictions that this augmentation brings to bear in the task of AV in an adversarial setting. Ineparticular, weeexperiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT model) and with two training strategies (one inspired by standard Language Models, and another inspired by Wasserstein Generative Adversarial Networks). Weeevaluate our hypothesis on five datasets (three of which have been specifically collected to represent an adversarial setting) and using two learning algorithms for the AV classifier (Support Vector Machines and Convolutional Neural Networks). This experimentation yields negative results, revealing that, although our methodology proves effective in many adversarial settings, its benefits are too sporadic for a pragmatical application.},
  keywords={Training;Generators;Generative adversarial networks;Classification algorithms;Data augmentation;Support vector machines;Standards;Writing;Transformers;Data models;Authorship identification;authorship verification;data augmentation;text classification},
  doi={10.1109/ACCESS.2024.3481161},
  ISSN={2169-3536},
  month={},}@ARTICLE{10792919,
  author={Mambile, Cesilia and Kaijage, Shubi and Leo, Judith},
  journal={IEEE Access}, 
  title={Application of Deep Learning in Forest Fire Prediction: A Systematic Review}, 
  year={2024},
  volume={12},
  number={},
  pages={190554-190581},
  abstract={Forests are among the worldâ€™s most valuable ecological resources. However, they face significant threats from Forest Fires (FFs), causing environmental damage and impacting wildlife and economies. The increasing global occurrence of FFs has created an urgent need for more accurate prediction methods. Traditional FF prediction approaches, reliant on meteorological data and human expertise, are often limited in accuracy and scalability. Deep Learning (DL) offers a promising solution for enhancing prediction capabilities. This systematic review evaluated various DL techniques for FF prediction, analyzing their methodologies, effectiveness, and challenges. Covering studies published between January 2017 and July 2024, 55 of 656 papers were selected for detailed analysis. The study revealed that Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models are the most frequently used, with most datasets being publicly available. These findings indicate that classification models and simulation-based studies dominate the field. Commonly used metrics include accuracy, precision, recall, F1 score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC). Key meteorological features, such as Temperature, Humidity, and Wind speed, have been extensively studied using the Normalized Difference Vegetation Index (NDVI), Land Surface Temperature (LST), and Normalized Difference Moisture Index (NDMI), the most commonly used satellite-derived features. However, integrating human activity data remains underexplored despite its potential to improve prediction accuracy significantly. Addressing this gap could enhance the practical applicability of DL models for FF predictions. This study provides insights into the most prevalent and effective DL techniques for FF prediction and highlights areas for future research.},
  keywords={Artificial intelligence;Forestry;Predictive models;Accuracy;Data models;Convolutional neural networks;Biological system modeling;Satellite images;Long short term memory;Computational modeling;Deep learning;forest fire;forest fire prediction;prediction features;satellite images;wildfire},
  doi={10.1109/ACCESS.2024.3515215},
  ISSN={2169-3536},
  month={},}@ARTICLE{9723518,
  author={Liu, Jingren and Bai, Haoyue and Zhang, Haofeng and Liu, Li},
  journal={IEEE MultiMedia}, 
  title={Beyond Normal Distribution: More Factual Feature Generation Network for Generalized Zero-Shot Learning}, 
  year={2022},
  volume={29},
  number={3},
  pages={69-79},
  abstract={Due to the prosperous development of generative models, research works have achieved great success on the generalized zero-shot learning (GZSL) task. In most generative methods of GZSL, researchers try to utilize attributes and normally distributed noise to generate visual features, which ignores whether the normal distribution can perfectly represent all categories. Therefore, in this article, we exploit variational auto-encoders (VAE) and visual features to generate image-level noise that can preserve class-level characteristics in more detail and propose a mechanism called more factual generative network (MFGN) to achieve more authentic generative process. In other words, it is to transfer the seen feature distribution to the unseen domains and regulate the knowledge to correct the generation of unseen samples. Extensive experiments are conducted on four popular datasets and the results demonstrate the effectiveness of the proposed work.},
  keywords={Visualization;Feature extraction;Training data;Semantics;Prototypes;Learning systems;Generative adversarial networks;Learning systems;Generalized ZSL;Generative Models;Data-driven Noise;Factual Features Generation},
  doi={10.1109/MMUL.2022.3155541},
  ISSN={1941-0166},
  month={July},}@ARTICLE{8697135,
  author={Li, Shutao and Song, Weiwei and Fang, Leyuan and Chen, Yushi and Ghamisi, Pedram and Benediktsson, JÃ³n Atli},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Deep Learning for Hyperspectral Image Classification: An Overview}, 
  year={2019},
  volume={57},
  number={9},
  pages={6690-6709},
  abstract={Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral-spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.},
  keywords={Feature extraction;Deep learning;Hyperspectral imaging;Training;Logistics;Classification;deep learning;feature extraction;hyperspectral image (HSI)},
  doi={10.1109/TGRS.2019.2907932},
  ISSN={1558-0644},
  month={Sep.},}@ARTICLE{8485427,
  author={Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
  journal={IEEE Transactions on Image Processing}, 
  title={CamStyle: A Novel Data Augmentation Method for Person Re-Identification}, 
  year={2019},
  volume={28},
  number={3},
  pages={1176-1190},
  abstract={Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.},
  keywords={Cameras;Training;Task analysis;Adaptation models;Machine learning;Data models;Australia;Person re-identification;CamStyle;one-view learning;unsupervised domain adaptation},
  doi={10.1109/TIP.2018.2874313},
  ISSN={1941-0042},
  month={March},}@ARTICLE{9585378,
  author={Ma, Fuyan and Sun, Bin and Li, Shutao},
  journal={IEEE Transactions on Affective Computing}, 
  title={Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion}, 
  year={2023},
  volume={14},
  number={2},
  pages={1236-1248},
  abstract={Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.},
  keywords={Face recognition;Feature extraction;Transformers;Visualization;Image recognition;Head;Task analysis;Facial expression recognition in the wild;global-local attention;transformers;global self-attention},
  doi={10.1109/TAFFC.2021.3122146},
  ISSN={1949-3045},
  month={April},}@INPROCEEDINGS{10204315,
  author={Zhang, Weixia and Zhai, Guangtao and Wei, Ying and Yang, Xiaokang and Ma, Kede},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective}, 
  year={2023},
  volume={},
  number={},
  pages={14071-14081},
  abstract={We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) using a textual template, and compute the joint probability from the cosine similarities of the visual-textual embeddings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learning three tasks - BIQA, scene classification, and distortion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and distortion type identification tasks and outperforms the state-of-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) realigns the quality annotations from different IQA datasets more effectively. The source code is available at https://github.com/zwx8981/LIQE.},
  keywords={Image quality;Training;Scene classification;Computational modeling;Source coding;Distortion;Multitasking;Datasets and evaluation},
  doi={10.1109/CVPR52729.2023.01352},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9162561,
  author={Zheng, Zhedong and Ruan, Tao and Wei, Yunchao and Yang, Yi and Mei, Tao},
  journal={IEEE Transactions on Multimedia}, 
  title={VehicleNet: Learning Robust Visual Representation for Vehicle Re-Identification}, 
  year={2021},
  volume={23},
  number={},
  pages={2683-2693},
  abstract={One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of $\text{86.07}\%$ mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.},
  keywords={Training;Robustness;Adaptation models;Data models;Automobiles;Cameras;Feature extraction;Vehicle re-identification;image representation;convolutional neural networks},
  doi={10.1109/TMM.2020.3014488},
  ISSN={1941-0077},
  month={},}@ARTICLE{9730792,
  author={Quan, Weize and Zhang, Ruisong and Zhang, Yong and Li, Zhifeng and Wang, Jue and Yan, Dong-Ming},
  journal={IEEE Transactions on Image Processing}, 
  title={Image Inpainting With Local and Global Refinement}, 
  year={2022},
  volume={31},
  number={},
  pages={2405-2420},
  abstract={Image inpainting has made remarkable progress with recent advances in deep learning. Popular networks mainly follow an encoder-decoder architecture (sometimes with skip connections) and possess sufficiently large receptive field, i.e., larger than the image resolution. The receptive field refers to the set of input pixels that are path-connected to a neuron. For image inpainting task, however, the size of surrounding areas needed to repair different kinds of missing regions are different, and the very large receptive field is not always optimal, especially for the local structures and textures. In addition, a large receptive field tends to involve more undesired completion results, which will disturb the inpainting process. Based on these insights, we rethink the process of image inpainting from a different perspective of receptive field, and propose a novel three-stage inpainting framework with local and global refinement. Specifically, we first utilize an encoder-decoder network with skip connection to achieve coarse initial results. Then, we introduce a shallow deep model with small receptive field to conduct the local refinement, which can also weaken the influence of distant undesired completion results. Finally, we propose an attention-based encoder-decoder network with large receptive field to conduct the global refinement. Experimental results demonstrate that our method outperforms the state of the arts on three popular publicly available datasets for image inpainting. Our local and global refinement network can be directly inserted into the end of any existing networks to further improve their inpainting performance. Code is available at https://github.com/weizequan/LGNet.git.},
  keywords={Image reconstruction;Generators;Convolution;Semantics;Image edge detection;Deep learning;Task analysis;Image inpainting;neural networks;receptive field},
  doi={10.1109/TIP.2022.3152624},
  ISSN={1941-0042},
  month={},}@ARTICLE{9849507,
  author={Tao, Xian and Gong, Xinyi and Zhang, Xin and Yan, Shaohua and Adak, Chandranath},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Deep Learning for Unsupervised Anomaly Localization in Industrial Images: A Survey}, 
  year={2022},
  volume={71},
  number={},
  pages={1-21},
  abstract={Currently, deep learning-based visual inspection has been highly successful with the help of supervised learning methods. However, in real industrial scenarios, the scarcity of defect samples, the cost of annotation, and the lack of  $a$  priori knowledge of defects may render supervised-based methods ineffective. In recent years, unsupervised anomaly localization (AL) algorithms have become more widely used in industrial inspection tasks. This article aims to help researchers in this field by comprehensively surveying recent achievements in unsupervised AL in industrial images using deep learning. The survey reviews more than 120 significant publications covering different aspects of AL, mainly covering various concepts, challenges, taxonomies, benchmark datasets, and quantitative performance comparisons of the methods reviewed. In reviewing the achievements to date, this article provides detailed predictions and analysis of several future research directions. This review provides detailed technical information for researchers interested in industrial AL and who wish to apply it to the localization of anomalies in other fields.},
  keywords={Image reconstruction;Image segmentation;Taxonomy;Semantics;Location awareness;Feature extraction;Autonomous aerial vehicles;Anomaly localization (AL);deep learning;industrial inspection;literature survey;unsupervised learning},
  doi={10.1109/TIM.2022.3196436},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10377015,
  author={Zhu, Xiangyang and Zhang, Renrui and He, Bowei and Guo, Ziyu and Zeng, Ziyao and Qin, Zipeng and Zhang, Shanghang and Gao, Peng},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2639-2650},
  abstract={Large-scale pre-trained models have shown promising open-world performance for both vision and language tasks. However, their transferred capacity on 3D point clouds is still limited and only constrained to the classification task. In this paper, we first collaborate CLIP and GPT to be a unified 3D open-world learner, named as Point-CLIP V2, which fully unleashes their potential for zero-shot 3D classification, segmentation, and detection. To better align 3D data with the pre-trained language knowledge, Point-CLIP V2 contains two key designs. For the visual end, we prompt CLIP via a shape projection module to generate more realistic depth maps, narrowing the domain gap between projected point clouds with natural images. For the textual end, we prompt the GPT model to generate 3D-specific text as the input of CLIPâ€™s textual encoder. Without any training in 3D domains, our approach significantly surpasses PointCLIP by +42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D classification. On top of that, V2 can be extended to few-shot 3D classification, zero-shot 3D part segmentation, and 3D object detection in a simple manner, demonstrating our generalization ability for unified 3D open-world learning. Code is available at https://github.com/yangyangyang127/PointCLIP_V2.},
  keywords={Point cloud compression;Training;Visualization;Solid modeling;Adaptation models;Image segmentation;Three-dimensional displays},
  doi={10.1109/ICCV51070.2023.00249},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9775582,
  author={Li, Mingkun and Li, Chun-Guang and Guo, Jun},
  journal={IEEE Transactions on Image Processing}, 
  title={Cluster-Guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification}, 
  year={2022},
  volume={31},
  number={},
  pages={3606-3617},
  abstract={Unsupervised person re-identification (Re-ID) aims to match pedestrian images from different camera views in an unsupervised setting. Existing methods for unsupervised person Re-ID are usually built upon the pseudo labels from clustering. However, the result of clustering depends heavily on the quality of the learned features, which are overwhelmingly dominated by colors in images. In this paper, we attempt to suppress the negative dominating influence of colors to learn more effective features for unsupervised person Re-ID. Specifically, we propose a Cluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised person Re-ID, in which clustering result is leveraged to guide the feature learning in a properly designed asymmetric contrastive learning framework. In CACL, both instance-level and cluster-level contrastive learning are employed to help the siamese network learn discriminant features with respect to the clustering result within and between different data augmentation views, respectively. In addition, we also present a cluster refinement method, and validate that the cluster refinement step helps CACL significantly. Extensive experiments conducted on three benchmark datasets demonstrate the superior performance of our proposal.},
  keywords={Image color analysis;Training;Proposals;Representation learning;Neural networks;Gray-scale;Measurement;Unsupervised person re-identification;asymmetric contrastive learning;cluster refinement},
  doi={10.1109/TIP.2022.3173163},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{9577816,
  author={Ye, Yuntong and Chang, Yi and Zhou, Hanyu and Yan, Luxin},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation}, 
  year={2021},
  volume={},
  number={},
  pages={2053-2062},
  abstract={Existing deep learning-based image deraining methods have achieved promising performance for synthetic rainy images, typically rely on the pairs of sharp images and simulated rainy counterparts. However, these methods suffer from significant performance drop when facing the real rain, because of the huge gap between the simplified synthetic rain and the complex real rain. In this work, we argue that the rain generation and removal are the two sides of the same coin and should be tightly coupled. To close the loop, we propose to jointly learn real rain generation and removal procedure within a unified disentangled image translation framework. Specifically, we propose a bidirectional disentangled translation network, in which each unidirectional network contains two loops of joint rain generation and removal for both the real and synthetic rain image, respectively. Meanwhile, we enforce the disentanglement strategy by decomposing the rainy image into a clean background and rain layer (rain removal), in order to better preserve the identity background via both the cycle-consistency loss and adversarial loss, and ease the rain layer translating between the real and synthetic rainy image. A counterpart composition with the entanglement strategy is symmetrically applied for rain generation. Extensive experiments on synthetic and real-world rain datasets show the superiority of proposed method compared to state-of-the-arts.},
  keywords={Computer vision;Rain;Pattern recognition},
  doi={10.1109/CVPR46437.2021.00209},
  ISSN={2575-7075},
  month={June},}@ARTICLE{8894380,
  author={Li, Huafeng and Yan, Shuanglin and Yu, Zhengtao and Tao, Dapeng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Attribute-Identity Embedding and Self-Supervised Learning for Scalable Person Re-Identification}, 
  year={2020},
  volume={30},
  number={10},
  pages={3472-3485},
  abstract={Due to the domain shift between source dataset and target dataset, most of the existing person re-identification (PRID) algorithms trained by a supervised learning framework often fail to be well generalized to another domain. To address this challenge, we propose a self-supervised learning algorithm based on attribute-identity embedding, which can incrementally optimize the model by selecting unlabeled samples from target domain. Thus the gap between source domain and target domain is bridged. Specifically, we first develop an attribute-identity joint prediction dictionary learning model for simultaneously learning a latent attribute space, a semantic attribute dictionary and an identifier. In our method, the predicted attribute from latent attribute space is used as a bridge to establish a preliminary link between different domains so as to predict the label of the target data sample. Second, to exploit the latent label contained in the predicted samples, we propose a prediction-training cycle self-supervised learning to tune the model variables to make them more adaptive in the target domain. Finally, the similarity measurement of pedestrians is achieved by combining the attribute space with latent identity space. The experiments show that the developed method outperforms some state-of-the-art supervised PRID methods and unsupervised PRID algorithms.},
  keywords={Visualization;Semantics;Dictionaries;Training;Machine learning;Predictive models;Adaptation models;Person re-identification;self-supervised learning;domain shift problem;attribute space},
  doi={10.1109/TCSVT.2019.2952550},
  ISSN={1558-2205},
  month={Oct},}@INPROCEEDINGS{9878492,
  author={Jia, Shuai and Ma, Chao and Yao, Taiping and Yin, Bangjie and Ding, Shouhong and Yang, Xiaokang},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Exploring Frequency Adversarial Attacks for Face Forgery Detection}, 
  year={2022},
  volume={},
  number={},
  pages={4093-4102},
  abstract={Various facial manipulation techniques have drawn seri-ous public concerns in morality, security, and privacy. Al- though existing face forgery classifiers achieve promising performance on detecting fake images, these methods are vulnerable to adversarial examples with injected impercep- tible perturbations on the pixels. Meanwhile, many face forgery detectors always utilize the frequency diversity be-tween real and fake faces as a crucial clue. In this paper, in- stead of injecting adversarial perturbations into the spatial domain, we propose a frequency adversarial attack method against face forgery detectors. Concretely, we apply dis-crete cosine transform (DCT) on the input images and in-troduce a fusion module to capture the salient region of ad-versary in the frequency domain. Compared with existing adversarial attacks (e.g. FGSM, PGD) in the spatial do-main, our method is more imperceptible to human observers and does not degrade the visual quality of the original images. Moreover, inspired by the idea of meta-learning, we also propose a hybrid adversarial attack that performs at-tacks in both the spatial and frequency domains. Exten-sive experiments indicate that the proposed method fools not only the spatial-based detectors but also the state-of- the-art frequency-based detectors effectively. In addition, the proposed frequency attack enhances the transferability across face forgery detectors as black-box attacks.},
  keywords={Visualization;Face recognition;Frequency-domain analysis;Perturbation methods;Computational modeling;Detectors;Transforms;Face and gestures},
  doi={10.1109/CVPR52688.2022.00407},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9238405,
  author={Wan, Jun and Lai, Zhihui and Liu, Jun and Zhou, Jie and Gao, Can},
  journal={IEEE Transactions on Image Processing}, 
  title={Robust Face Alignment by Multi-Order High-Precision Hourglass Network}, 
  year={2021},
  volume={30},
  number={},
  pages={121-133},
  abstract={Heatmap regression (HR) has become one of the mainstream approaches for face alignment and has obtained promising results under constrained environments. However, when a face image suffers from large pose variations, heavy occlusions and complicated illuminations, the performances of HR methods degrade greatly due to the low resolutions of the generated landmark heatmaps and the exclusion of important high-order information that can be used to learn more discriminative features. To address the alignment problem for faces with extremely large poses and heavy occlusions, this paper proposes a heatmap subpixel regression (HSR) method and a multi-order cross geometry-aware (MCG) model, which are seamlessly integrated into a novel multi-order high-precision hourglass network (MHHN). The HSR method is proposed to achieve high-precision landmark detection by a well-designed subpixel detection loss (SDL) and subpixel detection technology (SDT). At the same time, the MCG model is able to use the proposed multi-order cross information to learn more discriminative representations for enhancing facial geometric constraints and context information. To the best of our knowledge, this is the first study to explore heatmap subpixel regression for robust and high-precision face alignment. The experimental results from challenging benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in the literature.},
  keywords={Heating systems;Faces;Face recognition;Shape;Task analysis;Predictive models;Robustness;Heatmap regression;face alignment;geometirc constraints;heavy occlusions;large poses},
  doi={10.1109/TIP.2020.3032029},
  ISSN={1941-0042},
  month={},}@ARTICLE{10091771,
  author={Chang, Zhihao and Feng, Zhixi and Yang, Shuyuan and Gao, Quanwei},
  journal={IEEE Transactions on Image Processing}, 
  title={AFT: Adaptive Fusion Transformer for Visible and Infrared Images}, 
  year={2023},
  volume={32},
  number={},
  pages={2077-2092},
  abstract={In this paper, an Adaptive Fusion Transformer (AFT) is proposed for unsupervised pixel-level fusion of visible and infrared images. Different from the existing convolutional networks, transformer is adopted to model the relationship of multi-modality images and explore cross-modal interactions in AFT. The encoder of AFT uses a Multi-Head Self-attention (MSA) module and Feed Forward (FF) network for feature extraction. Then, a Multi-head Self-Fusion (MSF) module is designed for the adaptive perceptual fusion of the features. By sequentially stacking the MSF, MSA, and FF, a fusion decoder is constructed to gradually locate complementary features for recovering informative images. In addition, a structure-preserving loss is defined to enhance the visual quality of fused images. Extensive experiments are conducted on several datasets to compare our proposed AFT method with 21 popular approaches. The results show that AFT has state-of-the-art performance in both quantitative metrics and visual perception.},
  keywords={Transformers;Feature extraction;Decoding;Convolution;Visualization;Image fusion;Fuses;Multi-modality images;transformer;adaptive fusion;multi-head self-attention;multi-head self-fusion},
  doi={10.1109/TIP.2023.3263113},
  ISSN={1941-0042},
  month={},}@ARTICLE{9265290,
  author={Zhang, Weixia and Ma, Chao and Wu, Qi and Yang, Xiaokang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Language-Guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning}, 
  year={2021},
  volume={31},
  number={9},
  pages={3469-3481},
  abstract={The emerging vision-and-language navigation (VLN) problem aims at learning to navigate an agent to the target location in unseen photo-realistic environments according to the given language instruction. The main challenges of VLN arise mainly from two aspects: first, the agent needs to attend to the meaningful paragraphs of the language instruction corresponding to the dynamically-varying visual environments; second, during the training process, the agent usually imitate the expert demonstrations, i.e., the shortest-path to the target location specified by associated language instructions. Due to the discrepancy of action selection between training and inference, the agent solely on the basis of imitation learning does not perform well. Existing VLN approaches address this issue by sampling the next action from its predicted probability distribution during the training process. This allows the agent to explore diverse routes from the environments, yielding higher success rates. Nevertheless, without being presented with the golden shortest navigation paths during the training process, the agent may arrive at the target location through an unexpected longer route. To overcome these challenges, we design a cross-modal grounding module, which is composed of two complementary attention mechanisms, to equip the agent with a better ability to track the correspondence between the textual and visual modalities. We then propose to recursively alternate the learning schemes of imitation and exploration to narrow the discrepancy between training and inference. We further exploit the advantages of both these two learning schemes via adversarial learning. Extensive experimental results on the Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning scheme is generalized and complementary to prior arts. Our method performs well against state-of-the-art approaches in terms of effectiveness and efficiency.},
  keywords={Navigation;Training;Trajectory;Visualization;Task analysis;Grounding;Generators;Vision-and-language;embodied navigation;attention mechanism;adversarial learning},
  doi={10.1109/TCSVT.2020.3039522},
  ISSN={1558-2205},
  month={Sep.},}@ARTICLE{9043584,
  author={Jin, Zhi and Iqbal, Muhammad Zafar and Zou, Wenbin and Li, Xia and Steinbach, Eckehard},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dual-Stream Multi-Path Recursive Residual Network for JPEG Image Compression Artifacts Reduction}, 
  year={2021},
  volume={31},
  number={2},
  pages={467-479},
  abstract={JPEG is the most widely used lossy image compression standard. When using JPEG with high compression ratios, visual artifacts cannot be avoided. These artifacts not only degrade the user experience but also negatively affect many low-level image processing tasks. Recently, convolutional neural network (CNN)-based compression artifact removal approaches have achieved significant success, however, at the cost of high computational complexity due to an enormous number of parameters. To address this issue, we propose a dual-stream recursive residual network (STRRN) which consists of structure and texture streams for separately reducing the specific artifacts related to high-frequency or low-frequency image components. The outputs of these streams are combined and fed into an aggregation network to further enhance the restored images. By using parameter sharing, the proposed network reduces the total number of training parameters significantly. Moreover, experiments conducted on five commonly used datasets confirm that the proposed STRRN can efficiently reduce the compression artifacts, while using up to 4.6 times less training parameters and 5 times less running time compared to the state-of-the-art approaches.},
  keywords={Image coding;Training;Transform coding;Automobiles;Task analysis;Hafnium;Dual-stream;structure-texture decomposition;compression artifacts reduction;recursive residual network},
  doi={10.1109/TCSVT.2020.2982174},
  ISSN={1558-2205},
  month={Feb},}@ARTICLE{9760113,
  author={Xia, Kun and Deng, Lingfei and Duch, Wlodzislaw and Wu, Dongrui},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Privacy-Preserving Domain Adaptation for Motor Imagery-Based Brain-Computer Interfaces}, 
  year={2022},
  volume={69},
  number={11},
  pages={3365-3376},
  abstract={Objective: Electroencephalogram (EEG) is one of the most widely used signals in motor imagery (MI) based brain-computer interfaces (BCIs). Domain adaptation has been frequently used to improve the accuracy of EEG-based BCIs for a new user (target domain), by making use of labeled data from a previous user (source domain). However, this raises privacy concerns, as EEG contains sensitive health and mental information. It is very important to perform privacy-preserving domain adaptation, which simultaneously improves the classification accuracy for a new user and protects the privacy of a previous user. Methods: We propose augmentation-based source-free adaptation (ASFA), which consists of two parts: 1) source model training, where a novel data augmentation approach is proposed for MI EEG signals to improve the cross-subject generalization performance of the source model; and, 2) target model training, which simultaneously considers uncertainty reduction for domain adaptation and consistency regularization for robustness. ASFA only needs access to the source model parameters, instead of the raw EEG data, thus protecting the privacy of the source domain. We further extend ASFA to a stricter privacy-preserving scenario, where the source modelâ€™s parameters are also inaccessible. Results: Experimental results on four MI datasets demonstrated that ASFA outperformed 15 classical and state-of-the-art MI classification approaches. Significance: This is the first work on completely source-free domain adaptation for EEG-based BCIs. Our proposed ASFA achieves high classification accuracy and strong privacy protection simultaneously, important for the commercial applications of EEG-based BCIs.},
  keywords={Brain modeling;Electroencephalography;Feature extraction;Data models;Privacy;Covariance matrices;Adaptation models;Brain-computer interface;domain adaptation;motor imagery;privacy-preserving},
  doi={10.1109/TBME.2022.3168570},
  ISSN={1558-2531},
  month={Nov},}@INPROCEEDINGS{9706778,
  author={Ju, Yeong-Joon and Lee, Gun-Hee and Hong, Jung-Ho and Lee, Seong-Whan},
  booktitle={2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Complete Face Recovery GAN: Unsupervised Joint Face Rotation and De-Occlusion from a Single-View Image}, 
  year={2022},
  volume={},
  number={},
  pages={1173-1183},
  abstract={Although various face-related tasks have significantly advanced in recent years, occlusion and extreme pose still impede the achievement of higher performance. Existing face rotation or de-occlusion methods only have emphasized the aspect of each problem. In addition, the lack of high-quality paired data remains an obstacle for both methods. In this work, we present a self-supervision strategy called Swap-R&R to overcome the lack of ground-truth in a fully unsupervised manner for joint face rotation and de-occlusion. To generate an input pair for self-supervision, we transfer the occlusion from a face in an image to an estimated 3D face and create a damaged face image, as if rotated from a different pose by rotating twice with the roughly de-occluded face. Furthermore, we propose Complete Face Recovery GAN (CFR-GAN) to restore the collapsed textures and disappeared occlusion areas by leveraging the structural and textural differences between two rendered images. Unlike previous works, which have selected occlusion-free images to obtain ground-truths, our approach does not require human intervention and paired data. We show that our proposed method can generate a de-occluded frontal face image from an occluded profile face image. Moreover, extensive experiments demonstrate that our approach can boost the performance of facial recognition and facial expression recognition. The code is publicly available 1},
  keywords={Training;Solid modeling;Computer vision;Three-dimensional displays;Codes;Face recognition;Computational modeling;Biometrics -> Face Processing Transfer; Few-shot; Semi- and Un- supervised Learning},
  doi={10.1109/WACV51458.2022.00124},
  ISSN={2642-9381},
  month={Jan},}@INPROCEEDINGS{9578089,
  author={Zheng, Yajing and Zheng, Lingxiao and Yu, Zhaofei and Shi, Boxin and Tian, Yonghong and Huang, Tiejun},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={High-speed Image Reconstruction through Short-term Plasticity for Spiking Cameras}, 
  year={2021},
  volume={},
  number={},
  pages={6354-6363},
  abstract={Fovea, located in the centre of the retina, is specialized for high-acuity vision. Mimicking the sampling mechanism of the fovea, a retina-inspired camera, named spiking camera, is developed to record the external information with a sampling rate of 40,000 Hz, and outputs asynchronous binary spike streams. Although the temporal resolution of visual information is improved, how to reconstruct the scenes is still a challenging problem. In this paper, we present a novel high-speed image reconstruction model through the short-term plasticity (STP) mechanism of the brain. We derive the relationship between postsynaptic potential regulated by STP and the firing frequency of each pixel. By setting up the STP model at each pixel of the spiking camera, we can infer the scene radiance with the temporal regularity of the spike stream. Moreover, we show that STP can be used to distinguish the static and motion areas and further enhance the reconstruction results. The experimental results show that our methods achieve state-of-the-art performance in both image quality and computing time.},
  keywords={Visualization;Streaming media;Reconstruction algorithms;Cameras;Brain modeling;Retina;Spatiotemporal phenomena},
  doi={10.1109/CVPR46437.2021.00629},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9880279,
  author={Huang, Peiliang and Han, Junwei and Cheng, De and Zhang, Dingwen},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Robust Region Feature Synthesizer for Zero-Shot Object Detection}, 
  year={2022},
  volume={},
  number={},
  pages={7612-7621},
  abstract={Zero-shot object detection aims at incorporating class semantic vectors to realize the detection of (both seen and) unseen classes given an unconstrained test image. In this study, we reveal the core challenges in this research area: how to synthesize robust region features (for unseen objects) that are as intra-class diverse and inter-class separable as the real samples, so that strong unseen object detectors can be trained upon them. To address these challenges, we build a novel zero-shot object detection framework that contains an Intra-class Semantic Diverging component and an Inter-class Structure Preserving component. The former is used to realize the one-to-more mapping to obtain diverse visual features from each class semantic vector, preventing miss-classifying the real unseen objects as image backgrounds. While the latter is used to avoid the synthesized features too scattered to mix up the inter-class and foreground-background relationship. To demonstrate the effectiveness of the proposed approach, comprehensive experiments on PASCAL VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the new state-of-the-art performance on PASCAL VOC and COCO and it is the first study to carry out zero-shot object detection in remote sensing imagery.},
  keywords={Visualization;Computer vision;Synthesizers;Semantics;Object detection;Detectors;Feature extraction;Recognition: detection;categorization;retrieval; Transfer/low-shot/long-tail learning},
  doi={10.1109/CVPR52688.2022.00747},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9272370,
  author={Zhong, Yue and Qi, Yonggang and Gryaditskaya, Yulia and Zhang, Honggang and Song, Yi-Zhe},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Towards Practical Sketch-Based 3D Shape Generation: The Role of Professional Sketches}, 
  year={2021},
  volume={31},
  number={9},
  pages={3518-3528},
  abstract={In this paper, for the first time, we investigate the problem of generating 3D shapes from professional 2D sketches via deep learning. We target sketches done by professional artists, as these sketches are likely to contain more details than the ones produced by novices, and thus the reconstruction from such sketches poses a higher demand on the level of detail in the reconstructed models. This is importantly different to previous work, where the training and testing was conducted on either synthetic sketches or sketches done by novices. Novices sketches often depict shapes that are physically unrealistic, while models trained with synthetic sketches could not cope with the level of abstraction and style found in real sketches. To address this problem, we collected the first large-scale dataset of professional sketches, where each sketch is paired with a reference 3D shape, with a total of 1,500 professional sketches collected across 500 3D shapes. The dataset is available at http://sketchx.ai/downloads/. We introduce two bespoke designs within a deep adversarial network to tackle the imprecision of human sketches and the unique figure/ground ambiguity problem inherent to sketch-based reconstruction. We show that existing 3D shapes generation methods designed for images fail to be naively applied to our problem, and demonstrate the effectiveness of our method both qualitatively and quantitatively.},
  keywords={Shape;Three-dimensional displays;Solid modeling;Image reconstruction;Two dimensional displays;Training;Task analysis;Professional sketch dataset;deep sketch modeling},
  doi={10.1109/TCSVT.2020.3040900},
  ISSN={1558-2205},
  month={Sep.},}@INPROCEEDINGS{9025656,
  author={Tang, Haotian and Zhao, Yiru and Lu, Hongtao},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Unsupervised Person Re-Identification With Iterative Self-Supervised Domain Adaptation}, 
  year={2019},
  volume={},
  number={},
  pages={1536-1543},
  abstract={In real applications, person re-identification (re-id) is an inherently domain adaptive computer vision task which often requires the model trained on a group of people to perform well on an unlabeled dataset consisting of another group of pedestrians without supervised fine-tuning. Furthermore, there are typically a large number of classes (people) with small number of samples belonging to each class. Based on the characteristics of person re-id and general assumptions related to domain adaptation, we put forward a novel algorithm for cross-dataset person re-id. Our idea is simple yet effective: first, we preprocess the source dataset with style transfer GAN and train a baseline on it in a supervised learning manner, then we assign pseudo labels to unlabeled samples in target dataset based on the model trained on labeled source dataset; finally, we train on the target dataset with pseudo labels in traditional supervised learning manner. We adopt the idea of co-training in the training process to make the pseudo labels more reliable. We show the superiority of our model over all state-of-the-art methods through extensive experiments.},
  keywords={Labeling;Gallium nitride;Adaptation models;Task analysis;Supervised learning;Training;Iterative algorithms},
  doi={10.1109/CVPRW.2019.00195},
  ISSN={2160-7516},
  month={June},}@ARTICLE{10196057,
  author={Yang, Chaoying and Liu, Jie and Xu, Qi and Zhou, Kaibo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Generalized Graph Contrastive Learning Framework for Few-Shot Machine Fault Diagnosis}, 
  year={2024},
  volume={20},
  number={2},
  pages={2692-2701},
  abstract={Graph data-driven machine fault diagnosis methods make success using sufficient data recently. However, in the actual industry, there are rare failure data in historical data, leading to insufficient graph representation ability and reducing diagnosis performance. In this article, a generalized graph contrastive learning (GCL) framework for few-shot machine fault diagnosis is proposed. First, spectrum features of vibration data-based samples are used to calculate Euclidean distance matrix for constructing K-nearest neighborhood graph (KNNG), where K adjacent neighbors of each sample are connected. Avoiding excess calculation cost for graph construction, positive and negative KNNGs are constructed by changing parameter K. To make full use of few-shot samples, an unsupervised GCL subtask is set for pretraining graph deep learning model. Further, the unsupervised pretrained model is semisupervised trained using original KNNGs for outputting unlabeled nodesâ€™ labels. The proposed method achieves 99.83%, 99.56% in bearing and gearbox dataset, respectively, and the proposed GCL framework works for different graph neural networks.},
  keywords={Data models;Fault diagnosis;Feature extraction;Matrix converters;Training;Task analysis;Representation learning;Few-shot learning (FSL);graph contrastive learning (GCL);K-nearest neighborhood graph;machine fault diagnosis;model pretraining},
  doi={10.1109/TII.2023.3297664},
  ISSN={1941-0050},
  month={Feb},}@ARTICLE{10015839,
  author={Chen, Junliang and Lu, Weizeng and Li, Yuexiang and Shen, Linlin and Duan, Jinming},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Adversarial Learning of Object-Aware Activation Map for Weakly-Supervised Semantic Segmentation}, 
  year={2023},
  volume={33},
  number={8},
  pages={3935-3946},
  abstract={Recent years have witnessed impressive advances in the area of weakly-supervised semantic segmentation (WSSS). However, most of existing approaches are based on class activation maps (CAMs), which suffer from the under-segmentation problem (i.e., objects of interest are segmented partially). Although a number of literature works have been proposed to tackle this under-segmentation problem, we argue that these solutions built on CAMs may not be optimal for the WSSS task. Instead, in this paper we propose a network based on the object-aware activation map (OAM). The proposed network, termed OAM-Net, consists of four loss functions (foreground loss, background loss, average pixel and consistency loss) which ensure exactness, completeness, compactness and consistency of segmented objects via adversarial training. Compared to conventional CAM-based methods, our OAM-Net overcomes the under-segmentation drawback and significantly improves segmentation accuracy with negligible computational cost. A thorough comparison between OAM-Net and CAM-based approaches is carried out on the PASCAL VOC2012 dataset, and experimental results show that our network outperforms state-of-the-art approaches by a large margin. The code will be available soon.},
  keywords={Cams;Semantic segmentation;Training;Task analysis;Streaming media;Semantics;Object segmentation;Weakly-supervised semantic segmentation;class activation map;object-aware activation map},
  doi={10.1109/TCSVT.2023.3236432},
  ISSN={1558-2205},
  month={Aug},}@INPROCEEDINGS{9711300,
  author={Zheng, Fang and Wang, Le and Zhou, Sanping and Tang, Wei and Niu, Zhenxing and Zheng, Nanning and Hua, Gang},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={13148-13157},
  abstract={Understanding complex social interactions among agents is a key challenge for trajectory prediction. Most existing methods consider the interactions between pairwise traffic agents or in a local area, while the nature of interactions is unlimited, involving an uncertain number of agents and non-local areas simultaneously. Besides, they treat heterogeneous traffic agents the same, namely those among agents of different categories, while neglecting peopleâ€™s diverse reaction patterns toward traffic agents in different categories. To address these problems, we propose a simple yet effective Unlimited Neighborhood Interaction Network (UNIN), which predicts trajectories of heterogeneous agents in multiple categories. Specifically, the proposed unlimited neighborhood interaction module generates the fused-features of all agents involved in an interaction simultaneously, which is adaptive to any number of agents and any range of interaction area. Meanwhile, a hierarchical graph attention module is proposed to obtain category-to-category interaction and agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model are estimated for generating the future trajectories. Extensive experimental results on benchmark datasets demonstrate a significant performance improvement of our method over the state-of-the-art methods.},
  keywords={Computer vision;Benchmark testing;Trajectory;Gaussian mixture model;Action and behavior recognition;Motion and tracking;Scene analysis and understanding;Video analysis and understanding;Vision for robotics and autonomous vehicles},
  doi={10.1109/ICCV48922.2021.01292},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9764648,
  author={Li, Hongchao and Li, Chenglong and Zheng, Aihua and Tang, Jin and Luo, Bin},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={MsKAT: Multi-Scale Knowledge-Aware Transformer for Vehicle Re-Identification}, 
  year={2022},
  volume={23},
  number={10},
  pages={19557-19568},
  abstract={Existing vehicle re-identification (Re-ID) methods usually suffer from intra-instance discrepancy and inter-instance similarity. The key to solving this problem lies in filtering out identity-irrelevant interference and collecting identity-relevant vehicle details. In this paper, we aim to design a robust vehicle Re-ID framework that trains a model guided by knowledge vectors yet is able to disentangle the identity-relevant features and identity-irrelevant features. Toward this end, we propose a novel Multi-scale Knowledge-Aware Transformer (MsKAT) to build a knowledge-guided multi-scale feature alignment framework. First, we construct a Knowledge-Aware Transformer (KAT) to interact with semantic knowledge and visual feature. KAT mainly includes State elimination Transformer (SeT) to eliminate state (camera, viewpoint) interference and Attribute aggregation Transformer (AaT) to gather attribute (color, type) information. Second, to learn the knowledge-guided sample differences, we propose to encourage the separation of identity-relevant features and identity-irrelevant features by a Knowledge-Guided Alignment loss ( $\mathcal {L}_{KGA}$ ). Specifically,  $\mathcal {L}_{KGA}$  suppresses the difference between knowledge-guided positive pairs and the similarity between knowledge-guided negative pairs. Third, with the multi-scale settings of KAT and  $\mathcal {L}_{KGA}$ , our model can capture knowledge-guided visual consistency features at different scales. Extensive evidence demonstrates our approach achieves new state-of-the-art on three widely-used vehicle re-identification benchmarks.},
  keywords={Transformers;Visualization;Feature extraction;Image color analysis;Semantics;Knowledge based systems;Interference;Vehicle re-identification;knowledge-aware;transformer;multi-scale},
  doi={10.1109/TITS.2022.3166463},
  ISSN={1558-0016},
  month={Oct},}@INPROCEEDINGS{9710114,
  author={Bhunia, Ayan Kumar and Sain, Aneeshan and Chowdhury, Pinaki Nath and Song, Yi-Zhe},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation}, 
  year={2021},
  volume={},
  number={},
  pages={963-972},
  abstract={Text recognition remains a fundamental and extensively researched topic in computer vision, largely owing to its wide array of commercial applications. The challenging nature of the very problem however dictated a fragmentation of research efforts: Scene Text Recognition (STR) that deals with text in everyday scenes, and Handwriting Text Recognition (HTR) that tackles hand-written text. In this paper, for the first time, we argue for their unification â€“ we aim for a single model that can compete favourably with two separate state-of-the-art STR and HTR models. We first show that cross-utilisation of STR and HTR models trigger significant performance drops due to differences in their inherent challenges. We then tackle their union by introducing a knowledge distillation (KD) based framework. This however is non-trivial, largely due to the variable-length and sequential nature of text sequences, which renders off-the-shelf KD techniques that mostly work with global fixed length data, inadequate. For that, we propose four distillation losses, all of which are specifically designed to cope with the aforementioned unique characteristics of text recognition. Empirical evidence suggests that our proposed unified model performs at par with individual models, even surpassing them in certain cases. Ablative studies demonstrate that naive baselines such as a two-stage framework, multi-task and domain adaption/generalisation alternatives do not work that well, further authenticating our design.},
  keywords={Adaptation models;Computer vision;Text recognition;Computational modeling;Multitasking;Scene text and document understanding},
  doi={10.1109/ICCV48922.2021.00102},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9931456,
  author={Huyan, Ning and Zhang, Xiangrong and Quan, Dou and Chanussot, Jocelyn and Jiao, Licheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={AUD-Net: A Unified Deep Detector for Multiple Hyperspectral Image Anomaly Detection via Relation and Few-Shot Learning}, 
  year={2024},
  volume={35},
  number={5},
  pages={6835-6849},
  abstract={This article addresses the problem of the building an out-of-the-box deep detector, motivated by the need to perform anomaly detection across multiple hyperspectral images (HSIs) without repeated training. To solve this challenging task, we propose a unified detector [anomaly detection network (AUD-Net)] inspired by few-shot learning. The crucial issues solved by AUD-Net include: how to improve the generalization of the model on various HSIs that contain different categories of land cover; and how to unify the different spectral sizes between HSIs. To achieve this, we first build a series of subtasks to classify the relations between the center and its surroundings in the dual window. Through relation learning, AUD-Net can be more easily generalized to unseen HSIs, as the relations of the pixel pairs are shared among different HSIs. Secondly, to handle different HSIs with various spectral sizes, we propose a pooling layer based on the vector of local aggregated descriptors, which maps the variable-sized features to the same space and acquires the fixed-sized relation embeddings. To determine whether the center of the dual window is an anomaly, we build a memory model by the transformer, which integrates the contextual relation embeddings in the dual window and estimates the relation embeddings of the center. By computing the feature difference between the estimated relation embeddings of the centers and the corresponding real ones, the centers with large differences will be detected as anomalies, as they are more difficult to be estimated by the corresponding surroundings. Extensive experiments on both the simulation dataset and 13 real HSIs demonstrate that this proposed AUD-Net has strong generalization for various HSIs and achieves significant advantages over the specific-trained detectors for each HSI.},
  keywords={Detectors;Training;Image reconstruction;Windows;Feature extraction;Anomaly detection;Transformers;Anomaly detection;few-shot learning;hyperspectral images (HSIs);relation learning;transformers;unified detection},
  doi={10.1109/TNNLS.2022.3213023},
  ISSN={2162-2388},
  month={May},}@INPROCEEDINGS{8793820,
  author={Sweeney, Chris and Izatt, Greg and Tedrake, Russ},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={A Supervised Approach to Predicting Noise in Depth Images}, 
  year={2019},
  volume={},
  number={},
  pages={796-802},
  abstract={Modern robotic systems are very complex and need to be tested in simulations with detailed sensor noise models to effectively verify robotic behavior. Depth imagery in particular comes with significant noise in the form of scene-dependent pixel-wise dropouts and distortions. Unfortunately, many depth camera simulations contain limited noise models, or can only support generating realistic depth images of simple scenes, which limits their usefulness in effectively testing perception algorithms. We propose a data driven approach to generate more realistic noise for complex simulated environments by using a convolutional neural network (CNN) to predict which pixels of a simulated noise-free depth image will not have returns (no-depth-return pixels, or NDP). We choose to focus on NDP here, as these dropouts are the most common and dramatic form of depth image noise. To train this network, we use reconstructed real-world scenes from the Label Fusion dataset to provide ground truth depth for each noisy depth image used to scan the scene. We use the resulting noise-free and noisy depth image pairs as labeled examples and train the network to predict which pixels of the noise-free image will be NDP. When used to post-process a simulation of a depth sensor, this system produces realistic depth images, even in cluttered scenes. To demonstrate that our approach successfully closes the reality gap for depth imagery, we show that the popular ICP algorithm for object pose estimation fails more realistically on our CNN-corrupted simulated depth images than on uncorrupted depth images and unsupervised domain adaptation baselines.},
  keywords={Cameras;Image reconstruction;Data models;Noise measurement;Robot vision systems},
  doi={10.1109/ICRA.2019.8793820},
  ISSN={2577-087X},
  month={May},}@ARTICLE{9999670,
  author={Shao, Yuanjie and Wu, Wenxiao and You, Xinge and Gao, Changxin and Sang, Nong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Improving the Generalization of MAML in Few-Shot Classification via Bi-Level Constraint}, 
  year={2023},
  volume={33},
  number={7},
  pages={3284-3295},
  abstract={Few-shot classification (FSC), which aims to identify novel classes in the presence of a few labeled samples, has drawn vast attention in recent years. One of the representative few-shot classification methods is model-agnostic meta-learning (MAML), which focuses on learning an initialization that can quickly adapt to novel categories with a few annotated samples. However, due to insufficient samples, MAML can easily fall into the dilemma of overfitting. Most existing MAML-based methods either improve the inner-loop update rule to achieve better generalization or constrain the outer-loop optimization to learn a more desirable initialization, without considering improving the two optimization processes jointly, resulting in unsatisfactory performance. In this paper, we propose a bi-level constrained MAML (BLC-MAML) method for few-shot classification. Specifically, in the inner-loop optimization, we introduce a supervised contrastive loss to constrain the adaptation procedure, which can effectively increase the intra-class aggregation and inter-class separability, thus improving the generalization of the adapted model. In the case of the outer loop, we propose a cross-task metric (CTM) loss to constrain the adapted model to perform well on the different few-shot task. The CTM loss can enforce the adapted model to learn more discriminative and generalized feature representations, further boosting the generalization of the learned initialization. By simultaneously constraining the bi-level optimization procedure, the proposed BLC-MAML can learn an initialization with better generalization. Extensive experiments on several FSC benchmarks show that our method can effectively improve the performance of MAML under both the within-domain and cross-domain settings, and also perform favorably against the state-of-the-art FSC algorithms.},
  keywords={Adaptation models;Task analysis;Optimization;Measurement;Power capacitors;Feature extraction;Data models;MAML;bi-level constraint;supervised contrastive loss;cross-task metric loss},
  doi={10.1109/TCSVT.2022.3232717},
  ISSN={1558-2205},
  month={July},}@ARTICLE{10247080,
  author={Han, Pengfei and Zhao, Bin and Li, Xuelong},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Edge-Guided Remote-Sensing Image Compression}, 
  year={2023},
  volume={61},
  number={},
  pages={1-15},
  abstract={Using high-fidelity image compression makes it possible to transmit remote-sensing images in real-time. Nevertheless, existing lossy remote-sensing image compression (RSIC) methods have some inherent potential issues, including blocking and blurring effects, which are particularly problematic in low-compression-ratio (CR) settings. Although numerous methods have been studied to address the aforementioned issue, the majority of them exploit the prior of local smoothness in images, which usually induces the over-smoothing of regions with noticeable structure (i.e., edges and textures). During this task, we developed an innovative end-to-end framework that enables high-fidelity RSIC while retaining sharp edge and texture information. Initially, we put forth an edge-guided adversarial network (EGA-Net) for simultaneously restoring edge structures and generating texture details. Second, we impose an edge fidelity constraint to direct our network to optimize image content and structural information jointly. In addition, to facilitate this task, we have constructed a large-scale RSIC dataset named NWPU-RS-Compression (NWPU-RSC). This dataset contains over 300000 images of 30 categories, all with a fixed resolution of 600 Ã— 600. Finally, a new quantitative metric for full reference image quality that takes into account signal statistics and the characteristics of the human visual system (HVS) has been developed, which helps evaluate reconstructed remote-sensing images more objectively and accurately. Experimental evidence has demonstrated that the EGA-Net surpasses several representative compression approaches regarding quality metrics on the NWPU-RSC, AID, and ISPR Vaihingen datasets. Code, dataset, and more experimental results can be accessed at https: //github.com/Chenxi1510/Remote-sensing-Image-Compression.},
  keywords={Image coding;Image edge detection;Remote sensing;Image reconstruction;Quantization (signal);Task analysis;Semantics;Edge fidelity constraint;edge-guided adversarial net;image quality evaluation metric;remote-sensing image compression (RSIC)},
  doi={10.1109/TGRS.2023.3314012},
  ISSN={1558-0644},
  month={},}@ARTICLE{9786762,
  author={Wang, Pingyu and Zhao, Zhicheng and Su, Fei and Meng, Hongying},
  journal={IEEE Transactions on Multimedia}, 
  title={LTReID: Factorizable Feature Generation With Independent Components for Long-Tailed Person Re-Identification}, 
  year={2023},
  volume={25},
  number={},
  pages={4610-4622},
  abstract={With the rapid increase of large-scale and real-world person datasets, it is crucial to address the problem of long-tailed data distributions, i.e., head classes have large number of images while tail classes occupy extremely few samples. We observe that the imbalanced data distribution is likely to distort the overall feature space and impair the generalization capability of trained models. Nevertheless, this long-tailed problem has been rarely investigated in previous person Re-Identification (ReID) works. In this paper, we propose a novel Long-Tailed Re-Identification (LTReID) framework to simultaneously alleviate class-imbalance and hard-imbalance problems. Specifically, each real feature is decomposed into multiple independent components with two decorrelation losses. Then these components are randomly aggregated to generate more fake features for tail classes than head ones, resulting in the class-balance between head and tail classes. For the hard-balance between easy and hard samples, we utilize adversarial learning to generate more hard features than easy ones. The proposed framework can be trained in an end-to-end manner and avoids increasing the space and time complexity of inference models. Moreover, comprehensive experiments are conducted on the four ReID datasets so as to validate the effectiveness of the overall framework and the advantage of each module. Our results show that when trained with either balanced or imbalanced datasets, the LTReID achieves superior performance over the state-of-the-art methods.},
  keywords={Tail;Head;Feature extraction;Training;Indexes;Task analysis;Representation learning;Person re-identification;long-tailed distribution;feature factorization;feature generation},
  doi={10.1109/TMM.2022.3179902},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{9710078,
  author={Ma, Xinhong and Gao, Junyu and Xu, Changsheng},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Active Universal Domain Adaptation}, 
  year={2021},
  volume={},
  number={},
  pages={8948-8957},
  abstract={Most unsupervised domain adaptation methods rely on rich prior knowledge about the source-target label set relationship, and they cannot recognize categories beyond the source classes, which limits their applicability in practical scenarios. This paper proposes a new paradigm for unsupervised domain adaptation, termed as Active Universal Domain Adaptation (AUDA), which removes all label set assumptions and aims for not only recognizing target samples from source classes but also inferring those from target-private classes by using active learning to annotate a small budget of target data. For AUDA, it is challenging to jointly adapt the model to the target domain and select informative target samples for annotations under a large domain gap and significant semantic shift. To address the problems, we propose an Active Universal Adaptation Network (AUAN). Specifically, we first introduce Adversarial and Diverse Curriculum Learning (ADCL), which progressively aligns source and target domains to classify whether target samples are from source classes. Then, we propose a Clustering Non-transferable Gradient Embedding (CNTGE) strategy, which utilizes the clues of transferability, diversity, and uncertainty to annotate target informative sample, making it possible to infer labels for target samples of target-private classes. Finally, we propose to jointly train ADCL and CNTGE with target supervision to promote domain adaptation and target-private class recognition. Extensive experiments demonstrate that the proposed AUDA model equipped with ADCL and CNTGE achieves significant results on four popular benchmarks.},
  keywords={Adaptation models;Computer vision;Uncertainty;Target recognition;Annotations;Computational modeling;Semantics;Transfer/Low-shot/Semi/Unsupervised Learning;Representation learning},
  doi={10.1109/ICCV48922.2021.00884},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9810515,
  author={Gao, Zan and Sun, Chao and Cheng, Zhiyong and Guan, Weili and Liu, Anan and Wang, Meng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={TBNet: A Two-Stream Boundary-Aware Network for Generic Image Manipulation Localization}, 
  year={2023},
  volume={35},
  number={7},
  pages={7541-7556},
  abstract={Finding tampered regions in images is a common research topic in machine learning and computer vision. Although many image manipulation location algorithms have been proposed, most of them only focus on RGB images with different color spaces, and the frequency information that contains the potential tampering clues is often ignored. Moreover, among the manipulation operations, splicing and copy-move are two frequently used methods, but as their characteristics are quite different, specific methods have been individually designed for detecting the operations of either splicing or copy-move, and it is very difficult to widely apply these methods in practice. To solve these issues, in this work, a novel end-to-end two-stream boundary-aware network (abbreviated as TBNet) is proposed for generic image manipulation localization where the RGB stream, the frequency stream, and the boundary artifact location are explored in a unified framework. Specifically, we first design an adaptive frequency selection module (AFS) to adaptively select the appropriate frequency to mine inconsistent statistics and eliminate the interference of redundant statistics. Then, an adaptive cross-attention fusion module (ACF) is proposed to adaptively fuse the RGB feature and the frequency feature. Finally, the boundary artifact location network (BAL) is designed to locate the boundary artifacts for which the parameters are jointly updated by the outputs of the ACF, and its results are further fed into the decoder. Thus, the parameters of the RGB stream, the frequency stream, and the boundary artifact location network are jointly optimized, and their latent complementary relationships are fully mined. The results of the extensive experiments performed on six public benchmarks of the image manipulation localization task, namely, CASIA1.0, COVER, Carvalho, In-The-Wild, NIST-16, and IMD-2020, demonstrate that the proposed TBNet can substantially outperform state-of-the-art generic image manipulation localization methods in terms of MCC, F1, and AUC while maintaining robustness with respect to various attacks. Compared with DeepLabV3+ on the CASIA1.0, COVER, Carvalho, In-The-Wild, and NIST-16 datasets, the improvements in MCC/F1 reach 11%/11.1%, 8.2%/10.3%, 10.2%/11.6%, 8.9%/6.2%, and 13.3%/16.0%, respectively. Moreover, when IMD2020 is utilized, its AUC improvement can achieve 14.7%.},
  keywords={Splicing;Location awareness;Streaming media;Frequency-domain analysis;Task analysis;Feature extraction;Image color analysis;Adaptive cross-attention fusion;adaptive frequency selection;boundary artifact localization;generic image manipulation localization;two-stream boundary-aware},
  doi={10.1109/TKDE.2022.3187091},
  ISSN={1558-2191},
  month={July},}@ARTICLE{9432749,
  author={Liu, Fang and Wang, Jingya and Tang, Xu and Liu, Jia and Zhang, Xiangrong and Xiao, Liang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Adaptive Graph Convolutional Network for PolSAR Image Classification}, 
  year={2022},
  volume={60},
  number={},
  pages={1-14},
  abstract={Polarimetric synthetic aperture radar (PolSAR) image classification is one of the hottest issues in remote sensing, where studies on pixel-level information and relationship are of great significance. In this article, graph convolutional network (GCN) is employed to accomplish this pixel-level task benefiting from its excellent capability in structure exploration and information propagation between different pixels. To reduce the communication burden between various PolSAR pixels and high computational cost for the whole PolSAR image, an adaptive GCN (AdapGCN) consisting of pixel-centered subgraphs is proposed in this article. In the AdapGCN, a data-adaptive kernel and a spatial-adaptive kernel are introduced to, respectively, model data structure and spatial structure for PolSAR image. Moreover, a multiscale learning structure is integrated to further explore complicated relations between pixels. Extensive comparative evaluations validate the superiority of our new AdapGCN model for PolSAR image classification over a wide range of state-of-the-art methods on three challenging benchmarks.},
  keywords={Task analysis;Kernel;Scattering;Earth;Data models;Covariance matrices;Symmetric matrices;Adaptive graph convolutional network (GCN);information propagation;polarimetric synthetic aperture radar (PolSAR) image;subgraphs},
  doi={10.1109/TGRS.2021.3076997},
  ISSN={1558-0644},
  month={},}@ARTICLE{10004998,
  author={Wang, Haoyu and Wang, Xuesong and Cheng, Yuhu},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Graph Meta Transfer Network for Heterogeneous Few-Shot Hyperspectral Image Classification}, 
  year={2023},
  volume={61},
  number={},
  pages={1-12},
  abstract={Since obtaining labeled hyperspectral images (HSIs) is difficult and time-consuming, the shortage of training samples has always been a challenge for HSI classification. In practical applications, only a few labeled samples are available in the task domain (target domain), while sufficient labeled samples are available in another domain (source domain). At the same time, these two domains are heterogeneous and contain different categories. This scenario makes it difficult to effectively transfer knowledge from the source domain to the target domain. To address this challenge, we propose a novel heterogeneous few-shot learning (FSL) method, namely graph meta transfer network (GMTN). Specifically, the graph sample and aggregate network (GraphSAGE) and meta-learning, which are both inductive learning, are integrated into a unified framework. In this way, the aggregation function is generalized from abundant few-shot tasks for feature extraction on the source and target domains. The spatial importance strategy (SIS) is designed to guide the feature propagation and alleviate the information interference caused by different categories. The neighborhood receptive field spectral attention (RFSA) mechanism is proposed to model the importance of spectral band using the information of the neighborhood pixels, which enables GMTN to pay more attention to bands with discriminative features in both domains. In addition, the node spatial information reset method is proposed to augment samples based on the spatial position relationship of nodes. Furthermore, to alleviate the domain shift in heterogeneous scenarios, the conditional domain adversarial strategy is used to achieve effective meta-knowledge transfer. Experiments show that GMTN outperforms the compared state-of-the-art methods.},
  keywords={Feature extraction;Training;Task analysis;Data mining;Principal component analysis;Aggregates;Transfer learning;Attention mechanism;few-shot learning (FSL);heterogeneous;hyperspectral image (HSI) classification;meta-learning;transfer learning},
  doi={10.1109/TGRS.2022.3233591},
  ISSN={1558-0644},
  month={},}@ARTICLE{10384769,
  author={Wang, Chuang and Wang, Zidong and Liu, Qinyuan and Dong, Hongli and Sheng, Weiguo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Support-Sample-Assisted Domain Generalization via Attacks and Defenses: Concepts, Algorithms, and Applications to Pipeline Fault Diagnosis}, 
  year={2024},
  volume={20},
  number={4},
  pages={6413-6423},
  abstract={This article is concerned with domain generalization (DG), a practical yet challenging scenario in transfer learning where the target data are not available in advance. The key insight of DG is focused on learning a robust model that can generalize to the unseen domain by leveraging knowledge from the source domain. To this end, we propose a novel algorithm known as support-sample-assisted Adversarial Attacks (SSAA) for DG. In the SSAA algorithm, an attackâ€“defense strategy is deployed to enhance the target model's generalizability and transferability. This strategy includes a nontargeted attack stage, during which attack samples are generated to form pseudotarget domains with near-realistic covariate shifts. Subsequently, in the model defense stage, a biclassifier structure is used to distinguish support samples from the generated attack samples. These support samples form a new decision boundary encompassing all unseen samples, prompting an extension of the existing decision boundary to meet these samples. Experimental results on cross-domain fault diagnosis tasks suggest that SSAA outperforms current state-of-the-art DG methods, indicating a promising avenue for further DG development.},
  keywords={Feature extraction;Training;Task analysis;Generators;Perturbation methods;Pipeline processing;Informatics;Transfer learning;Attackâ€“defense strategy;domain adaptation (DA);domain generalization (DG);support sample;transfer learning (TL)},
  doi={10.1109/TII.2023.3337364},
  ISSN={1941-0050},
  month={April},}@ARTICLE{9652466,
  author={Lu, Fan and Chen, Guang and Li, Zhijun and Zhang, Lijun and Liu, Yinlong and Qu, Sanqing and Knoll, Alois},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={MoNet: Motion-Based Point Cloud Prediction Network}, 
  year={2022},
  volume={23},
  number={8},
  pages={13794-13804},
  abstract={Predicting the future can significantly improve the safety of intelligent vehicles, which is a key component in autonomous driving. 3D point clouds can accurately model 3D information of surrounding environment and are crucial for intelligent vehicles to perceive the scene. Therefore, prediction of 3D point clouds has great significance for intelligent vehicles, which can be utilized for numerous further applications. However, due to point clouds are unordered and unstructured, point cloud prediction is challenging and has not been deeply explored in current literature. In this paper, we propose a novel motion-based neural network named MoNet. The key idea of the proposed MoNet is to integrate motion features between two consecutive point clouds into the prediction pipeline. The introduction of motion features enables the model to more accurately capture the variations of motion information across frames and thus make better predictions for future motion. In addition, content features are introduced to model the spatial content of individual point clouds. A recurrent neural network named MotionRNN is proposed to capture the temporal correlations of both features. Moreover, an attention-based motion align module is proposed to address the problem of missing motion features in the inference pipeline. Extensive experiments on two large-scale outdoor LiDAR point cloud datasets demonstrate the performance of the proposed MoNet. Moreover, we perform experiments on applications using the predicted point clouds and the results indicate the great application potential of the proposed method.},
  keywords={Point cloud compression;Feature extraction;Pipelines;Three-dimensional displays;Predictive models;Trajectory;Correlation;Point cloud;prediction;autonomous driving},
  doi={10.1109/TITS.2021.3128424},
  ISSN={1558-0016},
  month={Aug},}@INPROCEEDINGS{10203397,
  author={Li, Xingyi and Cao, Zhiguo and Sun, Huiqiang and Zhang, Jianming and Xian, Ke and Lin, Guosheng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={3D Cinemagraphy from a Single Image}, 
  year={2023},
  volume={},
  number={},
  pages={4595-4605},
  abstract={We present 3D Cinemagraphy, a new technique that mar-ries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emer-gence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthe-size novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.},
  keywords={Point cloud compression;Photography;Visualization;Three-dimensional displays;Motion estimation;Animation;Cameras;3D from single images},
  doi={10.1109/CVPR52729.2023.00446},
  ISSN={2575-7075},
  month={June},}
