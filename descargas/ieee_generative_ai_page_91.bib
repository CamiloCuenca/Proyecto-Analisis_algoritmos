@ARTICLE{9887825,
  author={Zaheer, Muhammad Zaigham and Lee, Jin-Ha and Mahmood, Arif and Astrid, Marcella and Lee, Seung-Ik},
  journal={IEEE Transactions on Image Processing}, 
  title={Stabilizing Adversarially Learned One-Class Novelty Detection Using Pseudo Anomalies}, 
  year={2022},
  volume={31},
  number={},
  pages={5963-5975},
  abstract={Recently, anomaly scores have been formulated using reconstruction loss of the adversarially learned generators and/or classification loss of discriminators. Unavailability of anomaly examples in the training data makes optimization of such networks challenging. Attributed to the adversarial training, performance of such models fluctuates drastically with each training step, making it difficult to halt the training at an optimal point. In the current study, we propose a robust anomaly detection framework that overcomes such instability by transforming the fundamental role of the discriminator from identifying real vs. fake data to distinguishing good vs. bad quality reconstructions. For this purpose, we propose a method that utilizes the current state as well as an old state of the same generator to create good and bad quality reconstruction examples. The discriminator is trained on these examples to detect the subtle distortions that are often present in the reconstructions of anomalous data. In addition, we propose an efficient generic criterion to stop the training of our model, ensuring elevated performance. Extensive experiments performed on six datasets across multiple domains including image and video based anomaly detection, medical diagnosis, and network security, have demonstrated excellent performance of our approach.},
  keywords={Training;Image reconstruction;Anomaly detection;Generators;Feature extraction;Detectors;Training data;Novelty detection;anomaly detection;adversarial learning;one-class classification;outliers detection;stabilizing adversarial models},
  doi={10.1109/TIP.2022.3204217},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10657666,
  author={Wu, Jianzong and Li, Xiangtai and Si, Chenyang and Zhou, Shangchen and Yang, Jingkang and Zhang, Jiangning and Li, Yining and Chen, Kai and Tong, Yunhai and Liu, Ziwei and Loy, Chen Change},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Towards Language-Driven Video Inpainting via Multimodal Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={12501-12511},
  abstract={We introduce a new task - language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpaintingrequests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We have made datasets, code, and models publicly available at https://github.com/jianzongwu/Language-Driven-Video-Inpainting.},
  keywords={Training;Computer vision;Codes;Computational modeling;Large language models;Scalability;Natural languages;Computer Vision;Multimodal Learning;Video Inpainting;Text-to-Video Generation},
  doi={10.1109/CVPR52733.2024.01188},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9816525,
  author={Pahwa, Ramanpreet Singh and Chang, Richard and Jie, Wang and Xun, Xu and Zaw Min, Oo and Sheng, Foo Chuan and Ser Choong, Chong and Rao, Vempati Srinivasa},
  booktitle={2022 IEEE 72nd Electronic Components and Technology Conference (ECTC)}, 
  title={Automated Detection and Segmentation of HBMs in 3D X-ray Images using Semi-Supervised Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1890-1897},
  abstract={Deep Learning is being widely used to identify and segment various 2D and 3D structures in voxelized data in fields such as robotics and medical imaging. Automated object detection and segmentation has had a rich history in semicon inspection and defect detection technologies for past few decades. Deep learning-based object detection and image segmentation has the potential to further improve defect detection accuracy and reduce manpower required for the quality inspection process. We develop a novel framework that utilizes the advancements in deep learning-based object detection and image segmentation techniques to leverage on partial labeled data and remaining unlabeled data to significantly improve the performance of locating microscopic bumps and defects such as voids for the defect detection process. We apply our Semi-Supervised Learning approach on various buried structures such as memory bumps and logic bumps. We briefly describe our fabrication and scanning process and thereafter, explain our approach in locating these different structures in 3D scans in detail. We extract the virtual 2D slices from 3D scans, perform Semi-Supervised object detection and image segmentation to classify each pixel of these individual slices into solders, voids, Cu-Pillars, and Cu-Pads. We compare our approach with state-of-the-art fully supervised techniques and perform a thorough analysis to discuss the advantages and disadvantages of our approach in both object detection and image segmentation steps.},
  keywords={Deep learning;Training;Image segmentation;Three-dimensional displays;Shape;Semantics;Object detection;Deep Learning;Semi-Supervised Learning;2D Semantic segmentation;Object detection},
  doi={10.1109/ECTC51906.2022.00297},
  ISSN={2377-5726},
  month={May},}@ARTICLE{9939091,
  author={Zhang, Ruisong and Quan, Weize and Zhang, Yong and Wang, Jue and Yan, Dong-Ming},
  journal={IEEE Transactions on Multimedia}, 
  title={W-Net: Structure and Texture Interaction for Image Inpainting}, 
  year={2023},
  volume={25},
  number={},
  pages={7299-7310},
  abstract={Recent literature has developed two advanced tools for image inpainting: appearance propagation and attention matching. However, given the ineffective feature reorganization and vulnerable attention maps, existing works yield suboptimal results with distorted structures and inconsistent contents. Furthermore, we observe that deep sampling layers (DSL) and shallow skip connections (SSC) in U-Net separately promote image structure inference and texture synthesis. To address the above two issues, we devise a W-shaped network (W-Net), which consists of two key components: a texture spatial attention (TSA) module in SSC and a structure channel excitation (SCE) module in DSL. W-Net is a two-stage network, with coarse and refined structures derived at each stage. Meanwhile, the TSA module fills incomplete textures with reliable attention scores under the guidance of coarse structures, which effectively diminishes inconsistency from appearance to semantics. The SCE module rectifies structures according to the difference between coarse structures and refined structures enhanced by texture features. Then the module motivates them to produce more reasonable shapes. Complete textures and refined structures constitute desired inpainted images, as the output of W-Net. Experiments on multiple datasets demonstrate the superior performance of W-Net.},
  keywords={Semantics;DSL;Image restoration;Image edge detection;Feature extraction;Detectors;Image reconstruction;Image inpainting;structure and texture;convolutional neural network;attention},
  doi={10.1109/TMM.2022.3219728},
  ISSN={1941-0077},
  month={},}@ARTICLE{10093791,
  author={Zhu, Zhanchen and Zhang, Daokun and Wang, Zhikang and Feng, Siyuan and Duan, Peibo},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Spectral Dual-Channel Encoding for Image Dehazing}, 
  year={2023},
  volume={33},
  number={11},
  pages={6236-6248},
  abstract={In recent years, deep learning-based dehazing models have presented a momentum of dramatic growth. Unfortunately, most deep learning-based approaches heavily rely on synthetically hazed images for model training, which makes these methods brittle to restore hazy images taken from real-world scenes, due to the sample distribution discrepancy between synthetic and realistic images. Although some attempts have been made to overcome this difficulty by augmenting image spatial features with spectral features, the power of the spectral features still remains underutilized. In this paper, we propose the Spectral Dual-Channel Encoding (SDCE) framework for high-quality image dehazing, by unleashing the power of spectral feature encoding. We argue that hazes impose more adverse impacts on high-frequency image features (e.g., outlines and textures) than low-frequency features (e.g., colors), with theoretical and empirical justifications. To better restore hazed high- and low-frequency features, we decompose the hazed images into high- and low-frequency feature components with spectral dual-channel encoding and respectively design effective neural network architectures to recover hazed images on the two feature components. To be specific, we recover the low-frequency feature components with an encoder-decoder, while we specially design a high-frequency aggregation component (HFAC) to recover hazed images on high-frequency feature components, by referring to neighboring feature distributions. We conduct extensive experiments on four real-world image dehazing benchmarks. The experimental results show that our proposed SDCE framework outperforms the state-of-the-art baselines significantly, with an average 4.4% improvement in PSNR and an average 7.7% gain in SSIM.},
  keywords={Image restoration;Image color analysis;Image coding;Atmospheric modeling;Visualization;Training;Learning systems;Dehazing;spectrum;disentanglement;encoding},
  doi={10.1109/TCSVT.2023.3264717},
  ISSN={1558-2205},
  month={Nov},}@INPROCEEDINGS{10376589,
  author={Gu, Sophia and Clark, Christopher and Kembhavi, Aniruddha},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={I can’t believe there’s no images! : Learning Visual Tasks Using Only Language Supervision}, 
  year={2023},
  volume={},
  number={},
  pages={2672-2683},
  abstract={Many high-level skills that are required for computer vision tasks, such as parsing questions, comparing and contrasting semantics, and writing descriptions, are also required in other domains such as natural language processing. In this paper, we ask whether it is possible to learn those skills from text data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between embedding spaces for different modalities in contrastive models, and we analyze how these differences affect our approach and study strategies to mitigate this concern. We produce models using only text training data on four representative tasks: image captioning, visual entailment, visual question answering and visual news captioning, and evaluate them on standard benchmarks using images. We find these models perform close to models trained on images, while surpassing prior work for captioning and visual entailment in this text-only setting by over 9 points, and outperforming all prior work on visual news by over 30 points. We also showcase a variety of stylistic image captioning models that are trained using no image data and no human-curated language data, but instead using readily-available text data from books, the web, or language models.},
  keywords={Training;Visualization;Solid modeling;Adaptation models;Computer vision;Semantics;Training data},
  doi={10.1109/ICCV51070.2023.00252},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{9892791,
  author={Li, Yuan and Wang, Huanjie and Li, Jingwei and Liu, Chengbao and Tan, Jie},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={ACT: Adversarial Convolutional Transformer for Time Series Forecasting}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Time series forecasting is an important problem involving many fields, including the prediction of extreme weather early warning, electricity consumption planning, and long-term traffic congestion. Compared with one-step-ahead prediction, multi-horizon forecasting demands high prediction capacity of the model. Recent studies have shown the great potential of Transformer to improve the prediction accuracy. However, there are three problems with Transformer that restrict its performance, i.e. error accumulation, short-term and long-term dependencies. First, due to the teacher forcing strategy, the ground truth of target values are given during training and replaced by previous step output during testing. This difference between training and testing can lead to error accumulation. Second, time series data have a strong dependence on their local time information. But in classical Transformer architecture, the dot-product self-attention is computed by point-wise values, which are insensitive to local context. Thus, they may fail to distinguish between a turning point, an outlier and the part of patterns. Third, most methods optimize only one objective function and don't model the distributions of data, which is difficult to capture the long-term intricate patterns of time series. To solve these issues, we propose a Transformer-based time series forecasting model in this paper, named Adversarial Convolutional Transformer(ACT). First, we change the decoding mode from step-by-step way to one-step way, which can predict the entire sequence at one forward step to relieve the error accumulation issue. Next, we propose the convolutional attention block, which incorporates local context into the self-attention mechanism and captures the short-term dependencies of data. Then, we introduce adversarial training to the model to capture the long-term repeating patterns. Experiments on five challenging datasets demonstrate that ACT can bring solid improvements in accuracy.},
  keywords={Training;Time series analysis;Predictive models;Transformers;Turning;Solids;Planning;time series forecasting;one-step decoding;convolutional self-attention;adversarial training},
  doi={10.1109/IJCNN55064.2022.9892791},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9063463,
  author={Wu, Weichang and Yan, Junchi and Yang, Xiaokang and Zha, Hongyuan},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Discovering Temporal Patterns for Event Sequence Clustering via Policy Mixture Model}, 
  year={2022},
  volume={34},
  number={2},
  pages={573-586},
  abstract={Temporal point process (TPP) is an expressive tool for modeling the temporal pattern of event sequences. However, discovering temporal patterns for event sequences clustering is rarely studied in TPP modeling. To solve this problem, we take a reinforcement learning view whereby the observed sequences are assumed to be generated from a mixture of latent policies. The purpose is to cluster the sequences with different temporal patterns into the underlying policies while learning each of the policy model. The flexibility of our model lies in: i) all the components are networks including the policy network for modeling the temporal point process; ii) to handle varying-length event sequences, we resort to inverse reinforcement learning by decomposing the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn a reward function, it helps to achieve better performance or increasing efficiency compared to existing methods using rewards over the entire sequence such as log-likelihood or Wasserstein distance. We adopt an Expectation-Maximization algorithm, in E-step estimating the cluster labels for each sequence, in M-step aiming to learn the respective policy. Extensive experiments on synthetic and real-world datasets show the efficacy of our method against the state-of-the-arts.},
  keywords={Data models;Clustering algorithms;Machine learning;Stochastic processes;Predictive models;Earthquakes;Social network services;Time-series analysis;temporal point processes;reinforcement learning;model-based clustering},
  doi={10.1109/TKDE.2020.2986206},
  ISSN={1558-2191},
  month={Feb},}@ARTICLE{10272678,
  author={Zhu, Huilin and Yuan, Jingling and Zhong, Xian and Liao, Liang and Wang, Zheng},
  journal={IEEE Transactions on Multimedia}, 
  title={Find Gold in Sand: Fine-Grained Similarity Mining for Domain-Adaptive Crowd Counting}, 
  year={2024},
  volume={26},
  number={},
  pages={3842-3855},
  abstract={The domain shift of crowd scenes significantly hinders the application of crowd counting models in open scenarios. Although domain adaptation methods for crowd counting have bridged this gap to some extent, they ignore one of the significant causes of domain shift, which is the inter-domain data distribution bias. We discover that there exists a connection between the known and unknown distribution, which can be utilized by similarity mining to address the domain shift. However, there are still challenges related to insufficient and inaccurate similarity mining. In this article, a novel Fine-grained Inter-domain Similarity Mining (FSIM) framework is proposed. To comprehensively explore the similar distributions between source and target domains, we propose a Multi-scale Distribution Alignment (MDA) module based on diffusion retrieval. To enhance the reliability of inter- domain similarity mining, we propose a Multi-retrieval Refinement (MR) module based on evidence theory, which serves as an uncertainty measurement method. Eventually, to eliminate the data distribution bias, we perform model retraining using a similar distribution. Extensive experiments conducted on five standard crowd counting benchmarks, SHA, SHB, QNRF, NWPU, and JHU-CROWD++, show that the proposed FSIM has strong generalizability.},
  keywords={Data mining;Adaptation models;Evidence theory;Data models;Task analysis;Computational modeling;Synthetic data;Crowd counting;domain adaptive;evidence theory;multi-scale similarity},
  doi={10.1109/TMM.2023.3316437},
  ISSN={1941-0077},
  month={},}@ARTICLE{10497868,
  author={Nan, Guozheng and Zhao, Yue and Fu, Liyong and Ye, Qiaolin},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Object Detection by Channel and Spatial Exchange for Multimodal Remote Sensing Imagery}, 
  year={2024},
  volume={17},
  number={},
  pages={8581-8593},
  abstract={Smart satellites and unmanned aerial vehicles (UAVs) are typically equipped with visible light and infrared (IR) spectrum sensors. However, achieving real-time object detection utilizing these multimodal data on such resource-limited devices is a challenging task. This article proposes HyperYOLO, a real-time lightweight object detection framework for multimodal remote sensing images. First, we propose a lightweight multimodal fusion module named channel and spatial exchange (CSE) to effectively extract complementary information from different modalities. The CSE module consists of two stages: channel exchange and spatial exchange. Channel exchange achieves global fusion by learning global weights to better utilize cross-channel information correlation, while spatial exchange captures details by considering spatial relationships to calibrate local fusion. Second, we propose an effective auxiliary branch module based on the feature pyramid network for super resolution (FPNSR) to enhance the framework's responsiveness to small objects by learning high-quality feature representations. Moreover, we embed a coordinate attention mechanism to assist our network in precisely localizing and attending to the objects of interest. The experimental results show that on the VEDAI remote sensing dataset, HyperYOLO achieves a 76.72% mAP50, surpassing the SOTA SuperYOLO by 1.63%. Meanwhile, the parameter size and GFLOPs of HyperYOLO are about 1.34 million (28%) and 3.97 (22%) less than SuperYOLO, respectively. In addition, HyperYOLO has a file size of only 7.3 MB after the removal of the auxiliary FPNSR branch, which makes it easier to deploy on these resource-constrained devices.},
  keywords={Remote sensing;Feature extraction;Forestry;Object detection;Superresolution;YOLO;Multimodal sensors;Multimodal feature fusion;remote sensing image (RSI);RGB-infrared object detection;super resolution (SR)},
  doi={10.1109/JSTARS.2024.3388013},
  ISSN={2151-1535},
  month={},}@ARTICLE{10695805,
  author={Zhao, Guangwei and Wu, Haitao and Luo, Dexiang and Ou, Xu and Zhang, Yu},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Spatial–Spectral Interaction Super-Resolution CNN–Mamba Network for Fusion of Satellite Hyperspectral and Multispectral Image}, 
  year={2024},
  volume={17},
  number={},
  pages={18489-18501},
  abstract={The tradeoff between spatial and spectral resolution in sensor design is inevitable, and spatial–spectral fusion aims to use low spatial resolution hyperspectral image (HSI) and high spatial resolution (HR) multispectral image (MSI) obtained at the same time and in the same area to reconstruct HR HSI. Recently, a large number of deep-learning methods have been applied in this field and achieved success. However, these methods do not fully utilize the characteristics of data for network design, and cannot guarantee effective computational efficiency in extracting local and global features. To solve the above problems, we designed a spatial–spectral interaction super-resolution convolutional neural network (CNN)–Mamba fusion network for satellite HSI and MSI, which uses mutual guidance to improve the spatial and spectral resolution of different data, and obtains the final fused image through feature fusion. In addition, we combined Mamba with CNN to effectively explore global and local features of images. Extensive experiments have proven that our method can reconstruct fused images of high quality and is superior to current state-of-the-art fusion methods.},
  keywords={Feature extraction;Superresolution;Spatial resolution;Data mining;Image reconstruction;Image fusion;Distortion;Convolutional neural network (CNN);fusion;hyperspectral;Mamba;multispectral},
  doi={10.1109/JSTARS.2024.3469184},
  ISSN={2151-1535},
  month={},}@ARTICLE{10347237,
  author={Li, Qi and Guo, Dan and Qian, Wei and Tian, Xilan and Sun, Xiao and Zhao, Haifeng and Wang, Meng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Channel-Wise Interactive Learning for Remote Heart Rate Estimation From Facial Video}, 
  year={2024},
  volume={34},
  number={6},
  pages={4542-4555},
  abstract={Remote photoplethysmography measurement (also called rPPG prediction) is a vision-based technique that allows for the non-contact monitoring of human physiological activity using facial video. However, precisely detecting subtle color changes on facial skin, especially in less-constrained real-life scenarios, remains a formidable challenge for rPPG prediction. In this work, we address a rPPG-based heart rate estimation task by proposing an end-to-end Channel-wise Interaction Network (CIN-rPPG), in which the core idea contains two specialized units: channel-temporal interactive learning (CIT) and channel-spatial interactive learning (CIS). The CIT unit gets the periodicity of the rPPG signal by using temporal-wise shifting and channel-wise scaling to measure the interaction between channels and temporal dimensions. The CIS unit does both spatial-wise scaling and channel-wise scaling at the same time to perform channel-spatial interaction. This is intended to reveal how rPPG-related visual responses are detected on the human face. We exploit the rPPG recovery through the alternation of CIT and CIS implementations. The CIN-rPPG is completely conducted by convolutional operations on the sequential 2D feature maps of facial video in an end-to-end manner. Extensive experiments on three heart rate estimation datasets (UBFC-rPPG, PURE, and MMSE-HR) demonstrate that CIN-rPPG achieves state-of-the-art performance on both intra-dataset and cross-dataset testing.},
  keywords={Heart rate;Estimation;Color;Physiology;Skin;Signal resolution;Task analysis;Remote photoplethysmography;facial videos;end-to-end;channel-wise interaction;heart rate},
  doi={10.1109/TCSVT.2023.3332408},
  ISSN={1558-2205},
  month={June},}@ARTICLE{10138509,
  author={Zhou, Chaofan and Liu, Meiqin and Zhang, Senlin and Wei, Ping and Chen, Badong},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Few-Shot Classification of Screen Defects With Class-Agnostic Mask and Context-Based Classifier}, 
  year={2023},
  volume={72},
  number={},
  pages={1-16},
  abstract={In the real mobile screen production line, the yield of normal products is relatively high, so some defects are difficult to collect. Besides, samples of emerging defects are difficult to collect in large numbers in a short time. To address the few-shot problem caused by these, we propose a few-shot classification method to simultaneously recognize both the known defects with adequate training data and novel defects with scarce labeled training samples. First, we design a feature extractor with the class-agnostic mask. This module applies normal images to weaken irrelevant background information, thus extracting defect-highlighted features. Then, to avoid drowning limited important features in noise, the class prototype generator (CPG) is introduced to produce more representative class prototypes. Finally, a context-based classifier (CBC), which applies global context information, is proposed. This module can make better classification decisions among all classes. Experiments demonstrate that our method significantly outperforms other few-shot methods on the mobile screen defect dataset. The proposed method achieves 89.19% accuracy, 96.96% accuracy, and 97.55% accuracy in the one-shot, five-shot, and ten-shot test scenarios, respectively. The code is available at https://github.com/CFZ1/FSL_Cls_Screen.},
  keywords={Feature extraction;Training;Mobile handsets;Training data;Prototypes;Tin;Production;Deep learning;defect inspection;few-shot learning;mobile screen defects;surface defect classification},
  doi={10.1109/TIM.2023.3280532},
  ISSN={1557-9662},
  month={},}@ARTICLE{10274678,
  author={Hu, Dianlin and Zhang, Yikun and Li, Wangyao and Zhang, Weijie and Reddy, Krishna and Ding, Qiaoqiao and Zhang, Xiaoqun and Chen, Yang and Gao, Hao},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={SEA-Net: Structure-Enhanced Attention Network for Limited-Angle CBCT Reconstruction of Clinical Projection Data}, 
  year={2023},
  volume={72},
  number={},
  pages={1-13},
  abstract={This work aims to improve limited-angle (LA) cone-beam computed tomography (CBCT) by developing deep learning (DL) methods for real clinical CBCT projection data, which is the first feasibility study of clinical-projection-data-based LA-CBCT, to the best of our knowledge. In radiation therapy (RT), CBCT is routinely used as the onboard imaging modality for patient setup. Compared to diagnostic computed tomography (CT), CBCT has a long acquisition time, e.g., 60 s for a full 360° rotation, which is subject to the motion artifact. Therefore, the LA-CBCT, if achievable, is of great interest for the purpose of RT, for its proportionally reduced scanning time in addition to the radiation dose. However, LA-CBCT suffers from severe wedge artifacts and image distortions. Targeting at real clinical projection data, we have explored various DL methods such as image/data/hybrid-domain methods and finally developed the so-called structure-enhanced attention network (SEA-Net) method that has the best image quality from clinical projection data among the DL methods we have implemented. Specifically, the proposed SEA-Net employs a specialized structure enhancement subnetwork to promote texture preservation. Based on the observation that the distribution of wedge artifacts in reconstruction images is nonuniform, the spatial attention (SA) module is utilized to emphasize the relevant regions while ignoring the irrelevant ones, which leads to more accurate texture restoration.},
  keywords={Image reconstruction;Iterative methods;Computed tomography;Image restoration;Feature extraction;Convolutional neural networks;Radiation therapy;Clinical projection data;cone-beam computed tomography (CBCT);limited-angle (LA) computed tomography (CT);spatial attention (SA);structure enhancement},
  doi={10.1109/TIM.2023.3318712},
  ISSN={1557-9662},
  month={},}@ARTICLE{10478970,
  author={Liu, Jinduo and Han, Lu and Ji, Junzhong},
  journal={IEEE Transactions on Medical Imaging}, 
  title={MCAN: Multimodal Causal Adversarial Networks for Dynamic Effective Connectivity Learning From fMRI and EEG Data}, 
  year={2024},
  volume={43},
  number={8},
  pages={2913-2923},
  abstract={Dynamic effective connectivity (DEC) is the accumulation of effective connectivity in the time dimension, which can describe the continuous neural activities in the brain. Recently, learning DEC from functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) data has attracted the attention of neuroinformatics researchers. However, the current methods fail to consider the gap between the fMRI and EEG modality, which can not precisely learn the DEC network from multimodal data. In this paper, we propose a multimodal causal adversarial network for DEC learning, named MCAN. The MCAN contains two modules: multimodal causal generator and multimodal causal discriminator. First, MCAN employs a multimodal causal generator with an attention-guided layer to produce a posterior signal and output a set of DEC networks. Then, the proposed method uses a multimodal causal discriminator to unsupervised calculate the joint gradient, which directs the update of the whole network. The experimental results on simulated data sets show that MCAN is superior to other state-of-the-art methods in learning the network structure of DEC and can effectively estimate the brain states. The experimental results on real data sets show that MCAN can better reveal abnormal patterns of brain activity and has good application potential in brain network analysis.},
  keywords={Functional magnetic resonance imaging;Electroencephalography;Generators;Time series analysis;Task analysis;Learning systems;Feature extraction;Brain effective connectivity;functional magnetic resonance imaging;electroencephalog;multimodal causal learning;adversarial training},
  doi={10.1109/TMI.2024.3381670},
  ISSN={1558-254X},
  month={Aug},}@ARTICLE{10410042,
  author={Zhang, Zhe and Shang, Zhenqiao and Wang, Xin and Ma, Jie},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Combined Anomaly Aware Weakly Supervised Lightweight Model for Surface Defect Inspection}, 
  year={2024},
  volume={20},
  number={4},
  pages={6652-6663},
  abstract={Surface defect inspection plays a vital role in the industrial production process. Many detection methods based on deep learning have been gradually applied because of their better generalization performance. However, achieving accurate annotations for training deep learning models remains a challenge due to the difficult definition of defect boundaries and the high cost of manual annotation work. Meanwhile, the detection performance of the current deep-learning methods still cannot meet the needs of industrial applications. To address these issues, this article proposes a combined anomaly aware weakly supervised lightweight model that only requires image-level labels for training and outputs defect localization. In the framework, we first design a lightweight backbone to obtain feature maps. Then, we propose a novel weakly supervised localization (WSL) method to obtain anomaly responses and use them as prior knowledge of the downstream network. Finally, the final defect detection result is obtained through the work of the designed downstream fine inspection network. In addition, we will employ multiple supervisions throughout the framework for full data use. The results of the evaluation on four real-world defect datasets demonstrate that the proposed method is superior and more generalized than state-of-the-art WSL methods and defect detection methods on average precision.},
  keywords={Inspection;Feature extraction;Location awareness;Deep learning;Training;Anomaly detection;Defect detection;Supervised learning;Industrial engineering;Production management;Surface cracks;Anomaly aware;lightweight model;surface defect inspection;weakly supervised localization (WSL)},
  doi={10.1109/TII.2023.3348835},
  ISSN={1941-0050},
  month={April},}@ARTICLE{10253954,
  author={Yuan, Yuan and Zhao, Yiru and Ma, Dandan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={NACAD: A Noise-Adaptive Context-Aware Detector for Remote Sensing Small Objects}, 
  year={2023},
  volume={61},
  number={},
  pages={1-13},
  abstract={Small object detection in remote sensing faces significant challenges, such as their offset sensitivity caused by the small area coverage, the dim targets in images, and their vulnerability to complex backgrounds, which often result in missed detections and false alarms. In this work, we propose a noise-adaptive context-aware detector (NACAD) to alleviate the above problems, which mainly consists of a region proposal network (RPN) with noise-adaptive module (NAM), a context-aware module (CAM), and a position-refined module (PRM). The main contributions are threefold. First, we leverage the information around small objects as positive-incentive noise (also known as  $\pi $ -noise); through enlarging the range of small objects by NAM, more anchors of them are preserved as positive samples, thus stimulating the model to detect small objects. Second, the CAM is designed to provide multiple observation perspectives and abundant contextual representations for the enhancement of object features. Third, to reduce the interference of pure noise in the complicated backgrounds around small objects, the spatial calibration along two coordinate axes is devised by PRM to optimally use information beyond object regions. The effectiveness of our proposed detector, particularly on small objects, has been validated by the experiments on two public datasets, ITCVD and HRRSD. In particular, the NAM improves the recall of small objects, CAM enhances small object features, and PRM helps address the pure noise in complicated backgrounds around small objects.},
  keywords={Object detection;Feature extraction;Detectors;Task analysis;Remote sensing;Proposals;Location awareness;Context aware;noise adaptive;offset sensitive;remote sensing;small object detection},
  doi={10.1109/TGRS.2023.3316277},
  ISSN={1558-0644},
  month={},}@ARTICLE{10198396,
  author={Wang, Shuyuan and Lv, Chengkan and Zhang, Zhengtao and Wei, Xueyan},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Dual-Branch Learning With Prior Information for Surface Anomaly Detection}, 
  year={2023},
  volume={72},
  number={},
  pages={1-11},
  abstract={Visual surface anomaly detection focuses on the classification (CLS) and location (LOC) of regions that deviate from the normal appearance, and generally, only normal samples are provided for training. The reconstruction-based method is widely used, which locates the anomalies by analyzing the reconstruction error. However, there are two problems unsettled in the reconstruction-based method. First, the reconstruction error in the normal regions is sometimes large. This might mislead the model to take the normal regions as anomalies, which is named an overkill problem. Second, it has been observed that the anomalous regions sometimes cannot be repaired to normal, which results in a small reconstruction error in the anomalous regions. This misleads the model to take the anomalies as normal, which is called an anomaly escape problem. Aiming at the above two problems, we propose a model named dual-branch autoencoder with prior information (DBPI) which is mainly composed of a dual-branch AE structure and a GA unit. To alleviate the overkill problem, a natural idea is to reduce the reconstruction error in the normal regions, and therefore a dual-branch AE is proposed. The dual-branch AE reconstructs two images with consistent normal regions and different anomalous regions. By analyzing the reconstruction error between the above two reconstructed images, the anomalies can be detected without causing overkill. For the anomaly escape problem, an effective solution is to add prior information of normal appearance to the reconstructive network, which assists in repairing the anomalous regions and increasing the reconstruction error in the anomalous regions. Since the mathematical expectation map of the training data contains crucial features of the normal appearance, we utilize it as the prior information of the normal appearance. And the prior information is selectively introduced by the proposed gated attention (GA) unit, which effectively assists in reconstructing a normal image and further mitigates the anomaly escape problem. On the average precision (AP) metric for the anomaly detection benchmark dataset MVTec, the proposed unsupervised method outperforms the current state-of-the-art reconstruction-based method self-supervised predictive convolutional attentive block (SSPCAB) by 7.4%. Meanwhile, our unsupervised method also exhibits comparable performance to the best supervised methods on the surface defect detection DAGM dataset.},
  keywords={Image reconstruction;Decoding;Anomaly detection;Training data;Feature extraction;Surface reconstruction;Training;Anomaly detection;anomaly escape;defect detection;dual-branch (DB) autoencoder (AE);gated attention (GA);overkill},
  doi={10.1109/TIM.2023.3300458},
  ISSN={1557-9662},
  month={},}@ARTICLE{10224654,
  author={Manchanda, Sunny and Bhagwatkar, Kaushik and Balutia, Kavita and Agarwal, Shivang and Chaudhary, Jyoti and Dosi, Muskan and Chiranjeev, Chiranjeev and Vatsa, Mayank and Singh, Richa},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science}, 
  title={D-LORD: DYSL-AI Database for Low-Resolution Disguised Face Recognition}, 
  year={2024},
  volume={6},
  number={2},
  pages={147-157},
  abstract={Face recognition in a low-resolution video stream captured from a surveillance camera is a challenging problem. The problem becomes even more complicated when the subjects appearing in the video wear disguise artifacts to hide their identity or try to impersonate someone. The lack of labeled datasets restricts the current research on low-resolution face recognition systems under disguise. With this paper, we propose a large-scale database, D-LORD, that will facilitate the research on face recognition. The proposed D-LORD dataset includes high-resolution mugshot images of 2,100 individuals and 14,098 low-resolution surveillance videos, collectively containing over 1.2 million frames. Each frame in the dataset has been annotated with five facial keypoints and a single bounding box for each face. In the videos, subjects’ faces are occluded by various disguise artifacts, such as face masks, sunglasses, wigs, hats, and monkey caps. To the best of our knowledge, D-LORD is the first database to address the complex problem of low-resolution face recognition with disguise variations. We also establish the benchmark results of several state-of-the-art face detectors, frame selection algorithms, face restoration, and face verification algorithms using well-structured experimental protocols on the D-LORD dataset. The research findings indicate that the Genuine Acceptance Rate (GAR) at 1% False Acceptance Rate (FAR) varies between 86.44% and 49.45% across different disguises and distances. The dataset is publicly available to the research community at https://dyslai.org/datasets/D-LORD/.},
  keywords={Face recognition;Surveillance;Image resolution;Lighting;Detection algorithms;Databases;Data collection;Face recognition;low-resolution;disguise},
  doi={10.1109/TBIOM.2023.3306703},
  ISSN={2637-6407},
  month={April},}@ARTICLE{9872529,
  author={Li, Xuelong and Li, Guanlin and Zhao, Bin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Low-Light Hyperspectral Image Enhancement}, 
  year={2022},
  volume={60},
  number={},
  pages={1-13},
  abstract={Due to inadequate energy captured by the hyperspectral camera sensor in poor illumination conditions, low-light hyperspectral images (LHSIs) usually suffer from low visibility, spectral distortion, and various noises. A range of hyperspectral image (HSI) restoration methods have been developed, yet their effectiveness in enhancing low-light HSIs is constrained. This work focuses on the low-light HSI enhancement task, which aims to reveal the spatial–spectral information hidden in darkened areas. To facilitate the development of low-light HSI processing, we collect an LHSI dataset of both indoor and outdoor scenes. Based on Laplacian pyramid decomposition and reconstruction, we developed an end-to-end data-driven low-light HSI enhancement (HSIE) approach trained on the LHSI dataset. With the observation that illumination is related to the low-frequency component of HSI, while textural details are closely correlated with the high-frequency component, the proposed HSIE is designed to have two branches. The illumination enhancement branch is adopted to enlighten the low-frequency component with reduced resolution. The high-frequency refinement branch is utilized for refining the high-frequency component via a predicted mask. In addition, to improve information flow and boost performance, we introduce an effective channel attention block (CAB) with residual dense connection, which served as the basic block of the illumination enhancement branch. The effectiveness and efficiency of HSIE both in quantitative assessment measures and visual effects are demonstrated by experimental results on the LHSI dataset. According to the classification performance on the remote-sensing Indian Pines dataset, downstream tasks benefit from the enhanced HSI. Datasets and codes are available at https://github.com/guanguanboy/HSIE.},
  keywords={Noise reduction;Laplace equations;Lighting;Task analysis;Image enhancement;Hyperspectral imaging;Image restoration;Denoising;hyperspectral images (HSIs);Laplacian pyramid;low-light enhancement},
  doi={10.1109/TGRS.2022.3201206},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9959441,
  author={Zhang, Quan and Huang, Yanrong and Song, Rui},
  booktitle={2022 18th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)}, 
  title={A Ship Detection Model Based on YOLOX with Lightweight Adaptive Channel Feature Fusion and Sparse Data Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Accurate and real-time detection of ships has become an essential part in maritime video surveillance and plays a vital role in national territorial water security. However, the existing ship detection models have poor recognition performance for some types of ships, such as small ships and ultra-long ships. Meanwhile, light and weather factors also affect the accuracy of the ship detection model, which blurs the boundaries between ships and the background In this paper, a new ship detection model ShipYOLOX based on YOLOX is designed to solve those problems in existing models. Specifically, the modelrs ability to discriminate ships with complex contours is enhanced by adding a feature fusion module called Lightweight Adaptive Channel Feature Fusion (LACFF). Additionally, a new data augmentation algorithm Sparse Target Mosaic is designed to replace the original Mosaic. The new model ShipYOLOX has been validated in experiments with results showing that it can improve the accuracy of ship detection, which achieves an excellent performance of 87.4% AP75 on Seaships.},
  keywords={Location awareness;Adaptation models;Fuses;Video surveillance;Feature extraction;Data models;Real-time systems},
  doi={10.1109/AVSS56176.2022.9959441},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9575641,
  author={Da Cruz, Steve Dias and Taetz, Bertram and Wasenmüller, Oliver and Stifter, Thomas and Stricker, Didier},
  booktitle={2021 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Autoencoder Based Inter-Vehicle Generalization for In-Cabin Occupant Classification}, 
  year={2021},
  volume={},
  number={},
  pages={1296-1303},
  abstract={Common domain shift problem formulations consider the integration of multiple source domains, or the target domain during training. Regarding the generalization of machine learning models between different car interiors, we formulate the criterion of training in a single vehicle: without access to the target distribution of the vehicle the model would be deployed to, neither with access to multiple vehicles during training. We performed an investigation on the SVIRO dataset for occupant classification on the rear bench and propose an autoencoder based approach to improve the transferability. The autoencoder is on par with commonly used classification models when trained from scratch and sometimes out-performs models pre-trained on a large amount of data. Moreover, the autoencoder can transform images from unknown vehicles into the vehicle it was trained on. These results are corroborated by an evaluation on real infrared images from two vehicle interiors.},
  keywords={Training;Intelligent vehicles;Transforms;Machine learning;Data models;Automobiles},
  doi={10.1109/IV48863.2021.9575641},
  ISSN={},
  month={July},}@ARTICLE{9833385,
  author={Li, Leida and Chen, Pengfei and Lin, Weisi and Xu, Mai and Shi, Guangming},
  journal={IEEE Transactions on Image Processing}, 
  title={From Whole Video to Frames: Weakly-Supervised Domain Adaptive Continuous-Time QoE Evaluation}, 
  year={2022},
  volume={31},
  number={},
  pages={4937-4951},
  abstract={Due to the rapid increase in video traffic and relatively limited delivery infrastructure, end users often experience dynamically varying quality over time when viewing streaming videos. The user quality-of-experience (QoE) must be continuously monitored to deliver an optimized service. However, modern approaches for continuous-time video QoE estimation require densely annotating the continuous-time QoE labels, which is labor-intensive and time-consuming. To cope with such limitations, we propose a novel weakly-supervised domain adaptation approach for continuous-time QoE evaluation, by making use of a small amount of continuously labeled data in the source domain and abundant weakly-labeled data (only containing the retrospective QoE labels) in the target domain. Specifically, given a pair of videos from source and target domains, effective spatiotemporal segment-level feature representation is first learned by a combination of 2D and 3D convolutional networks. Then, a multi-task prediction framework is developed to simultaneously achieve continuous-time and retrospective QoE predictions, where a quality attentive adaptation approach is investigated to effectively alleviate the domain discrepancy without hampering the prediction performance. This approach is enabled by explicitly attending to the video-level discrimination and segment-level transferability in terms of the domain discrepancy. Experiments on benchmark databases demonstrate that the proposed method significantly improves the prediction performance under the cross-domain setting.},
  keywords={Quality of experience;Streaming media;Adaptation models;Predictive models;Databases;Three-dimensional displays;Task analysis;Quality of experience;domain adaptation;weakly-supervised learning;deep learning},
  doi={10.1109/TIP.2022.3190711},
  ISSN={1941-0042},
  month={},}@ARTICLE{8616891,
  author={Zhou, Mantong and Huang, Minlie and Zhu, Xiaoyan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Story Ending Selection by Finding Hints From Pairwise Candidate Endings}, 
  year={2019},
  volume={27},
  number={4},
  pages={719-729},
  abstract={The ability of story comprehension is a strong indicator of natural language understanding. Recently, Story Cloze Test has been introduced as a new task of machine reading comprehension, i.e., selecting a correct ending from two candidate endings given a four-sentence story context. Most existing methods for Story Cloze Test are essentially matching-based that operate by comparing an individual ending with a given context, therefore suffering from the evidence bias issue: both candidate endings can obtain supporting evidence from the story context, which misleads the classifier to choose an incorrect ending. To address this issue, we present a novel idea to improve story comprehension by utilizing the hints that are obtained through comparing two candidate endings. The proposed model firstly anticipates a feature vector for a possible ending solely based on the context, and then refines the feature prediction using the hints which encode the difference between two candidates. The candidate ending whose feature vector is more similar to the predicted ending vector is regarded as correct. Experimental results demonstrate that our approach can alleviate the evidence bias issue and improve story comprehension.},
  keywords={Task analysis;Context modeling;Predictive models;Speech processing;Computational modeling;Natural languages;Semantics;Machine reading comprehension;story comprehension;commonsense reasoning;neural networks},
  doi={10.1109/TASLP.2019.2893499},
  ISSN={2329-9304},
  month={April},}@INPROCEEDINGS{10030950,
  author={Chandra, Dupati Srikar and Varshney, Sakshi and Srijith, P.K. and Gupta, Sunil},
  booktitle={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Continual Learning with Dependency Preserving Hypernetworks}, 
  year={2023},
  volume={},
  number={},
  pages={2338-2347},
  abstract={Humans learn continually throughout their lifespan by accumulating diverse knowledge and fine-tuning it for future tasks. When presented with a similar goal, neural networks suffer from catastrophic forgetting if data distributions across sequential tasks are not stationary over the course of learning. An effective approach to address such continual learning (CL) problems is to use hypernetworks which generate task dependent weights for a target network. However, the continual learning performance of existing hypernetwork based approaches are affected by the assumption of independence of the weights across the layers in order to maintain parameter efficiency. To address this limitation, we propose a novel approach that uses a dependency preserving hypernetwork to generate weights for the target network while also maintaining the parameter efficiency. We propose to use recurrent neural network (RNN) based hypernetwork that can generate layer weights efficiently while allowing for dependencies across them. In addition, we propose novel regularisation and network growth techniques for the RNN based hypernetwork to further improve the continual learning performance. To demonstrate the effectiveness of the proposed methods, we conducted experiments on several image classification continual learning tasks and settings. We found that the proposed methods based on the RNN hypernetworks outperformed the baselines in all these CL settings and tasks.},
  keywords={Computer vision;Recurrent neural networks;Computational modeling;Task analysis;Image classification;Algorithms: Machine learning architectures;formulations;and algorithms (including transfer);Image recognition and understanding (object detection;categorization;segmentation;scene modeling;visual reasoning)},
  doi={10.1109/WACV56688.2023.00237},
  ISSN={2642-9381},
  month={Jan},}@INPROCEEDINGS{10763088,
  author={P, Yashini and G, Karthika and T, Sunitha and Renugadevi, R. and Magthalin R, Berlin},
  booktitle={2024 4th International Conference on Sustainable Expert Systems (ICSES)}, 
  title={Machine Learning-Based Textile Fabric Defect Detection Network}, 
  year={2024},
  volume={},
  number={},
  pages={1470-1477},
  abstract={The Deep Learning technology known as Convolutional Neural Network (CNNs) are relatively new in the machine learning that has been employed for computer vision in the field of identification of defects in textile fabrics which needs to be done accurately and also autonomously. In this paper, we design a new machine learning specialized network to reduce the decision action for LSTM based on CNNs. That is a rather positive result; however, there are several critical concerns which can be addressed further: the differences in the texture of the fabric, the unpredictable nature of the defects' distribution, and the lack of large-labeled datasets. owing to the advancements in feature extraction and segmentation new approaches like transfer learning and deep CNN based models like U-net, Res net etc. have begun to solve the issue. Unfortunately, efforts to improve these systems' effectiveness and reliability are still challenged by issues related to high calculability costs, sample over-learning and the need for vast annotated dataset. Therefore, the aim of this work is to establish a defect detecting model with high reliability and generalization ability for different textile fabrics using CNN technique. Besides, this study helps in gaining insights into other strategies that could be deployed in order to leverage on those strategies while using slightly more complex mixed structures and modifying loss function.},
  keywords={Costs;Clothing;Transfer learning;Transforms;Fabrics;Convolutional neural networks;Reliability;Textile industry;Standards;Defect detection;Specific algorithms;acronyms Convolutional Neural Network CNN;Deep Learning Network;Machine;Learning;Image Processing Methods},
  doi={10.1109/ICSES63445.2024.10763088},
  ISSN={},
  month={Oct},}@ARTICLE{10216316,
  author={Fang, Zhen and Lu, Jie and Zhang, Guangquan},
  journal={IEEE Transactions on Cybernetics}, 
  title={An Extremely Simple Algorithm for Source Domain Reconstruction}, 
  year={2024},
  volume={54},
  number={3},
  pages={1921-1933},
  abstract={The aim of unsupervised domain adaptation (UDA) is to utilize knowledge from a source domain to enhance the performance of a given target domain. Due to the lack of accessibility to the target domain’s labels, UDA’s efficacy is highly reliant on the source domain’s quality. However, it is often impractical and expensive to obtain an appropriate transferable source domain. To address this issue, we propose a novel UDA setting, source domain reconstruction (SDR), which seeks to construct a new transferable source domain utilizing labeled source samples and unlabeled target samples. SDR has a significant advantage over the conventional method as it is much less expensive to construct a suitable pseudo-source domain rather than collecting an actual transferable source domain in real-world scenarios. To test the practice of SDR, we investigate SDR theoretically. We propose an easily implementable algorithm, the domain MixUp (DMU), which is motivated by the MixUp strategy, to solve the SDR problem. The algorithm can be used to design a UDA framework to significantly enhance the performance of several existing UDA algorithms. Results from extensive experiments conducted on seven benchmarks  $(66$  UDA tasks) indicate that the reconstructed source domain has stronger transferability than the original source domain.},
  keywords={Indexes;Prediction algorithms;Task analysis;Transfer learning;Training;Interpolation;Image reconstruction;Domain adaptation;transfer learning},
  doi={10.1109/TCYB.2023.3298830},
  ISSN={2168-2275},
  month={March},}@ARTICLE{10510481,
  author={Qi, Lei and Liu, Ziang and Shi, Yinghuan and Geng, Xin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Generalizable Metric Network for Cross-Domain Person Re-Identification}, 
  year={2024},
  volume={34},
  number={10},
  pages={9039-9052},
  abstract={Person Re-identification (Re-ID) is a crucial technique for public security and has made significant progress in supervised settings. However, the cross-domain (i.e., domain generalization) scene presents a challenge in Re-ID tasks due to unseen test domains and domain-shift between the training and test sets. To tackle this challenge, most existing methods aim to learn domain-invariant or robust features for all domains. In this paper, we observe that the data-distribution gap between the training and test sets is smaller in the sample-pair space than in the sample-instance space. Based on this observation, we propose a Generalizable Metric Network (GMN) to further explore sample similarity in the sample-pair space. Specifically, we add a Metric Network (M-Net) after the main network and train it on positive and negative sample-pair features, which is then employed during the test stage. Additionally, we introduce the Dropout-based Perturbation (DP) module to enhance the generalization capability of the metric network by enriching the sample-pair diversity. Moreover, we develop a Pair-Identity Center (PIC) loss to enhance the model’s discrimination by ensuring that sample-pair features with the same pair-identity are consistent. We validate the effectiveness of our proposed method through a lot of experiments on multiple benchmark datasets and confirm the value of each module in our GMN.},
  keywords={Task analysis;Training;Extraterrestrial measurements;Public security;Labeling;Feature extraction;Cameras;Identification of persons;Generalizable metric network;domain generalization;person re-identification},
  doi={10.1109/TCSVT.2024.3395411},
  ISSN={1558-2205},
  month={Oct},}@ARTICLE{10292696,
  author={Chang, Dongliang and Sain, Aneeshan and Ma, Zhanyu and Song, Yi-Zhe and Wang, Ruiping and Guo, Jun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Mind the Gap: Open Set Domain Adaptation via Mutual-to-Separate Framework}, 
  year={2024},
  volume={34},
  number={6},
  pages={4159-4174},
  abstract={Unsupervised domain adaptation aims to leverage labeled data from a source domain to learn a classifier for an unlabeled target domain. Amongst its many variants, open set domain adaptation (OSDA) is perhaps the most challenging one, as it further assumes the presence of unknown classes in the target domain. In this paper, we study OSDA with a particular focus on enriching its ability to traverse across larger domain gaps, and we show that existing state-of-the-art methods suffer a considerable performance drop in the presence of larger domain gaps, especially on a new dataset (PACS) that we re-purposed for OSDA. Exploring this is pivotal for OSDA as with increasing domain shift, identifying unknown samples in the target domain becomes harder for the model, thus making negative transfer between source and target domains more challenging. Accordingly, we propose a Mutual-to-Separate (MTS) framework to address the larger domain gaps. Essentially we design two networks – (a) Sample Separation Network (SSN): which is trained to learn a hyperplane for separating unknown samples from known ones, and (b) Distribution Matching Network (DMN): which is trained to maximise domain confusion between source and target domains without unknown samples under the guidance of the SSN. The key insight lies in how we exploit the mutually beneficial information between these two networks. On closer observation, we see that SSN can reveal which samples in the target domain belong to the unknown class by instance weighting whereas, DMN pushes apart the samples that most likely belong to the unknown class in the target domain, which in turn reduces the difficulty of SSN in identifying unknown samples. It follows that (a) and (b) will mutually supervise each other and alternate until convergence, which can better align the source and target domains in the shared label space. Extensive experiments on five datasets (Office-31, Office-Home, PACS, VisDA, and  $mini$ _DomainNet) demonstrate the efficiency of the proposed method. Detailed ablation experiments also validate the effectiveness of each component and the generality of the proposed framework. Codes are available at: https://github.com/PRIS-CV/Mutual-to-Separate.},
  keywords={Picture archiving and communication systems;Training;Task analysis;Labeling;Adaptation models;Visualization;Information exchange;Domain adaptation;open set;mutual learning;transfer learning},
  doi={10.1109/TCSVT.2023.3326862},
  ISSN={1558-2205},
  month={June},}@INPROCEEDINGS{10611650,
  author={Wang, Yongliang and Mokhtar, Kamal and Heemskerk, Cock and Kasaei, Hamidreza},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Self-supervised Learning for Joint Pushing and Grasping Policies in Highly Cluttered Environments}, 
  year={2024},
  volume={},
  number={},
  pages={13840-13847},
  abstract={Robotic systems often face challenges when attempting to grasp a target object due to interference from surrounding items. We propose a Deep Reinforcement Learning (DRL) method that develops joint policies for grasping and pushing, enabling effective manipulation of target objects within untrained, densely cluttered environments. In particular, a dual RL model is introduced, which presents high resilience in handling complicated scenes, reaching an average of 98% task completion in simulation and real-world scenes. To evaluate the proposed method, we conduct comprehensive simulation experiments in three distinct environments: densely packed building blocks, randomly positioned building blocks, and common household objects. Further, real-world tests are conducted using actual robots to confirm the robustness of our approach in various untrained and highly cluttered environments. The results from experiments underscore the superior efficacy of our method in both simulated and real-world scenarios, outperforming recent state-of-the-art methods. To ensure reproducibility and further the academic discourse, we make available a demonstration video, the trained models, and the source code for public access. https://sites.google.com/view/pushandgrasp/home.},
  keywords={Source coding;Grasping;Self-supervised learning;Interference;Robustness;Reproducibility of results;Task analysis},
  doi={10.1109/ICRA57147.2024.10611650},
  ISSN={},
  month={May},}@ARTICLE{10584066,
  author={Wang, Hanlin and Yang, Shuyuan and Feng, Zhixi and Huang, Bincheng},
  journal={IEEE Internet of Things Journal}, 
  title={Semi-Supervised Modulation Classification via an Ensemble SigMatch Method}, 
  year={2024},
  volume={11},
  number={20},
  pages={32985-32997},
  abstract={In recent years, data-driven deep learning methods have significantly improved the performance of automatic modulation classification (AMC). However, labeling the vast number of signal samples obtained in a complex electromagnetic environment is challenging due to data security concerns and the drain on manpower and material resources. The scarcity of labeled samples constrains the applicability of these methods. In this article, an ensemble SigMatch (ESM) semi-supervised AMC method is proposed to fully leverage the unlabeled modulated signals. First, a SigMatch (SM) semi-supervised AMC framework is proposed, combining pseudo-labeling, consistency regularization, and modulated signal augmentation for direct identification of raw timing signals. Three different types of signal augmentation methods are investigated through mathematical analysis of the signal model. Second, based on SM and multiview learning, the ESM method is proposed to further enhance the performance of semi-supervised AMC through consistency learning of multiple augmentation views of unlabeled signals. A multiview consistency loss is designed in ESM, with additional data augmentation as complementary views. Multiple perturbed views are guided by the same sample to achieve consistent classification through a shared classification model, thus achieving more robust feature representation. Our method demonstrates remarkable performance on data sets RML2016.10A and RML2016.04C, especially with few labeled samples. On RML2016.10A, with only 110 labeled samples, the ESM enhances the overall classification accuracy from 35.77% to 70.44% compared with supervised learning.},
  keywords={Feature extraction;Modulation;Convolutional neural networks;Adaptation models;Internet of Things;Training;Automatic modulation classification (AMC);modulated signal augmentation;multiview learning;semi-supervised learning},
  doi={10.1109/JIOT.2024.3422648},
  ISSN={2327-4662},
  month={Oct},}@INPROCEEDINGS{9956318,
  author={Li, Youyu and Song, Xiaoning and Xu, Tianyang and Feng, Zhenhua},
  booktitle={2022 26th International Conference on Pattern Recognition (ICPR)}, 
  title={Memory-Token Transformer for Unsupervised Video Anomaly Detection}, 
  year={2022},
  volume={},
  number={},
  pages={3325-3332},
  abstract={Video anomaly detection is crucial for behavior analysis, which has witnessed continuous progress in recent years with the auto-encoder based reconstruction framework. However, in some cases, abnormal frames may also be reconstructed well due to the strong representation ability of deep networks, increasing missed detection. To mitigate this issue, the existing methods usually the memory bank method. This method records normal patterns and assigns high errors for the reconstruction of abnormal frames into normal frames. In this paper, to better use the semantic information of normal videos recorded in the memory module, we introduce the Memory-Token Transformer (MTT) to boost the reconstruction performance on normal frames. We assume that the anomalies in a video mainly concentrate on the regions containing people and relevant objects. Therefore, during the decoding stage, we first extract the semantic concepts of a feature map and generate the corresponding semantic tokens. Then the tokens are combined with the proposed memory module. Last, we introduce a transformer to fuse the complex relationship among different tokens, and use 3D convolution with the pooling operator in our encoder to enhance spatio-temporal feature extraction as compared with 2D models. The experimental results obtained on various benchmarks demonstrate the effectiveness of the proposed method.},
  keywords={Three-dimensional displays;Convolution;Semantics;Video sequences;Memory modules;Benchmark testing;Transformers},
  doi={10.1109/ICPR56361.2022.9956318},
  ISSN={2831-7475},
  month={Aug},}@ARTICLE{9520398,
  author={Ma, Hongbin and Yang, Shuyuan and He, Guangjun and Wu, Ruowu and Hao, Xiaojun and Li, Tingpeng and Feng, Zhixi},
  journal={IEEE Access}, 
  title={Faking Signals to Fool Deep Neural Networks in AMC via Few Data Points}, 
  year={2021},
  volume={9},
  number={},
  pages={124425-124433},
  abstract={The recent years has witnessed a rapid development of Deep Learning (DL) based Automation Modulation Classification (AMC) methods, which has proved to outperform traditional classification approaches. In order to disturb the deep neural networks for AMC, in this paper, we propose an adversarial attack method to generate fake signals for fooling DL-based classifiers. Firstly, some constraints on visual difference and recoverability of fake signals are defined. Next, a Few Data Point Attacker (FDPA) is proposed to generate fake signals with few perturbed data points via differential evolution algorithm. Some experiments are taken on a public dataset, RML 2016.10a, and the results show that fake signals generated by the FDPA can remarkably reduce the accuracies of three types of DL-based AMC classifiers, a Convolutional Neural Network (CNN) based classifier, a Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) based classifier, and a classifier combined with CNN and LSTM-RNN. The code will be available.},
  keywords={Modulation;Visualization;Perturbation methods;Convolutional neural networks;Deep learning;Statistics;Sociology;Automatic modulation classification;fake signals;few data point attacker (FDPA);differential evolution},
  doi={10.1109/ACCESS.2021.3106704},
  ISSN={2169-3536},
  month={},}@ARTICLE{10168168,
  author={Cen, Jiazhong and Jiang, Zekun and Xie, Lingxi and Jiang, Dongsheng and Shen, Wei and Tian, Qi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Consensus Synergizes With Memory: A Simple Approach for Anomaly Segmentation in Urban Scenes}, 
  year={2024},
  volume={34},
  number={2},
  pages={1086-1097},
  abstract={Anomaly segmentation is a critical task for safety-critical applications, such as autonomous driving in urban environments. Its objective is to detect out-of-distribution (OOD) samples with unseen categories, given a pre-trained segmentation model. The core challenge of this task is how to distinguish hard in-distribution samples from OOD samples, which has not been explicitly discussed in previous research. In this paper, we propose a simple yet effective approach named CosMe (Consensus Synergizes with Memory) to address this challenge. CosMe consists of two key components: 1) building a memory bank comprising seen prototypes extracted from multiple layers of the given segmentation model, and 2) training an auxiliary model that mimics the behavior of the given model and using the consensus of their mid-level features as complementary cues that synergize with the memory bank. The former serves as a baseline that can detect all potential outliers, including both OOD and hard in-distribution samples; the latter assists in distinguishing between these two types of outliers. Experimental results on several urban scene anomaly segmentation datasets demonstrate that CosMe outperforms previous approaches by a significant margin.},
  keywords={Training;Task analysis;Uncertainty;Prototypes;Feature extraction;Image reconstruction;Autonomous vehicles;Semantic segmentation;anomaly detection;clustering},
  doi={10.1109/TCSVT.2023.3290604},
  ISSN={1558-2205},
  month={Feb},}@ARTICLE{10495745,
  author={Jiao, Yifan and Yao, Hantao and Bao, Bing-Kun and Xu, Changsheng},
  journal={IEEE Transactions on Image Processing}, 
  title={Source-Guided Target Feature Reconstruction for Cross-Domain Classification and Detection}, 
  year={2024},
  volume={33},
  number={},
  pages={2808-2822},
  abstract={Existing cross-domain classification and detection methods usually apply a consistency constraint between the target sample and its self-augmentation for unsupervised learning without considering the essential source knowledge. In this paper, we propose a Source-guided Target Feature Reconstruction (STFR) module for cross-domain visual tasks, which applies source visual words to reconstruct the target features. Since the reconstructed target features contain the source knowledge, they can be treated as a bridge to connect the source and target domains. Therefore, using them for consistency learning can enhance the target representation and reduce the domain bias. Technically, source visual words are selected and updated according to the source feature distribution, and applied to reconstruct the given target feature via a weighted combination strategy. After that, consistency constraints are built between the reconstructed and original target features for domain alignment. Furthermore, STFR is connected with the optimal transportation algorithm theoretically, which explains the rationality of the proposed module. Extensive experiments on nine benchmarks and two cross-domain visual tasks prove the effectiveness of the proposed STFR module, e.g., 1) cross-domain image classification: obtaining average accuracy of 91.0%, 73.9%, and 87.4% on Office-31, Office-Home, and VisDA-2017, respectively; 2) cross-domain object detection: obtaining mAP of 44.50% on Cityscapes  $\rightarrow $  Foggy Cityscapes, AP on car of 78.10% on Cityscapes  $\rightarrow $  KITTI, MR $^{-2}$  of 8.63%, 12.27%, 22.10%, and 40.58% on COCOPersons  $\rightarrow $  Caltech, CityPersons  $\rightarrow $  Caltech, COCOPersons  $\rightarrow $  CityPersons, and Caltech  $\rightarrow $  CityPersons, respectively.},
  keywords={Image reconstruction;Feature extraction;Visualization;Task analysis;Image classification;Object detection;Telecommunications;Source-guided target feature reconstruction;cross-domain image classification;cross-domain object detection},
  doi={10.1109/TIP.2024.3384766},
  ISSN={1941-0042},
  month={},}@ARTICLE{10401239,
  author={Zhao, Zhicheng and Wang, Chun and Li, Chenglong and Zhang, Yong and Tang, Jin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Modality Conversion Meets Superresolution: A Collaborative Framework for High- Resolution Thermal UAV Image Generation}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={Due to the limitations and costs of thermal sensors, unmanned aerial vehicle (UAV) platforms often equip with high-resolution (HR) visible imaging and low-resolution (LR) thermal imaging cameras for all-day monitoring capability. Existing works generate the HR thermal UAV images by either superresolution (SR) from HR visible and LR thermal images or modality conversion (MC) from HR visible images. However, the modality gap between visible and thermal sources might degrade the generation quality. We observe that the MC task is beneficial in addressing the cross-modal gap in the SR task, while the SR task can provide the condition of thermal information to boost the MC task. Moreover, these two tasks have the same output and can thus be carried out simultaneously without any additional annotation. Based on this observation, we propose a collaborative enhancement network (CENet), which performs thermal UAV image SR and visible image MC in a joint manner, for HR thermal UAV image generation. In particular, we design a mutual guidance module (MGM) to interact the features from SR and MC tasks in an alternating bidirectional manner. Considering that low-level vision tasks are position-sensitive, to further enhance the feature alignment between the two tasks, we design a bidirectional alignment fusion module (BAFM) to maintain feature consistency of the MC and SR branches. The proposed collaborative framework not only achieves joint and unified training of the two tasks, but also generates two types of complementary HR images. Extensive experiments on public datasets demonstrate that the proposed CENet outperforms current state-of-the-art SR methods in generating HR thermal UAV images, as quantified by peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM).},
  keywords={Task analysis;Superresolution;Autonomous aerial vehicles;Feature extraction;Remote sensing;Imaging;Collaboration;Collaborative learning;modality conversion (MC);remote sensing;thermal image superresolution (SR);unmanned aerial vehicle (UAV)},
  doi={10.1109/TGRS.2024.3354878},
  ISSN={1558-0644},
  month={},}@ARTICLE{9698977,
  author={Yu, Weijie and Xu, Chen and Xu, Jun and Pang, Liang and Wen, Ji-Rong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Distribution Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains}, 
  year={2022},
  volume={30},
  number={},
  pages={721-733},
  abstract={Projecting the input text pair into a common semantic space where the matching function can be readily learned is an essential step for asymmetrical text matching. In the practice, it is often observed that the feature vectors from asymmetrical texts show a tendency to be gradually undistinguishable in the semantic space as the model is trained. However, the phenomenon is overlooked in existing studies. As a result, the feature vectors are constructed without any regularization, which inevitably hinders the learning of the downstream matching functions. In this paper, we first exploit the phenomenon and propose DDR-Match, a novel matching framework tailored for asymmetrical text matching. Specifically, in DDR-Match, a distribution distance-based regularizer is devised to accelerate the fusion of sequence representations corresponding to different domains in the semantic space. Then, we provide three instances of DDR-Match and make a comparison among them. DDR-Match is compatible with existing text matching methods by incorporating them as the underlying matching model. Four popular text matching methods are exploited in the paper. Extensive experimental results based on five publicly available benchmarks showed that DDR-Match consistently outperformed its underlying methods.},
  keywords={Semantics;Neural networks;Training;Task analysis;Measurement;Speech processing;Electronic mail;Text matching;sequence representation;natural language processing},
  doi={10.1109/TASLP.2022.3145289},
  ISSN={2329-9304},
  month={},}@ARTICLE{10411896,
  author={Tan, Xi and Liu, Xuan and Xiang, Kai and Wang, Jing and Tan, Shan},
  journal={IEEE Access}, 
  title={Deep Filtered Back Projection for CT Reconstruction}, 
  year={2024},
  volume={12},
  number={},
  pages={20962-20972},
  abstract={Filtered back projection (FBP) is a classic analytical algorithm for computed tomography (CT) reconstruction, with high computational efficiency. However, images reconstructed by FBP often suffer from excessive noise and artifacts. The original FBP algorithm uses a window function to smooth signals and a linear interpolation to estimate projection values at un-sampled locations. In this study, we propose a novel framework named DeepFBP in which an optimized filter and an optimized nonlinear interpolation operator are learned with neural networks. Specifically, the learned filter can be considered as the product of an optimized window function and the ramp filter, and the learned interpolation can be considered as an optimized way to utilize projection information of nearby locations through nonlinear combination. The proposed method remains the high computational efficiency of the original FBP and achieves much better reconstruction quality at different noise levels. It also outperforms the TV-based statistical iterative algorithm, with computational time being reduced in an order of two, and state-of-the-art post-processing deep learning methods that have deeper and more complicated network structures.},
  keywords={Image reconstruction;Information filters;Interpolation;Filtering algorithms;Computed tomography;Deep learning;Iterative algorithms;Analytical reconstruction;deep learning;FBP;neural network},
  doi={10.1109/ACCESS.2024.3357355},
  ISSN={2169-3536},
  month={},}@ARTICLE{9316308,
  author={Luo, Ying and Zhao, Hai and Zhang, Zhuosheng and Tang, Bingjie},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Open Named Entity Modeling From Embedding Distribution}, 
  year={2022},
  volume={34},
  number={11},
  pages={5472-5483},
  abstract={In this paper, we report our discovery on named entity distribution in a general word embedding space, which helps an open definition on multilingual named entity definition rather than previous closed and constraint definition on named entities through a named entity dictionary, which is usually derived from human labor and replies on schedule update. Our initial visualization of monolingual word embeddings indicates named entities tend to gather together despite of named entity types and language difference, which enable us to model all named entities using a specific geometric structure inside embedding space, namely, the named entity hypersphere. For monolingual cases, the proposed named entity model gives an open description of diverse named entity types and different languages. For cross-lingual cases, mapping the proposed named entity model provides a novel way to build a named entity dataset for resource-poor languages. At last, the proposed named entity model may be shown as a handy clue to enhance state-of-the-art named entity recognition systems generally.},
  keywords={Dictionaries;Task analysis;Hidden Markov models;Annotations;Organizations;Internet;Encyclopedias;Named entity recognition;embedding distribution;hypersphere;cross-lingual},
  doi={10.1109/TKDE.2021.3049654},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{10568963,
  author={Cheng, Cheng and Wu, Yudie and Liu, Qianjun and Hua, Feng and Zhang, Yong and He, Xin},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Fast High-Dimensional Parameter Optimization for Turbine Blade Manufacturing Using the Powerball L-BFGS Method Under Incomplete Measurements}, 
  year={2024},
  volume={73},
  number={},
  pages={1-12},
  abstract={Proper optimization of machining parameters can effectively improve production efficiency and precision in high-precision machining, such as turbine blade manufacturing. However, in industrial practice, current studies on the optimization of process parameters suffer from two drawbacks: the first is missing measurement values due to network latency and packet loss, while the second is the slow convergence rate of high-dimensional parameters to be optimized. This article proposes a prediction and optimization framework based on an accelerated limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm; the framework addresses the high-dimensional parameter optimization issues in high-precision machining processes, under incomplete sensor measurement scenarios. Incomplete measurements are imputed based on learned data distributions, significantly improving data utilization of the subsequent deep learning model trained after imputation. Then, to achieve fast high-dimensional parameter optimization, we introduce an accelerated L-BFGS algorithm, termed Powerball L-BFGS, with a Powerball function to optimize the search direction. To evaluate the effectiveness of the proposed framework, we conduct experiments on two series of aviation turbine blades (named Blade A and Blade B) with complex curved surfaces. Incomplete coordinate measuring machine data at key stages (i.e., blade root milling and comprehensive accurate milling) are used as inputs to predict and optimize the final geometrical errors. The proposed Powerball L-BFGS reduces the final optimization root mean square error (RMSE) to 0.053 mm for Blade A and 0.028 mm for Blade B; it decreases optimization iterations from 60 to 21 for Blade A and from 85 to 32 for Blade B, and achieving maximum process capability values of 5.23 and 2.39, respectively.},
  keywords={Blades;Optimization;Milling;Turbines;Manufacturing;Accuracy;Predictive models;Accelerated optimization algorithm;deep learning;high-precision machining;incomplete measurement imputation;parameter optimization},
  doi={10.1109/TIM.2024.3418071},
  ISSN={1557-9662},
  month={},}@ARTICLE{10131985,
  author={Liu, Tao and Liu, Jiahao and Li, Dong and Tan, Shan},
  journal={IEEE Journal of Selected Topics in Quantum Electronics}, 
  title={Improving Reconstruction of Structured Illumination Microscopy Images via Dual-Domain Learning}, 
  year={2023},
  volume={29},
  number={6: Photonic Signal Processing},
  pages={1-12},
  abstract={Structured illumination microscopy (SIM) provides an enhanced resolving power surpassing the optical diffraction limit by optical modulation of patterned illuminations. Although end-to-end deep learning techniques have recently advanced the reconstruction of SIM images, the reconstruction fidelity of existing networks is still moderate. We experimentally point out the crux lies in the inability of these models for faithful frequency learning. As a remedy, we propose a dual-domain learning strategy for SIM reconstruction, namely DDL-SIM, which learns to reconstruct SIM images from raw images in the spatial domain and raw image spectra in the frequency domain simultaneously, with the goal of narrowing the reconstruction gaps in both domains, thereby better recovering modulated frequencies and resolving more fine structures. Reconstruction experiments across various biological structures demonstrate the proposed DDL-SIM significantly improves the reconstruction fidelity of SIM images and shows great robustness against reconstruction artifacts.},
  keywords={Image reconstruction;Frequency-domain analysis;Imaging;Convolutional neural networks;Lighting;Data models;Deep learning;Complex networks;Deep learning;Structured illumination microscopy;super-resolution;complex neural network;deep learning},
  doi={10.1109/JSTQE.2023.3279341},
  ISSN={1558-4542},
  month={Nov},}@INPROCEEDINGS{10446049,
  author={Liu, Tao and Du, Chenpeng and Fan, Shuai and Chen, Feilong and Yu, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={DiffDub: Person-Generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-Encoder}, 
  year={2024},
  volume={},
  number={},
  pages={3630-3634},
  abstract={Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, including data augmentation and supplementary eye guidance. Moreover, we encapsulated a conformer-based reference encoder and motion generator fortified by a cross-attention mechanism. This enables our model to learn person-specific textures with varying references and reduces reliance on paired audio-visual data. Our rigorous experiments comprehensively highlight that our ground-breaking approach outpaces existing methods with considerable margins and delivers seamless, intelligible videos in person-generic and multilingual scenarios.},
  keywords={Training;Visualization;Technological innovation;Semantics;Synchronization;Speech processing;Videos;Talking Face;Diffusion;Face Animation;Dubbing},
  doi={10.1109/ICASSP48485.2024.10446049},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10013940,
  author={Li, Wei and Guo, Haiyun and Dong, Honghui and Tang, Ming and Zhou, Yue and Wang, Jinqiao},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Bi-Level Implicit Semantic Data Augmentation for Vehicle Re-Identification}, 
  year={2023},
  volume={24},
  number={4},
  pages={4364-4376},
  abstract={Vehicle re-identification (Re-ID) aims at finding the target vehicle identity from multi-camera surveillance videos, which plays an important role in the intelligent transportation system (ITS). It suffers from the subtle discrepancy among vehicles from the same vehicle model and large variation across different viewpoints of the same vehicle. To enhance the robustness of Re-ID models, many methods exploit additional detection or segmentation models to extract discriminative local features. Some others employ data-driven methods to enrich the diversity of the training data, such as the data augmentation and 3D-based data generation, so that the Re-ID model can obtain stronger robustness against intra-class variations. However, these methods either rely on extra annotations or greatly increase the computational cost. In this paper, we propose the Bi-level Implicit semantic Data Augmentation (BIDA) framework to solve this problem from two aspects. (1) We implicitly augment the images semantically in the feature space according to the identity-level and superclass-level intra-class variations, which can generate more diverse semantic augmentations beyond the intra-identity variations. (2) We introduce the similarity ranking constraints on the augmented training set by extending the sample-wise triplet loss to the distribution-wise one, which can effectively reduce meaningless semantic transformations and improve the discrimination of the feature. We conduct extensive experiments on VeRi-776, VehicleID and Cityflow benchmarks to reveal the effectiveness of our method. And we achieve new state-of-the-art performance on VeRi-776.},
  keywords={Semantics;Training;Space vehicles;Data models;Feature extraction;Training data;Robustness;Vehicle re-identification;implicit semantic data augmentation;covariance matrices;triplet-based ranking constraint},
  doi={10.1109/TITS.2023.3234644},
  ISSN={1558-0016},
  month={April},}@INPROCEEDINGS{9694140,
  author={Elhakiem, Ahmed. A. and Elsaid Ghoniemy, Tarek and Salama, Gouda I.},
  booktitle={2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
  title={Astronomical image denoising based on Convolutional Neural Network}, 
  year={2021},
  volume={},
  number={},
  pages={51-56},
  abstract={Astronomical images captured using optical telescopes usually suffer from severe noise effects which makes the denoising step inevitable for image analysis. This paper proposes a denoising framework for astronomical images based on Convolutional Neural Network (Astro U-net). The modified Astro U-net model has been learned in four ways, the first method is using astronomical images from the Hubble Space Telescope data set with three types of noise (dark noise, read- out noise, shot noise) added, the second method is learned using the same data set with the dark noise (dn) added only, the third method is using the same data set with the read-out noise (ron) overlaid, the fourth method is using the same data set with the shot noise (sn) added. The proposed framework for denoising the astronomical images is based on a fusion of the image that was improved by the model learned in the first method with the image that was improved by the three models that were learned by the second, third and fourth methods sequentially. Experimentally, the proposed framework shows a significant improvement in both the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM) as compared to the Astro U-net model on different exposure time ratios.},
  keywords={Adaptation models;PSNR;Noise reduction;Optical computing;Telescopes;Optical imaging;Data models;Astronomical image processing;Image denoising;Convolutional Neural Networks;Image Fusion},
  doi={10.1109/ICICIS52592.2021.9694140},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10580849,
  author={He, Pinyao and Huang, Jingyue and Li, Ming},
  booktitle={2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Text Keyword Extraction Based on GPT}, 
  year={2024},
  volume={},
  number={},
  pages={1394-1398},
  abstract={This study investigates text keyword and phrase extraction methods based on the GPT-3.5 model,and validates their effectiveness through comparative analysis. Initially, researchers employ the GPT-3.5 model for extracting keywords and phrases from text to uncover crucial information within the text. Subsequently, the extracted data from GPT-3.5 is compared with the key text from the original dataset to assess extraction performance and consistency. Lastly, extracted keywords are utilized for sentiment analysis, conducting comparative experiments with the BERT-TextCNN model, and validation across diverse datasets. The research findings demonstrate the GPT-3.5 model's capability to efficiently extract crucial textual information and significantly enhance sentiment classification precision. This enhancement contributes to improved performance and interpretability in text analysis, thereby providing substantial support for the field of natural language processing.},
  keywords={Training;Sentiment analysis;Analytical models;Technological innovation;Text analysis;Annotations;Social networking (online);GPT;Key text extraction;BERT-TextCNN;Sentiment analysis},
  doi={10.1109/CSCWD61410.2024.10580849},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{10219633,
  author={Zhou, Xun and Sun, Wujin and Shi, Xiaodong},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={A High-Quality Melody-Aware Peking Opera Synthesizer Using Data Augmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1092-1097},
  abstract={The performing art of Peking Opera places great demands on the singing skills of singers, including pronunciation, melody, role, personal style and emotional expression, which poses a great challenge to Peking Opera singing voice synthesis. In this paper, we propose OperaSinger, following the main architecture of FastSpeech 2, using the features from the musical score as input, while improving the encoder in FastSpeech 2 by employing a stack of melody-aware location-variable convolution blocks in parallel with feed-forward Transformer blocks to alleviate the lack of naturalness caused by ignoring relatively local features. Due to the limitation of publicly available opera data, we explore several novel data augmentation methods to boost the training of OperaSinger. Extensive experiment results have demonstrated that 1) OperaSinger can generate high-quality Peking Opera samples (MOS 3.80) with naturalness and expressiveness; 2) the proposed data augmentation methods effectively improve performance on both subjective and objective evaluations.},
  keywords={Training;Art;Convolution;Synthesizers;Training data;Focusing;Data augmentation;Singing Voice Synthesis;Feed-forward Transformer;Data Augmentation},
  doi={10.1109/ICME55011.2023.00191},
  ISSN={1945-788X},
  month={July},}@ARTICLE{10955329,
  author={Li, Jing and Bai, Lu and Yang, Bin and Li, Chang and Ma, Lingfei},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Graph Representation Learning for Infrared and Visible Image Fusion}, 
  year={2025},
  volume={22},
  number={},
  pages={13801-13813},
  abstract={Infrared and visible image fusion aims to extract complementary features to synthesize a single fused image. In our method, we covert the regular image format into the graph space and conduct graph convolutional networks (GCNs) to extract NLss for the reliable infrared and visible image fusion. More specifically, GCNs are first performed on each intra-modal set to aggregate the features and propagate the inherent information, thereby extracting independent intra-modal NLss. Then, such intra-modal non-local self-similarity (NLss) features of infrared and visible images are concatenated to explore cross-domain NLss inter-modally and reconstruct the fused images. Extensive experiments show the superior performance of our method with the qualitative and quantitative analysis on the TNO, RoadScene and M3FD datasets, respectively, outperforming many state-of-the-art (SOTA) methods for the robust and effective infrared and visible image fusion. Note to Practitioners—This paper was motivated by the problem that the most existing methods that employ convolutional neural networks (CNNs) and transformer-based frameworks mainly extract local features and long-range dependence. However, they often cause overlooking the image’s NLss or information redundancy, resulting in poor infrared and visible image fusion. To address these problems, graph-based data representations can construct relationships among spatially repeatable details or textures with far-space distances, which are more suitable for handling irregular objects. Therefore, it is significant to covert the regular image format into the graph space and conduct graph convolutional networks (GCNs) to extract NLss for the reliable infrared and visible image fusion. In this paper, we develop an infrared and visible image fusion method based on graph representation learning strategy.},
  keywords={Feature extraction;Transformers;Image fusion;Graph convolutional networks;Data mining;Electronic mail;Fuses;Convolution;Training;Image reconstruction;Infrared image;visible image;image fusion;graph convolutional networks},
  doi={10.1109/TASE.2025.3557234},
  ISSN={1558-3783},
  month={},}@ARTICLE{10697214,
  author={Li, Chengyang and Sun, Fangwei and Zhou, Heng and Xie, Yongqiang and Li, Zhongbo and Zhu, Liping},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Multi-Weather Restoration: An Efficient Prompt-Guided Convolution Architecture}, 
  year={2025},
  volume={35},
  number={2},
  pages={1436-1450},
  abstract={Addressing degraded weather conditions plays a vital role in practical applications. Many existing restoration approaches are limited to specific weather types, which limits their applicability to different weather scenarios. Advanced technologies, encompassing Transformer and diffusion model, have been harnessed to confront this challenge. However, these methods often heighten network complexity and prolong inference duration. To this end, we present MW-ConvNet, a U-shaped convolution-based network for multi-weather restoration. Specifically, the MW-Enc block and MW-Dec block are introduced to achieve simple yet strong feature extraction, which rely entirely on traditional 2D convolution. To improve adaptability to multiple weather conditions, a prompt generation module is designed to generate a representative weather prompt at the encoder’s terminus. Drawing inspiration from style transfer, the weather prompt is used to guide the decoder learning through a progressive restoration procedure. For future high-fidelity restoration, we introduce frequency separation through wavelet pooling blocks in encoder phase and corresponding up-sampling blocks in decoder phase. The segregated treatment of low-frequency and high-frequency features curbs the loss of textural information during network computation. It also future improves the quality and accuracy of generated weather prompt. Extensive experiments demonstrate that the proposed MW-ConvNet obtains superior performance compared to state-of-the-art methods across both weather-specific and real-world restoration tasks. Significantly, our method achieves an impressive inference speed of 0.12 seconds per  $256\times 256$  image, outpacing transformer-based and diffusion-based models.},
  keywords={Image restoration;Meteorology;Transformers;Computer architecture;Decoding;Rain;Feature extraction;Degradation;Convolution;Training;Image restoration;multi-weather;deraining;desnowing;raindrop removal},
  doi={10.1109/TCSVT.2024.3469190},
  ISSN={1558-2205},
  month={Feb},}@ARTICLE{10345728,
  author={Wang, Yajie and Chen, Mulin and Li, Xuelong},
  journal={IEEE Transactions on Multimedia}, 
  title={Continuous Emotion-Based Image-to-Music Generation}, 
  year={2024},
  volume={26},
  number={},
  pages={5670-5679},
  abstract={Image-to-music generation aims to generate realistic pure music according to a given image. Although many previous works are conducted on bridging image and music, they mainly focus on the content-based cross-modal matching. For example, matching the Christmas song to an image that contains a Christmas tree. By comparison, image-to-music generation is a more challenging task due to its ambiguity and subjectivity. Specifically, there is no explicit correlation between the image content and music melody, without any lyric and human sound. Meanwhile, the perception of generated music varies from person to person. Inspired by the synesthesia phenomenon, we think that if an image tends to elicit a certain emotion on human, the generated music should also leave a similar impression. Therefore, in this paper, we propose a continuous emotion-based image-to-music generation framework, which uses emotion as the key for cross-modal generation. Specifically, a new image-music dataset is established, which uses valence-arousal (VA) space to capture the complex and nuanced nature of emotions. After that, a plug and play model is proposed to translate an image into a piece of music with similar emotion, which projects the emotions into continuous-valued labels, and explores both the intra-modal and inter-modal emotional consistency with contrastive learning. To our best knowledge, this is the first end-to-end framework towards the task of pure music generation from natural images. Extensive experiments show that the generated music achieves satisfactory emotional consistency with the input images, as well as impressive quality.},
  keywords={Task analysis;Music;Distance measurement;Annotations;Aerospace electronics;Training;Standards;Image-to-music generation;valence-arousal space;multi-modal cognitive computing;vicinagearth security},
  doi={10.1109/TMM.2023.3338089},
  ISSN={1941-0077},
  month={},}@ARTICLE{10292891,
  author={Zhang, Han and Li, Yiding and Li, Xuelong},
  journal={IEEE Transactions on Multimedia}, 
  title={Constrained Bipartite Graph Learning for Imbalanced Multi-Modal Retrieval}, 
  year={2024},
  volume={26},
  number={},
  pages={4502-4514},
  abstract={Real-worldmulti-modal retrieval tasks always encounter modal imbalance scenarios. The scales of instances from different modalities are inconsistent and unaligned with each other. Though several methods alleviate the issue by establishing miscellaneous representations of multi-modal data, they still suffer from difficulties like laborious human-being annotations and complex common-space optimization. In the research, we present Constrained Bipartite Graph Learning (CBGL) for imbalanced correlations, where a size-flexible correlation graph is learned from instances' representations. To guide the graph learning, we take advantage of prior side information, including positive pairs and negative pairs, which readily express intra-modality affinity and inter-modality discrepancy. Accordingly, both positive and negative correlations are propagated over instances, and a similarity graph with satisfactory neighbors is achieved. Benefiting from the probabilistic similarities, a query graph is then naturally constructed that directly achieves multi-modal retrieval. To validate the effect, we build a Music-Video-Image (MVI) dataset in regard to music and images with imbalanced scales. Experimental results reported on MVI dataset and three benchmarks demonstrate our prominent superiority over ten representative competitors in multi-modal retrieval.},
  keywords={Correlation;Bipartite graph;Semantics;Task analysis;Optimization;Visualization;Annotations;Constrained bipartite graph;imbalanced modalities;multi-modal retrieval;query graph},
  doi={10.1109/TMM.2023.3323884},
  ISSN={1941-0077},
  month={},}@ARTICLE{10504144,
  author={Zheng, Aihua and Yuan, Fan and Zhang, Haichuan and Wang, Jiaxiang and Tang, Chao and Li, Chenglong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Public-Private Attributes-Based Variational Adversarial Network for Audio-Visual Cross-Modal Matching}, 
  year={2024},
  volume={34},
  number={9},
  pages={8698-8709},
  abstract={Existing audio-visual cross-modal matching methods focus on mitigating cross-modal heterogeneity but ignore the impact of intra-class discrepancy of the same identity in different scenarios, which might greatly limit the matching performance. To simultaneously handle both problems of intra-class discrepancy and cross-modal heterogeneity, we propose a novel public-private attributes-based variational adversarial network ( $P^{2}$ VANet), which captures the consistency within and between classes, for audio-visual cross-modal matching. In particular,  $P^{2}$ VANet first uses a variational auto-encoder, which captures the inherent global information in diverse scenarios from the hidden variable through reconstruction, to reduce the intra-class discrepancy. Then it integrates a public attributes guidance module to capture the consistency of audio and visual by supervision of the common high-level semantic information to mitigate cross-modal heterogeneity. In addition,  $P^{2}$ VANet designs a private attributes embedding module to enhance the discriminative features inherent in each class to decrease inter-class similarity. Extensive experiments on audio-visual cross-modal matching demonstrate the effectiveness of the proposed approach compared with the state-of-the-art methods.},
  keywords={Visualization;Semantics;Feature extraction;Face recognition;Adversarial machine learning;Task analysis;Decoding;Audio-visual cross-modal matching;variational adversarial learning;public-private attributes;metric learning},
  doi={10.1109/TCSVT.2024.3390573},
  ISSN={1558-2205},
  month={Sep.},}@INPROCEEDINGS{10650979,
  author={Zhu, Yaolong and Zhu, Ding and Liu, Juan},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={RA-LENet:R-Wave Attention and Local Enhancement for Noise Reduction in ECG Signals}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Cardiovascular disease is a major life-threatening condition commonly monitored through electrocardiogram (ECG) signals. However, the ECG signals currently generated by sensors are often accompanied by a plethora of diverse types of noise with different intensities, which causes a lot of interference in downstream tasks. In this work, we propose a deep learning based method for ECG signal denoising. Due to the different frequency characteristics of different types of noises, we use a Transformer with local enhancement as a feature extractor which can capture global dependencies. In addition, we introduce an R-wave attention mechanism to improve the most difficult R-wave reconstruction. Our experimental results demonstrate the effectiveness of our approach in denoising different types of strong noises, outperforming the state-of-the-art (SOTA) methods. The code is available at https://github.com/caprilovel/ECG_Denoise.},
  keywords={Noise;Noise reduction;Neural networks;Interference;Electrocardiography;Sensor phenomena and characterization;Feature extraction;ECG signal;denoising;Transformer},
  doi={10.1109/IJCNN60899.2024.10650979},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10150393,
  author={Behnke, Maciej and Saganowski, Stanislaw and Kaczmarek, Łukasz D. and Kazienko, Przernysław},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Emotions Studied by Computer Scientists and Psychologists — A Complementary Perspective}, 
  year={2023},
  volume={},
  number={},
  pages={206-211},
  abstract={Affective scientists from different fields work together to describe, understand, and harness human emotions. One specific collaboration between computer scientists and psychologists created the new field of affective computing. Computer scientists bring to the field perspective resulting from education in Physical Sciences and Engineering, whereas psychologists from Social Sciences and Humanities. This article first addresses three issues in studying emotions that are usually approached differently in psychology and computer science: defining emotions, ethical considerations, and exploring emotional data. With the bibliometric data, we show that interdisciplinary teams are exceptions, which may limit the scope of research on emotions. Next, we argue that presented different perspectives complement rather than exclude each other. Thus, we advocate for creating multidisciplinary initiatives that bring unique benefits. Our perspective provides an additional impulse to bring our fields together and encourage multidisciplinary discussions on advancing the science of emotions.},
  keywords={Computer science;Pervasive computing;Ethics;Conferences;Redundancy;Psychology;Medical services;emotions;affect;affective science;affective computing},
  doi={10.1109/PerComWorkshops56833.2023.10150393},
  ISSN={2766-8576},
  month={March},}@INPROCEEDINGS{10520555,
  author={Abdelkader, Hala and Abdelrazek, Mohamed and Schneider, Jean-Guy and Rani, Priya and Vasa, Rajesh},
  booktitle={2023 IEEE Engineering Informatics}, 
  title={Robustness Attributes to Safeguard Machine Learning Models in Production}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Machine learning (ML) has revolutionized various industries by enabling the development of complex models that learn from data and make accurate predictions. However, moving from prototyping ML models to production software systems poses robustness challenges due to the lack of standardization around ML tools and processes. To tackle these challenges in deployed ML models, different elements of robustness must be addressed, including transparency, safety, and security along with adopting a unified language and terminologies. This paper aims to highlight the key robustness challenges that ML models encounter when deployed in production environments, and to emphasize the significance of proactively tackling these challenges. We also introduce various patterns for safeguarding ML models that need to be implemented on both the ML models and the software system sides.},
  keywords={Terminology;Production;Machine learning;Standardization;Predictive models;Software systems;Robustness;Machine learning;robustness;trustworthiness;production environments},
  doi={10.1109/IEEECONF58110.2023.10520555},
  ISSN={},
  month={Nov},}@ARTICLE{10729275,
  author={Zhao, Xingyu and An, Yuexuan and Qi, Lei and Geng, Xin},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Scalable Label Distribution Learning for Multi-Label Classification}, 
  year={2025},
  volume={36},
  number={7},
  pages={13232-13246},
  abstract={Multi-label classification (MLC) refers to the problem of tagging a given instance with a set of relevant labels. Most existing MLC methods are based on the assumption that the correlation of two labels in each label pair is symmetric, which is violated in many real-world scenarios. Moreover, most existing methods design learning processes associated with the number of labels, which makes their computational complexity a bottleneck when scaling up to large-scale output space. To tackle these issues, we propose a novel method named scalable label distribution learning (SLDL) for MLC, which can describe different labels as distributions in a latent space, where the label correlation is asymmetric and the dimension is independent of the number of labels. Specifically, SLDL first converts labels into continuous distributions within a low-dimensional latent space and leverages the asymmetric metric to establish the correlation between different labels. Then, it learns the mapping from the feature space to the latent space, resulting in the computational complexity is no longer related to the number of labels. Finally, SLDL leverages a nearest neighbor-based strategy to decode the latent representations and obtain the final predictions. Extensive experiments illustrate that SLDL achieves very competitive classification performances with little computational consumption.},
  keywords={Correlation;Vectors;Training;Symmetric matrices;Computational modeling;Computational complexity;Accuracy;Scalability;Measurement;Transforms;Label correlation;label distribution learning (LDL);large-scale output space;multi-label classification (MLC)},
  doi={10.1109/TNNLS.2024.3475469},
  ISSN={2162-2388},
  month={July},}@ARTICLE{10343142,
  author={Zheng, Tong and Chen, Jialun and Feng, Wenbin and Yu, Chongchong},
  journal={IEEE Access}, 
  title={A Graph Aggregation Convolution and Attention Mechanism Based Semantic Segmentation Method for Sparse Lidar Point Cloud Data}, 
  year={2024},
  volume={12},
  number={},
  pages={10459-10469},
  abstract={In recent years, following the development of sensor and computer techniques, it is favored by many fields, i.e. automatic drive, intelligent home, etc., which the deep learning based semantic segmentation method for point cloud data collected by LiDAR. This type method can automatic extract features of point cloud, helping label semantic categories. However, compared to 2D images, 3D point cloud data is more expensive to acquire. Hence, to save research and production costs, the low-thread LiDAR is a good choice. For one observation scenario, following the decrease of the line, the point cloud becomes sparse, which may cause the information loss. To balance the cost and the segmentation effect, we provide a point cloud completion auxiliary semantic segmentation method. Here, the baseline of the proposed method is Bilateral Augmentation and Adaptive Fusion (BAAF) model. It is the main contribution that a completion module introduction in feature extraction part of BAAF. Under the premise of using low-thread LiDAR sensor to collect data, the semantic segmentation effect of 3D field point cloud is improved as much as possible. It provides theoretical basis for cost saving in practical industrial application. The feature extraction of completion module consists of Graph Aggregation Convolution (GAC) and attention mechanism. Then, we use shuffle transform to upsampling data. In addition, to analyze the effectiveness of the proposed method, we make a new dataset with sparse point cloud data, i.e. Sparse-SemanticKITTI dataset, based on public SemanticKITTI dataset. Furthermore, in experiment part, we prove the research significance. Moreover, we compare the segmentation results between classical methods to the proposed one based on point cloud data in SemanticKITTI, Sparse-SemanticKITTI and Semantic3D dataset, respectively. The effectiveness of the proposed one is obvious. Finally, the model complexity is analyzed. In sum, we provide a sparse point cloud semantic segmentation method to balance the cost and the effect.},
  keywords={Point cloud compression;Feature extraction;Semantic segmentation;Convolution;Laser radar;Three-dimensional displays;Costs;Graph theory;Point cloud semantic segmentation;Lidar;bilateral augmentation and adaptive fusion (BAAF);point cloud completion;graph aggregation convolution},
  doi={10.1109/ACCESS.2023.3339657},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10943590,
  author={Arrabi, Ahmad and Zhang, Xiaohan and Sultani, Waqas and Chen, Chen and Wshah, Safwan},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance}, 
  year={2025},
  volume={},
  number={},
  pages={5356-5366},
  abstract={Aerial imagery analysis is critical for many research fields. However, obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However, G2A is rarely studied, because of its challenges, including but not limited to, the drastic view changes, occlusion, and range of visibil-ity. In this paper, we present a novel Geometric Preserving Ground-to-Aerial (G2A) image synthesis (GPG2A) model that can generate realistic aerial images from ground images. GPG2A consists of two stages. The first stage predicts the Bird's Eye View (BEV) segmentation (referred to as the BEV layout map) from the ground image. The second stage synthesizes the aerial image from the predicted BEV layout map and text descriptions of the ground image. To train our model, we present a new multimodal cross-view dataset, namely VIGORv2, built upon VIGOR [64] with newly collected aerial images, maps, and text descriptions. Our extensive experiments illustrate that GPG2A synthesizes better geometry-preserved aerial images than existing models. We also present two applications, data augmentation for cross-view geo-localization and sketch-based region search, to further verify the effectiveness of our GPG2A. The code and dataset are available at https://github.com/AhmadArrabi/GPG2A.},
  keywords={Geometry;Image segmentation;Computer vision;Costs;Image synthesis;Geology;Computational modeling;Layout;Diffusion models;Data augmentation;diffusion model;cross-view geolocalization;cross-view image synthesis;multimodality},
  doi={10.1109/WACV61041.2025.00523},
  ISSN={2642-9381},
  month={Feb},}@INPROCEEDINGS{11004655,
  author={Anjaneyulu, Gudla and Reddy, Pundru Chandra Shaker and Praveen, Pappula and Banoth, Shobhan and Pandey, Deependra and Koka, Sridhar N},
  booktitle={2025 International Conference on Advanced Computing Technologies (ICoACT)}, 
  title={A Hybrid Optimization Deep Learning Frame Work for Efficient Stock Market Forecasting}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread need to forecast future market trends has prompted the incorporation of cutting-edge technology that go beyond the realm of conventional statistical models. Predicting the future of the stock market using ML and DL methods is the focus of this article. We offer a holistic strategy that incorporates effective methods for feature selection, data preparation, and classification. To clean up the data and lower the noise level, the wavelet transform technique is used. By determining which input features are most important, the Dandelion-Optimization-Algorithm(DOA) optimizes feature selection. An innovative model for analyzing stock market data is created, 3DCNN-GRU, which combines a 3DCNNwith a gated-recurrent-unit(GRU). The Blood-Coagulation-Algorithm(BCA) makes it easier to tune hyperparameters, which improves the performance of the model. The outstanding prediction accuracy of 99.14% achieved by our methodology demonstrates its durability and efficacy in stock market anticipating applications. The proposed model has a lot of potential, but it can only handle the Nifty 50 index because that's all the dataset covers. The work's broader implications imply that the model might be further validated and its applicability improved by including additional datasets and investigating alternative market conditions. To guarantee robustness and generalizability, future research could concentrate on using this model in different financial circumstances.},
  keywords={Analytical models;Accuracy;Predictive models;Feature extraction;Market research;Data models;Indexes;Stock markets;Forecasting;Resilience;Stock prediction;deep learning;Nifty 50 index;CNN;Gated recurrent units},
  doi={10.1109/ICoACT63339.2025.11004655},
  ISSN={},
  month={March},}@ARTICLE{10848195,
  author={Wang, Mengzhu and Su, Houcheng and Wang, Sijia and Wang, Shanshan and Yin, Nan and Shen, Li and Lan, Long and Yang, Liang and Cao, Xiaochun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Graph Convolutional Mixture-of-Experts Learner Network for Long-Tailed Domain Generalization}, 
  year={2025},
  volume={35},
  number={7},
  pages={6936-6947},
  abstract={The goal of single domain generalization is to use data from a single domain (source domain) to train a model, which is then deployed over several unknown domains for testing (target domains). This study introduces a practical approach diverging from traditional DG, which typically relies on multiple source domains. We focus on Single Long-Tailed Domain Generalization, which refers to a scenario in the context of long-tail distribution, where although minority classes may have fewer samples in a single domain, these minority classes could become more prevalent and dominant in other domains. We introduce the Graph Convolutional Mixture-of-Experts Learners Network for Long-Tailed Domain Generalization (GCML) as a solution to this problem. Our approach presents two novel tactics. Initially, we utilize an expert learning technique that is skill-diverse. In order to properly manage the unknown target domain, this entails training multiple specialists inside a single long-tailed source domain and combining their knowledge. Then, we use a graph convolutional network to facilitate domain generalization, leveraging joint data structure modeling to learn more domain-invariant feature. Experiments conducted on four established benchmarks reveal that our GCML algorithm outperforms contemporary domain generalization techniques, demonstrating its efficacy in this complex task.},
  keywords={Heavily-tailed distribution;Adaptation models;Training;Data models;Circuits and systems;Data augmentation;Predictive models;Integrated circuit modeling;Data structures;Computational modeling;Domain generalization;mixture-of-experts learner;data structure;domain-invariant feature},
  doi={10.1109/TCSVT.2025.3532309},
  ISSN={1558-2205},
  month={July},}@ARTICLE{10659202,
  author={Zhang, Hui and Luo, Guiyang and Cao, Yuanzhouhan and Wang, Xiao and Li, Yidong and Wang, Fei-Yue},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Scale-Disentangled and Uncertainty-Guided Alignment for Domain-Adaptive Object Detection}, 
  year={2024},
  volume={25},
  number={12},
  pages={19507-19521},
  abstract={Unsupervised domain adaptive object detection methods aim to transfer knowledge from the label-sufficient domain to the unlabeled domain. Most existing works minimize domain disparity by concentrating on different levels through adversarial learning. However, adversarial learning do not consider the different influences on under-aligned and well-aligned samples as they merely match distinct distributions with consistent weight. To address this issue, we design a novel scale-disentangled and uncertainty-guided alignment for domain-adaptive object detection (SDUGA), consisting of three main components: (1) Disentangled scale coarse module, which decouples scale information from global image features and performs individual alignment across domains for the corresponding scale by training domain classifiers in an adversarial learning manner; (2) Disentangled scale fine module, which generalizes the disentangled scale alignment to instance-level adaptation, reinforcing the distribution alignment across domains from multi-scale local instance level; (3) Uncertainty-guided coarse-to-fine attention alignment, which adjusts weights for various samples adaptively by generating the uncertainty-guided attention map, thus enforcing the detector to converge more on alignment for under-aligned samples and avoid misaligning well-aligned ones. Extensive experiments over three challenging domain-shift object detection scenarios demonstrate that SDUGA gains superior performance compared to state-of-the-art methods.},
  keywords={Object detection;Feature extraction;Proposals;Training;Representation learning;Deep learning;Adversarial machine learning;Detection algorithms;Object detection;domain adaptation;deep learning;adversarial feature learning},
  doi={10.1109/TITS.2024.3447052},
  ISSN={1558-0016},
  month={Dec},}@INPROCEEDINGS{10801623,
  author={Wang, Sheng and Sun, Ge and Ma, Fulong and Hu, Tianshuai and Qin, Qiang and Song, Yongkang and Zhu, Lei and Liang, Junwei},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={DragTraffic: Interactive and Controllable Traffic Scene Generation for Autonomous Driving}, 
  year={2024},
  volume={},
  number={},
  pages={14241-14247},
  abstract={Evaluating and training autonomous driving systems require diverse and scalable corner cases. However, most existing scene generation methods lack controllability, accuracy, and versatility, resulting in unsatisfactory generation results. Inspired by DragGAN in image generation, we propose DragTraffic, a generalized, interactive, and controllable traffic scene generation framework based on conditional diffusion. DragTraffic enables non-experts to generate a variety of realistic driving scenarios for different types of traffic agents through an adaptive mixture expert architecture. We employ a regression model to provide a general initial solution and a refinement process based on the conditional diffusion model to ensure diversity. User-customized context is introduced through cross-attention to ensure high controllability. Experiments on a real-world driving dataset show that DragTraffic outperforms existing methods in terms of authenticity, diversity, and freedom. Demo videos and code are available at https://chantsss.github.io/Dragtraffic/.},
  keywords={Training;Adaptation models;Accuracy;Noise reduction;Process control;Controllability;Diffusion models;Autonomous vehicles;Intelligent robots;Videos},
  doi={10.1109/IROS58592.2024.10801623},
  ISSN={2153-0866},
  month={Oct},}@ARTICLE{9478335,
  author={Geisler, Simon and Cunha, Carlos and Satzoda, Ravi Kumar},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Better, Faster Small Hazard Detection: Instance-Aware Techniques, Metrics and Benchmarking}, 
  year={2022},
  volume={23},
  number={7},
  pages={9062-9077},
  abstract={Vision-based detection of hazards in the path of ego-vehicle is a challenging task because of the variability in the type of hazards. In this paper, we present a detailed review of vision-based hazard detection methods followed by a set of new architectures and methods include semantic segmentation, instance segmentation, object detection, monocular vision with depth fusion based methods and ensembles. Additionally, we propose a set of new (and some old) benchmarking metrics that accurately capture the effectiveness of hazard detection algorithms, in terms of both algorithmic accuracy and deployability in vehicles. Detailed performance evaluations show that the proposed methods using Mask-RCNN, ensembles and monocular-stereo fusion surpass current state-of-the-art techniques in terms of accuracy and computational speed. Additionally, our fusion based object detection architectures provide a good tradeoff between accuracy (e.g. Average Precision) and computation requirements, with operating speeds that are 15 times faster than existing techniques.},
  keywords={Hazards;Semantics;Roads;Image segmentation;Object detection;Measurement;Benchmark testing;Small hazard detection;road hazards;road debris;lost cargo;object detection;instance segmentation;semantic segmentation;depth;disparity},
  doi={10.1109/TITS.2021.3090338},
  ISSN={1558-0016},
  month={July},}@ARTICLE{10807204,
  author={Matasyoh, Nevin M. and Mathis-Ullrich, Franziska and Zeineldin, Ramy A.},
  journal={IEEE Access}, 
  title={SAMSurg: Surgical Instrument Segmentation in Robotic Surgeries Using Vision Foundation Model}, 
  year={2024},
  volume={12},
  number={},
  pages={193950-193959},
  abstract={Integrating Minimally Invasive Surgery (MIS) with robotic systems has revolutionized surgical procedures by enhancing precision, reducing patient discomfort, and shortening recovery times. However, the dynamic and visually complex environment of robotic surgeries poses significant challenges in the accurate semantic segmentation of surgical instruments, a critical task for navigating surgical robots and ensuring surgical safety. This necessitates the development of advanced computational techniques capable of overcoming these challenges. To address these limitations in robotic surgeries, we introduce SAMSurg, an adaptation of the Segment Anything Model (SAM). This vision foundation model accurately segments diverse objects in images for various computer vision applications. SAMSurg leverages the pre-trained representations of SAM’s image and prompt encoders, focusing on fine-tuning the mask decoder to adapt to the specific demands of surgical instrument segmentation in MIS. The model is evaluated using over 77K image-mask pairs from various surgical datasets, demonstrating its adaptability across different surgical interventions, disciplines, and instrument types. This extensive evaluation underscores SAMSurg’s superior performance against state-of-the-art models like SAM and MedSAM, with Dice Similarity Coefficient (DSC) scores as high as 96.90%. Furthermore, it demonstrates strong generalization capabilities across various surgical contexts, achieving higher DSC scores than state-of-the-art models, such as SAM and MedSAM, on unseen datasets. SAMSurg’s performance shows a significant improvement in the segmentation of surgical instruments for robotic surgeries, promising to enhance surgical techniques and patient care outcomes.},
  keywords={Instruments;Image segmentation;Surgery;Robots;Biomedical imaging;Training;Robot kinematics;Decoding;Adaptation models;Foundation models;Box prompting;robotic surgery;surgical instrument segmentation;vision foundation model},
  doi={10.1109/ACCESS.2024.3520386},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9956635,
  author={Dias Da Cruz, Steve and Taetz, Bertram and Stifter, Thomas and Stricker, Didier},
  booktitle={2022 26th International Conference on Pattern Recognition (ICPR)}, 
  title={Autoencoder for Synthetic to Real Generalization: From Simple to More Complex Scenes}, 
  year={2022},
  volume={},
  number={},
  pages={5060-5066},
  abstract={Learning on synthetic data and transferring the resulting properties to their real counterparts is an important challenge for reducing costs and increasing safety in machine learning. In this work, we focus on autoencoder architectures and aim at learning latent space representations that are invariant to inductive biases caused by the domain shift between simulated and real images showing the same scenario. We train on synthetic images only, present approaches to increase generalizability and improve the preservation of the semantics to real datasets of increasing visual complexity. We show that pre-trained feature extractors (e.g. VGG) can be sufficient for generalization on images of lower complexity, but additional improvements are required for visually more complex scenes. To this end, we demonstrate a new sampling technique, which matches semantically important parts of the image, while randomizing the other parts, leads to salient feature extraction and a neglection of unimportant parts. This helps the generalization to real data and we further show that our approach outperforms fine-tuned classification models.},
  keywords={Visualization;Semantics;Machine learning;Feature extraction;Complexity theory;Safety;Pattern recognition},
  doi={10.1109/ICPR56361.2022.9956635},
  ISSN={2831-7475},
  month={Aug},}@ARTICLE{10767173,
  author={Tao, Lei and Qian, Jin and Gong, Changhao and Zhang, Dingfa and Luo, Yuemei},
  journal={IEEE Sensors Journal}, 
  title={Cross-Domain Retinopathy Classification Based on Optical Coherence Tomography Sensors via Domain Adversarial Graph Convolutional Network}, 
  year={2025},
  volume={25},
  number={2},
  pages={3473-3483},
  abstract={Optical coherence tomography (OCT) sensor system, known for their high precision and noninvasive imaging capabilities, can capture detailed structural information of retinal tissue. Analyzing OCT sensing images allows for the identification of various retinopathies, providing significant references for clinical diagnosis and treatment. In a typical OCT sensing system, a sample beam illuminates the retina to capture reflected interference signals, which are converted to electronic form. The interference pattern is processed using the Michelson interferometry and diffraction gratings, and the resulting spectral data are transformed into a B-scan image through the Fourier transform, revealing detailed cross-sectional and 3-D organization. The application of deep learning techniques has become a prevalent methodology for the analysis of OCT sensing images. However, given the varying light source wavelengths, detector sensitivities, and imaging modalities of OCT sensing devices, images generated for the same patient at different medical centers may show significant discrepancies. These differences can affect image quality and resolution, as well as the representation of specific pathological features. This can result in distributions that are incongruent between the training and test datasets, thereby impeding the efficacy of deep learning models. Existing studies have focused mainly on domain and class labels, overlooking the importance of data structure and limiting their effectiveness. To address this issue, we propose a novel approach, namely, domain adversarial graph convolutional network (DAGCN), introducing two innovative modules to enhance the adaptability of OCT sensing image classification models. The first module, data structure-aware alignment, leverages small batch OCT image data to reduce domain shift. The second, class centroid alignment (CA), minimizes the distance between class centroids across domains, improving generalization and adaptability. We validated DAGCN’s performance through extensive testing on three public datasets. The results show that DAGCN enhances the adaptability of OCT retinopathy image classification techniques by effectively learning domain invariants and semantic representations that mitigates the impact of feature fluctuations resulting from discrepancies in sensor sensitivity and noise levels across OCT sensing devices.},
  keywords={Sensors;Feature extraction;Retinopathy;Imaging;Sensor systems;Lenses;Adaptation models;Semantics;Optical coherence tomography;Training;Deep learning;domain adaption;optical coherence tomography (OCT) images;retinopathy classification},
  doi={10.1109/JSEN.2024.3502470},
  ISSN={1558-1748},
  month={Jan},}@ARTICLE{10557653,
  author={Zhang, Chengcheng and Li, Wei and Deng, Ming and Jiang, Yizhang and Cui, Xiaohui and Chen, Ping},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={SIG: Graph-Based Cancer Subtype Stratification With Gene Mutation Structural Information}, 
  year={2024},
  volume={21},
  number={6},
  pages={1752-1764},
  abstract={Somatic tumors have a high-dimensional, sparse, and small sample size nature, making cancer subtype stratification based on somatic genomic data a challenge. Current methods for improving cancer clustering performance focus on dimension reduction, integrating multi-omics data, or generating realistic samples, yet ignore the associations between mutated genes within the patient-gene matrix. We refer to these associations as gene mutation structural information, which implicitly includes cancer subtype information and can enhance subtype clustering. We introduce a novel method for cancer subtype clustering called SIG(Structural Information within Graph). As cancer is driven by a combination of genes, we establish associations between mutated genes within the same patient sample, pair by pair, and use a graph to represent them. An association between two mutated genes corresponds to an edge in the graph. We then merge these associations among all mutated genes to obtain a structural information graph, which enriches the gene network and improves its relevance to cancer clustering. We integrate the somatic tumor genome with the enriched gene network and propagate it to cluster patients with mutations in similar network regions. Our method achieves superior clustering performance compared to SOTA methods, as demonstrated by clustering experiments on ovarian and LUAD datasets.},
  keywords={Cancer;Tumors;Genomics;Bioinformatics;Clustering algorithms;Germanium;Task analysis;Cancer clustering;feature selection;gene mutat ion structural information;genomic data;graph},
  doi={10.1109/TCBB.2024.3414498},
  ISSN={1557-9964},
  month={Nov},}@INPROCEEDINGS{10281117,
  author={Zhang, Zilun and Zhao, Qunfei and Yan, Tingman},
  booktitle={2023 5th International Conference on Electronics and Communication, Network and Computer Technology (ECNCT)}, 
  title={Capsule defect detection method based on Improved Faster RCNN}, 
  year={2023},
  volume={},
  number={},
  pages={246-253},
  abstract={The capsule defect detection has always been a challenging issue. In previous research, it has been difficult to strike a balance between accuracy and detection speed for the capsule defect detection system to meet the requirements of industrial production. In light of this, I introduce an innovative approach to identify defects in capsules using an Enhanced Faster RCNN framework. This method is not only high in accuracy, but also meets the real-time requirements of industrial detection. This method effectively identifies common capsule anomalies like scratches, compression marks, and faulty imprints. Ensuring capsule quality is pivotal in production processes. To align with the demands of real-time industrial inspection, I incorporate the MobileNetV1 network, replacing the Vgg16 network in the traditional Faster RCNN for feature extraction. Network parameter learning is achieved using the Adam algorithm, a substitution for the conventional Momentum algorithm. Furthermore, recognizing the limitations of Faster R-CNN in precisely localizing smaller objects, I enhanced its capabilities by integrating the Feature Pyramid Network (FPN). This adaptation optimized the utilization of finely detailed shallow features, thereby improving the detection performance of smaller objects.},
  keywords={Fault diagnosis;Semantics;Buildings;Production;Computer architecture;Inspection;Feature extraction;Faster RCNN;Defect detection;Feature pyramid network;Capsule;Feature classification;Target recognition;Multiscale feature fusion},
  doi={10.1109/ECNCT59757.2023.10281117},
  ISSN={},
  month={Aug},}@ARTICLE{10714416,
  author={Zheng, Ruochen and Han, Chuchu and Gao, Changxin and Sang, Nong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Linear Feature Source Prediction and Recombination Network for Noisy Label Learning}, 
  year={2025},
  volume={35},
  number={2},
  pages={1648-1659},
  abstract={Collecting training data for deep models from the Internet is a common data acquisition approach. However, there are challenges in using these data directly, as they often contain inaccurate annotations. This situation has increased the attention and importance of noisy label learning, the process of training a deep model with unreliable annotations. The typical strategy in noisy label learning is to identify potential mislabeled samples and assign pseudo-labels generated by the network to them, replacing the original labels. However, existing methods encounter the following problems: 1) they typically do not evaluate the pseudo-labels and directly use all of them, and 2) empirical parameter settings are often dataset-specific. These shortcomings limit the application of these methods in real-world scenarios. In this paper, we propose the Linear Feature Source Prediction and Recombination Network (LFSPR), trying to solve the problem above by proposing a new pretext task. The pretext task is designed to build the linear connection between the high-dimensional feature and the low-dimensional feature. The source of the latter is regarded as the high-dimensional feature, which follows a non-linear head network to obtain the low-dimensional feature. The pretext task is designed in low-dimensional space by predicting the linear composition weights of the potential source. Based on the pretext task, our method can generate pseudo-labels for uncertain samples while dynamically evaluating and selecting them, rather than simply using all pseudo-labels or discarding a fixed proportion of pseudo-labels for a given dataset. To the best of our knowledge, this is the first approach in the noisy label learning domain to employ pretext task for the pseudo-labels generation, evaluation and selection. The experiments on CIFAR-10, CIFAR-100 and Clothing1M demonstrate the effectiveness of our method.},
  keywords={Noise measurement;Training;Feature extraction;Annotations;Predictive models;Manifolds;Head;Estimation error;Data models;Circuits and systems;Noisy label learning;pretext task;semi-supervised learning},
  doi={10.1109/TCSVT.2024.3478771},
  ISSN={1558-2205},
  month={Feb},}@ARTICLE{10794602,
  author={Zhao, Junhong and Xue, Bing and Zhang, Mengjie},
  journal={IEEE Transactions on Image Processing}, 
  title={SALENet: Structure-Aware Lighting Estimations From a Single Image for Indoor Environments}, 
  year={2024},
  volume={33},
  number={},
  pages={6806-6820},
  abstract={High Dynamic Range (HDR) lighting plays a pivotal role in modern augmented and mixed-reality (AR/MR) applications, facilitating immersive experiences through realistic object insertion and dynamic relighting. However, the acquisition of precise HDR environment maps remains cost-prohibitive and impractical when using standard devices. To bridge this gap, this paper introduces SALENet, a novel deep network for estimating global lighting conditions from a single image, to effectively mitigate the need for resource-intensive acquisition methods. In contrast to earlier studies, we focus on exploring the inherent structural relationships within the lighting distribution. We design a hierarchical transformer-based neural network architecture with a proposed cross-attention mechanism between different resolution lighting source representations, optimizing the spatial distribution of lighting sources simultaneously for enhanced consistency. To further improve accuracy, a structure-based contrastive learning method is proposed to select positive-negative pairs based on lighting distribution similarity. By harnessing the synergy of hierarchical transformers and structure-based contrastive learning, our framework yields a significant enhancement in lighting prediction accuracy, enabling high-fidelity augmented and mixed reality to achieve cost-effectively immersive and realistic lighting effects.},
  keywords={Lighting;Estimation;Accuracy;Contrastive learning;Transformers;Rendering (computer graphics);Reflectivity;Light sources;Indoor environment;Deep learning;Lighting estimation;spherical Gaussian;environment map;transformer;contrastive learning;augmented reality},
  doi={10.1109/TIP.2024.3512381},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{9412569,
  author={Ding, Yuhe and Ma, Xin and Luo, Mandi and Zheng, Aihua and He, Ran},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Unsupervised Contrastive Photo-to-Caricature Translation based on Auto-distortion}, 
  year={2021},
  volume={},
  number={},
  pages={4520-4527},
  abstract={Photo-to-caricature translation aims to synthesize the caricature as a rendered image exaggerating the features through sketching, pencil strokes, or other artistic drawings. Style rendering and geometry deformation are the most important aspects in photo-to-caricature translation task. To take both into consideration, we propose an unsupervised contrastive photo-to-caricature translation architecture. Considering the intuitive artifacts in the existing methods, we propose a contrastive style loss for style rendering to enforce the similarity between the style of rendered photo and the caricature, and simultaneously enhance its discrepancy to the photos. To obtain an exaggerating deformation in an unpaired/unsupervised fashion, we propose a Distortion Prediction Module (DPM) to predict a set of displacements vectors for each input image while fixing some controlling points, followed by the thin plate spline interpolation for warping. The model is trained on unpaired photo and caricature while can offer bidirectional synthesizing via inputting either a photo or a caricature. Extensive experiments demonstrate that the proposed model is effective to generate hand-drawn like caricatures compared with existing competitors.},
  keywords={Geometry;Interpolation;Rendering (computer graphics);Distortion;Pattern recognition;Task analysis;Splines (mathematics)},
  doi={10.1109/ICPR48806.2021.9412569},
  ISSN={1051-4651},
  month={Jan},}@ARTICLE{10891548,
  author={Lu, Yuwu and Huang, Haoyu and Hu, Xue and Lai, Zhihui},
  journal={IEEE Transactions on Multimedia}, 
  title={Multiple Adaptation Network for Multi-Source and Multi-Target Domain Adaptation}, 
  year={2025},
  volume={27},
  number={},
  pages={5731-5745},
  abstract={Multi-source domain adaptation (MSDA) has garnered significant attention due to its emphasis on transferring knowledge from multiple labeled source domains to a single unlabeled target domain. MSDA requires sufficient labeled data from multiple source domains, but in practice, massive unlabeled data exist instead of well-labeled data. Multiple target domains also provide plenty of information, which is useful for domain adaptation. However, most MSDA studies overlook the critical scenario of multi-source and multi-target domain adaptation (MMDA). To address these problems, we propose a Multiple Adaptation Network (MAN) approach for MMDA, which utilizes multiple alignment strategies for each source-target domain pair-group to align relevant specific feature spaces. MAN also aligns multiple classifiers for the relevant feature spaces to optimize the decision boundaries of multiple target domains. Moreover, to consider the task relations of multiple classifiers, we minimize the semantic differences between the target-conditioned classifiers and utilize a weight learning category to optimize this process. To fully utilize the information from multiple target domains, we transfer the style information of the target data to the source data, aiding in the training of multiple classifiers. Extensive experiments in challenge domain adaptation benchmarks, including the ImageCLEF-DA, Office-Home, DomainNet, and RGB-to-thermal datasets, demonstrate the superiority of our method over the state-of-the-art approaches.},
  keywords={Feature extraction;Training;Adaptation models;Semantics;Adversarial machine learning;Generators;Data models;Data mining;Reviews;Faces;Domain adaptation;multi-source domain adaptation;multi-target;thermal imagery},
  doi={10.1109/TMM.2025.3543094},
  ISSN={1941-0077},
  month={},}@ARTICLE{10310270,
  author={Wu, Qiong and Li, Jiahan and Dai, Pingyang and Ye, Qixiang and Cao, Liujuan and Wu, Yongjian and Ji, Rongrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Unsupervised Domain Adaptation on Person Reidentification Via Dual-Level Asymmetric Mutual Learning}, 
  year={2025},
  volume={36},
  number={1},
  pages={1371-1382},
  abstract={Unsupervised domain adaptation (UDA) person reidentification (Re-ID) aims to identify pedestrian images within an unlabeled target domain with an auxiliary labeled source-domain dataset. Many existing works attempt to recover reliable identity information by considering multiple homogeneous networks. And take these generated labels to train the model in the target domain. However, these homogeneous networks identify people in approximate subspaces and equally exchange their knowledge with others or their mean net to improve their ability, inevitably limiting the scope of available knowledge and putting them into the same mistake. This article proposes a dual-level asymmetric mutual learning (DAML) method to learn discriminative representations from a broader knowledge scope with diverse embedding spaces. Specifically, two heterogeneous networks mutually learn knowledge from asymmetric subspaces through the pseudo label generation in a hard distillation manner. The knowledge transfer between two networks is based on an asymmetric mutual learning (AML) manner. The teacher network learns to identify both the target and source domain while adapting to the target domain distribution based on the knowledge of the student. Meanwhile, the student network is trained on the target dataset and employs the ground-truth label through the knowledge of the teacher. Extensive experiments in Market-1501, CUHK-SYSU, and MSMT17 public datasets verified the superiority of DAML over state-of-the-arts (SOTA).},
  keywords={Knowledge engineering;Microstrip;Heterogeneous networks;Adaptation models;Training;Task analysis;Feature extraction;Person reidentification (Re-ID);retrieval;transfer learning;unsupervised domain adaptation (UDA)},
  doi={10.1109/TNNLS.2023.3326477},
  ISSN={2162-2388},
  month={Jan},}@INPROCEEDINGS{10385691,
  author={Liu, Zhenhong and Wang, Xingce and Wu, Zhongke and Zhu, Yi-Cheng and Frangi, Alejandro F.},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Simultaneous Super-Resolution and Denoising on MRI via Conditional Stochastic Normalizing Flow}, 
  year={2023},
  volume={},
  number={},
  pages={1313-1318},
  abstract={Magnetic resonance imaging (MRI) scans often suffer from noise and low-resolution (LR), which affect the diagnosis and treatment results obtained for patients. LR images and noise come together with MRI, and the existing methods solve image super-resolution (SR) reconstruction and denoising tasks in a step-by-step manner, which influences the overall real distribution of the MRI data. In this paper, we present a simultaneous SR and denoising algorithm based on a stochastic normalizing flow (SNF), named the MR image SR and denoising model based on an SNF (SRDSNF). SRDSNF adds the encoded information of the input image as the conditional information to each reverse step of the stochastic normalizing flow, which realizes a consistent description of the spatial distribution between the reconstruction result and the input image. We introduce rangenull space decomposition and subsequence sampling strategies to enhance the consistency of the input and output data and increase the generation speed of the model. Simultaneous SR and denoising tasks experiment is carried out using the BrainWeb and NFBS datasets. The experimental results show that good SR and denoising results are obtained with fewer sampling steps, these results are consistent with the ground truths, and the structural similarity and peak signal-to-noise ratio of the results are also higher than those of the comparison methods. The proposed method demonstrates potential clinical promise.},
  keywords={Training;PSNR;Magnetic resonance imaging;Noise reduction;Superresolution;Transforms;Markov processes;Stochastic normalizing flow;Diffusion model;MR image;Super-resolution;Denoising},
  doi={10.1109/BIBM58861.2023.10385691},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{11042916,
  author={Chen, Cheng and Hong, Haokai and Lin, Wanyu and Tan, Kay Chen},
  booktitle={2025 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Physics-Informed Evolutionary Transfer Optimization Framework for Material Design}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={The design of new crystal materials is of significant scientific importance to society. In recent years, machine learning-based approaches have shown their potential in crystal material design. However, their effectiveness relies heavily on the availability of high-quality and extensive training data, which is difficult to collect in practice. To this end, this paper presents a novel physics-informed evolutionary transfer optimization framework that can design new crystal materials without the need for extensive data. Specifically, we first propose a novel physics-informed encoding for materials, enabling the use of multi-objective evolutionary optimization to simultaneously optimize multiple physical objectives, including the validity, properties, and energy of crystal materials. These physical objectives are critical to the effective design of crystal materials. Additionally, to mitigate the slow optimization speed of evolutionary computation, we propose a physics-informed evolutionary transfer optimization technique to enhance the design speed of optimized materials. We conducted comprehensive experiments to analyze the designed crystals from the perspectives of validity, density functional theory (DFT) validation, formation energy, and energy above hull. The experimental results validate the immense potential of the proposed physics-informed multi-objective evolutionary optimization framework in crystal material design.},
  keywords={Discrete Fourier transforms;Training data;Crystals;Evolutionary computation;Density functional theory;Encoding;Optimization;Evolutionary algorithms;material design;evolutionary transfer optimization},
  doi={10.1109/CEC65147.2025.11042916},
  ISSN={},
  month={June},}@ARTICLE{10742517,
  author={Han, Hongcheng and Tian, Zhiqiang and Guo, Qinbo and Jiang, Jue and Du, Shaoyi and Wang, Juan},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={HSC-T: B-Ultrasound-to-Elastography Translation via Hierarchical Structural Consistency Learning for Thyroid Cancer Diagnosis}, 
  year={2025},
  volume={29},
  number={2},
  pages={799-806},
  abstract={Elastography ultrasound imaging is increasingly important in the diagnosis of thyroid cancer and other diseases, but its reliance on specialized equipment and techniques limits widespread adoption. This paper proposes a novel multimodal ultrasound diagnostic pipeline that expands the application of elastography ultrasound by translating B-ultrasound (BUS) images into elastography images (EUS). Additionally, to address the limitations of existing image-to-image translation methods, which struggle to effectively model inter-sample variations and accurately capture regional-scale structural consistency, we propose a BUS-to-EUS translation method based on hierarchical structural consistency. By incorporating domain-level, sample-level, patch-level, and pixel-level constraints, our approach guides the model in learning a more precise mapping from BUS to EUS, thereby enhancing diagnostic accuracy. Experimental results demonstrate that the proposed method significantly improves the accuracy of BUS-to-EUS translation on the MTUSI dataset and that the generated elastography images enhance nodule diagnostic accuracy compared to solely using BUS images on the STUSI and the BUSI datasets. This advancement highlights the potential for broader application of elastography in clinical practice.},
  keywords={Elastography;Ultrasonic imaging;Thyroid;Feature extraction;Accuracy;Bioinformatics;Vectors;Medical diagnostic imaging;Training;Pipelines;Medical image translation;elastograghy ultrasound;hierarchical structural consistency;thyroid cancer},
  doi={10.1109/JBHI.2024.3491905},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{11162599,
  author={Zhang, Xu and Jia, Yuheng and Song, Mofei and Wang, Ran},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Similarity and Dissimilarity Guided Co-association Matrix Construction for Ensemble Clustering}, 
  year={2025},
  volume={},
  number={},
  pages={1-23},
  abstract={Ensemble clustering aggregates multiple weak clusterings to achieve a more accurate and robust consensus result. The Co-Association matrix (CA matrix) based method is the mainstream ensemble clustering approach that constructs the similarity relationships between sample pairs according the weak clustering partitions to generate the final clustering result. However, the existing methods neglect that the quality of cluster is related to its size, i.e., a cluster with smaller size tends to higher accuracy. Moreover, they also do not consider the valuable dissimilarity information in the base clusterings which can reflect the varying importance of sample pairs that are completely disconnected. To this end, we propose the Similarity and Dissimilarity Guided Co-association matrix (SDGCA) to achieve ensemble clustering. First, we introduce normalized ensemble entropy to estimate the quality of each cluster, and construct a similarity matrix based on this estimation. Then, we employ the random walk to explore high-order proximity of base clusterings to construct a dissimilarity matrix. Finally, the adversarial relationship between the similarity matrix and the dissimilarity matrix is utilized to construct a promoted CA matrix for ensemble clustering. We compared our method with 13 state-of-the-art methods across 12 datasets, and the results demonstrated the superior clustering ability and robustness of the proposed approach. The code is available at https://github.com/xuz2019/SDGCA.},
  keywords={Uncertainty;Accuracy;Noise;Entropy;Indexes;Clustering methods;Training;Robustness;Partitioning algorithms;Distributed databases;Cluster ensemble;Co-association matrix;Adversarial relationship},
  doi={10.1109/TKDE.2025.3608721},
  ISSN={1558-2191},
  month={},}@ARTICLE{10981431,
  author={Gao, Yu and Qi, Jin and Sun, Ying and Zhu, Xingjian and Chen, Genxin and Dong, Zhenjiang and Sun, Yanfei},
  journal={IEEE Internet of Things Journal}, 
  title={Cross-Domain Open-Set Fault Diagnosis for Rotating Machinery Based on Frequency-Aware Model With Neighborhood Invariance}, 
  year={2025},
  volume={12},
  number={14},
  pages={28390-28400},
  abstract={Domain adaptation (DA) is a frequently used technique in intelligent fault diagnosis. However, existing DA methods presume that the source and target domains have the same label space. Due to the complexity of industrial operation conditions, new fault types will inevitably occur. Thus, the above assumption is only sometimes satisfied. To overcome this issue, we propose a novel frequency-aware model with neighborhood invariance (FAN) for cross-domain open-set fault diagnosis. First, we comprehensively consider the domain shift phenomenon in time and frequency features and construct an encoder based on the Fourier neural operator (FNO) to extract potential invariant information efficiently. Second, we expect known class samples to be mapped to an invariant neighborhood to separate unknown classes. Based on this, we adopt neighborhood invariance learning to reduce the intradomain variations in the target domain and form robust discriminative boundaries. Extensive experiments on public and real-world datasets demonstrate that FAN outperforms the comparison methods and has flexibility.},
  keywords={Fault diagnosis;Feature extraction;Adaptation models;Sun;Fans;Discrete Fourier transforms;Data mining;Training;Time-frequency analysis;Telecommunications;Fault diagnosis;frequency-aware model;open-set domain adaptation (OSDA);rotating machinery},
  doi={10.1109/JIOT.2025.3565810},
  ISSN={2327-4662},
  month={July},}@INPROCEEDINGS{10978451,
  author={Zhang, Shuwen and Wang, Siye and Li, Jiaxing},
  booktitle={2025 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={SecureSemComs: Dynamic Adversarial Defense Framework for Semantic Communications}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Semantic communication is a current research hotspot emphasizing the transmission of content-related semantics. However, this focus increases vulnerability to attacks that exploit high-dimensional features, underscoring the urgent need for robust defense mechanisms against evolving threats. Traditional defense strategies are often computationally intensive, require continuous updates, and perform inadequately against sophisticated adversarial tactics in noisy environments. In this paper, we propose a Dynamic Adversarial Defense Framework (DADF), a model-agnostic, plug-and-play solution for seamless integration with semantic communication systems, requiring no modifications for deployment. The Adversarial Pattern Learning (APL) module aligns adversarial and genuine audio samples within a unified feature space, facilitating noise pattern extraction and significantly reducing interference. Subsequently, a Dynamic Noise Filter (DNF) employs adaptive mechanisms to eliminate harmful noise based on noise patterns extracted from the APL. Testing across five public benchmarks demonstrates DADF's superior performance, with peak achievements including a 9.31-fold reduction in mean squared error (MSE) and a 269.78% increase in signal-to-distortion ratio (SDR), significantly outperforming existing solutions. This breakthrough establishes a new bench-mark for resilience and adaptability in semantic communications, enhancing security against evolving adversarial attacks. The code is available at https://github.com/sweet0516IDADF.},
  keywords={Military communication;Adaptation models;Noise;Interference;Semantic communication;Feature extraction;Security;Noise measurement;Signal to noise ratio;Resilience;Semantic communication;adversarial defense;adversarial pattern learning (APL);dynamic noise filter (DNF)},
  doi={10.1109/WCNC61545.2025.10978451},
  ISSN={1558-2612},
  month={March},}@INPROCEEDINGS{10823306,
  author={Archana, R. and Govindraj, S. and Adhil, M Y},
  booktitle={2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)}, 
  title={Enhanced Kidney Diagnosis by CT Scan Stone Annotation}, 
  year={2024},
  volume={},
  number={},
  pages={1437-1442},
  abstract={The increasing prevalence of kidney stone disease necessitates efficient and accurate diagnostic methods to alleviate the burden on healthcare systems and professionals. Traditional manual methods of CT scan analysis are time-intensive and prone to human error, often delaying critical diagnoses. This study introduces an automated detection framework utilizing the YOLO NAS model, specifically optimized for real-time kidney stone annotation in CT scans. The dataset includes over 10,000 CT images sourced from Kaggle and Roboflow, enriched with additional scans manually annotated some image using the VGG Image Annotator tool to ensure comprehensive coverage of kidney stone types, sizes, and densities. The YOLO NAS model was selected due to its superior performance in object detection, leveraging Neural Architecture Search for optimization and trained using the SuperGradients library. The proposed model achieves a mean average precision (mAP) of 93% at a 0.50 Intersection over Union (IoU) threshold, demonstrating its high accuracy and efficiency. This automated solution reduces radiologists' workload, enhances diagnostic precision, and enables timely interventions, ultimately improving patient care. Future enhancements include expanding the dataset, integrating multi-modal data, and optimizing deployment for real-time clinical applications. By automating kidney stone detection, this system offers a robust approach to improving medical diagnostics.},
  keywords={YOLO;Accuracy;Annotations;Computed tomography;Manuals;Real-time systems;Libraries;Neural architecture search;Medical diagnosis;Optimization;Kidney Stone Detection;YOLO NAS;CT Scan Imaging;Object Detection;Medical Imaging;Bounding Box Annotation;Neural Architecture Search (NAS);Automated Medical Diagnostics},
  doi={10.1109/ICICNIS64247.2024.10823306},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10611144,
  author={Kim, Jisong and Bang, Geonho and Choi, Kwangjin and Seong, Minjae and Yoo, Jaechang and Pyo, Eunjong and Choi, Jun Won},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network}, 
  year={2024},
  volume={},
  number={},
  pages={9117-9124},
  abstract={In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into bird’s eye view object detection, a significant improvement in detection accuracy is achieved.},
  keywords={Point cloud compression;Accuracy;Focusing;Radar;Object detection;Predictive models;Encoding},
  doi={10.1109/ICRA57147.2024.10611144},
  ISSN={},
  month={May},}@INPROCEEDINGS{11004341,
  author={Haider, Khadija and Abbas, Muhammad Sohail and Muhammad, Asif and Qaiser, Muhid},
  booktitle={2024 26th International Multi-Topic Conference (INMIC)}, 
  title={Enhanced Synthetic Images Classification with Vision Transformers and Effective Data Augmentation Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Synthetic images have made significant advancements, resulting in highly realistic and high-quality visuals generated at an unprecedented speed. However, the proliferation of synthetic images raises growing concerns regarding their potential misuse, ethical implications, and associated risks in various sectors. This study focuses on leveraging data augmentation techniques to enhance the accuracy and reliability of synthetic image detection. In our study, we fine-tuned a Vision Transformer (ViT) on the CIFAKE dataset, comprising real and synthetic images, augmented using various techniques such as transformations, photometric adjustments, geometric changes, and noise addition. The best results were obtained with ViT fine-tuned by distortions and noise augmentation, achieving an accuracy rate of 98.66%, demonstrating that sophisticated augmentation techniques improve performance. Data augmentation plays a crucial role in handling the increasing realism and sophistication of synthetic images, contributing to more effective content moderation, digital forensics efforts, and the mitigation of potential risks associated with synthetic media.},
  keywords={Performance evaluation;Computer vision;Visualization;Accuracy;Computational modeling;Transformers;Distortion;Data augmentation;Reliability;Image classification;Vision Transformers;CIFAKE;Synthetic Images;Image Classification;Augmentation Techniques},
  doi={10.1109/INMIC64792.2024.11004341},
  ISSN={2835-8864},
  month={Dec},}@INPROCEEDINGS{11038573,
  author={Geißler, Daniel and Zhou, Bo and Lukowicz, Paul},
  booktitle={2025 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Strategies and Challenges of Efficient White-Box Training for Human Activity Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={225-230},
  abstract={Human Activity Recognition using time-series data from wearable sensors poses unique challenges due to complex temporal dependencies, sensor noise, placement variability, and diverse human behaviors. These factors, combined with the nontransparent nature of black-box Machine Learning models impede interpretability and hinder human comprehension of model behavior. This paper addresses these challenges by exploring strategies to enhance interpretability through white-box approaches, which provide actionable insights into latent space dynamics and model behavior during training. By leveraging human intuition and expertise, the proposed framework improves explainability, fosters trust, and promotes transparent Human Activity Recognition systems. A key contribution is the proposal of a Human-in-the-Loop framework that enables dynamic user interaction with models, facilitating iterative refinements to enhance performance and efficiency. Additionally, we investigate the usefulness of Large Language Model as an assistance to provide users with guidance for interpreting visualizations, diagnosing issues, and optimizing workflows. Together, these contributions present a scalable and efficient framework for developing interpretable and accessible Human Activity Recognition systems.},
  keywords={Training;Pervasive computing;Conferences;Noise;Closed box;Behavioral sciences;Human activity recognition;Proposals;Wearable sensors;Glass box;White-Box;Explainability;Efficency;HAR},
  doi={10.1109/PerComWorkshops65533.2025.00069},
  ISSN={2766-8576},
  month={March},}@ARTICLE{10813422,
  author={Xi, Zuping and Qu, Zuomin and Lu, Wei and Luo, Xiangyang and Cao, Xiaochun},
  journal={IEEE Transactions on Cybernetics}, 
  title={Invisible DNN Watermarking Against Model Extraction Attack}, 
  year={2025},
  volume={55},
  number={2},
  pages={800-811},
  abstract={Deep neural network (DNN) models are widely used in various fields, such as pattern recognition and natural language processing, and provide considerable commercial value to their owners. Embedding a digital watermark in the model allows the legitimate owner to detect unauthorized use of the model. However, the existing DNN watermarking methods are vulnerable to model extraction attacks since the watermark task and the original model task are independent. In this article, a novel collaborative DNN watermarking framework is proposed to defend against model extraction attacks by establishing cooperation between the watermark generation and embedding. Specifically, the trigger samples are not only imperceptible to ensure perceptual stealth security but also infused with target-label information to guide the following feature associations. In the process of watermark embedding, the feature representation of trigger samples is forced to be similar to that of the task distribution samples via feature coupling. Consequently, the trigger samples from our framework can be recognized in the stolen model as task distribution samples, so that the ownership of the model can be successfully verified. Extensive experiments on CIFAR10, CIFAR100, and ImageNet demonstrate the effectiveness and superior performance of the proposed watermarking framework against various model extraction attacks.},
  keywords={Watermarking;Data models;Feature extraction;Computational modeling;Training;Security;Closed box;Collaboration;Robustness;Data mining;Backdoor watermarking;model extraction attack;model watermarking;watermarking security},
  doi={10.1109/TCYB.2024.3514838},
  ISSN={2168-2275},
  month={Feb},}@ARTICLE{10891424,
  author={Liu, Tong and Li, Jing and Wu, Jia and Du, Bo and Zhan, Yibing and Tao, Dapeng and Wan, Jun},
  journal={IEEE Transactions on Multimedia}, 
  title={Facial Expression Recognition With Heatmap Neighbor Contrastive Learning}, 
  year={2025},
  volume={27},
  number={},
  pages={4795-4807},
  abstract={Many supervised learning-based facial expression recognition (FER) methods achieve good performance with the assistance of expression labels and a complex framework. However, there are inconsistent annotations in different expression datasets, making the above methods disadvantageous for new expression datasets or datasets with limited training data. The objective of this paper is to learn self-supervised facial expression features that enable the FER model not to rely on the annotation consistency of the different datasets. Most current self-supervised learning algorithms based on contrastive learning learn the representation by forcing different augmented views of the same image close in the embedding space, but they cannot cover all variances within a semantic class. We propose a heatmap neighbor contrastive learning (HNCL) method for FER. It treats the images corresponding to the heatmap nearest neighbors of expressions as other positives, providing more semantic variations than pre-defined augmented transformations. Therefore, our HNCL can learn better expression features covering more intra-class variances, improving the performance of the FER model based on self-supervised learning. After fine-tuning, HNCL with a simple framework achieves top-three performance on the in-the-lab datasets and even matches the performance of state-of-the-art supervised learning methods on the in-the-wild datasets.},
  keywords={Heating systems;Contrastive learning;Semantics;Feature extraction;Training;Supervised learning;Convolutional neural networks;Annotations;Electronic mail;Predictive models;Facial expression recognition;heatmap neighbor contrastive learning;self-supervised learning;semantic variations},
  doi={10.1109/TMM.2025.3543029},
  ISSN={1941-0077},
  month={},}@ARTICLE{11024010,
  author={Hu, Dinghan and Cui, Xiaonan and Jiang, Tiejia and Hu, Wenbin and Cao, Jiuwen},
  journal={IEEE Signal Processing Letters}, 
  title={Mutual Information Driven Representation Learning for Cross-Subject Seizure Detection}, 
  year={2025},
  volume={32},
  number={},
  pages={2434-2438},
  abstract={Developing a generalizable model across subjects is crucial for the practical application of Electroencephalogram (EEG) based seizure detection model. However, inter-subject variability poses a challenge to the accurate identification of epileptic EEG, and applications often require recalibration and training of the base model using individual labeled data. To overcome this limitation, we propose a cross-subject transfer learning algorithm based on mutual information decomposition driven representation learning (MIDRL). The algorithm first introduces the structured state space sequence model to capture the long-term dependencies of epileptic EEG, and the residual module is used to mine the deep information between channels. Additionally, the mutual information estimation is employed to decompose the middle layer features of the network into domain-invariant representations and domain-specific representations, with the dynamically learnable weight updating mechanism to adaptively balance the learning tasks associated with the two representations. Finally, to address the problem of target samples being easily confused near the classification boundary, the minimum class confusion loss is introduced to reduce the class correlation predicted by the classifier. Experimental results demonstrate that the proposed algorithm effectively retains patterns of seizure region and exhibits strong performance for cross-subject seizure detection.},
  keywords={Electroencephalography;Brain modeling;Mutual information;Training;Epilepsy;Feature extraction;Representation learning;Adaptation models;Temperature measurement;Spatiotemporal phenomena;Seizure detection;individual variability;transfer learning;mutual information},
  doi={10.1109/LSP.2025.3576642},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{9408926,
  author={Zhang, Weiran and Chen, Xi and Li, Wei},
  booktitle={2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={Minimizing Outputs’ Differences of Classifiers with Different Responsibilities for Unsupervised Adversarial Domain Adaptation}, 
  year={2021},
  volume={},
  number={},
  pages={1188-1195},
  abstract={The adversarial domain adaptation has made many achievements in the field of Unsupervised Domain Adaptation (UDA). However, initial adversarial domain adaptation methods do not consider the category-level domain adaptation, causing confusion between different categories in the different domains. This paper propose a UDA approach called Minimizing Outputs' Differences of Classifiers with Different Responsibilities (MODCDR) to improve this situation. Its framework consists of one generator and two different task-specific classifiers, one working on the source domain, while the other working on the target domain. By training the feature generator to minimize the outputs' discrepancy of these two classifiers, this method not only generates discriminative features, but also achieves the category-level feature distribution alignment simultaneously. In addition, these two classifiers can also produce and filter the reliable pseudo-labels on the target domain, being used for assisting our training. This paper tested this method on three publicly available transfer learning object recognition datasets, with comparison to recent unsupervised adversarial domain adaptation approaches. The results show that the proposed method has superior performance.},
  keywords={Training;Transfer learning;Signal processing;Generators;Reliability;Object recognition;Task analysis;adversarial domain adaptation;discriminative features;category-level feature distribution alignment;pseudo-label},
  doi={10.1109/ICSP51882.2021.9408926},
  ISSN={},
  month={April},}@INPROCEEDINGS{11113016,
  author={Xiang, Sheng and Xu, Chenhao and Cheng, Dawei and Wang, Xiaoyang and Zhang, Ying},
  booktitle={2025 IEEE 41st International Conference on Data Engineering (ICDE)}, 
  title={Efficient Learning-Based Graph Simulation for Temporal Graphs}, 
  year={2025},
  volume={},
  number={},
  pages={251-264},
  abstract={Graph simulation has recently received a surge of attention in graph processing and analytics. In real-life applications, e.g. social science, biology, and chemistry, many graphs are composed of a series of evolving graphs (i.e., temporal graphs). While most of the existing graph generators focus on static graphs, the temporal information of the graphs is ignored. In this paper, we focus on simulating temporal graphs, which aim to reproduce the structural and temporal properties of the observed real-life temporal graphs. In this paper, we first give an overview of the existing temporal graph generators, including recently emerged learning-based approaches. Most of these learning-based methods suffer from one of the limitations: low efficiency in training or slow generating, especially for temporal random walk-based methods. Therefore, we propose an efficient learning-based approach to generate graph snapshots, namely temporal graph autoencoder (TGAE). Specifically, we propose an attention-based graph encoder to encode temporal and structural characteristics on sampled ego-graphs. And we proposed an ego-graph decoder that can achieve a good trade-off between simulation quality and efficiency in temporal graph generation. Finally, the experimental evaluation is conducted among our proposed TGAE and representative temporal graph generators on real-life temporal graphs and synthesized graphs. It is reported that our proposed approach outperforms the state-of-the-art temporal graph generators by means of simulation quality and efficiency.},
  keywords={Training;Learning systems;Social sciences;Autoencoders;Memory management;Generators;Biology;Graph neural networks;Surges;Information technology;Graph Simulation;Temporal Graphs;Graph Neural Network},
  doi={10.1109/ICDE65448.2025.00026},
  ISSN={2375-026X},
  month={May},}@ARTICLE{11071375,
  author={Chen, Mulin and Wang, Yajie and Li, Xuelong},
  journal={IEEE Transactions on Multimedia}, 
  title={PIMG: Progressive Image-to-Music Generation with Contrastive Diffusion Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={The goal of Image-to-Music Generation is to create pure music according to the given image. Unlike existing tasks such as text-to-image generation, there is no explicit connection between image content and musical melody. Some existing studies attempt to generate music by directly mapping image features (such as color, edges, etc.) into musical notes, which may result in the melodic incoherence. Inspired by neuroscience, it is desirable to employ emotion to bridge these two modalities. However, the continuity and complexity of emotions make it difficult to capture the cross-modal correlation. Drawing from human perception mechanisms of emotions, a Progressive Image-to-Music Generation (PIMG) framework is proposed. The framework designs a mean-teacher based association network to guide the music generation process progressively, starting from highly correlated image-music pairs. The generation network receives more challenging sample pairs gradually, eventually capturing complex cross-modal emotional correspondences. Additionally, a contrastive learning strategy is introduced into the diffusion models to better capture the consistency between pieces of music with the similar emotions. Extensive experimental results demonstrate that the proposed framework is able to generate high-quality and emotionally consistent music from images.},
  keywords={Music;Image color analysis;Training;Diffusion models;Feature extraction;Data models;Complexity theory;Videos;Visualization;Transformers;Image-to-music generation;semi-supervised learning;diffusion models;multi-modal cognitive computing},
  doi={10.1109/TMM.2025.3586119},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{11084295,
  author={Yadav, Nand Kumar and Mehmood, Rayeesa and Rizk, Rodrigue and Santosh, KC},
  booktitle={2025 IEEE International Conference on Image Processing (ICIP)}, 
  title={SCL-GAN: Spatially-Correlative Lightweight GAN for Efficient and High-Fidelity Thermal-Visible Face Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={2049-2054},
  abstract={This work introduces SCL-GAN (Spatially-Correlative Lightweight GAN), a novel architecture for facial image reconstruction using thermal face images, designed for efficient execution on edge devices such as the NVIDIA Jetson board. The proposed architecture leverages spatial feature correlations across thermal-visible modalities while maintaining a low parameter count and FLOPs. Experimental results show that SCL-GAN achieves a 68.57% reduction in computational cost (GMac) and a 71.71% reduction in trainable parameters, compared to baseline models. Moreover, we observe consistent improvements in image quality metrics, including a 5.05% increase in SSIM, 4.49% reduction in VGG-FaceLoss, and a 27.83% reduction in FID on the WHU-IIP dataset. On the CVBL-CHILD dataset, SCL-GAN demonstrates an 11.70% SSIM improvement, 18.21% VGG-FaceLoss reduction, and a 47.88% drop in FID. The code is available at: https://github.com/GANGREEK/SCL-GAN.git.},
  keywords={Image quality;Training;Measurement;Translation;Image edge detection;Computational modeling;Computer architecture;Real-time systems;Computational efficiency;Faces;Spatially-Correlative Lightweight GAN;Thermal-Visible Face Synthesis;Edge Devices},
  doi={10.1109/ICIP55913.2025.11084295},
  ISSN={2381-8549},
  month={Sep.},}@ARTICLE{10960542,
  author={Zhang, Jun and Zhang, Xupeng and Liu, Shuai and Pan, Bin and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={FIE-Net: Foreground Instance Enhancement Network for Domain Adaptation Object Detection in Remote Sensing Imagery}, 
  year={2025},
  volume={63},
  number={},
  pages={1-14},
  abstract={Domain adaptation methods can mitigate performance degradation in remote sensing image object detection that arises from interdomain differences. However, current approaches often overlook the focused attention on foreground features, and the differences between the foreground and the background in remote sensing images implicitly diminish the characteristics of the foreground. To address this challenge, we propose a foreground instance enhancement network (FIE-Net) to balance the differences between the foreground and the background in remote sensing images, while enhancing the alignment and application of foreground features. The FIE-Net cooperates with the foreground-focused multigranularity feature alignment (FMA) module with the label filtering and application (LFA) module, progressively focusing on the salient foreground features during the processes of feature alignment and label application. Specifically, FMA directs feature alignment toward the foreground focus during the multigranularity feature alignment process, through foreground-focus perception attention and instance-centered emphasis approach. LFA balances the foreground and background difference information contained in labels, through the hybrid threshold label filtering method and the progressive label switching strategy. The experimental results indicate superior performance and generalization capabilities of our proposed FIE-Net in multiple remote sensing adaptation scenarios. The code is released at https://github.com/Lab-PANbin/},
  keywords={Remote sensing;Feature extraction;Object detection;Training;Filtering;Computational modeling;Fast Fourier transforms;Detectors;Data mining;Classification algorithms;Domain adaptation;foreground focus;object detection;remote sensing imagery},
  doi={10.1109/TGRS.2025.3557171},
  ISSN={1558-0644},
  month={},}@ARTICLE{11018804,
  author={Zhou, Yi and Wang, Zhangyun and Ning, Nianwen and Lu, Ning and Dong, Mianxiong and Shen, Xuemin},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Vehicular Multimodal Motion Forecasting with Scenario-Guided Diffusion Probabilistic Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Ensuring the safe driving and decision-making of autonomous vehicles requires reliable modeling of the multimodal motion of vehicles. In this paper, we propose a novel scenario-guided diffusion probabilistic model called RoadDiff to address these issues. By modeling and interacting with scene-contextual heterogeneous data at the feature level, we achieve lightweight and high-quality representations of traffic dynamics. We interpolate these representations as guiding conditions into a stochastic iterative denoising process of reverse diffusion, progressively transforming a known prior distribution into a multimodal trajectory distribution that includes the ground truth mode. Our well-designed Transformer-based neural network approximates the denoising matching function of the reverse diffusion process, and our region of interest-based training strategy ensures that the sampled multimodal trajectories conform to road and physical constraints to overcome mode blur. We carefully design two loss functions to ensure multimodal prediction for regions of interest and efficient trajectory guidance during this process. Experimental results on real-world motion datasets demonstrate that RoadDiff outperforms state-of-the-art methods in terms of prediction multimodality and accuracy, indicating its potential as a novel paradigm for vehicular multimodal motion prediction.},
  keywords={Trajectory;Vehicle dynamics;Training;Predictive models;Diffusion models;Context modeling;Semantics;Data models;Accuracy;Stochastic processes;Connected and automated vehicles;heterogeneous representation;multimodal motion prediction;diffusion probabilistic model},
  doi={10.1109/TVT.2025.3575478},
  ISSN={1939-9359},
  month={},}@INPROCEEDINGS{10649947,
  author={Ji, Hongling and Liu, Hao and Liang, Jiuzhen},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={PICLAnony: Anonymous Face Generation with Controllable Attributes based on Parametric Imitative Contrastive Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={As personal photos are widely shared on social media, face anonymization becomes an effective solution to avoid identity leakage. Aiming at the problems of low quality and uncontrollable attributes of anonymized faces in existing algorithms, we propose a face anonymization algorithm PICLAnony based on parametric imitation contrast learning. It transfers the four visual information of identity, expression, pose and illumination from the source image to the generated anonymized face image by parametric imitation contrast learning. And it edits these attribute features that reflect sensitive behavioral intentions under the premise of controllable background. In the parameter imitation learning stage, high-quality and pose-controllable anonymized faces are generated by imitating the semantic parameters of source images. In the parameter contrast learning stage, the semantic parameters of the edited generated image and the source image are compared and learned, which solves the problem of insufficient decoupling of expression and illumination attributes. In addition, a background control module is designed to keep the background controllable during the editing process of anonymous face facial attributes. The subjective and objective results demonstrate that PICLAnony outperforms the state-of-the-art methods in terms of image quality and editing of facial attributes of anonymized faces.},
  keywords={Data privacy;Visualization;Social networking (online);Imitation learning;Semantics;Lighting;Information filtering;anonymous face generation;sensitive behavioral intent protection;parameter imitation contrast learning;attribute control},
  doi={10.1109/IJCNN60899.2024.10649947},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10884211,
  author={Abdu-Aguye, Mubarak G. and Zaheer, Muhammad Zaigham and Nandakumar, Karthik},
  booktitle={2024 IEEE International Conference on Data Mining (ICDM)}, 
  title={Feature Map Purification for Enhancing Adversarial Robustness of Deep Timeseries Classifiers}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Deep-learning based timeseries classifiers are known to be susceptible to adversarial attacks, where the adversary adds imperceptible perturbations to the input sample to cause mis-classification. While principled adversarial defense mechanisms such as adversarial training and certified robustness have been proposed for image classifiers, they have seldom been studied in the timeseries domain. Existing defenses for timeseries classifiers are primarily centered around adversarial sample detection, but have mixed performance and fail to generalize well across attacks. This work proposes an alternative approach based on purifying the intermediate representations within a deep convolutional timeseries (DCT) classifier. We design a learnable sub-network with residual connections that filters the feature maps in multiple wavelet basis spaces to suppress the adversarial perturbations. Given any pretrained non-robust DCT classifier, the proposed feature map purification (FeMPure) module can be trained in isolation without affecting the given classifier and can be seamlessly plugged back in to enhance the adversarial robustness of the original classifier. Experiments based on 2 well-known architectures for DCT classifiers, 6 adversarial attacks, and 80 public-domain datasets demonstrate that the proposed FeMPure approach can provide good adversarial robustness, irrespective of whether the adversary is unaware or has full knowledge of the defense mechanism. With minor modifications, the FeMPure approach can also be employed for adversarial sample detection or for enhancing certified robustness.},
  keywords={Training;Filters;Accuracy;Purification;Perturbation methods;Computer architecture;Robustness;Discrete cosine transforms;Data mining;Computational complexity;adversarial robustness;feature maps;purification;timeseries;wavelets},
  doi={10.1109/ICDM59182.2024.00007},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{10919586,
  author={Liu, Dongyu and Wang, Xuhong and Chen, Cen and Wang, Yanhao and Yao, Shengyue and Lin, Yilun},
  booktitle={2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={SVIA: A Street View Image Anonymization Framework for Self-Driving Applications}, 
  year={2024},
  volume={},
  number={},
  pages={3567-3574},
  abstract={In recent years, there has been an increasing interest in image anonymization, particularly focusing on the de-identification of faces and individuals. However, for self-driving applications, merely de-identifying faces and individuals might not provide sufficient privacy protection since street views like vehicles and buildings can still disclose locations, trajectories, and other sensitive information. Therefore, it remains crucial to extend anonymization techniques to street view images to fully preserve the privacy of users, pedestrians, and vehicles. In this paper, we propose a Street View Image Anonymization (SVIA) framework for self-driving applications. The SVIA framework consists of three integral components: a semantic segmenter to segment an input image into functional regions, an inpainter to generate alternatives to privacy-sensitive regions, and a harmonizer to seamlessly stitch modified regions to guarantee visual coherence. Compared to existing methods, SVIA achieves a much better trade-off between image generation quality and privacy protection, as evidenced by experimental results for five common metrics on two widely used public datasets.},
  keywords={Data privacy;Privacy;Image segmentation;Visualization;Semantics;Streaming media;Information filtering;Protection;Faces;Information integrity},
  doi={10.1109/ITSC58415.2024.10919586},
  ISSN={2153-0017},
  month={Sep.},}@ARTICLE{11002732,
  author={Liu, Yici and Qin, Lang and Chen, Xin and Le Bouquin Jeannes, Regine and Louis Coatrieux, Jean and Shu, Huazhong},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Advancing Cross-Subject Domain Generalization in Brain–Computer Interfaces With Multiadversarial Strategies}, 
  year={2025},
  volume={74},
  number={},
  pages={1-12},
  abstract={A cross-subject domain generalization (DG) approach with multiadversarial strategies (DGMA) is introduced to reduce brain-computer interfaces (BCIs) systems’ dependency on high-quality, subject-specific electroencephalographic (EEG) data, making it adaptable to unseen domains. DGMA leverages annotated training data from other subjects and consists of three modules: 1) prefeature extraction (PFE), enhancing EEG signal separability through preprocessing, data augmentation, and tangent space mapping; 2) distribution feature updater (DFU), aligning intersubject feature distributions with marginal maximum mean discrepancy (MMD); and 3) multiadversarial training (MAT), initially using gradient reversal layer (GRL) to amplify domain differences and classification loss, allowing the model to learn diverse domain-specific features before minimizing these differences to balance domain transferability and discriminability. DGMA is capable of better capturing domain-specific features while achieving stronger generalization compared with traditional methods focused solely on minimizing domain differences. Validated on four motor imagery datasets, DGMA achieved state-of-the-art accuracies of 76.1% on BCI Competition IV 2a and 72.4% on the 002-2014 dataset. Additional tests on a private fatigue dataset and the SEED dataset yielded accuracies of 99.5% and 86.6%, respectively. The code can be found at https://github.com/liuyici/DGMA-BCI},
  keywords={Electroencephalography;Brain modeling;Training;Feature extraction;Data models;Data mining;Data augmentation;Manifolds;Costs;Space mapping;Adversarial neural network;domain generalization (DG);electroencephalographic (EEG);transfer learning},
  doi={10.1109/TIM.2025.3566804},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10032408,
  author={Liao, Haohong and Zheng, Silin and Shen, Xuelin and Li, Mark Junjie and Wang, Xu},
  booktitle={2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Semi-automatic Data Annotation System for Multi-Target Multi-Camera Vehicle Tracking}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Multi-target multi-camera tracking (MTMCT) plays an important role in intelligent video analysis, surveillance video retrieval, and other application scenarios. Nowadays, the deep-learning-based MTMCT has been the mainstream and has achieved fascinating improvements regarding tracking accuracy and efficiency. However, according to our investigation, the lacking of datasets focusing on real-world application scenarios limits the further improvements for current learning-based MTMCT models. Specifically, the learning-based MTMCT models training by common datasets usually cannot achieve satisfactory results in real-world application scenarios. Motivated by this, this paper presents a semi-automatic data annotation system to facilitate the real-world MTMCT dataset establishment. The proposed system first employs a deep-learning-based single-camera trajectory generation method to automatically extract trajectories from surveillance videos. Subsequently, the system provides a recommendation list in the following manual cross-camera trajectory matching process. The recommendation list is generated based on side information, including camera location, timestamp relation, and background scene. In the experimental stage, extensive results further demonstrate the efficiency of the proposed system.},
  keywords={Training;Annotations;Surveillance;Loading;Manuals;Cameras;Trajectory;Multi-Object Tracking;Multi-Target Multi-Camera Tracking;Vehicle Re-Identification;Semi-Automatic;Data Annotation},
  doi={10.1109/DSAA54385.2022.10032408},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11010556,
  author={Zhao, Hengheng and Huang, Ping and Wang, Ruolong and Wang, Xiying},
  booktitle={2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT)}, 
  title={Efficient Additive 3D Modeling from Single Images via Layered Morphing Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={577-584},
  abstract={A novel technique for single-image-driven rapid additive 3D modeling is presented, integrating principles of 3D printing with topological relationships between adjacent layers. By incorporating innovative curve morphing techniques, efficient transformation from a single image to complex three-dimensional models is achieved. Experimental evaluations demonstrate that superior performance in modeling efficiency, geometric accuracy, and visual fidelity is attained, particularly for regular geometric shapes and transitional forms. While potential for refinement in detailed reconstruction of complex cultural artifacts is noted, competitive overall results are still observed. Compared to traditional manual modeling, significant enhancements in speed are realized, while high levels of accuracy and appearance similarity are maintained, and data requirements along with computational complexity are substantially reduced. Practical solutions for rapid product design, digital heritage preservation, and augmented/virtual reality (AR/VR) applications are provided, with optimized layer structuring and topological control addressing precision limitations inherent in deep learning approaches. This research advances the field of 3D modeling by offering a robust, efficient framework with broad theoretical and practical implications.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Additives;Accuracy;Shape;Computational modeling;Three-dimensional printing;Product design;Cultural differences;Additive Modeling;Curve Morphing;Inter-Layer Topology;Single-Image Modeling},
  doi={10.1109/ISCAIT64916.2025.11010556},
  ISSN={},
  month={March},}@INPROCEEDINGS{10411812,
  author={Zhang, Zunhao and Chen, Zhenhao and Shen, Yifan and Guan, Dayan and Kot, Alex},
  booktitle={2023 IEEE Ninth Multimedia Big Data (BigMM)}, 
  title={Temporality-guided Masked Image Consistency for Domain Adaptive Video Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={56-63},
  abstract={Video semantic segmentation has witnessed substantial advancements, largely due to the vast volume of labeled training samples. Nevertheless, domain adaptive video segmentation that adapts from a labeled source domain to an unlabeled target domain remains insufficiently delved into. In this paper, we propose Temporality-guided Masked Image Consistency (TgMIC), a simple yet effective approach that leverages the concept of Masked Image Modeling (MIM) to learn semantic features in the target domain. Unlike random masking strategy applied in traditional MIM, TgMIC introduces a novel temporality-guided masking strategy that samples the mask according to the distribution of optical flow, which facilitate the learning of spatial context relations in video sequence. Specifically, TgMIC masks the patches in vision transformers where the variance of optical flow is large, as these patches are known to contain noisy estimates of optical flow. In order to learn semantic information for video segmentation, TgMIC reconstructs the predictions of original frames from the masked frames. Comprehensive tests and detailed analysis on various public datasets show that our mechanism stands out, outpacing contemporaneous techniques, while ensuring no additional time costs.},
  keywords={Training;Image segmentation;Adaptation models;Video sequences;Semantics;Transformers;Optical flow;Deep Learning;Big Video Data;Semantic Segmentation;Domain Adaptation;Masked Image Modeling},
  doi={10.1109/BigMM59094.2023.00015},
  ISSN={},
  month={Dec},}@ARTICLE{10654578,
  author={Li, Mingxu and Peng, Bo and Zhai, Donghai},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Latent Space Segmentation Model for Visual Surface Defect Inspection}, 
  year={2024},
  volume={73},
  number={},
  pages={1-11},
  abstract={There are a huge number of models that claim to enhance visual surface defect inspection accuracy. However, as these models generally function directly within the pixel space, optimizing advanced segmentation techniques frequently demands substantial computational resources and poses challenges for inference on devices with limited computing power. In addition, many current methodologies are deeply reliant on extensive surface defect datasets. In response to these challenges, our research presents a novel approach based on an auto-encoder structure that uses “latent space” to refine defect segmentation. Within the encoder segment of the autoencoder, we’ve incorporated contrastive learning, amplifying both feature extraction and segmentation capabilities. This architectural choice not only tailors the strategy for prompt response scenarios and underscores its precision in high-accuracy applications, but also addresses the challenges posed by the scarcity of defect samples. As a means to assess our approach and better cater to industrial applications that prioritize sample-level accuracy, we introduce innovative sample-level metrics, namely, mostly segmented (MS) and mostly lost (ML). Experiments conducted on the RSDD and Neuseg datasets underscore the strategy’s steadfast performance under diverse data circumstances. Synthesizing the benefits of latent space and contrastive learning, this article delineates proficient methodology for surface defect segmentation.},
  keywords={Inspection;Image segmentation;Accuracy;Training;Contrastive learning;Rail transportation;Defect detection;Contrastive learning;image segmentation;latent segmentation;railway inspection;surface defect inspection},
  doi={10.1109/TIM.2024.3446650},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10651383,
  author={Sun, Zheyi and Liu, Hao and Liang, Jiuzhen},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Eyes Attribute Editing Assisted by Dual-Coordinate System and Multi-Probability Fusion Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The eye area, which can effectively convey social information and behavioral intentions, contains most of the information in facial images. However, the occlusion of glasses makes it difficult to accurately obtain the eye information in some facial images. Although the existing attribute editing method can remove glasses from face images, there are existing drawbacks lay in incomplete eye information and insufficient disentanglement after the glasses were removed (e.g. the background and hair color also changed when the glasses were removed), unable to get real and natural eye editing results. To address this problem, we propose an eye attribute editing model assisted by dual-coordinate system and multi-probability fusion prediction. The former one is designed to enhance the position awareness of inpainting to directly focus on the eyes area while the later one could integrate both local and global information of the target area. Specifically, the first stage of our method is to remove the glasses, and then use multi-probability fusion prediction method to inpaint the eye area to obtain a reasonably complete eye editing result supported by dual-coordinate system. On the one hand, we designed a dual-coordinate positioning module that uses a cascaded classifier and a dual-coordinate system to locate the human eye height area mask, which effectively solves the problem of inconsistent eye position height in facial images and determines the editing area for inpainting after the glasses are removed. On the other hand, we employed a multi-probability fusion prediction algorithm to supplement and enhance the ocular information, which can combine a distance-based local probability and an adaptive global probability, resulting in improved editing results. Quantitative and qualitative evaluations show that our method outperforms the state-of-the-art methods.},
  keywords={Location awareness;Hair;Adaptation models;Image color analysis;Neural networks;Glass;Predictive models;attribute editing;multi-probability fusion prediction;dual-coordinate eye height positioning},
  doi={10.1109/IJCNN60899.2024.10651383},
  ISSN={2161-4407},
  month={June},}
