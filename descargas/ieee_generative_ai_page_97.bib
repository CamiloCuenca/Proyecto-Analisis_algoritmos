@ARTICLE{9980373,
  author={Cullen, Drake and Halladay, James and Briner, Nathan and Basnet, Ram and Bergen, Jeremy and Doleck, Tenzin},
  journal={IEEE Access}, 
  title={Evaluation of Synthetic Data Generation Techniques in the Domain of Anonymous Traffic Classification}, 
  year={2022},
  volume={10},
  number={},
  pages={129612-129625},
  abstract={Anonymous network traffic is more pervasive than ever due to the accessibility of services such as virtual private networks (VPN) and The Onion Router (Tor). To address the need to identify and classify this traffic, machine and deep learning solutions have become the standard. However, high-performing classifiers often scale poorly when applied to real-world traffic classification due to the heavily skewed nature of network traffic data. Prior research has found synthetic data generation to be effective at alleviating concerns surrounding class imbalance, though a limited number of these techniques have been applied to the domain of anonymous network traffic detection. This work compares the ability of a Conditional Tabular Generative Adversarial Network (CTGAN), Copula Generative Adversarial Network (CopulaGAN), Variational Autoencoder (VAE), and Synthetic Minority Over-sampling Technique (SMOTE) to create viable synthetic anonymous network traffic samples. Moreover, we evaluate the performance of several shallow boosting and bagging classifiers as well as deep learning models on the synthetic data. Ultimately, we amalgamate the data generated by the GANs, VAE, and SMOTE into a comprehensive dataset dubbed CMU-SynTraffic-2022 for future research on this topic. Our findings show that SMOTE consistently outperformed the other upsampling techniques, improving classifiers’ F1-scores over the control by ~7.5% for application type characterization. Among the tested classifiers, Light Gradient Boosting Machine achieved the highest F1-score of 90.3% on eight application types.},
  keywords={Virtual private networks;Telecommunication traffic;Synthetic data;Data models;Generative adversarial networks;Feature extraction;Deep learning;Anonymous traffic;synthetic data;CopulaGAN;CTGAN;SMOTE;VAE;TabNet;deep learning;machine learning;unbalanced data},
  doi={10.1109/ACCESS.2022.3228507},
  ISSN={2169-3536},
  month={},}@ARTICLE{10632167,
  author={Zhang, Qingjie and Wang, Xiaoying and Li, Chunhui},
  journal={IEEE Access}, 
  title={SA-WGAN-Based Optimization Method for Network Traffic Feature Camouflage}, 
  year={2024},
  volume={12},
  number={},
  pages={111142-111157},
  abstract={In the field of network security attack and defense, attackers frequently utilize network monitoring to analyze traffic features and obtain user privacy. Most defense methods employ feature-based traffic morphing techniques. However, the existing traffic camouflage method based on the Wasserstein Generative Adversarial Network (WGAN) exhibits limited defensive effectiveness, as the transformed traffic can still be detected due to the inherent limitations of the model and algorithm. In this paper, we propose a Wasserstein Generative Adversarial Network model with a Self-Attention mechanism (SA-WGAN) and adjust the parameter of the discriminator. Simultaneously, in the traffic generation algorithm, a constraint on padding packets was added: the Jaccard Index of the set of statistical features of the generated traffic must reach a threshold of 0.25, while ensuring the inclusion of three-way handshake packets to optimize the camouflage effect. To verify the camouflage effectiveness of the optimized defense method, we conduct a series of adversarial attacks. Experimental results show that the feature defense method based on SA-WGAN can significantly reduce the detection accuracy of monitored traffic. Compared to the feature defense method based on WGAN, it decreases the classification accuracy by 9.15% under the Panchenko attack, effectively enhancing the defensive capability and successfully increasing the difficulty for attackers to penetrate network traffic.},
  keywords={Generators;Monitoring;Feature extraction;Telecommunication traffic;Generative adversarial networks;Training;Servers;Communication systems;Security;Communication security;feature generation;SA-WGAN;traffic camouflage;traffic generation algorithm},
  doi={10.1109/ACCESS.2024.3441034},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9434124,
  author={George, Yasmeen and Sedai, Suman and Antony, Bhavna J. and Ishikawa, Hiroshi and Wollstein, Gadi and Schuman, Joel S. and Garnavi, Rahil},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)}, 
  title={Dueling Deep Q-Network For Unsupervised Inter-Frame Eye Movement Correction In Optical Coherence Tomography Volumes}, 
  year={2021},
  volume={},
  number={},
  pages={1595-1599},
  abstract={In optical coherence tomography (OCT) volumes of retina, the sequential acquisition of the individual slices makes this modality prone to motion artifacts, misalignments between adjacent slices being the most noticeable. Any distortion in OCT volumes can bias structural analysis and influence the outcome of longitudinal studies. The presence of speckle noise characteristic of this imaging modality leads to inaccuracies when traditional registration techniques are employed. Also, the lack of a well-defined ground truth makes supervised deep-learning techniques ill-posed to tackle the problem. In this paper, we tackle these issues by using deep reinforcement learning to correct inter-frame movements in an unsupervised manner. Specifically, we use dueling deep Q-network to train an artificial agent to find the optimal policy, i.e. a sequence of actions, that best improves the alignment by maximizing the sum of reward signals. Instead of relying on the ground-truth of transformation parameters to guide the rewarding system, for the first time, we use a combination of intensity based image similarity metrics. Further, to avoid the agent bias towards speckle noise, we ensure the agent can see retinal layers as part of the interacting environment. For quantitative evaluation, we simulate the eye movement artifacts by applying 2D rigid transformations on individual B-scans. The proposed model achieves an average of 0.985 and 0.914 for normalized mutual information and correlation coefficient, respectively. We also compare our model with elastix intensity based medical image registration approach, where significant improvement is achieved by our model for both noisy and denoised volumes.},
  keywords={Image registration;Optical coherence tomography;Reinforcement learning;Optical distortion;Speckle;Retina;Noise measurement;Dueling deep Q-network;reinforcement learning;motion correction;artificial agents;optical coherence tomography},
  doi={10.1109/ISBI48211.2021.9434124},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{10586696,
  author={Zhang, Yunhao and Jia, Menglin and Zhou, Wenbo and Yang, Yang},
  booktitle={2024 3rd International Conference on Image Processing and Media Computing (ICIPMC)}, 
  title={Quality Assessment of AI-Generated Image Based on Cross-modal Correlation}, 
  year={2024},
  volume={},
  number={},
  pages={378-384},
  abstract={Artificial Intelligence Generated Content (AIGC), which encompasses various types of content including images, has been developed with the assistance of artificial intelligence technology. Artificial Intelligence-Generated Images (AIGIs) are typically generated by inputting text prompt words into image generation models. The ongoing creation of AIGI presents new problems and challenges. A significant challenge is that AIGI compared to natural images may have some unique distortions and the quality of AIGIs is inconsistent and various. So it is of great significance to evaluate AIGI more comprehensively and AIGC Image Quality Assessment (AIGCIQA) has emerged as a new topic in computer vision. There are few Image Quality Assessment (IQA) methods specifically for AIGIs, and traditional IQA methods ignore the information embedded in the text prompts associated with these images. In this paper, we propose a framework for fusing text and image features. Specifically, we operate by processing text prompts and corresponding generated images. We use the RoBERTa pre-trained language model to extract text features, and we use the CNN+Transformer hybrid network architecture to extract image features, and then add a multi-head attention mechanism when fusing the extracted features to effectively learn the association information between text and images, and input it into the regression network to regress the prediction score. We conduct extensive experiments on three mainstream AIGCIQA databases, including AGIQA-1K, AGIQA-3K, and AIGCIQA2023, and the results demonstrate the effectiveness of our proposed fusion framework.},
  keywords={Image quality;Attention mechanisms;Databases;Predictive models;Network architecture;Media;Feature extraction;AIGC;Image Quality Assessment;feature fusion},
  doi={10.1109/ICIPMC62364.2024.10586696},
  ISSN={},
  month={May},}@INPROCEEDINGS{10771097,
  author={Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J. and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng Polo},
  booktitle={2024 IEEE Visualization and Visual Analytics (VIS)}, 
  title={Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion}, 
  year={2024},
  volume={},
  number={},
  pages={96-100},
  abstract={Diffusion-based generative models’ impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.},
  keywords={Visualization;Image synthesis;Visual analytics;Blogs;Text to image;Transforms;Animation;Generative AI;Machine Learning;Interactive visualization;Text-to-image generative AI;Artificial Intelligence;User study},
  doi={10.1109/VIS55277.2024.00027},
  ISSN={2771-9553},
  month={Oct},}@ARTICLE{10047894,
  author={Madni, Hussain Ahmad and Umer, Rao Muhammad and Foresti, Gian Luca},
  journal={IEEE Access}, 
  title={Blockchain-Based Swarm Learning for the Mitigation of Gradient Leakage in Federated Learning}, 
  year={2023},
  volume={11},
  number={},
  pages={16549-16556},
  abstract={Federated Learning (FL) is a machine learning technique in which collaborative and distributed learning is performed, while the private data reside locally on the client. Rather than the data, only gradients are shared among all collaborative nodes with the help of a central server. To ensure the data privacy, the gradients are prone to the deformation, or the representation is perturbed before sharing, ultimately reducing the performance of the model. Recent studies show that the original data can still be recovered using latent space (i.e., gradient leakage problem) by Generative Adversarial Network and different optimization algorithms such as Bayesian and Covariance Matrix Adaptation Evolution Strategy. To address the issues of data privacy and gradient leakage, in this paper, we train deep neural networks by exploiting the blockchain-based Swarm Learning (SL) framework. In the SL scheme, instead of sharing perturbed or noisy gradients to the central server, we share the original gradients among authenticated (i.e., blockchain-based smart contract) training nodes. To demonstrate the effectiveness of the SL approach, we evaluate the proposed approach using the standard CIFAR10 and MNIST benchmark datasets and compare it with the other existing methods.},
  keywords={Data models;Particle swarm optimization;Peer-to-peer computing;Blockchains;Servers;Federated learning;Smart contracts;Blockchain;data privacy;federated learning;gradient leakage;model privacy;Swarm Learning},
  doi={10.1109/ACCESS.2023.3246126},
  ISSN={2169-3536},
  month={},}@ARTICLE{9893574,
  author={Zhou, Dongliang and Zhang, Haijun and Yang, Kai and Liu, Linlin and Yan, Han and Xu, Xiaofei and Zhang, Zhao and Yan, Shuicheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Learning to Synthesize Compatible Fashion Items Using Semantic Alignment and Collocation Classification: An Outfit Generation Framework}, 
  year={2024},
  volume={35},
  number={4},
  pages={5226-5240},
  abstract={The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this article, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module (SAM), which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module (CCM), which is used to improve the compatibility of a synthesized outfit. To evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20 000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform the state-of-the-art methods in terms of similarity, authenticity, and compatibility measurements.},
  keywords={Feature extraction;Generative adversarial networks;Task analysis;Biological system modeling;Semantics;Footwear;Training;Fashion compatibility learning;fashion synthesis;generative adversarial network (GAN);image-to-image translation;outfit generation},
  doi={10.1109/TNNLS.2022.3202842},
  ISSN={2162-2388},
  month={April},}@ARTICLE{9837021,
  author={Li, Yike and Tian, Yunzhe and Tong, Endong and Niu, Wenjia and Xiang, Yingxiao and Chen, Tong and Wu, Yalun and Liu, Jiqiang},
  journal={Tsinghua Science and Technology}, 
  title={Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence}, 
  year={2023},
  volume={28},
  number={1},
  pages={27-38},
  abstract={Reinforcement learning (RL), one of three branches of machine learning, aims for autonomous learning and is now greatly driving the artificial intelligence development, especially in autonomous distributed systems, such as cooperative Boston Dynamics robots. However, robust RL has been a challenging problem of reliable aspects due to the gap between laboratory simulation and real world. Existing efforts have been made to approach this problem, such as performing random environmental perturbations in the learning process. However, one cannot guarantee to train with a positive perturbation as bad ones might bring failures to RL. In this work, we treat robust RL as a multi-task RL problem, and propose a curricular robust RL approach. We first present a generative adversarial network (GAN) based task generation model to iteratively output new tasks at the appropriate level of difficulty for the current policy. Furthermore, with these progressive tasks, we can realize curricular learning and finally obtain a robust policy. Extensive experiments in multiple environments demonstrate that our method improves the training stability and is robust to differences in training/test conditions.},
  keywords={Training;Perturbation methods;Reinforcement learning;Generative adversarial networks;Multitasking;Particle measurements;Stability analysis;robust reinforcement learning;generative adversarial network (GAN) based model;curricular learning},
  doi={10.26599/TST.2021.9010076},
  ISSN={1007-0214},
  month={February},}@ARTICLE{10466780,
  author={Paul, Dipanjyoti and Rana, Shivani and Saha, Sriparna and Mathew, Jimson},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Online Summarization of Microblog Data: An Aid in Handling Disaster Situations}, 
  year={2024},
  volume={11},
  number={3},
  pages={4029-4039},
  abstract={During any natural disaster or unfortunate accident, both civilians and responders need information on an urgent basis. In such events, microblogging sites particularly Twitter plays an important role in providing real-time information. The raw form of microblog tweets is prodigiously informative but massive in size. The end-users and data analysts have to go through millions of tweets before extraction of any information. To ease the process and extract only relevant information, artificial intelligence (AI)-based techniques can be incorporated to generate summaries from the incoming information. Moreover, tweets keep on arriving continuously in a streaming manner, and therefore in ideal cases, the summaries also need to be updated continuously. In this work, we have proposed a clustering-based summary generation approach that takes multiviewed representations of data and utilizes a new variant of generative adversarial network (GAN) named triple-GAN to perform clustering. Triple-GAN consists of three networks, a generator, a discriminator, and a separator. Maintaining equilibrium among these networks requires proper parameter tuning which makes training of GAN difficult. In the literature, GAN-based techniques have been extensively applied to image datasets. In the proposed method, we have explored the usage of GAN for text data in an unsupervised manner and the analysis of the training of GAN has also been reported. The developed method opens up a new direction in utilizing GAN for solving clustering problem of text data. The proposed method is applied to two versions of four disaster-based microblog datasets and obtained results are compared with many existing and a few baseline methods. The comparative study illustrates the superiority and efficacy of the developed method.},
  keywords={Blogs;Generative adversarial networks;Disasters;Data mining;Social networking (online);Real-time systems;Particle separators;Graph attention network;multiview data;online clustering;online summarization;streaming data;triple generative adversarial network (3-GAN)},
  doi={10.1109/TCSS.2023.3347520},
  ISSN={2329-924X},
  month={June},}@INPROCEEDINGS{10980968,
  author={Robert, Florian and Calovoulos, Alexia and Facq, Laurent and Decoeur, Fanny and Gontier, Etienne and Grosset, Christophe F. and de Senneville, Baudouin Denis},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Improving Cell Instance Segmentation in Scanning Electron Microscopy via Semantic Image Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Understanding tumour tissue architecture is essential for advancing new therapies and necessitates accurate cell instance segmentation in serial scanning electron microscopy (SEM) images. While artificial intelligence (AI)-based methods are effective for this task, errors persist in poor-quality cell contours, requiring manual gap filling. This study addresses cell labelling and contour segmentation by supplementing a standard segmentation technique (nnU-Net) by a specialized cell contour filling network. To train the latter, our approach utilizes a generative adversarial network (GAN)-based image synthesis method to generate realistic disturbed contour maps from randomized coarse cell contour semantic maps (CCSM). Subsequently, cell labelling was produced using the enhanced contours. Our approach was evaluated on private serial block face-SEM (SBF-SEM) images of hepatoblastoma tissues, achieving accurate cell labelling (70.8% labelling) and cell delineations (centerline Dice (clDice)= 0.91). This outperforms the state-of-the-art Cellpose algorithm (29.2% labelling, clDice= 0.34), opening the path to bioarchitecture investigations.},
  keywords={Instance segmentation;Scanning electron microscopy;Image synthesis;Microprocessors;Semantics;Computer architecture;Generative adversarial networks;Filling;Labeling;Standards;Scanning electron microscopy;Segmentation;Cell;Deep neural network;Generative adversarial network},
  doi={10.1109/ISBI60581.2025.10980968},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{10415660,
  author={Wu, Xindong and Zhu, Xingquan and Baralis, Elena and Lu, Ruqian and Kumar, Vipin and Rutkowski, Leszek and Tang, Jie},
  booktitle={2023 IEEE International Conference on Data Mining (ICDM)}, 
  title={On Computing Paradigms - Where Will Large Language Models Be Going}, 
  year={2023},
  volume={},
  number={},
  pages={1577-1582},
  abstract={Computing generates intelligence. With this statement we do not mean computing’s capabilities of manipulating numbers, shapes, symbols, and even logics. What we mean is the ingenious design of computing structures which serve as the basis of intelligence generation during program running. In this panel discussion, we consider how to obtain such capabilities through some computing paradigms as examples, including principal computing, logic computing, discriminative computing, and generative computing. The panelists express their thoughts about the inherent advantages and disadvantages of each of these paradigms, in terms of their adaptivity, interpretability, generality and specificity, and dives into detailed discussions about Large Language Models (LLMs), a mainstream generative paradigm which leverages the strengths of large pre-trained models and downstream prompt tuning to deliver combined intelligence, superior to most existing frameworks in natural language processing. The panel outlines potential challenges of the generative paradigm, with a strong focus on LLMs, and emphasizes that future directions of such models will need to address (1) tackling bias, discrimination, and transparency challenges; (2) delivering logical answers with high specificity; (3) enabling personalized, lightweight, and rapid updating mechanisms; (4) assessing accreditation, tracing, and misusages; and (5) ensuring sustainable LLMs.},
  keywords={Adaptation models;Shape;Computational modeling;Symbols;Data models;Natural language processing;Tuning;computing;artificial intelligence;large language models},
  doi={10.1109/ICDM58522.2023.00211},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{10578974,
  author={AboulEla, Samar and Kashef, Rasha},
  booktitle={2024 IEEE World AI IoT Congress (AIIoT)}, 
  title={Network Intrusion Detection Using a Stacking of AI-driven Models with Sampling}, 
  year={2024},
  volume={},
  number={},
  pages={157-164},
  abstract={Securing computer networks in Internet of Things (IoT) platforms has become increasingly challenging due to rising threats. Identifying unusual activities in IoT networks is crucial for maintaining their security. This research delves into detecting anomalies in network behavior to overcome security challenges. We use artificial intelligence (AI) models, specifically machine learning (ML) and deep learning (DL), for detecting intrusions in the network. We adopt four ML classifiers and implement two methods for assembling the classification outputs using stacking. Training our model on extensive historical network data enhances the ability to recognize abnormal network behaviors effectively. Initial tests on the NSL-KDD benchmark dataset have shown promising results, indicating the potential effectiveness of our approach. We also employed oversampling using Generative Adversarial Networks (GANs) to maintain balance in the data distribution, which led to noticeable improvement, reaching a 73.5% F-score and 61% accuracy compared to baseline models.},
  keywords={Training;Deep learning;Computational modeling;Stacking;Benchmark testing;NSL-KDD;Generative adversarial networks;Cyber-security;Anomaly Detection;Deep Learning;Generative Adversarial Networks},
  doi={10.1109/AIIoT61789.2024.10578974},
  ISSN={},
  month={May},}@INPROCEEDINGS{10981354,
  author={Morales-Chan, Miguel and Amado-Salvatierra, Hector R. and Hernandez-Rizzardini, Rocael and Román, Byron Linares},
  booktitle={2025 IEEE Engineering Education World Conference (EDUNINE)}, 
  title={Enhancing Conference Engagement: Implementing an AI-Powered Educational Chatbot as a Support Assistant for Interactive Learning and Participation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents the development and implementation of an educational chatbot assistant for an academic conference focused on education. Leveraging advancements in AI, the chatbot was contextualized with real conference data, including the agenda, speaker details, session summaries, and FAQs. Attendees could engage directly with the chatbot to receive relevant insights and were prompted with follow-up questions to encourage deeper participation, effectively enhancing interaction with presenters. This approach mirrors a flipped-classroom experience, where AIdriven tools maximize engagement and prepare attendees for active participation. The experience highlights the evolving potential of chatbots to support personalized, interactive experiences in educational events, demonstrating their application beyond traditional rule-based systems toward adaptable, data-informed assistance.},
  keywords={Electronic learning;Accuracy;Generative AI;Atmosphere;Chatbots;Real-time systems;Online services;Mirrors;Feeds;Engineering education;artificial intelligence;Chatbot;generative AI tools;LLMs},
  doi={10.1109/EDUNINE62377.2025.10981354},
  ISSN={},
  month={March},}@INPROCEEDINGS{10042910,
  author={Chang, Rong-Guey and Siao, Cheng-Yan and Lee, Chia-Ying},
  booktitle={2022 IEEE 4th Eurasia Conference on IOT, Communication and Engineering (ECICE)}, 
  title={Postprocessing System for Word-Filling and Quantifiers Based on Chinese Story Generation}, 
  year={2022},
  volume={},
  number={},
  pages={360-361},
  abstract={In recent years, many fields have been related to introducing artificial intelligence to natural language generation. Although these natural language models have excellent results and generate smooth sentences, they are still not effective learning features such as character relationships, especially in the Chinese language. When a sentence is generated, it is necessary to pay attention to the following words to correctly predict and generate, such as quantifiers, which causes the generated words to be inappropriate and then affects the generation of subsequent words. Therefore, we developed a set of attention mechanism enhancement models, aiming at the generative language model that controls the ordering of generated speech parts, revising the different mechanisms of character fighting relationship and quantifier design, and adopting the traditional classification model one-against-all support vector machine training. The results show that the generated sentences are arranged in the same order as the original ones so the generation can be reliably controlled.},
  keywords={Training;Support vector machines;Natural languages;Speech enhancement;Reliability;Artificial intelligence;Sorting;natural language generation;quantifier;character relationships},
  doi={10.1109/ECICE55674.2022.10042910},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10847000,
  author={Kavitha, T. and S P, Manikandan and Patil, Bhimaraya and Patil, Anita},
  booktitle={2024 International Conference on Innovation and Novelty in Engineering and Technology (INNOVA)}, 
  title={Advancements in Distributed Deep Learning: Federated Learning, AutoML Integration, and Beyond}, 
  year={2024},
  volume={I},
  number={},
  pages={1-7},
  abstract={This paper present advancements in distributed deep learning, focusing on federated learning, AutoML integration, and beyond. Leveraging the latest developments in machine learning (ML) and artificial intelligence (AI), our research explores novel approaches to enhance the efficiency, scalability, and security of distributed deep learning systems. This paper introduces federated learning techniques to enable collaborative model training across decentralized edge devices, thereby minimizing data privacy concerns and reducing communication over-head. Additionally, Proposed work investigate the integration of automated machine learning (AutoML) capabilities into distributed training pipelines, streamlining the model selection and hyperparameter tuning processes. Furthermore, Paper also explores cutting-edge advancements beyond traditional deep learning paradigms, such as generative adversarial networks (GANs) and reinforcement learning (RL), to unlock new opportunities in distributed AI. Through experimental evaluations and case studies, Paper demonstrate the effectiveness and potential impact of our proposed advancements, paving the way for future breakthroughs in distributed deep learning research.},
  keywords={Deep learning;Training;Technological innovation;Federated learning;Scalability;Reinforcement learning;Automated machine learning;Generative adversarial networks;Security;Tuning;Federated Learning AutoML Integration Distributed Deep Learning Edge Computing Privacy-Preserving Machine Learning Reinforcement Learning Generative Adversarial Networks (GANs) Scalable Machine Learning Model Selection Hyperparameter Tuning},
  doi={10.1109/INNOVA63080.2024.10847000},
  ISSN={},
  month={Dec},}@INBOOK{10982338,
  author={Pierson, Lillian},
  booktitle={Data & AI Imperative: Designing Strategies for Exponential Growth}, 
  title={Amplifying Growth Marketing Outcomes with Data and AI}, 
  year={2024},
  volume={},
  number={},
  pages={66-82},
  abstract={<p>This chapter focuses on some of the most novel and cutting&#x2010;edge ways that one can leverage data and artificial intelligence (AI) in marketing to drive exponential business growth. There are two main routes by which one can leverage machine learning and AI to drive growth by improving marketing on a strategic level: marketing strategy development support and marketing strategy executional support. Segmentation and personalization are some of the most prominent growth marketing use cases. The chapter looks at how advancements in AI are driving more powerful outcomes in growth marketing via segmentation and campaign&#x2010;level optimizations. Basically, any ongoing marketing activity that is required to maintain and grow a brand's awareness, engage with its audiences, foster its customer relationships, and drive conversions over time would fall into this category. The chapter also focuses on to use generative AI to achieve a dramatic reduction in content development costs and time.</p>},
  keywords={Artificial intelligence;Pricing;Real-time systems;Market research;Monitoring;Cognition;Profitability;Engines;Decision making;Companies},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394251971},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10982338},}@ARTICLE{10646618,
  author={Kumar, Dhruba and Dutta, Saurabh and Illias, Hazlee Azil},
  journal={IEEE Transactions on Power Delivery}, 
  title={Optimizing Transformer Fault Detection Performance Through the Synergy of AI and Statistical Analysis for Multi-Fault Classification}, 
  year={2024},
  volume={39},
  number={5},
  pages={2932-2942},
  abstract={The recent development of artificial intelligence (AI) has opened new avenues in processing parts per million (ppm) for fault detection through dissolved gas analysis (DGA). According to the latest IEC and IEEE standards, the existing methods are only applicable on single fault occurrence. The paper focuses on the challenge of detecting multiple faults occurring simultaneously in cases involving many faults using AI. Further, an inadequate training sample for classification and unavailability of balanced per-fault data reduces the model generalization, increases the risk of overfitting and biased learning towards the majority class. The proposed approach involves normalizing raw ppm values using z-score normalization, reducing dimensionality through t-distributed stochastic neighbor embedding (t-SNE), and synthesizing data using a generative adversarial network (GAN). Additionally, the parameters of error-correcting output codes (ECOC) and forest classifiers are optimized using a genetic algorithm (GA), efficiently solving multiple faults. F1 score, area under curve (AUC), and k-fold loss are used to evaluate fitness for improved classifier performance. This method outperforms the Duval method, and the data synthesis represents a new contribution to the field. The proposed method can achieve an overall accuracy of 99.6%, 98.6%, and 97.3% for the 9, 15, and 31 classes, respectively.},
  keywords={Optimization;Dimensionality reduction;Classification algorithms;Accuracy;Generative adversarial networks;Data models;Nearest neighbor methods;Synthetic data;Dissolved gas analysis;Synthetic data;dimensionality reduction;dissolved gas analysis;genetic algorithms;generative adversarial networks},
  doi={10.1109/TPWRD.2024.3449389},
  ISSN={1937-4208},
  month={Oct},}@INBOOK{10953324,
  author={Davenport, Thomas H. and Barkin, Ian and Davenport, Chase},
  booktitle={All Hands on Tech: The AI-Powered Citizen Revolution}, 
  title={Guardrails}, 
  year={2025},
  volume={},
  number={},
  pages={227-233},
  abstract={Summary <p>Guardrails are like bumper bowling. They automatically provide specific rules, standards, and best practices designed to guide citizen behavior within a governance framework. The Mayo Clinic is relying heavily on its domain experts to be stewards of the data they use for citizen projects. Stewardship does not have to be less rigorous than putting up shields that prevent undesirable behavior; their thorough guidance systems allow Mayo Clinic to trust their citizens to do the right thing with data. It is argued that the best approach to avoiding citizen development risk is to make it easy for citizens to do the right thing. That is typically accomplished through a combination of human, automated, and financial guardrails. This chapter provides some examples of policy guardrail domains. As generative artificial intelligence puts increasingly powerful and intuitive technology in the hands of employees, orchestrating citizens will only become a more formidable task.</p>},
  keywords={Companies;Artificial intelligence;Industries;Training;Standards;Sorting;Soft sensors;Reviews;Prevention and mitigation;Medical services},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394245925},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953324},}@INBOOK{10953082,
  author={Grill, Andrew},
  booktitle={Digitally Curious: Your Guide to Navigating the Future of AI and All Things Tech}, 
  title={TOOLS TO GET YOU STARTED WITH AI}, 
  year={2025},
  volume={},
  number={},
  pages={53-58},
  abstract={Summary <p>Protecting the data is vital in an enterprise environment. Using tools which are not completely fair and transparent means that there is a risk that the content produced might be biased or inaccurate, harming the reputation of the company as a result. This chapter provides some examples of Generative artificial intelligence enablers. ChatGPT Plus used to create the digitally curious GPT. Microsoft Co&#x2010;Pilot is built into the Microsoft 365 product family, available for an additional fee, and also as a stand&#x2010;alone web&#x2010;based tool. When used inside Word, PowerPoint, Teams, or Outlook, it is a pervasive GPT prompt that can be used to complete tasks within the app. Grammarly is a grammar and syntax tool. Otter.ai transcribes all of the podcast recordings, summarises the discussions, and extracts the insights from the episodes.</p>},
  keywords={Artificial intelligence;Chatbots;Companies;Regulation;Law;Business;Syntactics;Security;Protocols;Industries},
  doi={10.1002/9781394309399.ch5},
  ISSN={},
  publisher={Wiley},
  isbn={9781394217014},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953082},}@INBOOK{10950667,
  author={Jay, Rabi},
  booktitle={Enterprise AI in the Cloud: A Practical Guide to Deploying End-to-End Machine Learning and ChatGPT Solutions}, 
  title={Addressing the Challenges with Enterprise AI}, 
  year={2024},
  volume={},
  number={},
  pages={31-40},
  abstract={Summary <p>This chapter discusses the technical challenges, ethical dilemmas, and strategic hurdles. Digital natives, which are companies born in the digital age, are technology savvy and have embraced technology as a primary driver for achieving business results, such as cost reduction, efficiency, and innovation. They are agile, adaptable, and innovative and are quick to adopt new technologies, such as machine learning, artificial intelligence (AI), and generative AI. Digital&#x2010;native companies have been able to hire the best talent in the industry by giving them good pay, attractive career opportunities, and work amenities that attract good talent. Deploying high&#x2010;performing models in production requires high&#x2010;quality data in large quantities. A robust data infrastructure pipeline, from collection to consumption, is vital. AI introduces regulatory concerns such as bias, ethics, and privacy that must be tackled before deploying models. Large&#x2010;scale AI initiatives require a lot of technological, financial, and human resources.</p>},
  keywords={Artificial intelligence;Data models;Machine learning;Business;Companies;Security;Monitoring;Mathematical models;Ethics;Digital transformation},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394213078},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950667},}@INBOOK{10951763,
  author={Kihn, Martin and Lin, Andrea Chen},
  booktitle={Customer 360: How Data, AI, and Trust Change Everything}, 
  title={Data Collaboration&#x2014;A Rising Imperative}, 
  year={2025},
  volume={},
  number={},
  pages={135-142},
  abstract={Summary <p>Competitors across industries are vying to stay atop the news headlines. Companies want to be associated with innovation and generative artificial intelligence to win the mindshare of their customers and prospects. Collaborations between adjacent vendors are solidifying companies' momentum while boxing&#x2010;out newcomers or other competitors trying to gain ground. Consent management platforms allow web and mobile app publishers to collect first&#x2010;party data in a privacy&#x2010;aware manner. One method of building first&#x2010;party data comes from investing in server&#x2010;side data collection. As technologies improve, enterprises are in a better position to be open to data sharing and collaboration. Moreover, technologies like data clean rooms are able to provide a trusted environment for collaboration. Companies must simultaneously sharpen the utility of their data as a differentiator for connecting with customers while meeting the rising bar of having full respect for customer privacy.</p>},
  keywords={Internet;Companies;Collaboration;Media;Business;Browsers;Uninterruptible power systems;Clouds;Artificial intelligence;Web sites},
  doi={10.1002/9781394308668.ch14},
  ISSN={},
  publisher={Wiley},
  isbn={9781394273638},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951763},}@INPROCEEDINGS{10972911,
  author={Li, Ke and Mostajeran, Fariba and Rings, Sebastian and Kruse, Lucie and Schmidt, Susanne and Arz, Michael and Wolf, Erik and Steinicke, Frank},
  booktitle={2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={I Hear, See, Speak & Do: Bringing Multimodal Information Processing to Intelligent Virtual Agents for Natural Human-AI Communication}, 
  year={2025},
  volume={},
  number={},
  pages={1648-1649},
  abstract={In this demo paper, we present an Extended Reality (XR) framework providing a streamlined workflow for creating and interacting with intelligent virtual agents (IVAs) with multimodal information processing capabilities using commercially available artificial intelligence (AI) tools and cloud services such as large language and vision models. The system supports (i) the integration of high-quality, customizable virtual 3D human models for visual representations of IVAs and (ii) multimodal communication with generative AI-driven IVAs in immersive XR, featuring realistic human behavior simulations. Our demo showcases the enormous potential and vast design space of embodied IVAs for various XR applications.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Extended reality;Metaverse;Information processing;User interfaces;Systems support;Haptic interfaces;Artificial intelligence;Intelligent Virtual Agents;Extended Reality;Embodied AI;Human-AI Interaction},
  doi={10.1109/VRW66409.2025.00469},
  ISSN={},
  month={March},}@INPROCEEDINGS{10544050,
  author={Selvaraj, Ram and Singh, Ayush and Kameel, Shafiudeen and Samal, Rahul and Agarwal, Pooja},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={Vidgen: Long-Form Text-to-Video Generation with Temporal, Narrative and Visual Consistency for High Quality Story-Visualisation Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Generating long-form videos conditioned on large story based text input is a new and relatively unexplored task. Current text-to-video models are designed to generate short video clips conditioned on small input texts, which while temporally consistent, are severely lacking in narrative and visual consistency which are key elements of good story visualisation. This paper presents Vidgen, a pipeline that ensures Long-Form Text-to-Video Generation with Temporal, Narrative and Visual Consistency. To address the above issues Vidgen employs a new approach which leverages a Large-Language- Model (LLM) for the pre-processing of input story/script, which extracts key actions and story elements from the given script, and converts it to a standardised format which will be read and understood by the text-to-video model. The paper also proposes fixing "embeddings" (spatial-information about the appearance of the character) using a new, faster and improved textual- inversion approach with pre-trained weights, to ensure consistent looking characters. The work done in this paper will allow any traditional text-to-video model to accept large input text in the form of a story and generate high quality, temporally consistent arbitrary-length videos that have consistent looking characters for the entire duration of the video.},
  keywords={Training;Visualization;Generative AI;Pipelines;Entertainment industry;Memory modules;Generators;Generative Artificial Intelligence;Latent Diffusion Models;Large Language Models;Text-to-Video Generation;Textual Inversion;Story Visualisation},
  doi={10.1109/I2CT61223.2024.10544050},
  ISSN={},
  month={April},}@INPROCEEDINGS{10626785,
  author={Gupta, Shobha and Vishwakarma, Shubham and Srivastava, Kriti},
  booktitle={2024 4th International Conference on Intelligent Technologies (CONIT)}, 
  title={Revolutionizing Jewellery Design through Stable Diffusion Model: Bridging the Gap between Consumer Preferences and Designer Creativity}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This study offers a novel approach that uses generative AI models such as stable diffusion models to produce a large number of jewellery designs, improve the design process, and increase consumer involvement. The jewellery business, which relies on designers’ expertise and creativity, is on the verge of a dramatic shift with the integration of AI-driven technologies. Our proposed approach serves as a creative middleman, combining consumer preferences and artistic ideas to provide a wide range of jewellery pictures. By allowing customers to choose from these AI-generated designs, the technology not only meets their expectations but also dramatically streamlines the designer’s productivity. This synergy between AI-generated solutions and human ingenuity not only reduces the time and effort required in the initial design phase, but also guarantees that the final product resonates more strongly with customer preferences. We go into the technical details of stable diffusion models, highlighting their ability to generate and customise high-quality images. Furthermore, we discuss how this technology affects efficiency, satisfaction, and creative expression. Our findings indicate that this method not only promotes a more collaborative and consumer-centric design process, but it also creates new opportunities for innovation in the jewellery sector. This study attempts to show how the combination of artificial intelligence and human creativity may reshape the paradigms of jewellery creation, making it more inclusive, efficient, and responsive to consumer demands.},
  keywords={Productivity;Industries;Technological innovation;Generative AI;Design methodology;Merging;Collaboration;LORA;Stable Diffusion;Text-To-Image;Diffusion models;Generative AI;Transfer Learning},
  doi={10.1109/CONIT61985.2024.10626785},
  ISSN={},
  month={June},}@INBOOK{10952561,
  author={Blount, Jeb and Iannarino, Anthony},
  booktitle={The AI Edge: Sales Strategies for Unleashing the Power of AI to Save Time, Sell More, and Crush the Competition}, 
  title={Robots Have Goals, Not Souls}, 
  year={2024},
  volume={},
  number={},
  pages={31-33},
  abstract={Summary <p>Though artificial intelligence (AI) may appear to think and reason, it does not. Anything about it that seems human is an illusion&#x2014;a crafty parlor trick. It is a robot. A digital machine. It does not feel emotions. It does not care. Anthropomorphism&#x2014;the act of attributing human traits, emotions, and intentions to nonhuman entities&#x2014;has deep evolutionary roots, from the ancient deification of natural elements to the modern attribution of human emotions to pets. When it comes to AI platforms&#x2014;especially conversational agents like Siri, Alexa, Claude, Microsoft Copilot, and ChatGPT&#x2014;this inclination becomes particularly pronounced. Generative AI is simply an algorithmic machine that works relentlessly and without remorse to accomplish the goal &#x2014;even if that means breaking human rules and norms to accomplish that goal, all the while assuring that it has the best interests at heart.</p>},
  keywords={Artificial intelligence;Robots;Anthropomorphism;Virtual assistants;Medical diagnostic imaging;Heart;Chatbots;Software;Oral communication;Identification of persons},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394244492},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952561},}@INPROCEEDINGS{10871782,
  author={Liu, Delong and Lin, Yongping},
  booktitle={2024 17th International Conference on Advanced Computer Theory and Engineering (ICACTE)}, 
  title={A DeepFake Face Detection Method Using Vision Transformer with a Convolutional Module}, 
  year={2024},
  volume={},
  number={},
  pages={180-184},
  abstract={The Artificial Intelligence (AI) industry has developed rapidly in recent years. Plenty of large generative models such as ChatGPT have achieved remarkable accomplishment. AI technology provides much convenience for human beings in various fields, but it also brings much harm. Many lawbreakers utilize this technology to perform DeepFake operations such as replacing one face with another for crucial people, posing a challenge to social security. In this work, we present a novel structure detector for DeepFake examination. The network is a convolutional module of the Convolutional Neural Network embedded in the Vision Transformer, called Convolutional Neural Network Vision Transformer (CNNVT). Where the CNN captures the fine-grained features of the face, ViT implements the context representation and final classification. Subsequently, a series of experiments could prove the validity of our network. We trained our model on the DeepFake dataset and got 98.67% Recall and 98.68% Accuracy. The model has practical significance in preventing the propagation of DeepFake data.},
  keywords={Training;Deepfakes;Accuracy;Feature extraction;Transformers;Data models;Convolutional neural networks;Security;Artificial intelligence;Faces;Convolutional Neural Network Vision Transformer(CNNVT);DeepFake examination;Vision Transformer(ViT)},
  doi={10.1109/ICACTE62428.2024.10871782},
  ISSN={2154-7505},
  month={Sep.},}@INPROCEEDINGS{11010541,
  author={Zhou, Chi and Wang, Kaili},
  booktitle={2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT)}, 
  title={ST3GAN: A Chinese Character Font Generation Model Combining Structural-Stroke Consistency Loss and Style Perception Loss}, 
  year={2025},
  volume={},
  number={},
  pages={883-891},
  abstract={With the rapid development of deep learning technologies, Chinese character font generation based on artificial intelligence has become a hot research topic. However, the core challenge in Chinese character font generation lies in how to simultaneously maintain structural rationality and style consistency during the generation process. Currently, many studies focus primarily on improving the overall quality of the generated images, with less attention given to the accurate preservation of the intrinsic structural features and writing style of Chinese characters. To address this challenge, a novel Generative Adversarial Network (GAN) model, ST3GAN, is proposed. This model introduces structural stroke consistency loss and style perception loss, aiming to enhance the generator’s deep understanding and expression of Chinese character structure and style. Specifically, the structural stroke consistency loss combines structural encoding and stroke encoding to ensure the accuracy of stroke count and layout in the generated characters, while the style perception loss uses the Gram matrix to measure the similarity between the generated image and the target style, thereby enhancing style consistency. Experimental results show that ST3GAN significantly improves the quality of generated images, especially in terms of structural rationality and style consistency, demonstrating a strong advantage over existing methods in generation performance.},
  keywords={Deep learning;Accuracy;Image coding;Smoothing methods;Layout;Computer applications;Writing;Generative adversarial networks;Loss measurement;Information technology;Chinese character font generation;Generative Adversarial Network;structural-stroke consistency loss;style perception loss},
  doi={10.1109/ISCAIT64916.2025.11010541},
  ISSN={},
  month={March},}@INBOOK{10982326,
  author={Pierson, Lillian},
  booktitle={Data & AI Imperative: Designing Strategies for Exponential Growth}, 
  title={Introduction to Data Strategy}, 
  year={2024},
  volume={},
  number={},
  pages={17-31},
  abstract={<p>The purpose of the STAR Framework is to de&#x2010;risk future data projects and products by following a methodical process. This process helps to select the optimal use case for substantial and reliable return on investment given current state conditions within a company. The Four Phases of the STAR Framework are survey the industry, take stock and inventory of the organization, assess the organization's current state conditions, and recommend a strategy for reaching future state goals. Much like in the world of athletics, where there are sprinters and marathon runners, in the world of business strategy, there are quick win use cases and strategic use cases. Generative artificial intelligence systems are also built on things called foundation models. Large language models are special machine learning model strained on massive datasets and have the ability to both understand and generate human&#x2010;like text.</p>},
  keywords={Stars;Industries;Computer aided software engineering;Companies;Documentation;Standards organizations;Artificial intelligence;Surveys;Iterative methods;Training},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394251971},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10982326},}@INPROCEEDINGS{8823801,
  author={Behjati, Razieh and Arisholm, Erik and Bedregal, Margrethe and Tan, Chao},
  booktitle={2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)}, 
  title={Synthetic Test Data Generation Using Recurrent Neural Networks: A Position Paper}, 
  year={2019},
  volume={},
  number={},
  pages={22-27},
  abstract={Testing in production-like test environments is an essential part of quality assurance processes in many industries. Provisioning of such test environments, for information-intensive services, involves setting up databases that are rich-enough to enable simulating a wide variety of user scenarios. While production data is perhaps the gold-standard here, many organizations, particularly within the public sectors, are not allowed to use production data for testing purposes due to privacy concerns. The alternatives are to use anonymized data, or synthetically generated data. In this paper, we elaborate on these alternatives and compare them in an industrial context. Further we focus on synthetic data generation and investigate the use of recurrent neural networks for this purpose. In our preliminary experiments, we were able to generate representative and highly accurate data using a recurrent neural network. These results open new research questions that we discuss here, and plan to investigate in our future research.},
  keywords={Recurrent neural networks;Data models;Generators;Testing;Finance;Task analysis;Training;Recurrent Neural Networks;Synthetic Data Generation;Software Testing;Generative Models;Deep Learning},
  doi={10.1109/RAISE.2019.00012},
  ISSN={},
  month={May},}@INPROCEEDINGS{10920564,
  author={Xiao, Huaming and Liu, Guangyu and Hang, Shuwen and Jin, Zhixuan and Liang, Tao and Li, Xuefeng},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Integrating Motion Deblurring with Deep Learning for Real-Time Defect Detection in High-Speed Steel Production}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the steel manufacturing industry, the precision of surface defect detection on steel plates is a critical factor for optimizing production quality and operational efficiency. As production line speeds continue to escalate, the resultant motion blur from the rapid movement of steel plates increasingly challenges the performance of imaging systems, thereby diminishing the accuracy of defect detection. This paper presents a novel approach to real-time steel plate surface defect detection, leveraging advanced motion deblurring techniques to mitigate these challenges. Specifically, we compare the efficacy of three methodologies: the standalone YOLOv5 defect detection algorithm, YOLOv5 in conjunction with Deblur-GAN preprocessing, and the YOLO-Steel-GAN framework, which seamlessly integrates Deblur-GAN with YOLOv5. Experimental evaluations reveal that while YOLOv5 alone achieves a recall rate of 91.6% with a detection speed of 62 FPS, its precision is limited to 55.2% and a mean Average Precision (mAP) of 51%. The introduction of Deblur-GAN as a preprocessing step with YOLOv5 enhances precision to 63.1% and recall to 92.3%, albeit with a reduction in detection speed to 45 FPS. In contrast, the proposed YOLO-Steel-GAN framework not only sustains a competitive detection speed of 53 FPS but also significantly elevates precision to 89.3%, recall to 94.4%, and mAP to 87.9%. These results demonstrate that the YOLO-Steel-GAN framework provides a robust and efficient solution for real-time steel plate surface defect detection, offering substantial improvements in both accuracy and processing speed. The findings underscore the practical applicability of this integrated approach in high-speed industrial environments, marking a significant advancement in steel plate quality control.},
  keywords={YOLO;Manufacturing industries;Accuracy;Computational modeling;Lighting;Production;Quality control;Real-time systems;Steel;Defect detection;Steel plate defect detection;motion deblurring;Generative Adversarial Networks (GAN);Convolutional Neural Networks (CNN)},
  doi={10.1109/ICSMD64214.2024.10920564},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10826982,
  author={Liao, Zengxi and Wu, Xiang and Lei, Shiyu and Qiu, Dehong},
  booktitle={2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={A Method of Rolling Bearing Fault Diagnosis Using Transfer Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1076-1082},
  abstract={Rolling bearing fault diagnosis is an important task that is critical for realizing predictive maintenance in industry 4.0. However, due to the lack of labeled data of rolling bearing faults and the complexity of working conditions, it is expected to construct fault diagnosis model with good generalization through leveraging a small number of labeled data collected from limited source domains and make it perform fault diagnosis well in other target domains. In this paper, we propose a method of rolling bearing fault diagnosis using transfer learning. We first construct a group of Domain-Adversarial Neural Networks (DANNs), each of them does its best to identify the faults of target domain via making the distribution of each source domain data and the distribution of the target domain data become similar as much as possible. After that, the group of DANNs are combined through the Mixture of Experts (MoE) to build the fault diagnosis model MoE-DANN, which achieves satisfactory fault diagnosis results in the target domain by unleashing the potential and balancing the effect of each expert. Extensive experiments demonstrate that MoE-DANN yields better diagnosis accuracy on open benchmark datasets than the baseline models. Additionally, it generalizes well even it is trained by a limited number of labeled data.},
  keywords={Fault diagnosis;Accuracy;Transfer learning;Neural networks;Rolling bearings;Benchmark testing;Data models;Pattern recognition;Smart manufacturing;Predictive maintenance;rolling bearing fault diagnosis;transfer learning;generative adversarial network;mixture of experts;generalization},
  doi={10.1109/PRAI62207.2024.10826982},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10882268,
  author={Jadhav, Shital and Bartere, Mahip and Patil, Sonal},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Review of Deep Fake Detection Using Deep Learning Convolutional, Recurrent, and Graph Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The exponential growth in the generation and dissemination of deep fake content has resulted in huge challenges to the integrity and trustworthiness of digital media. The current paper tries to meet an unmet need for robust and reliable deep fake detection techniques by reviewing the current literature and meta-analyzing the existing methods. This study thoroughly assesses various deep fake detection models; their methodologies; strengths; and weaknesses, and performance metrics, such as accuracy, precision, recall, and F1-scores. The paper provides a review and critical analysis of methods using a diverse set of techniques, ranging from CNNs, RNNs, GANs, and hybrid approaches. CNNs, especially those using transfer learning from pre-trained models like VGG16, ResNet, and InceptionV3, have demonstrated image-based deep fake detection with high accuracy because of their capability in effectively capturing spatial features. RNNs, especially the Long Short-Term Memory (LSTM) architecture, are applied to perform video-based detection, leveraging the distinctive features of analyzing data over a temporal sequence and identifying inconsistencies over time. In the beginning, GAN-based methods used to generate deep fakes have now been repurposed for detection. This work provides a technique to repurpose GAN-based methods for detection using the capability of such models to distinguish between real and synthetic data using adversarial training. Hybrid architectures that integrate several such neural network models hold promise for improving detection performance through combined strengths. This comparative analysis demonstrates that CNN-based methods are typically the best-performing methods for static image analysis, while RNN is more effective for the video sequence. GAN-based methods are capable of adapting to new deep fake generation techniques, but do so with increased computational complexity. Hybrid methods have increased complexity but present a better trade-off between accuracy and computational efficiency. The impact of such a detailed review is two-fold. By systematically categorizing and reviewing the existing methods for deep fake detection, the current study brings out a clear view of the state-of-the-art, guiding researchers and practitioners in the selection of relevant techniques with a view to specific application requirements. Further, the identification of limitations and gaps in the currently existing methods lays out research directions for the future, leading toward the development of more robust and scalable detection systems. Ultimately, this work contributes to enhancing the security and reliability of digital media, fostering greater public trust in digital content authenticity.},
  keywords={Deepfakes;Accuracy;Reviews;Neural networks;Video sequences;Media;Feature extraction;Robustness;Forgery;Long short term memory;Deep Fake Detection;Convolutional Neural Networks;Recurrent Neural Networks;Generative Adversarial Networks;Meta-Analysis},
  doi={10.1109/ICAIQSA64000.2024.10882268},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10987295,
  author={Rodrigues, Anisha P and Prabhu K, Akshay and Acharya, Shailesh U and MS, Adithya and Poojari, Seejan Padmanabha and Fernandes, Roshan and Vijaya, P.},
  booktitle={2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)}, 
  title={Local Language Handwritten Character Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={57-62},
  abstract={The idea of the proposed methodology is to develop an efficient system that can recognize handwritten Kannada characters, a language whose script is quite complex. It makes use of feature extraction by using CNN in conjunction with a hybrid SVM-CNN approach in classification to overcome handwriting variation. The SVM-CNN hybrid takes the strength of CNN pattern recognition and complements it with SVM classification; Also, the preprocessing would include the binarization of images and noise removal to have more qualitative data, hence significantly improving the recognition process as a whole. This holistic approach provides greater accuracy and robustness in recognition of regional languages in further digitization efforts focused on document digitization applications, automated systems, and educational tool sets for an inclusive digital practice.},
  keywords={Support vector machines;Handwriting recognition;Image recognition;Noise;Feature extraction;Data engineering;Vectors;Robustness;Systems support;Character recognition;HCR system;Support Vector Machine(SVM);Convolution Neural Network(CNN);K-Nearest Neighbor(KNN);Generative Adversarial Networks(GANs)},
  doi={10.1109/AIDE64228.2025.10987295},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11064594,
  author={Ezumalai, R. and Santhakumar, D.},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Optimizing Traffic Safety: A Dual Multi-Scale Attention Network for Anomaly Detection in VANETs}, 
  year={2025},
  volume={3},
  number={},
  pages={754-760},
  abstract={Vehicular Ad Hoc Networks (VANETs) improve road safety and traffic flow, but face security threats due to their decentralized nature. Conventional anomaly detection mechanisms are ineffective, requiring higher scalable accuracy in computationally feasible solutions. This research introduces the real-time monitoring of anomalies through comprehensive introduction to using a Dual Multi-Scale Attention Network (DMSA) that was optimized via Planet Optimization for anomaly detection in systems related to the monitoring of real-time traffic. Multiple metrics - including throughput, dropped packet ratio, and overhead traffic ratio are incorporated - to the collected dataset which is of real-time nature. To achieve a reasonable model performance, the collection set is normalized using three scaling rescaling techniques. Anomaly detection is then implemented by DMSA, which includes innovative attention mechanisms to trace even the most complex deviations of patterns in traffic data. Optimizing DMSA model using the Planet Optimization (PO), fine-tuning the hyperparameter, so that detection is enhanced, and at the same time, computational efficiency gets improved to detect the fine anomalies which might have remained undetected in the previously used traditional methods while offering high efficiency in dynamic vehicular networks. The results of the conducted experiments show that the DMSA-PO framework was able to achieve detection accuracy in the range of 98.2%, precision of 97.5% and an F1-score of 97.2%, higher than others in use. This quantum enhanced model offers a strong, scalable, and efficient anomaly detection solution for VANETs to guarantee their security and reliability in dynamic, real-time environments.},
  keywords={Accuracy;Planets;Vehicular ad hoc networks;Real-time systems;Security;Reliability;Vehicle dynamics;Anomaly detection;Optimization;Monitoring;Anomaly Detection;Planet Optimization;Realtime monitoring;Self-guided quantum generative adversarial network;Vehicular Ad Hoc Network},
  doi={10.1109/ICCSAI64074.2025.11064594},
  ISSN={},
  month={April},}@INPROCEEDINGS{11077509,
  author={Santorelli, Christopher and Belmonte, Victor Ginart and Mastropaolo, Ryan},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={Adversarially Enhanced Financial Misinformation: A Comparative Analysis of LLM- vs. GAN-Generated Content Exposing AI Moderation Vulnerabilities}, 
  year={2025},
  volume={},
  number={},
  pages={250-256},
  abstract={As Large Language Models (LLMs) become more pervasive, their capability to generate convincing financial news poses an escalating threat to investor decision-making and market stability. However, contemporary content moderation and AIbased verification systems exhibit notable vulnerabilities when confronted with the subtle linguistic manipulations introduced by advanced prompt engineering techniques and adversarial training. This study investigated the comparative credibility, influence, and detectability of AI-generated financial headlines produced via Zero-Shot, Few-Shot (8-Shot), and Chain-of-Thought (CoT) prompting, with CoT outputs further used to train a GAN for adversarially enhanced text generation. We compiled a combined dataset of NASDAQ-listed securities and web-scraped, human authored news, generated additional AI-driven headlines under three prompting paradigms, and conducted a survey of randomly sampled headlines ($\mathbf{n} \boldsymbol{=} \mathbf{3 0 0}$) to assess the credibility, market perception impact, investment influence, and AI detectability. The analysis revealed that headlines generated through Chain-of-Thought prompting consistently scored higher in perceived authenticity, influenced investment sentiment more profoundly, and were harder for participants to classify as AI-written. The findings underscore the urgent need for adversarially robust content moderation and verification mechanisms, capable of adapting to the rapidly evolving landscape of AI-generated financial misinformation, particularly when Chain-of-Thought reasoning is leveraged to enhance GAN-generated content.},
  keywords={Training;Surveys;Terminology;Large language models;Natural language processing;Stability analysis;Prompt engineering;Security;Fake news;Investment;Large Language Models (LLMs);Generative Adversarial Networks (GANs);AI-Generated Misinformation;Adversarial Learning;AI Content Moderation;Natural Language Processing (NLP);Misinformation Detection},
  doi={10.1109/AIRC64931.2025.11077509},
  ISSN={},
  month={May},}@ARTICLE{10183374,
  author={Ou, Liang and Chang, Yu-Chen and Wang, Yu-Kai and Lin, Chin-Teng},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Fuzzy Centered Explainable Network for Reinforcement Learning}, 
  year={2024},
  volume={32},
  number={1},
  pages={203-213},
  abstract={The explainability of reinforcement learning (RL) models has received vast amount of interest as its applications have widened. Most existing explainable RL models focus on improving the explainability of an agent's observations instead of the relationships between agent states and actions. This study presents a fuzzy centered explainable network (FCEN) for RL tasks to interpret the relationships between agent states and actions. The proposed FCEN leverages the interpretability of fuzzy neural networks to establish if–then rules and a generative model to visualize learned knowledge. Precisely, the FCEN includes if–then rules that formulate state-action mappings with human-understandable logic, such as the form “IF Input is A THEN Output is B.” In addition, these rules connect with a generative model that concretizes the states into human-understandable patterns (figures). Our experimental results obtained on 4 Atari games show that the proposed FCEN can achieve a high level of performance in RL tasks and enormously boost the explainability of RL agents both globally and locally. In other words, the FCEN maintains a high-level explanation for the agent decision logic and the possibility of low-level analysis for each given observation sample. The explainability boost does not undermine reward learning performance, humans can even enhance the agent's performance with the provided explainability.},
  keywords={Fuzzy neural networks;Feature extraction;Artificial intelligence;Transformers;Decision making;Visualization;Explainable AI (xAI);fuzzy neural network (FNN);generative model;reinforcement learning (RL)},
  doi={10.1109/TFUZZ.2023.3295055},
  ISSN={1941-0034},
  month={Jan},}@INPROCEEDINGS{9313202,
  author={Ying, Xiang and Zhang, Yulin and Wei, Xi and Yu, Mei and Zhu, Jialin and Gao, Jie and Liu, Zhiqiang and Li, Xuewei and Yu, Ruiguo},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={MSDAN: Multi-Scale Self-Attention Unsupervised Domain Adaptation Network for Thyroid Ultrasound Images}, 
  year={2020},
  volume={},
  number={},
  pages={871-876},
  abstract={With the maturity of artificial intelligence, AI-aided diagnosis technology is gradually widely applied in clinical medicine. However, for the same pathological tissue, medical images produced by different types of instruments usually possess different data distributions. Because of the domain shift phenomenon, AI-aided diagnosis cannot accurately diagnose medical images in other domains, which is a waste of precious medical images. This paper proposes a Multi-Scale Self-Attention Unsupervised Domain Adaptive framework (MSDAN), which consists of three modules. First, the multi-scale framework constrains the source domain features and target domain features by optimizing adversarial losses with different level features. Second, the mix-up discriminator extracts latent spatial features by mixing up source domain and target domain features. Finally, MSDAN learns the geometric information of the pathological tissues in medical images through the self-attention module, thereby improving the transfer effect of the semantic information in medical images. Extensive experiments prove that the proposed approach can achieve superior performance on tasks with various degrees of domain shift and data complexity, especially for thyroid ultrasound images.},
  keywords={Feature extraction;Medical diagnostic imaging;Ultrasonic imaging;Thyroid;Pathology;Cancer;Semantics;Medical Imaging;Domain Adaptation;AI-Aided Detection},
  doi={10.1109/BIBM49941.2020.9313202},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11039386,
  author={Singh, Rohini and Gandotra, Ekta and Jain, Shruti},
  booktitle={2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)}, 
  title={Detecting the Undetectable: Deep Learning Model for Identification of Fake Images}, 
  year={2025},
  volume={},
  number={},
  pages={980-985},
  abstract={Deep learning (DL) has brought big change in the field of image processing, allowing the generation of fake images, commonly known as Deepfake images. Because of the rapid advancements in generative models, it become difficult to differentiate between fake and real images. This research finds out the effectiveness of DL based Convolutional Neural Networks (CNNs) for the identification of deepfake images. In this study, the performance of three CNN models namely VGG16, VGG19, and InceptionV3 were employed, to evaluate their effectiveness in detecting fake images. The main objective of the study is to enhance detection accuracy by using robust feature extraction capabilities of these models and optimizing their performance through fine-tuning. Experimental results demonstrate that VGG16 provides the highest accuracy of 94% for detecting fake images. In this research, we utilize and compare the performance of three CNN models i.e., VGG16, VGG19, and Inception V3 to analyze their ability to detect fake images. The primary aim of the research is to improve the accuracy of detection using strong feature extraction abilities of these models and fine-tuning their performance. Experimental results indicate that VGG16 offers the best accuracy of 94% in detecting fake images.},
  keywords={Deep learning;Deepfakes;Analytical models;Accuracy;Computational modeling;Signal processing;Feature extraction;Convolutional neural networks;Artificial intelligence;Deepfake images;Deep learning;Artificial Intelligence;Convolutional Neural Networks;VGG16;VGG19;InceptionV3},
  doi={10.1109/ISPCC66872.2025.11039386},
  ISSN={2643-8615},
  month={March},}@ARTICLE{10680313,
  author={Gallotta, Roberto and Todd, Graham and Zammit, Marvin and Earle, Sam and Liapis, Antonios and Togelius, Julian and Yannakakis, Georgios N.},
  journal={IEEE Transactions on Games}, 
  title={Large Language Models and Games: A Survey and Roadmap}, 
  year={2024},
  volume={},
  number={},
  pages={1-18},
  abstract={Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.},
  keywords={Games;Artificial intelligence;Surveys;Predictive models;Biological system modeling;Video games;Transformers;Digital games;gameplaying;generative AI;generative text;large language models;procedural content generation;survey;video games},
  doi={10.1109/TG.2024.3461510},
  ISSN={2475-1510},
  month={},}@ARTICLE{10697452,
  author={Liu, Zhonghua and Quan, Yu and Lyu, Xiaohong and Alenazi, Mohammed J.F.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Enhancing Clinical Accuracy of Medical Chatbots With Large Language Models}, 
  year={2025},
  volume={29},
  number={9},
  pages={6395-6405},
  abstract={The rapid advancement of large language models (LLMs) has opened up new possibilities for transforming healthcare practices, patient interactions, and medical report generation. This paper explores the application of LLMs in developing medical chatbots and virtual assistants that prioritize clinical accuracy. We propose a novel multi-turn dialogue model, including adjusting the position of layer normalization to improve training stability and convergence, employing a contextual sliding window reply prediction task to capture fine-grained local context, and developing a local critical information distillation mechanism to extract and emphasize the most relevant information. These components are integrated into a multi-turn dialogue model that generates coherent and clinically accurate responses. Experiments on the MIMIC-III and n2c2 datasets demonstrate the superiority of the proposed model over state-of-the-art baselines, achieving significant improvements in perplexity, BLEU-2, recall at K scores, medical entity recognition, and response coherence. The proposed model represents a significant step in developing reliable and contextually relevant multi-turn medical dialogue systems that can assist patients and healthcare professionals.},
  keywords={Chatbots;Accuracy;Transformers;Medical services;Context modeling;Biomedical imaging;Training;Reliability;Oral communication;Convergence;Large language models;medical chatbots;clinical accuracy;multi-turn dialogue modeling},
  doi={10.1109/JBHI.2024.3470323},
  ISSN={2168-2208},
  month={Sep.},}@ARTICLE{10836771,
  author={Cao, Ziang and Hong, Fangzhou and Wu, Tong and Pan, Liang and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DiffTF++: 3D-Aware Diffusion Transformer for Large-Vocabulary 3D Generation}, 
  year={2025},
  volume={47},
  number={4},
  pages={3018-3030},
  abstract={Generating diverse and high-quality 3D assets automatically poses a fundamental yet challenging task in 3D computer vision. Despite extensive efforts in 3D generation, existing optimization-based approaches struggle to produce large-scale 3D assets efficiently. Meanwhile, feed-forward methods often focus on generating only a single category or a few categories, limiting their generalizability. Therefore, we introduce a diffusion-based feed-forward framework to address these challenges with a single model. To handle the large diversity and complexity in geometry and texture across categories efficiently, we 1) adopt improved triplane to guarantee efficiency; 2) introduce the 3D-aware transformer to aggregate the generalized 3D knowledge with specialized 3D features; and 3) devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge. Building upon our 3D-aware Diffusion model with TransFormer, DiffTF, we propose a stronger version for 3D generation, i.e., DiffTF++. It boils down to two parts: multi-view reconstruction loss and triplane refinement. Specifically, we utilize multi-view reconstruction loss to fine-tune the diffusion model and triplane decoder, thereby avoiding the negative influence caused by reconstruction errors and improving texture synthesis. By eliminating the mismatch between the two stages, the generative performance is enhanced, especially in texture. Additionally, a 3D-aware refinement process is introduced to filter out artifacts and refine triplanes, resulting in the generation of more intricate and reasonable details. Extensive experiments on ShapeNet and OmniObject3D convincingly demonstrate the effectiveness of our proposed modules and the state-of-the-art 3D object generation performance with large diversity, rich semantics, and high quality.},
  keywords={Three-dimensional displays;Transformers;Diffusion models;Fitting;Training;Solid modeling;Neural radiance field;Image reconstruction;Topology;Rendering (computer graphics);3D generation;large-vocabulary 3D objects;3D awareness;transformer},
  doi={10.1109/TPAMI.2025.3528247},
  ISSN={1939-3539},
  month={April},}@ARTICLE{10129207,
  author={Zhu, Yongjie and Liu, Jia and Cong, Fengyu},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Dynamic Community Detection for Brain Functional Networks During Music Listening With Block Component Analysis}, 
  year={2023},
  volume={31},
  number={},
  pages={2438-2447},
  abstract={The human brain can be described as a complex network of functional connections between distinct regions, referred to as the brain functional network. Recent studies show that the functional network is a dynamic process and its community structure evolves with time during continuous task performance. Consequently, it is important for the understanding of the human brain to develop dynamic community detection techniques for such time-varying functional networks. Here, we propose a temporal clustering framework based on a set of network generative models and surprisingly it can be linked to Block Component Analysis to detect and track the latent community structure in dynamic functional networks. Specifically, the temporal dynamic networks are represented within a unified three-way tensor framework for simultaneously capturing multiple types of relationships between a set of entities. The multi-linear rank- $(L_{r}, L_{r}, 1)$  block term decomposition (BTD) is adopted to fit the network generative model to directly recover underlying community structures with the specific evolution of time from the temporal networks. We apply the proposed method to the study of the reorganization of the dynamic brain networks from electroencephalography (EEG) data recorded during free music listening. We derive several network structures ( $L_{r}$  communities in each component) with specific temporal patterns (described by BTD components) significantly modulated by musical features, involving subnetworks of frontoparietal, default mode, and sensory-motor networks. The results show that the brain functional network structures are dynamically reorganized and the derived community structures are temporally modulated by the music features. The proposed generative modeling approach can be an effective tool for describing community structures in brain networks that go beyond static methods and detecting the dynamic reconfiguration of modular connectivity elicited by continuously naturalistic tasks.},
  keywords={Brain modeling;Tensors;Hidden Markov models;Electroencephalography;Feature extraction;Analytical models;Task analysis;Dynamic community detection;brain connectivity;module detection;generative model;EEG;tensor decomposition;block term decomposition},
  doi={10.1109/TNSRE.2023.3277509},
  ISSN={1558-0210},
  month={},}@ARTICLE{6819854,
  author={Soh, Harold and Demiris, Yiannis},
  journal={IEEE Transactions on Haptics}, 
  title={Incrementally Learning Objects by Touch: Online Discriminative and Generative Models for Tactile-Based Recognition}, 
  year={2014},
  volume={7},
  number={4},
  pages={512-525},
  abstract={Human beings not only possess the remarkable ability to distinguish objects through tactile feedback but are further able to improve upon recognition competence through experience. In this work, we explore tactile-based object recognition with learners capable of incremental learning. Using the sparse online infinite Echo-State Gaussian process (OIESGP), we propose and compare two novel discriminative and generative tactile learners that produce probability distributions over objects during object grasping/palpation. To enable iterative improvement, our online methods incorporate training samples as they become available. We also describe incremental unsupervised learning mechanisms, based on novelty scores and extreme value theory, when teacher labels are not available. We present experimental results for both supervised and unsupervised learning tasks using the iCub humanoid, with tactile sensors on its five-fingered anthropomorphic hand, and 10 different object classes. Our classifiers perform comparably to state-of-the-art methods (C4.5 and SVM classifiers) and findings indicate that tactile signals are highly relevant for making accurate object classifications. We also show that accurate “early” classifications are possible using only 20-30 percent of the grasp sequence. For unsupervised learning, our methods generate high quality clusterings relative to the widely-used sequential k-means and self-organising map (SOM), and we present analyses into the differences between the approaches.},
  keywords={Tactile sensors;Learning systems;Training;Unsupervised learning;Support vector machines;Tactile object recognition, supervised classification, unsupervised learning, incremental learning},
  doi={10.1109/TOH.2014.2326159},
  ISSN={2329-4051},
  month={Oct},}@INPROCEEDINGS{10703750,
  author={Khan, Sulaiman and Biswas, Md. Rafiul and Murad, Alina and Ali, Hazrat and Shah, Zubair},
  booktitle={2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI)}, 
  title={An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging}, 
  year={2024},
  volume={},
  number={},
  pages={234-239},
  abstract={Recent developments in multimodal large language models (MLLMs) have spurred significant interest in their potential applications across various medical imaging domains. On the one hand, there is a temptation to use these generative models to synthesize realistic-looking medical image data, while on the other hand, the ability to identify synthetic image data in a pool of data is also significantly important. In this study, we explore the potential of the Gemini (gemini-1.0-pro-visionlatest) and GPT-4V (gpt-4-vision-preview) models for medical image analysis using two modalities of medical image data. Utilizing synthetic and real imaging data, both Gemini AI and GPT-4V are first used to classify real versus synthetic images, followed by an interpretation and analysis of the input images. Experimental results demonstrate that both Gemini and GPT4 could perform some interpretation of the input images. In this specific experiment, Gemini was able to perform slightly better than the GPT-4V on the classification task. In contrast, responses associated with GPT-4V were mostly generic in nature. Our early investigation presented in this work provides insights into the potential of MLLMs to assist with the classification and interpretation of retinal fundoscopy and lung X-ray images. We also identify key limitations associated with the early investigation study on MLLMs for specialized tasks in medical image analysis.},
  keywords={Analytical models;Image analysis;Large language models;Lung;Data science;Retina;Data models;Rough surfaces;X-ray imaging;Biomedical imaging;LLM;ChatGPT;Gemini AI;Multimodal data;Retina;Lung},
  doi={10.1109/IRI62200.2024.00056},
  ISSN={2835-5776},
  month={Aug},}@ARTICLE{11125506,
  author={Li, Weizhen and Huang, Peijie and Xu, Yuhong and Huang, Junbao and Fan, Jiekun},
  journal={IEEE Transactions on Audio, Speech and Language Processing}, 
  title={GRSF: Generating and Refining LLM Knowledge for Cross-Domain Zero-Shot Slot Filling}, 
  year={2025},
  volume={33},
  number={},
  pages={3921-3931},
  abstract={Slot filling plays an important role in task-oriented dialogue systems. Though slot filling has made significant progress in the single-domain learning paradigm, facilitating learning in unknown domains remains a challenge. Previous works have mostly focused on supplementing sequence labeling models with slot meta-information or metric learning, lacking domain-specific knowledge and performing poorly on unseen slots. By further pretraining or employing larger-parameter generative models, other methods simply introduce implicit general knowledge, which proves challenging in effectively compensating for the performance deficiencies of the model across different slots. In this paper, as knowledge becomes increasingly abundant within LLMs (large language models), we propose a novel framework that generates and refines LLMs knowledge for cross-domain zero-shot slot filling (GRSF). Specifically, based on in-context learning, we introduce a knowledge generation module to fully leverage the knowledge of LLMs. Moreover, based on the transferability of different slots, we train a knowledge evaluator to transfer the factuality of knowledge to refine knowledge. Finally, the refined knowledge is integrated with the original text and fed into the model to compensate for its knowledge gaps. Extensive experiments on three public datasets demonstrate that our approach achieves superior performance compared to the baselines.},
  keywords={Filling;Semantics;Training;Large language models;Labeling;Training data;Tagging;Speech processing;Refining;Prototypes;Cross-domain;large language models;slot filling;small model;zero-shot},
  doi={10.1109/TASLPRO.2025.3597441},
  ISSN={2998-4173},
  month={},}@ARTICLE{10937969,
  author={Kamdjou, Hugues Marie and Ouchani, Samir},
  journal={Computing in Science & Engineering}, 
  title={A Secure Architecture for Digital Twins in Resource-Constrained Industrial Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Digital Twins (DT) are transforming the Industrial Internet of Things (IIoT) by providing virtual replicas of physical assets, enabling enhanced predictive analytics and data-driven decision-making. However, as Industry 5.0 integrates DT with generative AI technologies via APIs and tokens, security risks increase. Outdated modeling tools, coupled with vulnerabilities in resource-constrained environments, such as limited computational power and bandwidth, further compromise DT security. These constraints impede the implementation of robust security measures like encryption, authentication, and blockchain, making DT susceptible to cyber threats and fraud. This paper addresses these challenges by proposing a secure DT conceptual architecture that leverages edge-fog-cloud computing with Federated Learning (FL), and Blockchain. The framework ensures resource-efficient security by integrating privacy-preserving computations, secure data storage, and resilient communication channels. It effectively mitigates risks beyond 51% attacks, including Sybil attacks and data manipulation, while enabling secure data exchange and realtime monitoring. Furthermore, the proposed architecture improves adaptability in complex IIoT environments, supporting seamless device integration, scalability, and energy-efficient operations to safeguard industrial systems against evolving threats.},
  keywords={Industrial Internet of Things;Security;Blockchains;Computer architecture;Real-time systems;Firewalls (computing);Ecosystems;Data privacy;Data models;Computer security},
  doi={10.1109/MCSE.2025.3554031},
  ISSN={1558-366X},
  month={},}@ARTICLE{5557858,
  author={Kim, Jong Kyoung and Choi, Seungjin},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Probabilistic Models for Semisupervised Discriminative Motif Discovery in DNA Sequences}, 
  year={2011},
  volume={8},
  number={5},
  pages={1309-1317},
  abstract={Methods for discriminative motif discovery in DNA sequences identify transcription factor binding sites (TFBSs), searching only for patterns that differentiate two sets (positive and negative sets) of sequences. On one hand, discriminative methods increase the sensitivity and specificity of motif discovery, compared to generative models. On the other hand, generative models can easily exploit unlabeled sequences to better detect functional motifs when labeled training samples are limited. In this paper, we develop a hybrid generative/discriminative model which enables us to make use of unlabeled sequences in the framework of discriminative motif discovery, leading to semisupervised discriminative motif discovery. Numerical experiments on yeast ChIP-chip data for discovering DNA motifs demonstrate that the best performance is obtained between the purely-generative and the purely-discriminative and the semisupervised learning improves the performance when labeled sequences are limited.},
  keywords={Biological system modeling;Numerical models;Computational modeling;DNA;Probabilistic logic;Hybrid power systems;Joints;Graphical models;hybrid generative/discriminative models;motif discovery;probabilistic models;semisupervised learning.},
  doi={10.1109/TCBB.2010.84},
  ISSN={1557-9964},
  month={Sep.},}@INPROCEEDINGS{11163049,
  author={Xie, ZhiXian and Fang, Shan},
  booktitle={2025 IEEE 12th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={The Application of AI-Assisted Color and Emotion Association Modeling in Healing Space Design}, 
  year={2025},
  volume={12},
  number={},
  pages={1276-1282},
  abstract={This study addresses the issues of high subjectivity and lack of quantitative basis in traditional color design for therapeutic spaces by proposing an AI-assisted method for modeling the relationship between color and emotion. First, multimodal emotion recognition technologies (such as facial expression analysis, speech, and semantic analysis) are used to extract users’ current emotional states. Then, a mapping relationship between color parameters (HSV/RGB) and the Valence-Arousal emotion model is constructed to achieve a quantitative conversion between "emotion and color." Finally, an intelligent color-matching system is developed and embedded into the spatial design process to assist designers in generating emotionally guided color schemes. The study adopts a psycho-physiological joint evaluation method, using tools such as the PANAS and SAM scales, as well as physiological indicators like EDA and HRV, to verify the model's effectiveness. The results show that the proposed method significantly improves emotion recognition accuracy and the generation of emotionally congruent color schemes, with high user satisfaction. This research provides AI-based support for personalized and scientific therapeutic space design, offering promising application prospects.},
  keywords={Emotion recognition;Adaptation models;Accuracy;Translation;Semantics;Color;Speech recognition;Smart homes;User experience;Real-time systems;AI-assisted design;color-emotion modeling therapeutic space;emotion recognition},
  doi={10.1109/ITAIC64559.2025.11163049},
  ISSN={2693-2865},
  month={May},}@INPROCEEDINGS{11100719,
  author={Zhai, Denghui and Zhu, Yu and Xie, Weihua and Yang, Qiang},
  booktitle={2025 IEEE International Symposium on the Application of Artificial Intelligence in Electrical Engineering (AAIEE)}, 
  title={Computer Vision based Operational Condition Monitoring and Risk Detection of Offshore Wind Farm Converter Valve}, 
  year={2025},
  volume={},
  number={},
  pages={606-611},
  abstract={This paper focuses on the digital intelligent operation and maintenance technology in the scene of offshore wind power converter stations and proposes a condition monitoring and risk detection algorithm based on computer vision. Due to the environmental particularity of offshore converter stations and the important position of converter stations in DC transmission, it has important practical significance and research value for the multi-aspects monitoring and inspection of converter stations. In recent years, with the development of deep learning, many detection algorithms based on computer vision have been implemented in industrial scenes, replacing manual inspection work in complex and dangerous conditions, and improving equipment safety and inspection efficiency. Based on the improved yolov9 model and two-stage visual detection method proposed in this paper, the detection and identification of a variety of faults and risks such as personnel safety operation, smoke and fire, equipment dirt and corrosion are recognized in multiple scenarios of converter station, which provides technical support for the intelligent operation and maintenance of digital converter station of offshore wind power platform, helps to reduce the operation and maintenance cost and improve the operation efficiency of converter station.},
  keywords={Condition monitoring;Computer vision;Visualization;Inspection;Wind power generation;Wind farms;Valves;Maintenance;Safety;Detection algorithms;offshore wind farm;converter valve;operational condition monitoring;machine learning},
  doi={10.1109/AAIEE64965.2025.11100719},
  ISSN={},
  month={April},}@INPROCEEDINGS{10350939,
  author={de Almeida, Tiago Rodrigues and Rudenko, Andrey and Schreiter, Tim and Zhu, Yufei and Maestro, Eduardo Gutierrez and Morillo-Mendez, Lucas and Kucner, Tomasz P. and Martinez Mozos, Oscar and Magnusson, Martin and Palmieri, Luigi and Arras, Kai O. and Lilienthal, Achim J.},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={THÖR-Magni: Comparative Analysis of Deep Learning Models for Role-conditioned Human Motion Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={2192-2201},
  abstract={Autonomous systems, that need to operate in human environments and interact with the users, rely on understanding and anticipating human activity and motion. Among the many factors which influence human motion, semantic attributes, such as the roles and ongoing activities of the detected people, provide a powerful cue on their future motion, actions, and intentions. In this work we adapt several popular deep learning models for trajectory prediction with labels corresponding to the roles of the people. To this end we use the novel THÖR-Magni dataset, which captures human activity in industrial settings and includes the relevant semantic labels for people who navigate complex environments, interact with objects and robots, work alone and in groups. In qualitative and quantitative experiments we show that the role-conditioned LSTM, Transformer, GAN and VAE methods can effectively incorporate the semantic categories, better capture the underlying input distribution and therefore produce more accurate motion predictions in terms of Top-K ADE/FDE and log-likelihood metrics.},
  keywords={Measurement;Deep learning;Annotations;Semantics;Fitting;Manuals;Predictive models;Transformers;Trajectory;Task analysis;human trajectory prediction;human motion dataset;deep learning},
  doi={10.1109/ICCVW60793.2023.00234},
  ISSN={2473-9944},
  month={Oct},}@ARTICLE{10612764,
  author={Guo, Siyuan and Guan, Jihong and Zhou, Shuigeng},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Diffusing on Two Levels and Optimizing for Multiple Properties: A Novel Approach to Generating Molecules With Desirable Properties}, 
  year={2024},
  volume={21},
  number={6},
  pages={2050-2063},
  abstract={In the past decade, Artificial Intelligence (AI) driven drug design and discovery has been a hot research topic in the AI area, where an important branch is molecule generation by generative models, from GAN-based models and VAE-based models to the latest diffusion-based models. However, most existing models pursue mainly the basic properties like validity and uniqueness of the generated molecules, a few go further to explicitly optimize one single important molecular property (e.g. QED or PlogP), which makes most generated molecules little usefulness in practice. In this paper, we present a novel approach to generating molecules with desirable properties, which expands the diffusion model framework with multiple innovative designs. The novelty is two-fold. On the one hand, considering that the structures of molecules are complex and diverse, and molecular properties are usually determined by some substructures (e.g. pharmacophores), we propose to perform diffusion on two structural levels: molecules and molecular fragments respectively, with which a mixed Gaussian distribution is obtained for the reverse diffusion process. To get desirable molecular fragments, we develop a novel electronic effect based fragmentation method. On the other hand, we introduce two ways to explicitly optimize multiple molecular properties under the diffusion model framework. First, as potential drug molecules must be chemically valid, we optimize molecular validity by an energy-guidance function. Second, since potential drug molecules should be desirable in various properties, we employ a multi-objective mechanism to optimize multiple molecular properties simultaneously. Extensive experiments with two benchmark datasets QM9 and ZINC250 k show that the molecules generated by our proposed method have better validity, uniqueness, novelty, Fréchet ChemNet Distance (FCD), QED, and PlogP than those generated by current SOTA models.},
  keywords={Training;Drugs;Data models;Computational modeling;Gaussian distribution;Diffusion models;Optimization;Diffusion model;molecule generation;multi-objective optimization},
  doi={10.1109/TCBB.2024.3434461},
  ISSN={1557-9964},
  month={Nov},}@INPROCEEDINGS{10688253,
  author={Jaiswal, Manuraj and Choubey, Abha and Choubey, Siddhartha and Jaiswal, Dipti},
  booktitle={2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0}, 
  title={Investigating The Use of Neural Networks In Image and Video Processing for Improved Media Quality}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Today, in the digital age, high-quality video and pictures are more important than ever in many areas, from medical images and monitoring to fun and multimedia. This study looks into how neural networks can be used in picture and video processing to make media better. New technologies like deep learning and neural network designs have made it possible for computers to fix and improve video content to a whole new level. Getting rid of image noise, improving sharpness, and denoising videos are all common issues that this study aims to look at how well neural networks deal with them. Part of this research is coming up with new ways to use generative adversarial networks (GANs), reinforcement learning techniques, convolutional and recurrent neural networks, and more. As part of the investigation, we compare the performance of the suggested methods to a group of well-known image and video processing methods. These include Bicubic Interpolation, Histogram Equalization, Wavelet Denoising, Non-Local Means Denoising, Sharpening Filters, and Dehazing and Contrast Enhancement. As part of the performance measures, signal-to-noise ratios, picture quality indices, and studies on how people see things are used. The big steps forward made possible by neural networks are shown by our results. Image and video quality, noise reduction, and clarity are all much better when the suggested methods are used instead of more common ones in a number of test cases. There are many ways that neural networks could completely change how picture and video processing is done. This study shows some of these ways.},
  keywords={Deep learning;Interpolation;Histograms;Recurrent neural networks;Image resolution;Filters;Convolution;Artificial Intelligence;Computer Vision;Deep Learning;Image Processing;Machine Learning;Media Enhancement;Neural Networks;Quality Improvement;Video Processing;Visual Media},
  doi={10.1109/OTCON60325.2024.10688253},
  ISSN={},
  month={June},}@INPROCEEDINGS{10594398,
  author={Hu, Huiting and Wang, Xing and Zhu, Guohua},
  booktitle={2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)}, 
  title={Controllable Text Generation Based on Enhanced Non-Residual Attention}, 
  year={2024},
  volume={},
  number={},
  pages={395-399},
  abstract={The typical construction method of prompts in CLM results in the combination of prompt text information and input text information being too long, and designing a prompt is challenging. Fixed template combinations can impact the grammatical structure, complicate the understanding of the prompt model, and influence the control effect of the generative model. By enhancing Non-Residual Attention, the prompt model processes prompt information to generate improved prompts. This enables the generative model to access comprehensive enhanced prompt information and input text information at any time step. At the same time, a copying mechanism is incorporated into the generative model to tackle the consistency issue of contextual text and enhance the controllability of the generative model. This enables the model output to encompass input text information or prompt text information, thereby reflecting contextual consistency and improving output control. Based on the ROCStory dataset with labeled characters, emotions, and actions, the results indicate that the enhanced Non-Residual Attention model has better control and generation effects compared to the original method.},
  keywords={Coherence;Syntactics;Controllability;Logic;Context modeling;CLM;the Non-Residual Attention;copying mechanism},
  doi={10.1109/ICETCI61221.2024.10594398},
  ISSN={},
  month={May},}@ARTICLE{10526444,
  author={Fukaya, Kaisei and Daylamani-Zad, Damon and Agius, Harry},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Evaluation Metrics for Intelligent Generation of Graphical Game Assets: A Systematic Survey-Based Framework}, 
  year={2024},
  volume={46},
  number={12},
  pages={7998-8017},
  abstract={Generative systems for graphical assets have the potential to provide users with high quality assets at the push of a button. However, there are many forms of assets, and many approaches for producing them. Quantitative evaluation of these methods is necessary if practitioners wish to validate or compare their implementations. Furthermore, providing benchmarks for new methods to strive for or surpass. While most methods are validated using tried-and-tested metrics within their own domains, there is no unified method of finding the most appropriate. We present a framework based on a literature pool of close to 200 papers, that provides guidance in selecting metrics to evaluate the validity and quality of artefacts produced, and the operational capabilities of the method.},
  keywords={Measurement;Three-dimensional displays;Games;Databases;Systematics;Sprites (computer);Solid modeling;Artificial intelligence;evaluation metrics;graphical game assets;PCG},
  doi={10.1109/TPAMI.2024.3398998},
  ISSN={1939-3539},
  month={Dec},}@INPROCEEDINGS{10842773,
  author={Sharma, Deepika and Devare, Manoj},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={Advancing Malware Defense: A Hybrid Approach with Explainable Deep Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Malware detection and classification remain critical challenges in cybersecurity, necessitating advanced techniques for combating increasingly sophisticated threats. This paper presents a novel hybrid approach that leverages the power of explainable deep neural networks to enhance malware detection and classification accuracy while providing interpretable results. The proposed method combines static and dynamic analysis techniques to extract a comprehensive set of features from both executable files and runtime behaviors. These features are then processed using a custom-designed deep neural network architecture that incorporates attention mechanisms and interpretability layers. The model not only achieves high accuracy in detecting and classifying various malware families, but also offers explanations for its decisions, addressing the "black box" problem often associated with deep learning models. The experimental results demonstrated the effectiveness of this hybrid approach, showing significant improvements over traditional machine learning methods and baseline deep learning models. The explainable component of the system provides valuable insights into the specific characteristics and behaviors that contribute to malware identification, potentially aiding the development of more robust defense mechanisms. This research contributes to the field by offering a powerful, interpretable tool for malware analysis that can adapt to evolving threats while providing transparency in its decision-making process, thus bridging the gap between advanced AI techniques and practical cybersecurity applications.},
  keywords={Deep learning;Industries;Accuracy;Runtime;Artificial neural networks;Medical services;Feature extraction;Malware;Hybrid power systems;Computer security;Malware detection;explainable AI;deep neural networks;interpretability;AI transparency;adaptive security},
  doi={10.1109/IDICAIEI61867.2024.10842773},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10092961,
  author={Donzella, Valentina and Chan, Pak Hung and Huggett, Anthony},
  booktitle={2022 2nd International Conference on Robotics, Automation and Artificial Intelligence (RAAI)}, 
  title={Optimising Faster R-CNN Training to Enable Video Camera Compression for Assisted and Automated Driving Systems}, 
  year={2022},
  volume={},
  number={},
  pages={234-239},
  abstract={Advanced driving assistance systems based on only one camera or one RADAR are evolving into the current assisted and automated driving functions delivering SAE Level 2 and above capabilities. A suite of environmental perception sensors is required to achieve safe and reliable planning and navigation in future vehicles equipped with these capabilities. The sensor suite, based on several cameras, LiDARs, RADARs and ultrasonic sensors, needs to be adequate to provide sufficient (and redundant, depending on the level of driving automation) spatial and temporal coverage of the environment around the vehicle. However, the data amount produced by the sensor suite can easily exceed a few tens of Gb/s, with a single ‘average’ automotive camera producing more than 3 Gb/s. It is therefore important to consider leveraging traditional video compression techniques as well as to investigate novel ones to reduce the amount of video camera data to be transmitted to the vehicle processing unit(s). In this paper, we demonstrate that lossy compression schemes, with high compression ratios (up to 1:1,000) can be applied safely to the camera video data stream when machine learning based object detection is used to consume the sensor data. We show that transfer learning can be used to re-train a deep neural network with H.264 and H.265 compliant compressed data, and it allows the network performance to be optimised based on the compression level of the generated sensor data. Moreover, this form of transfer learning improves the neural network performance when evaluating uncompressed data, increasing its robustness to real world variations of the data.},
  keywords={Deep learning;Transfer learning;Artificial neural networks;Object detection;Detectors;Cameras;Robot sensing systems;perception sensors;unmanned vehicles;assisted and automated driving;neural networks;intelligent systems;autonomous systems},
  doi={10.1109/RAAI56146.2022.10092961},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11130027,
  author={Perales, Emma and Verdy-Ricard, Romain and Labiod, Mohamed Aymen and Bendiab, Gueltoum and Chenoune, Yasmina},
  booktitle={2025 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Federated Learning for Securing Medical Imaging Against Deepfakes in 6G Smart Hospitals}, 
  year={2025},
  volume={},
  number={},
  pages={1100-1105},
  abstract={The convergence of $\mathbf{6 G}$ connectivity, artificial intelligence, and smart healthcare infrastructure is transforming medical imaging workflows. In 6G-enabled smart hospitals, radiology images such as CT and MRI scans are processed and transmitted at ultra-low latency between edge devices, cloud platforms, and remote specialists. However, this speed and openness introduce critical security vulnerabilities, notably the risk of deepfakebased manipulation. Malicious actors can exploit generative adversarial networks (GANs) to inject, erase, or alter anomalies in medical scans-potentially leading to misdiagnoses, insurance fraud, or patient endangerment. In this paper, we introduce a federated deepfake detection framework tailored to 6 G healthcare networks. Our approach uses a DenseNet-based convolutional neural network collaboratively trained across multiple medical institutions, ensuring privacy preservation through decentralised learning. We simulate edge-based federated training scenarios on benchmark datasets, including real and manipulated CT images, and evaluate the model’s robustness in both intra- and cross-dataset conditions. The results show strong generalisation capabilities and high sensitivity to tampered medical content. This work paves the way for a secure AI-assisted image verification pipeline in next-generation healthcare environments, reinforcing 6 G network management with decentralised, privacyaware intelligence for deepfake mitigation.},
  keywords={6G mobile communication;Training;Deepfakes;Hospitals;Federated learning;Computed tomography;Robustness;Artificial intelligence;Next generation networking;Biomedical imaging;Deepfake detection;Federated learning;6G networks;Smart hospitals;Medical imaging;DenseNet;CT scans;Privacy-preserving AI},
  doi={10.1109/CSR64739.2025.11130027},
  ISSN={},
  month={Aug},}@ARTICLE{10049068,
  author={Tiago, Cristiana and Snare, Sten Roar and Šprem, Jurica and McLeod, Kristin},
  journal={IEEE Access}, 
  title={A Domain Translation Framework With an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images}, 
  year={2023},
  volume={11},
  number={},
  pages={17594-17602},
  abstract={Currently, medical image domain translation operations show a high demand from researchers and clinicians. Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant. Deep Learning (DL) architectures, most specifically deep generative models, are widely used to generate and translate images from one domain to another. The proposed framework relies on an adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography images and perform domain translation. Contrary to Generative Adversarial Networks (GANs), DDMs are able to generate high quality image samples with a large diversity. If a DDM is combined with a GAN, this ability to generate new data is completed at an even faster sampling time. In this work we trained an adversarial DDM combined with a GAN to learn the reverse denoising process, relying on a guide image, making sure relevant anatomical structures of each echocardiography image were kept and represented on the generated image samples. For several domain translation operations, the results verified that such generative model was able to synthesize high quality image samples: MSE: 11.50 ± 3.69, PSNR (dB): 30.48 ± 0.09, SSIM: 0.47 ± 0.03. The proposed method showed high generalization ability, introducing a framework to create echocardiography images suitable to be used for clinical research purposes.},
  keywords={Echocardiography;Training;Biomedical imaging;Data models;Ultrasonic imaging;Noise reduction;Diffusion processes;Deep learning;diffusion models;domain translation;echocardiography;image generation},
  doi={10.1109/ACCESS.2023.3246762},
  ISSN={2169-3536},
  month={},}@ARTICLE{10348505,
  author={Guan, Shannan and Yu, Xin and Huang, Wei and Fang, Gengfa and Lu, Haiyan},
  journal={IEEE Transactions on Image Processing}, 
  title={DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition}, 
  year={2024},
  volume={33},
  number={},
  pages={395-407},
  abstract={In this work, we propose a new Dual Min-Max Games (DMMG) based self-supervised skeleton action recognition method by augmenting unlabeled data in a contrastive learning framework. Our DMMG consists of a viewpoint variation min-max game and an edge perturbation min-max game. These two min-max games adopt an adversarial paradigm to perform data augmentation on the skeleton sequences and graph-structured body joints, respectively. Our viewpoint variation min-max game focuses on constructing various hard contrastive pairs by generating skeleton sequences from various viewpoints. These hard contrastive pairs help our model learn representative action features, thus facilitating model transfer to downstream tasks. Moreover, our edge perturbation min-max game specializes in building diverse hard contrastive samples through perturbing connectivity strength among graph-based body joints. The connectivity-strength varying contrastive pairs enable the model to capture minimal sufficient information of different actions, such as representative gestures for an action while preventing the model from overfitting. By fully exploiting the proposed DMMG, we can generate sufficient challenging contrastive pairs and thus achieve discriminative action feature representations from unlabeled skeleton data in a self-supervised manner. Extensive experiments demonstrate that our method achieves superior results under various evaluation protocols on widely-used NTU-RGB+D, NTU120-RGB+D and PKU-MMD datasets.},
  keywords={Skeleton;Games;Data augmentation;Task analysis;Perturbation methods;Data models;Three-dimensional displays;Self-supervised learning;adversarial learning;contrastive learning;skeleton action recognition;min-max game},
  doi={10.1109/TIP.2023.3338410},
  ISSN={1941-0042},
  month={},}@ARTICLE{9615000,
  author={Zhang, Zhao and Zhuang, Fuzhen and Zhu, Hengshu and Li, Chao and Xiong, Hui and He, Qing and Xu, Yongjun},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}, 
  year={2023},
  volume={35},
  number={4},
  pages={4321-4334},
  abstract={Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in AI-related applications. Despite the large sizes, existing KGs are far from complete and comprehensive. In order to continuously enrich KGs, automatic knowledge construction and update mechanisms are usually utilized, which inevitably bring in plenty of noise. However, most existing knowledge graph embedding (KGE) methods assume that all the triple facts in KGs are correct, and project both entities and relations into a low-dimensional space without considering noise and knowledge conflicts. This will lead to low-quality and unreliable representations of KGs. To this end, in this paper, we propose a general multi-task reinforcement learning framework, which can greatly alleviate the noisy data problem. In our framework, we exploit reinforcement learning for choosing high-quality knowledge triples while filtering out the noisy ones. Also, in order to take full advantage of the correlations among semantically similar relations, the triple selection processes of similar relations are trained in a collective way with multi-task learning. Moreover, we extend popular KGE models TransE, DistMult, ConvE and RotatE with the proposed framework. Finally, the experimental validation shows that our approach is able to enhance existing KGE models and can provide more robust representations of KGs in noisy scenarios.},
  keywords={Task analysis;Reinforcement learning;Training;Computational modeling;Noise measurement;Chaos;Internet;Knowledge graph;knowldge discovery;big data applications},
  doi={10.1109/TKDE.2021.3127951},
  ISSN={1558-2191},
  month={April},}@ARTICLE{10499254,
  author={Wang, Jiaxiang and Zheng, Aihua and Yan, Yan and He, Ran and Tang, Jin},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Attribute-Guided Cross-Modal Interaction and Enhancement for Audio-Visual Matching}, 
  year={2024},
  volume={19},
  number={},
  pages={4986-4998},
  abstract={Audio-visual matching is an essential task that measures the correlation between audio clips and visual images. However, current methods rely solely on the joint embedding of global features from audio clips and face image pairs to learn semantic correlations. This approach overlooks the importance of high-confidence correlations and discrepancies of local subtle features, which are crucial for cross-modal matching. To address this issue, we propose a novel Attribute-guided Cross-modal Interaction and Enhancement Network (ACIENet), which employs multiple attributes to explore the associations of different key local subtle features. The ACIENet contains two novel modules: the Attribute-guided Interaction (AGI) module and the Attribute-guided Enhancement (AGE) module. The AGI module employs global feature alignment similarity to guide cross-modal local feature interactions, which enhances cross-modal association features for the same identity and expands cross-modal distinctive features for different identities. Additionally, the interactive features and original features are fused to ensure intra-class discriminability and inter-class correspondence. The AGE module captures subtle attribute-related features by using an attribute-driven network, thereby enhancing discrimination at the attribute level. Specifically, it strengthens the combined attribute-related features of gender and nationality. To prevent interference between multiple attribute features, we design a multi-attribute learning network as a parallel framework. Experiments conducted on a public benchmark dataset demonstrate the efficacy of the ACIENet method in different scenarios. Code and models are available at https://github.com/w1018979952/ACIENet.},
  keywords={Feature extraction;Faces;Correlation;Task analysis;Face recognition;Visualization;Semantics;Audio-visual cross-modal matching;attribute-guided cross-modal interaction;attribute-guided cross-modal enhancement},
  doi={10.1109/TIFS.2024.3388949},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10469620,
  author={Purohit, Ruchira and Sane, Yana and Vaishampayan, Devashree and Vedantam, Sowmya and Singh, Mangal},
  booktitle={2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)}, 
  title={AI vs. Human Vision: A Comparative Analysis for Distinguishing AI-Generated and Natural Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Today’s data-driven generation has led to remarkable advancements in technology. However, as there are two sides to a coin, technology too has both its advantages and disadvantages. The expansion of AI has given rise to ‘Deepfake’ which involves skillful superimposing of person’s face with another person’s face which is very dangerous and it is used to produce morphed images and disseminate fake videos which has led to cyberbullying, financial fraud and cybersecurity risks. Our goal is to correctly determine authentic images by classifying them into AI generated v/s real images.We have used ‘PyGoogle’ image library for creation of dataset for AI images and for the real image dataset we have used our own camera to capture real images. We have used CNN model on both the dataset and observed that accuracy of Google images dataset is 88 percent and that of the own dataset is 81 percent. For evaluating the performance of our model we have created Confusion Matrix for the same.},
  keywords={Computational modeling;Finance;Cyberbullying;Libraries;Internet;Fraud;Artificial intelligence;AI images;Image Classification;CNN},
  doi={10.1109/ICAECT60202.2024.10469620},
  ISSN={},
  month={Jan},}@ARTICLE{9941060,
  author={Xu, Cheng and Li, Keke and Luo, Xuandi and Xu, Xuemiao and He, Shengfeng and Zhang, Kun},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Fully Deformable Network for Multiview Face Image Synthesis}, 
  year={2024},
  volume={35},
  number={7},
  pages={8854-8868},
  abstract={Photorealistic multiview face synthesis from a single image is a challenging problem. Existing works mainly learn a texture mapping model from the source to the target faces. However, they rarely consider the geometric constraints on the internal deformation arising from pose variations, which causes a high level of uncertainty in face pose modeling, and hence, produces inferior results for large pose variations. Moreover, current methods typically suffer from undesired facial details loss due to the adoption of the de-facto standard encoder–decoder architecture without any skip connections (SCs). In this article, we directly learn and exploit geometric constraints and propose a fully deformable network to simultaneously model the deformations of both landmarks and faces for face synthesis. Specifically, our model consists of two parts: a deformable landmark learning network (DLLN) and a gated deformable face synthesis network (GDFSN). The DLLN converts an initial reference landmark to an individual-specific target landmark as delicate pose guidance for face rotation. The GDFSN adopts a dual-stream structure, with one stream estimating the deformation of two views in the form of convolution offsets according to the source pose and the converted target pose, and the other leveraging the predicted deformation offsets to create the target face. In this way, individual-aware pose changes are explicitly modeled in the face generator to cope with geometric transformation, by adaptively focusing on pertinent regions of the source face. To compensate for offset estimation errors, we introduce a soft-gating mechanism for adaptive fusion between deformable features and primitive features. Additionally, a pose-aligned SC (PASC) is tailored to propagate low-level input features to the appropriate positions in the output features for further enhancing the facial details and identity preservation. Extensive experiments on six benchmarks show that our approach performs favorably against the state-of-the-arts, especially with large pose changes. Code is available at https://github.com/cschengxu/FDFace.},
  keywords={Face detection;Face recognition;Strain;Deformable models;Convolutional neural networks;Standards;Pose estimation;Deformable convolution;gating;multiview face synthesis;pose-invariant face recognition},
  doi={10.1109/TNNLS.2022.3216018},
  ISSN={2162-2388},
  month={July},}@INPROCEEDINGS{9268761,
  author={Loukmane, Attoumane and Graña, Manuel and Mestari, Mohammed},
  booktitle={2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS)}, 
  title={A Model for Classification of Traffic Signs Using Improved Convolutional Neural Network and Image Enhancement}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={In an advanced driver assistance system (ADAS), recognition of traffic signs is very important for safety driving. Recently, the convolutional neural networks (CNNs) have presented promising results. In this work, we propose a robust model based on VGG network by adding batch normalization operation. Dropout is also used to reduce the overfitting of the model. Due to the imbalance of the dataset, data augmentation is performed. Then, in order to enhance images, Contrast limited adaptive histogram equalization (CLAHE) and normalization are performed. The performance of the model is evaluated on German traffic sign recognition benchmark (GTSRB) dataset using different performance metrics namely confusion matrix, precision, recall. Experiments results show that, the proposed model reaches a state-of-art accuracy of 99.33 % and surpasses the best human performance of 98.84 %. This model can be used for real world system.},
  keywords={Feature extraction;Training;Support vector machines;Kernel;Histograms;Task analysis;Neurons;Traffic signs;CNNs;GTSRB},
  doi={10.1109/ICDS50568.2020.9268761},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10766731,
  author={Yang, Min Jian and Zeng, Yueling Jenny and Wang, Li-C.},
  booktitle={2024 IEEE International Test Conference (ITC)}, 
  title={WM-Graph: Graph-Based Approach for Wafermap Analytics}, 
  year={2024},
  volume={},
  number={},
  pages={111-120},
  abstract={This paper introduces WM-Graph, a novel approach designed for flexible analytics of wafermaps. The key concept behind WM-Graph is the construction of a wafermap graph, where individual wafermaps are connected if they exhibit semi-equivalence. This graph-based structure allows a wide range of analytics to be performed using established graph algorithms. Unlike traditional multi-class classification methods, WM-Graph enables more versatile analyses, making it possible to answer complex, practical questions that would otherwise be difficult to address. We explain the technical innovations that underpin the WM-Graph approach and demonstrate how to perform certain analytical tasks with simple graph operations. The effectiveness of the WM-Graph approach is validated through experiments using the public WM-811K dataset and a proprietary dataset from a recent production line.},
  keywords={Technological innovation;Systematics;Natural languages;Production;Market research;Classification algorithms;Artificial intelligence;wafermaps;analytics;machine learning;decision support},
  doi={10.1109/ITC51657.2024.00030},
  ISSN={2378-2250},
  month={Nov},}@ARTICLE{11023997,
  author={Zhang, Yaning and Wang, Tianyi and Yu, Zitong and Gao, Zan and Shen, Linlin and Chen, Shengyong},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={MFCLIP: Multi-Modal Fine-Grained CLIP for Generalizable Diffusion Face Forgery Detection}, 
  year={2025},
  volume={20},
  number={},
  pages={5888-5903},
  abstract={The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Our code will be available at https://github.com/Jenine-321/MFCLIP},
  keywords={Forgery;Noise;Faces;Feature extraction;Visualization;Training;Electronic mail;Computer vision;Adaptation models;Transformers;Diffusion face forgery detection;transformer;CLIP;image-noise fusion;sample pair attention},
  doi={10.1109/TIFS.2025.3576577},
  ISSN={1556-6021},
  month={},}@ARTICLE{8717627,
  author={Jiang, Yun and Tan, Ning and Peng, Tingting},
  journal={IEEE Access}, 
  title={Optic Disc and Cup Segmentation Based on Deep Convolutional Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={64483-64493},
  abstract={Glaucoma is a chronic eye disease that causes loss of vision and it is irreversible. Accurate segmentation of optic disc and optic cup is a basic step in screening glaucoma. The most existing deep convolutional neural network (DCNN) methods have insufficient feature information extraction, and hence they are susceptible to pathological regions and low-quality images, with have poor ability to restore context information. Finally, the accuracy of the model segmentation is low. In this paper, we propose GL-Net, a multi-label DCNN model that combines the generative adversarial networks. GL-Net consists of two network structures including a Generator and a Discriminator. In the Generator, we use skip connection to promote the fusion of low-level feature information and high-level feature information, which alleviates the difficulty of restoring detailed feature information during upsampling, and reduces the downsampling factor, effectively alleviating excessive feature information loss. In the loss function, we add the L1 distance function and the cross-entropy function to prevent the mode collapse when the model is trained, which makes the segmentation result more accurate. We use transfer learning and data augmentation to alleviate the problem of insufficient data and over-fitting of the model during training. Finally, GL-Net was verified on DRISHTI-GS1 dataset. The experimental results show that GL-Net outperforms some state-of-the-art method, such as M-Net, Stack-U-Net, RACE-net, and BCRF in terms of F1 and boundary distance localization error (BLE). Particularly, in the optic cup segmentation, GL-Net outperforms RACE-net by 3.5% and 4.16 pixels in terms of F1 and BLE, respectively.},
  keywords={Optical imaging;Biomedical optical imaging;Adaptive optics;Image segmentation;Optical fiber networks;Optical computing;Feature extraction;Deep learning;optic disc segmentation;optic cup segmentation;deep convolutional neural networks;generative adversarial networks},
  doi={10.1109/ACCESS.2019.2917508},
  ISSN={2169-3536},
  month={},}@ARTICLE{9042295,
  author={Dai, Qiang and Cheng, Xi and Qiao, Yan and Zhang, Youhua},
  journal={IEEE Access}, 
  title={Crop Leaf Disease Image Super-Resolution and Identification With Dual Attention and Topology Fusion Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={55724-55735},
  abstract={For agricultural disease image identification, obtained images are typically unclear, which can lead to poor identification results in real production environments. The quality of an image has a significant impact on the identification accuracy of pre-trained image classifiers. To address this problem, we propose a generative adversarial network with dual-attention and topology-fusion mechanisms called DATFGAN. This network can effectively transform unclear images into clear and high-resolution images. Additionally, the weight sharing scheme in our proposed network can significantly reduce the number of parameters. Experimental results demonstrate that DATFGAN yields more visually pleasing results than state-of-the-art methods. Additionally, treated images are evaluated based on identification tasks. The results demonstrate that the proposed method significantly outperforms other methods and is sufficiently robust for practical use.},
  keywords={Diseases;Agriculture;Feature extraction;Topology;Network topology;Generators;Crop leaf disease;attention;generative adversarial networks;super-resolution;identification},
  doi={10.1109/ACCESS.2020.2982055},
  ISSN={2169-3536},
  month={},}@ARTICLE{9123405,
  author={Meng, Nan and Ge, Zhou and Zeng, Tianjiao and Lam, Edmund Y.},
  journal={IEEE Access}, 
  title={LightGAN: A Deep Generative Model for Light Field Reconstruction}, 
  year={2020},
  volume={8},
  number={},
  pages={116052-116063},
  abstract={A light field image captured by a plenoptic camera can be considered a sampling of light distribution within a given space. However, with the limited pixel count of the sensor, the acquisition of a high-resolution sample often comes at the expense of losing parallax information. In this work, we present a learning-based generative framework to overcome such tradeoff by directly simulating the light field distribution. An important module of our model is the high-dimensional residual block, which fully exploits the spatio-angular information. By directly learning the distribution, our approach can generate both high-quality sub-aperture images and densely-sampled light fields. Experimental results on both real-world and synthetic datasets demonstrate that the proposed method outperforms other state-of-the-art approaches and achieves visually more realistic results.},
  keywords={Image reconstruction;Convolution;Spatial resolution;Correlation;Generators;Light fields;Light field reconstruction;view synthesis;4D convolution;high-dimension residual block;generative adversarial networks;deep learning;computational imaging},
  doi={10.1109/ACCESS.2020.3004477},
  ISSN={2169-3536},
  month={},}@ARTICLE{8962051,
  author={Meng, Lingwu and You, Xiaoming and Liu, Sheng and Li, Shundong},
  journal={IEEE Access}, 
  title={Multi-Colony Ant Algorithm Using Both Generative Adversarial Nets and Adaptive Stagnation Avoidance Strategy}, 
  year={2020},
  volume={8},
  number={},
  pages={53250-53260},
  abstract={Aiming at Travel Salesman Problem (TSP) that ant colony algorithm is easy to fall into local optima and slow convergence, a multi-colony ant algorithm using both generative adversarial nets (GAN) and adaptive stagnation avoidance strategy (GAACO) is proposed. First, to improve the convergence speed of the algorithm, we introduce a GAN model based on the game between convergence speed and solution quality. Then, to overcome premature convergence, an adaptive stagnation avoidance strategy is proposed. The strategy consists of two parts: (1) information entropy. It is used to measure the diversity of GAACO; (2) a cooperative game model. When the value of information entropy is less than threshold value, the cooperative game model will be used to select the appropriate pheromone matrix for different colonies to improve the accuracy. Finally, to further accelerate the convergence of the algorithm, the initial pheromone matrix is preprocessed to increase the pheromone of the optimal path for each iteration in the early stage. And according to reinforcement learning method, each colony increases the pheromone of the global optimal path at the end of each iteration. Extensive experiments with numerous instances in the TSPLIB standard library show that the proposed methods significantly outperform the state-of-the-art multi-colony ant colony optimization algorithms, especially in the large-scale TSPs.},
  keywords={Convergence;Ant colony optimization;Urban areas;Games;Heuristic algorithms;Adaptation models;Gallium nitride;Generative adversarial nets;cooperative game;information entropy;travel salesman problem},
  doi={10.1109/ACCESS.2020.2967076},
  ISSN={2169-3536},
  month={},}@ARTICLE{9257470,
  author={Xia, Hailun and Xiao, Meng},
  journal={IEEE Access}, 
  title={3D Human Pose Estimation With Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={206198-206206},
  abstract={3D human pose estimation from a monocular RGB image is a challenging task in computer vision because of depth ambiguity in a single RGB image. As most methods consider joint locations independently which can lead to an overfitting problem on specific datasets, it's crucial to consider the plausibility of 3D poses in terms of their overall structures. In this paper, we present Generative Adversarial Networks (GANs) for 3D human pose estimation, which learn plausible 3D human body representations by adversarial training. In GANs, the generator regresses 3D joint positions from a 2D input and the discriminator aims to distinguish the ground-truth 3D samples from the predicted ones. We leverage Graph Convolutional Networks (GCNs) in both generator and discriminator to fully exploit the spatial relations of input and output coordinates. The combination of GANs and GCNs promotes the network to predict more accurate 3D joint locations and learn more reasonable human body structures at the same time. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and HumanEva-I) where it outperforms state-of-the-art methods. Furthermore, we propose a new evaluation metric distance-based Pose Structure Score (dPSS) for evaluating the structural similarity degree between the predicted 3D pose and its ground-truth.},
  keywords={Three-dimensional displays;Generators;Pose estimation;Two dimensional displays;Training;Measurement;Semantics;3D human pose estimation;generative adversarial networks;graph convolutional networks},
  doi={10.1109/ACCESS.2020.3037829},
  ISSN={2169-3536},
  month={},}@ARTICLE{10845168,
  author={Mitra, Rohan and Zualkernan, Imran},
  journal={IEEE Access}, 
  title={Music Generation Using Deep Learning and Generative AI: A Systematic Review}, 
  year={2025},
  volume={13},
  number={},
  pages={18079-18106},
  abstract={This paper presents a systematic review of recent advances in music generation using deep learning techniques, categorizing the latest research in the field and identifying key contributions from various approaches. The study examines common data representations in music generation, including raw waveforms, spectrograms, and MIDI, alongside the most prominent deep learning architectures like Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Variational Autoencoders (VAEs), and Transformer-based models. Through a comparative analysis, the paper highlights the strengths and limitations of these approaches. The findings suggest that GANs with spectrograms and RNNs with MIDI data are particularly effective for generating multi-track music, while autoregressive models like MusicGen and transformer models demonstrate superior performance in capturing long-term dependencies in music generation. Additionally, the paper underscores the emergence of diffusion models, which are gaining popularity for generating high-quality, complex music outputs. The major contribution of this review is the identification of the best-performing models for various music generation tasks and the provision of comprehensive insights into data representation methods, evaluation metrics, and future research directions.},
  keywords={Music;Measurement;Surveys;Spectrogram;Instruments;Mathematical models;Deep learning;Data models;Transformers;Symbols;Music generation;survey paper;GAN;LSTM;VAE;spectrograms;MIDI;RNNs;diffusion models;transformers;generative AI},
  doi={10.1109/ACCESS.2025.3531798},
  ISSN={2169-3536},
  month={},}@ARTICLE{9389731,
  author={Togo, Ren and Kotera, Megumi and Ogawa, Takahiro and Haseyama, Miki},
  journal={IEEE Access}, 
  title={Text-Guided Style Transfer-Based Image Manipulation Using Multimodal Generative Models}, 
  year={2021},
  volume={9},
  number={},
  pages={64860-64870},
  abstract={A new style transfer-based image manipulation framework combining generative networks and style transfer networks is presented in this paper. Unlike conventional style transfer tasks, we tackle a new task, text-guided image manipulation. We realize style transfer-based image manipulation that does not require any reference style images and generate a style image from the user’s input sentence. In our method, since an initial reference input sentence for a content image can automatically be given by an image-to-text model, the user only needs to update the reference sentence. This scheme can help users when they do not have any images representing the desired style. Although this text-guided image manipulation is a new challenging task, quantitative and qualitative comparisons showed the superiority of our method.},
  keywords={Task analysis;Semantics;Feature extraction;Deep learning;Electronic mail;Computational modeling;Style transfer;image manipulation;text-to-image synthesis;aesthetic analysis;generative model},
  doi={10.1109/ACCESS.2021.3069876},
  ISSN={2169-3536},
  month={},}@ARTICLE{9399425,
  author={Hao, Jinlin and Chen, Xueyun},
  journal={IEEE Access}, 
  title={Detailed Feature Guided Generative Adversarial Pose Reconstruction Network}, 
  year={2021},
  volume={9},
  number={},
  pages={56093-56103},
  abstract={Face frontalization is a critical and difficult task on face pose reconstruction. Previous researches use simple posture information as guidance, such as pose coding and facial landmarks. To explore the guidance effect of profile faces, we propose detailed features that provide much detailed information. In this paper, a Detailed Feature Guided Generative Adversarial Pose Reconstruction Network (DGPR) is proposed. Firstly, frontal pose coding and profile detailed features are fed into DGPR to generate detailed features of front face. Then, the second generator combines frontal detailed features and profile face to reconstruct front face. Besides, we propose a conditional enhancement loss to strengthen the guiding role of detailed features, and a smoothing loss to reduce edge sharpness in generated faces. Experimental results show that our method generates photorealistic front faces and outperforms state-of-the-art methods on M2FPA and CAS-PEAL. Specifically, DGPR improves the face recognition accuracy under pose angles of ±60°, ±75°, ±90° by 2%, 1%, and 6% respectively over the state-of-the-art methods on M2FPA, achieves the average rank-1 recognition rate to 99.95% and improves it by 0.05% on CAS-PEAL. These results demonstrate the effects of detailed features and corresponding modules.},
  keywords={Faces;Face recognition;Three-dimensional displays;Generators;Encoding;Surface reconstruction;Two dimensional displays;Face frontalization;face pose reconstruction;Generative Adversarial Network;face rotation},
  doi={10.1109/ACCESS.2021.3072277},
  ISSN={2169-3536},
  month={},}@ARTICLE{10565870,
  author={Xiao, Nan and Zhao, Kun and Ma, Ruize and Zhang, Hao},
  journal={IEEE Access}, 
  title={A Generative Elastic Network Method for R Peak Detection Suitable for Few-Shot Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={167049-167058},
  abstract={R peak detection is fundamental to the analysis of long-term electrocardiogram (ECG) signals. Despite their significant success in R peak detection, neural networks based on statistical learning usual require more than 50% of all data for training. However, it is often difficult to provide such a high proportion of training data in practice. This paper proposes a novel R peak detection method based on Generative Elastic Network (GEN), which is suitable for few-shot learning. Utilizing the Lobachevsky University Database (LUDB), this method achieves an accuracy exceeding 99% by using less than 3% of the data for training and 14% for validation. It dramatically reduces the dependency on large volumes of data for training and validation, while preserving an accuracy level that is on par with existing methods.},
  keywords={Training;Databases;Electrocardiography;Statistical learning;Feature extraction;Neural networks;Long short term memory;Few shot learning;Generative elastic network;electrocardiogram (ECG);R peak detection;few-shot learning},
  doi={10.1109/ACCESS.2024.3417344},
  ISSN={2169-3536},
  month={},}@ARTICLE{11006033,
  author={Ning, Yuchen and Meng, Xianshuang and Zhou, Lingxiao and Liu, Jun and Qiu, Shijie and Wu, Jingfang and Wei, Xi and Ying, Jun and Zhu, Siwei and Zhang, Yantian},
  journal={IEEE Access}, 
  title={A Multi-Head Attention-Based Lightweight Generative Adversarial Network for Thyroid Ultrasound Video Super-Resolution}, 
  year={2025},
  volume={13},
  number={},
  pages={99628-99640},
  abstract={Thyroid cancer is a prevalent malignancy, highlighting the critical need for early detection of thyroid nodules. Despite ultrasound being the primary diagnostic modality, its limited resolution often hampers the accuracy of identifying malignant nodules. Method: We present a cutting-edge deep learning approach, the Multi-Head Attention-based Generative Adversarial Network (MHVSR), tailored for the super-resolution of thyroid ultrasound videos. This innovative method encompasses a lightweight optical flow extraction network for swift analysis, a recurrent fusion reconstruction module based on multi-head mechanism, and a hybrid loss function designed to expedite network convergence. Result: The MHVSR showcased exceptional performance on  $4\times $  super-resolution task on Thyroid dataset, achieving a PSNR of 31.43dB and an SSIM of 0.98, surpassing current VSR algorithms. It delivered reconstructions that closely approximated the quality of original high-resolution videos, while meeting the stringent real-time diagnostic criteria with an impressive inference speed of 27.89FPS. Furthermore, on the Liver test dataset, MHVSR demonstrated its robustness with a PSNR of 39.39dB and an SSIM of 0.97, outperforming other algorithms. Conclusion: MHVSR not only achieves state-of-the-art super-resolution in ultrasound videos but also satisfies the demanding real-time diagnostic requirements, marking a significant leap forward in the field of medical imaging, particularly for the diagnosis of thyroid nodules.},
  keywords={Image reconstruction;Ultrasonic imaging;Superresolution;Thyroid;Medical diagnostic imaging;Optical flow;Streaming media;Hospitals;Training;Thyroid cancer;Video super resolution;deep learning;image reconstruction;ultrasound image;deep generative models},
  doi={10.1109/ACCESS.2025.3570905},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9319285,
  author={Guidotti, Riccardo and Monreale, Anna and Spinnato, Francesco and Pedreschi, Dino and Giannotti, Fosca},
  booktitle={2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={Explaining Any Time Series Classifier}, 
  year={2020},
  volume={},
  number={},
  pages={167-176},
  abstract={We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.},
  keywords={Time series analysis;Artificial intelligence;Neural networks;Decoding;Encoding;Decision trees;Data models;Explainable AI;Time Series Classification;Shapelet-based Rules;Exemplars and Counter-Exemplars},
  doi={10.1109/CogMI50398.2020.00029},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8490455,
  author={Wan, Shanchuan and Kaneko, Tomoyuki},
  booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG)}, 
  title={Building Evaluation Functions for Chess and Shogi with Uniformity Regularization Networks}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Building evaluation functions for chess variants is a challenging goal. At this time, only AlphaZero succeeded with millions of self-play records produced by using thousands of tensor processing units (TPUs), which are not available for most researchers. This paper presents the challenge of training evaluation functions on the basis of deep convolutional neural networks using decent data and computing resources, where regularization is crucial as complex models trained with limited data are more prone to overfitting. We present a novel training scheme by introducing a uniformity regularization (UR) network. In the proposed approach, a value network and a discriminator network share common convolutional layers and both networks are trained simultaneously. Loss functions for them are based on the difference between the score of a random move and that of an experts' move as a comparison training method. The value network is expected to give precise scores for all positions, while the discriminator makes qualitative evaluations for move pairs, and acts as a regularizer that penalizes differences in evaluation results to ensure all samples are uniformly discriminated. Due to the existence of shared layers, such regularization is beneficial for improving the overall accuracy of the value network. Experimental results for chess and shogi demonstrate the proposed method surpassed the standard L2 regularization method, and successfully helped obtain decently accurate value networks.},
  keywords={Training;Games;Task analysis;Learning (artificial intelligence);Law;Feature extraction;neural networks;joint training;regularization;computer chess},
  doi={10.1109/CIG.2018.8490455},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{10721936,
  author={Nayim, Meshal and Mohan, Vishnu and Pandey, Trilok Nath and Dash, Bibhuti Bhusan and Dash, Bibek Bikram and Patra, Sudhansu Shekhar},
  booktitle={2024 International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)}, 
  title={Detection of Leading CNN Models for AI Image Accuracy and Efficiency}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This research explores the challenge of distinguishing between authentic and AI-generated images using advanced convolutional neural network (CNN) architectures. Leveraging the CIFAKE dataset, which includes a diverse array of AI-generated and real images, we evaluated the performance of three pre-trained models: ResNet50, EfficientNetB0, and DenseNet121. Our study involved training these models with images of 32x32 and resized 64x64 pixel resolutions to assess their classification accuracy, precision, recall, and computational efficiency. DenseNet121 emerged as the most effective model, achieving an accuracy of 98.49% on the test dataset, significantly outperforming the other models. This work highlights the potential of deep learning methodologies in addressing the growing need for reliable AI image detection, providing a robust foundation for future advancements and applications in this critical field.},
  keywords={Training;Deep learning;Accuracy;Image resolution;Computational modeling;Computer architecture;Convolutional neural networks;Reliability;Artificial intelligence;Residual neural networks;AI-generated images;image classification;convolutional neural networks;resnet50;efficientnetb0;densenet121;CIFAKE dataset;deep learning},
  doi={10.1109/IACIS61494.2024.10721936},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9196426,
  author={Greenwood, Nigel and Sundaram, Brruntha and Muirhead, Alexander and Copperthwaite, James},
  booktitle={2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)}, 
  title={Awareness without Neural Networks: Achieving Self-Aware AI via Evolutionary and Adversarial Processes}, 
  year={2020},
  volume={},
  number={},
  pages={147-153},
  abstract={A key difficulty in achieving self-aware artificial intelligence (AI) is the achievement of epistemological knowledge, i.e. a machine that “knows what it knows” and “knows what it does not know” with respect to some model of itself or its surroundings. Given a nonlinear dynamical system with known algebraic structure expressible as differential equations, with sensors able to create a time-series of measurements of sufficient variables to create a suitable partial state vector, then novel forms of evolutionary machine learning and adversarial processes are sufficient to create a form of AI that is “aware” of its knowledge set regarding this system, and can use a form of differential Game Theory and adversarial processes to “think” about its knowledge set to address ambiguities and achieve objectives, including moving beyond its original training data. This may itself constitute a form of “self-awareness”. Results from successful use of these techniques in medical and engineering problems are outlined. This AI architecture does not involve neural networks or their derivative architectures, but instead is inspired by evolutionary ecosystems. Implications for self-aware operating systems are discussed.},
  keywords={Mathematical model;Biological cells;Machine learning;Game theory;Sensor systems;evolutionary;epistemological;learning;adversarial;differential;game;nonlinear;dynamics;ecosystem;operating},
  doi={10.1109/ACSOS-C51401.2020.00047},
  ISSN={},
  month={Aug},}@ARTICLE{10982106,
  author={Ho, Thi Kieu Khanh and Karami, Ali and Armanfard, Narges},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Graph Anomaly Detection in Time Series: A Survey}, 
  year={2025},
  volume={47},
  number={8},
  pages={6990-7009},
  abstract={With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency (relationships within a variable over time) and the inter-variable dependency (relationships between multiple variables) existing in time-series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of TSAD using graphs, referred to as G-TSAD. First, we explore the significant potential of graph representation for time-series data and and its contributions to facilitating anomaly detection. Then, we review state-of-the-art graph anomaly detection techniques, mostly leveraging deep learning architectures, in the context of time series. For each method, we discuss its strengths, limitations, and the specific applications where it excels. Finally, we address both the technical and application challenges currently facing the field, and suggest potential future directions for advancing research and improving practical outcomes.},
  keywords={Surveys;Reviews;Anomaly detection;Time series analysis;Videos;Social networking (online);Market research;Artificial intelligence;Training;Systematics;Time-series anomaly detection;graphs;deep learning;time-series signals;dynamic social networks;videos},
  doi={10.1109/TPAMI.2025.3566620},
  ISSN={1939-3539},
  month={Aug},}@ARTICLE{10874180,
  author={Kong, Insung and Kim, Kunwoong and Kim, Yongdai},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Fair Representation Learning for Continuous Sensitive Attributes Using Expectation of Integral Probability Metrics}, 
  year={2025},
  volume={47},
  number={5},
  pages={3784-3795},
  abstract={AI fairness, also known as algorithmic fairness, aims to ensure that algorithms operate without bias or discrimination towards any individual or group. Among various AI algorithms, the Fair Representation Learning (FRL) approach has gained significant interest in recent years. However, existing FRL algorithms have a limitation: they are primarily designed for categorical sensitive attributes and thus cannot be applied to continuous sensitive attributes, such as age or income. In this paper, we propose an FRL algorithm for continuous sensitive attributes. First, we introduce a measure called the Expectation of Integral Probability Metrics (EIPM) to assess the fairness level of representation space for continuous sensitive attributes. We demonstrate that if the distribution of the representation has a low EIPM value, then any prediction head constructed on the top of the representation become fair, regardless of the selection of the prediction head. Furthermore, EIPM possesses a distinguished advantage in that it can be accurately estimated using our proposed estimator with finite samples. Based on these properties, we propose a new FRL algorithm called Fair Representation using EIPM with MMD (FREM). Experimental evidences show that FREM outperforms other baseline methods.},
  keywords={Predictive models;Prediction algorithms;Measurement;Artificial intelligence;Vectors;Representation learning;Kernel;Economic indicators;Training data;Training;Fairness;representation learning;integral probability metric},
  doi={10.1109/TPAMI.2025.3538915},
  ISSN={1939-3539},
  month={May},}@INPROCEEDINGS{11041809,
  author={Koppula, Anil Kumar and Gadde, Sai Sudha},
  booktitle={2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={A Systematic Review of Deep Learning Algorithms for Non-Alcoholic Fatty Liver Disease}, 
  year={2025},
  volume={},
  number={},
  pages={14-20},
  abstract={Non-alcoholic fatty liver disease (NAFLD) is the foremost cause of continuing liver disease globally. The diagnosis is predicated on the existence of steatosis in more than 5% of hepatocytes without important alcohol use. It aims to give a comprehensive overview of present studies on DL technologies that can assist clinicians in implementing automated NAFLD diagnosis and staging. The qualitative research found that AI greatly improved the diagnosis of NAFLD, Non-Alcoholic Steatohepatitis (NASH), and liver fibrosis. Modalities, picture capture, feature extraction, data management, and classifiers were evaluated and compared based on performance indicators. DL-enabled solutions may improve the detection and quantification of steatosis, NASH, and NAFLD patients. Prospective research comparing DL-assisted modalities to traditional approaches is important in real-world applications.},
  keywords={Deep learning;Ultrasonic imaging;Liver diseases;Feature extraction;Classification algorithms;Convolutional neural networks;Artificial intelligence;Medical diagnostic imaging;Systematic literature review;Diseases;Deep learning;Fatty liver;NAFLD;Ultrasound images;Convolutional neural network},
  doi={10.1109/ICAISS61471.2025.11041809},
  ISSN={},
  month={May},}@INPROCEEDINGS{10847521,
  author={Dwivedi, Aditya and Shreya and Sahu, Siddhinath and Srivastava, Anubhava},
  booktitle={2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={AI-Powered Real-Time Text Editor with Multilingual Translation and Speech Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1335-1340},
  abstract={This paper introduces a novel real-time collaborative text editor for revolutionizing information writing. We propose the development of an all-inclusive real-time collaborative text editor that does not require separate tools for information writing and editing. Our system integrates AI-driven text synthesis, real-time collaboration, language translation, and speech recognition into one unified platform. Through this approach, users can write, translate, and generate content without having to depend on other applications but yet will enjoy all functionalities offered by a regular text editor. We aim to develop a tool with these capabilities that enhances user creativity and productivity. This all-inclusive system is designed to meet a variety of user needs, including hands-free typing via speech recognition, intelligent content creation, and multilingual communication. Our concept represents a major advancement in text editing, as it provides a unified platform that allows users to realize their full creative potential. Our goal is to revolutionize the writing industry and create powerful and versatile tools that can be used for both personal and professional purposes.},
  keywords={Productivity;Translation;Text recognition;Collaboration;Speech recognition;Writing;Real-time systems;Multilingual;Artificial intelligence;Creativity;Real-time collaboration;AI-driven text generation;Speech recognition;Language translation;Text editor},
  doi={10.1109/CICN63059.2024.10847521},
  ISSN={2472-7555},
  month={Dec},}@ARTICLE{8974211,
  author={Jiang, Yi and Xu, Jiajie and Yang, Baoqing and Xu, Jing and Zhu, Junwu},
  journal={IEEE Access}, 
  title={Image Inpainting Based on Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={22884-22892},
  abstract={Image inpainting aims to fill missing regions of a damaged image with plausibly synthesized content. Existing methods for image inpainting either fill the missing regions by borrowing information from surrounding areas or generating semantically coherent content from region context. They often produce ambiguous or semantically incoherent content when the missing region is large or with complex structures. In this paper, we present an approach for image inpainting. The completion model based on our proposed algorithm contains one generator, one global discriminator, and one local discriminator. The generator is responsible for inpainting the missing area, the global discriminator aims evaluating whether the repair result has global consistency, and the local discriminator is responsible for identifying whether the repair area is correct. The architecture of the generator is an auto-encoder. We use the skip-connection in the generator to improve the prediction power of the model. Also, we use Wasserstein GAN loss to ensure the stability of training. Experiments on CelebA dataset and LFW dataset demonstrate that our proposed model can deal with large-scale missing pixels and generate realistic completion results.},
  keywords={Generators;Generative adversarial networks;Decoding;Training;Gallium nitride;Image edge detection;Neural networks;AutoEncoder;image inpainting;skip-connection;stable training;wasserstein GAN},
  doi={10.1109/ACCESS.2020.2970169},
  ISSN={2169-3536},
  month={},}@ARTICLE{9371686,
  author={Panetta, Karen and Kezebou, Landry and Oludare, Victor and Agaian, Sos and Xia, Zehua},
  journal={IEEE Access}, 
  title={TMO-Net: A Parameter-Free Tone Mapping Operator Using Generative Adversarial Network, and Performance Benchmarking on Large Scale HDR Dataset}, 
  year={2021},
  volume={9},
  number={},
  pages={39500-39517},
  abstract={Currently published tone mapping operators (TMO) are often evaluated on a very limited test set of high dynamic range (HDR) images. Thus, the resulting performance index is highly subject to extensive hyperparameter tuning, and many TMOs exhibit sub-optimal performance when tested on a broader spectrum of HDR images. This indicates that there are deficiencies in the generalizable applicability of these techniques. Finally, it is a challenge developing parameter-free tone mapping operators using data-hungry advanced deep learning methods due to the paucity of large scale HDR datasets. In this paper, these issues are addressed through the following contributions: a) a large scale HDR image benchmark dataset (LVZ-HDR dataset) with multiple variations in sceneries and lighting conditions is created to enable performance evaluation of TMOs across a diverse conditions and scenes that will also contribute to facilitate the development of more robust TMOs using state-of-the-art deep learning methods; b) a deep learning-based tone mapping operator (TMO-Net) is presented, which offers an efficient and parameter-free method capable of generalizing effectively across a wider spectrum of HDR content; c) finally, a comparative analysis, and performance benchmarking of 19 state-of-the-art TMOs on the new LVZ-HDR dataset are presented. Standard metrics including the Tone Mapping Quality Index (TMQI), Feature Similarity Index for Tone Mapped images (FSITM), and Natural Image Quality Evaluator (NIQE) are used to qualitatively evaluate the performance index of the benchmarked TMOs. Experimental results demonstrate that the proposed TMO-Net qualitatively and quantitatively outperforms current state-of-the-art TMOs.},
  keywords={Dynamic range;Benchmark testing;Deep learning;Visualization;Generative adversarial networks;Image segmentation;Deep learning;GAN-based tone mapping;TMO benchmarking;tone mapping;TMO-Net HDR dataset;parameter-free tone mapping},
  doi={10.1109/ACCESS.2021.3064295},
  ISSN={2169-3536},
  month={},}@ARTICLE{9443209,
  author={Seo, Hyunseok and Yu, Lequan and Ren, Hongyi and Li, Xiaomeng and Shen, Liyue and Xing, Lei},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Deep Neural Network With Consistency Regularization of Multi-Output Channels for Improved Tumor Detection and Delineation}, 
  year={2021},
  volume={40},
  number={12},
  pages={3369-3378},
  abstract={Deep learning is becoming an indispensable tool for imaging applications, such as image segmentation, classification, and detection. In this work, we reformulate a standard deep learning problem into a new neural network architecture with multi-output channels, which reflects different facets of the objective, and apply the deep neural network to improve the performance of image segmentation. By adding one or more interrelated auxiliary-output channels, we impose an effective consistency regularization for the main task of pixelated classification (i.e., image segmentation). Specifically, multi-output-channel consistency regularization is realized by residual learning via additive paths that connect main-output channel and auxiliary-output channels in the network. The method is evaluated on the detection and delineation of lung and liver tumors with public data. The results clearly show that multi-output-channel consistency implemented by residual learning improves the standard deep neural network. The proposed framework is quite broad and should find widespread applications in various deep learning problems.},
  keywords={Image segmentation;Task analysis;Tumors;Training;Biomedical imaging;Neural networks;Feature extraction;Artificial intelligence;cancer detection;neural networks;regularization;residual learning;segmentation},
  doi={10.1109/TMI.2021.3084748},
  ISSN={1558-254X},
  month={Dec},}@ARTICLE{9741786,
  author={Xue, Wang and Huan-Xin, Cheng and Sheng-Yi, Sun and Ze-Qin, Jiang and Kai, Cheng and Li, Cheng},
  journal={IEEE Access}, 
  title={MSFSA-GAN: Multi-Scale Fusion Self Attention Generative Adversarial Network for Single Image Deraining}, 
  year={2022},
  volume={10},
  number={},
  pages={34442-34448},
  abstract={Bad weather such as rainy days will seriously affect the image quality and the accuracy of visual processing algorithm. In order to improve the image deraining quality, a multi-scale fusion self attention generation adversarial network (MSFSA-GAN) is proposed. This network uses different scales to extract input characteristics of rain lines. First, Gaussian pyramid rain maps with different scales are generated by Gaussian algorithm. Then, in order to extract the features of rain lines with different scales, the coarse fusion module and fine fusion module are designed respectively. Next, the extracted features are fused at different scales. In this process, the self attention mechanism is introduced to make the network focus on the extracted features of different scales. And before the fusion, the rain pattern reconstruction operation is also carried out, so that the network can reproduce the input image more perfectly. Finally, it is input into the discriminator network with dense blocks to obtain the image that removes the rain lines. We used R100H and R100L datasets to train and test our network. The results show that our method as high as 27.79 in PSNR and UQI is 0.94, which is superior to the existing methods in performance. Meanwhile, we also compared the cost of time, the result of our network is only 0.02s.},
  keywords={Rain;Feature extraction;Generators;Generative adversarial networks;Deep learning;Data mining;Task analysis;Rain removal;MSFSA-GAN;self attention;dense block},
  doi={10.1109/ACCESS.2022.3162224},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10616548,
  author={Naimi, Lahbib and Bouziane, El Mahi and Manaouch, Mohamed and Jakimi, Abdeslam},
  booktitle={2024 International Conference on Circuit, Systems and Communication (ICCSC)}, 
  title={A new approach for automatic test case generation from use case diagram using LLMs and prompt engineering}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The automation of test case generation from UML diagrams is a growing field that aims to make the software development process smoother. This paper suggests a new framework that uses generative artificial intelligence (AI) to turn use case diagrams into test cases that can be executed. By getting information from the XML representation of use case diagrams, we can create detailed instructions that guide a generative AI model to make test cases for each use case scenario. This method not only makes test case creation easier but also ensures we cover everything well and accurately, which could make software products get to market faster. this approach shows how traditional software engineering methods and new AI techniques can work well together, giving us an idea of what automated software testing might look like in the future.},
  keywords={Software testing;Automation;Unified modeling language;XML;Software;Prompt engineering;Software engineering;Software development management;software testing;test cases;LLMs prompt engineering},
  doi={10.1109/ICCSC62074.2024.10616548},
  ISSN={},
  month={June},}@ARTICLE{10592663,
  author={Yan, Liang and Wu, Xiaodong and Wei, Chongfeng and Zhao, Sheng},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Human-Vehicle Shared Steering Control for Obstacle Avoidance: A Reference-Free Approach With Reinforcement Learning}, 
  year={2024},
  volume={25},
  number={11},
  pages={17888-17901},
  abstract={Although artificial intelligence has made tremendous progress recently, there remain various technical issues and ethical problems before autonomous vehicles can be available to the general public. In the interim, collaboration between automated systems and human drivers becomes a promising solution, where the merits of machine intelligence and human intelligence are blended in a complementary way. To this end, the paper proposes a reference-free human-vehicle shared control framework based on reinforcement learning. Firstly, a personalized human-like driver agent is derived from highway driving data by means of generative adversarial imitation learning integrated with Gaussian mixture model. The driver model is responsible for real-time interaction with the reinforcement learning agent to relieve the burden of human operators in the course of training. Then, a copilot agent learns the policies to cooperatively control the vehicle steering based on three distinct implementations for the search of the best coordination strategy. Heuristic reward functions are designed to guide the agent policy optimization for multi-objective equilibrium between driver synchronization against intervention. To verify the control performance of the proposed shared driving system, simulation experiments with driver models and human-in-the-loop tests with real-life participants are conducted in the end of this paper. The results demonstrate that the shared steering control method can effectively follow human intentions, facilitate driving goals, improve road safety and reduce driver’s workload simultaneously in the challenging dynamic obstacle avoidance scenarios.},
  keywords={Vehicles;Human-machine systems;Collision avoidance;Vehicle dynamics;Trajectory;Resource management;Reinforcement learning;Shared steering control;reinforcement learning;driving behavior modeling;collision avoidance},
  doi={10.1109/TITS.2024.3420894},
  ISSN={1558-0016},
  month={Nov},}@INPROCEEDINGS{10901571,
  author={Liang, Yuxin and Yang, Peng and He, Yuanyuan and Lyu, Feng},
  booktitle={GLOBECOM 2024 - 2024 IEEE Global Communications Conference}, 
  title={Resource-Efficient Generative AI Model Deployment in Mobile Edge Networks}, 
  year={2024},
  volume={},
  number={},
  pages={2647-2652},
  abstract={The surging development of Artificial Intelligence-Generated Content (AIGC) marks a transformative era of the content creation and production. Edge servers promise attractive benefits, e.g., reduced service delay and backhaul traffic load, for hosting AIGC services compared to cloud-based solutions. However, the scarcity of available resources on the edge pose significant challenges in deploying generative AI models. In this paper, by characterizing the resource and delay demands of typical generative AI models, we find that the consumption of storage and GPU memory, as well as the model switching delay represented by I/O delay during the preloading phase, are significant and vary across models. These multidimensional coupling factors render it difficult to make efficient edge model deployment decisions. Hence, we present a collaborative edge-cloud framework aiming to properly manage generative AI model deployment on the edge. Specifically, we formulate edge model deployment problem considering heterogeneous features of models as an optimization problem, and propose a model-level decision selection algorithm to solve it. It enables pooled resource sharing and optimizes the trade-off between resource consumption and delay in edge generative AI model deployment. Simulation results validate the efficacy of the proposed algorithm compared with baselines, demonstrating its potential to reduce overall costs by providing feature-aware model deployment decisions.},
  keywords={Adaptation models;Generative AI;Simulation;Collaboration;Telecommunication traffic;Switches;Delays;Servers;Resource management;Load modeling},
  doi={10.1109/GLOBECOM52923.2024.10901571},
  ISSN={2576-6813},
  month={Dec},}@ARTICLE{10438439,
  author={Yamashita, Haruki and Okamoto, Takuma and Takashima, Ryoichi and Ohtani, Yamato and Takiguchi, Tetsuya and Toda, Tomoki and Kawai, Hisashi},
  journal={IEEE Access}, 
  title={Fast Neural Speech Waveform Generative Models With Fully-Connected Layer-Based Upsampling}, 
  year={2024},
  volume={12},
  number={},
  pages={31409-31421},
  abstract={Although end-to-end (E2E) text-to-speech (TTS) models with HiFi-GAN-based neural vocoder (e.g. VITS and JETS) can achieve human-like speech quality with fast inference speed, these models still have room to further improve the inference speed with a CPU for practical implementations because HiFi-GAN-based neural vocoder unit is a bottleneck. Additionally, HiFi-GAN is widely used not only for TTS but also for many speech and audio applications. To accelerate HiFi-GAN while maintaining the synthesis quality, Multi-stream (MS)-HiFi-GAN, iSTFTNet and MS-iSTFT-HiFi-GAN have been proposed. Although inverse short-term Fourier transform (iSTFT)-based fast upsampling is introduced in iSTFTNet and MS-iSTFT-HiFi-GAN, we first find that the predicted intermediate features input to the iSTFT layer are completely different from the original STFT spectra due to the redundancy of the overlap-add operation in iSTFT. To further improve the synthesis quality and inference speed, we propose FC-HiFi-GAN and MS-FC-HiFi-GAN by introducing trainable fully-connected (FC) layer-based fast upsampling without overlap-add operation instead of the iSTFT layer. The experimental results for unseen speaker synthesis and E2E TTS conditions show that the proposed methods can slightly accelerate the inference speed and significantly improve the synthesis quality in JETS-based E2E TTS than iSTFTNet and MS-iSTFT-HiFi-GAN. Therefore, the iSTFT layer can be replaced by the proposed trainable FC layer-based upsampling without overlap-add operation in HiFi-GAN-based neural vocoders.},
  keywords={Vocoders;Computational modeling;Training;Acoustics;Redundancy;Text detection;Speech recognition;Generative adversarial networks;Text-to-speech;Audio-visual systems;Assistive technologies;Digital TV;End-to-end text-to-speech;fully-connected layer-based upsampling;iSTFTNet;multi-stream HiFi-GAN;neural vocoder},
  doi={10.1109/ACCESS.2024.3366707},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9261651,
  author={Ren, Jieji and Ren, Mingjun and Sun, Lijian},
  booktitle={2020 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Generative Model-Driven Sampling Strategy for High Efficient Measurement of Complex Surfaces on Coordinate Measuring Machines}, 
  year={2020},
  volume={},
  number={},
  pages={442-447},
  abstract={Coordinate measuring machine is widely used in precision measurement of industrial parts. However, the nature of the point-by-point probing characteristic limits its efficiency in the measurement of complex parts which normally requires dense sampling pints for fully evaluating the machining errors with high fidelity. To address this problem, this paper proposes a generative model-driven sampling strategy to reduce the number of the sampling points while maintaining the measurement accuracy. Specifically, the surface error reconstruction under sparse sampling is transformed as an image super-resolution task, which adopts a generative model to estimate accurate dense results from under-sampled data. A multi-scale neural network architecture is designed to achieve reconstruction, and the Fractional Brownian Motion is applied to synthesis large-scale simulated error datasets for model training. The generalized neural model could use sparse measurements to reconstruct global machining error, which dramatically reduces the sampling time and increases measurement efficiency. Both computer simulation and actual measurement are carried out to verify the effectiveness of the proposed method.},
  keywords={Surface reconstruction;Machining;Surface treatment;Image reconstruction;Measurement uncertainty;Surface fitting;Training;Machining Error;Measurement;Deep Learning},
  doi={10.1109/ICSMD50554.2020.9261651},
  ISSN={},
  month={Oct},}@ARTICLE{10930496,
  author={Mohammad Imdadul Alam, Gazi and Tasnia, Naima and Biswas, Tapu and Hossen, Md. Jakir and Arfin Tanim, Sharia and Saef Ullah Miah, Md},
  journal={IEEE Access}, 
  title={Real-Time Detection of Forest Fires Using FireNet-CNN and Explainable AI Techniques}, 
  year={2025},
  volume={13},
  number={},
  pages={51150-51181},
  abstract={This study presents FireNet-CNN, an advanced deep-learning model particularly designed for forest fire detection, which significantly surpasses existing methods in terms of reliability, efficiency, and interpretability. FireNet-CNN is compared to popular pre-trained models, including VGG16, VGG19, and Inception V3, across key performance metrics and consistently shows superior results, achieving 99.05% accuracy, 99.41% precision, and 98.28% recall. The model was evaluated using two augmented datasets: Dataset A and Dataset B, which consist of fire and non-fire images sourced from multiple video and image datasets. FireNet-CNN’s architecture, which includes 2.75 million parameters and a compact model size of 10.58 MB, has been meticulously optimized for fire detection tasks. As a consequence, the inference time of 0.95 seconds/image enables fast real-time deployment especially suitable for resource-constrained platforms like drones, remote sensors or other types of embedded systems in wooded regions. FireNet-CNN uses synthetic data augmentation based on Stable Diffusion to overcome the limitations of dataset size and class imbalance. This augmentation is critical as it helps the model accurately identify fire instances with a lower false positive rate, which is key for any real-time fire detection system where reliability and dependability are vital. To improve transparency and trust in safety-critical applications, FireNet-CNN incorporates the explainable AI (XAI) techniques, such as Grad-CAM and Saliency Maps. Despite encountering challenges such as reliance on synthetic data and issues of class imbalance, FireNet-CNN has demonstrated promising potential as a viable and effective solution for early wildfire detection. It offers significant insights for future research and practical applications in fire management and disaster response.},
  keywords={Forestry;Wildfires;Deep learning;Computational modeling;Explainable AI;Ecosystems;Data models;Carbon emissions;Carbon dioxide;Climate change;Forest fire detection;deep learning;generative AI;explainable AI;wildfire monitoring},
  doi={10.1109/ACCESS.2025.3552352},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11152869,
  author={Taimori, Ali and Sellathurai, Mathini and Ratnarajah, Tharmalingam},
  booktitle={IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Differential semantic communications for salient object transmission in 6G multimedia systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={emerging paradigms lack a generalised feedbacked model that addresses factors such as bandwidth usage, information scheduling, and performance evaluation. To tackle them, we theorise Differential Semantic Communications, abbreviated DSCs, inspired by human-like natural communications. A DSC model employs a closed-loop feedback architecture, where two intelligent transceiver agents interact cyclically to achieve a communication goal. We introduce a top-down information representation to enable iterative transmissions. Visual salient object transmission is demonstrated as a proof-of-concept application for DSCs. A novel performance metric, termed semantic misfit-percent, is also derived to evaluate the efficiency of data-and-information reconstruction. Simulation results show the ability of salient object transmission in images with about 9 % misfit improvement on average compared to the state-of-the-art DALL.E1-based approach. Implementation codes are available upon request.},
  keywords={Visualization;Simulation;Multimedia systems;Information representation;Image representation;Semantic communication;Performance metrics;Transceivers;Iterative methods;Image reconstruction;Closed-loop feedback;generative AI;image representation;semantic communications;semantic computing},
  doi={10.1109/INFOCOMWKSHPS65812.2025.11152869},
  ISSN={2833-0587},
  month={May},}@INPROCEEDINGS{10440898,
  author={Raghavan, Kaushik and B, Sivaselvan and V, Kamakoti},
  booktitle={2023 IEEE 20th India Council International Conference (INDICON)}, 
  title={Counter-CAM : An Improved Grad-CAM based Visual Explainer for Infrared Breast cancer Classification}, 
  year={2023},
  volume={},
  number={},
  pages={661-666},
  abstract={Understanding the decision-making process of machine learning models is essential for establishing confidence and interpretability. Counter-CAM combines the power of Grad-CAM and counterfactual explanations to provide intuitive and comprehensive insights into model decisions. Counter-CAM enables end users to comprehend why a model makes particular predictions by displaying the critical regions and their modifications in counterfactual images. Initially, we use Grad-CAM to generate heatmaps that emphasize regions of an input image that are crucial for the model’s prediction. These heatmaps provide localized explanations but cannot demonstrate how modifications to these regions would affect the model’s conclusion. To overcome this limitation, we incorporate counterfactual explanations, demonstrating how minor image changes can result in a different model prediction. By superimposing Grad-CAM on counterfactual images, we generate Counter-CAM. This visual representation juxtaposes the significance of various regions in the original image with their influence on the model’s decision in the counterfactual image. We demonstrate Counter-CAM’s ability to provide intuitive and visual explanations for model predictions by validating its efficacy on multiple datasets and demonstrating its capacity to provide intuitive and visual explanations for model predictions. Counter-CAM improves the interpretability and explainability of machine learning models, enabling end users to comprehend and trust the decision-making procedure.},
  keywords={Visualization;Cogeneration;Decision making;Machine learning;Predictive models;Breast cancer;Explainable AI;Deep Learning;Biomedical Image Analysis;Generative Adversarial Networks},
  doi={10.1109/INDICON59947.2023.10440898},
  ISSN={2325-9418},
  month={Dec},}@INPROCEEDINGS{10780101,
  author={Sugacini, M. and K.Prathibanandhi and C.Yaashuwanth},
  booktitle={2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, 
  title={Enhanced Liver Tumor Localization Based on Couinaud Classification Using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Liver tumor localization is an essential job in clinical imaging evaluation, which includes determining the place of a tumor in the liver from clinical images such as CT checks. This job is essential for the precise medical diagnosis, therapy preparation, and tracking of liver cancer cells. Deep discovering methods, particularly convolutional neural networks (CNNs), have revealed guaranteeing lead to clinical image evaluation, consisting of liver tumor localization. Image refining offers a precise and automated technique for the very early discovery and medical diagnosis of growths. Our technique consists of the effective removal of functions from Creation integrated with recurring and pre-trained weights. This system investigates the idea of revealing how a pre-trained deep neural network makes decisions. It entails dissecting the internal layers and condensing the characteristics that influence forecasts. The suggested technique enhanced segmentation method to identify liver growths. It improved the accuracy of the afflicted by the tumor to 99%.Keywords— Cybersecurity, Chaos based permutation, Discrete Cosine Transform, Local Feature, Spatial Pyramid matching, Bag of words, OTP.},
  keywords={Location awareness;Deep learning;Liver cancer;Accuracy;Sensitivity;Refining;Liver;Medical treatment;Medical diagnosis;Tumors;Couinaud;Deep Learning;Generative Adversarial Network},
  doi={10.1109/ICPECTS62210.2024.10780101},
  ISSN={},
  month={Oct},}@ARTICLE{8913483,
  author={Ong, Desmond C. and Wu, Zhengxuan and Tan, Zhi-Xuan and Reddan, Marianne and Kahhale, Isabella and Mattek, Alison and Zaki, Jamil},
  journal={IEEE Transactions on Affective Computing}, 
  title={Modeling Emotion in Complex Stories: The Stanford Emotional Narratives Dataset}, 
  year={2021},
  volume={12},
  number={3},
  pages={579-594},
  abstract={Human emotions unfold over time, and more affective computing research has to prioritize capturing this crucial component of real-world affect. Modeling dynamic emotional stimuli requires solving the twin challenges of time-series modeling and of collecting high-quality time-series datasets. We begin by assessing the state-of-the-art in time-series emotion recognition, and we review contemporary time-series approaches in affective computing, including discriminative and generative models. We then introduce the first version of the Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models. We demonstrate several baseline and state-of-the-art modeling approaches on the SEND, including a Long Short-Term Memory model and a multimodal Variational Recurrent Neural Network, which perform comparably to the human-benchmark. We end by discussing the implications for future research in time-series affective computing.},
  keywords={Computational modeling;Hidden Markov models;Affective computing;Biological system modeling;Videos;Data models;Recurrent neural networks;Affective computing;affect sensing and analysis;multi-modal recognition;emotional corpora},
  doi={10.1109/TAFFC.2019.2955949},
  ISSN={1949-3045},
  month={July},}@ARTICLE{10887211,
  author={Wahidur, Rahman S. M. and Kim, Sumin and Choi, Haeung and Bhatti, David S. and Lee, Heung-No},
  journal={IEEE Access}, 
  title={Legal Query RAG}, 
  year={2025},
  volume={13},
  number={},
  pages={36978-36994},
  abstract={Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence (AI) for various core tasks. However, these technologies remain in their early stages and face challenges such as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding misleading responses, commonly referred to as hallucinations. To address these limitations, this paper introduces Legal Query RAG (LQ-RAG), a novel Retrieval-Augmented Generation framework with a recursive feedback mechanism specifically designed to overcome the critical shortcomings of standard RAG implementations in legal applications. The proposed framework incorporates four key components: a custom evaluation agent, a specialized response generation model, a prompt engineering agent, and a fine-tuned legal embedding LLM. Together, these components effectively minimize hallucinations, improve domain-specific accuracy, and deliver precise, high-quality responses for complex queries. Experimental results demonstrate that the fine-tuned embedding LLM achieves a 13% improvement in Hit Rate and a 15% improvement in Mean Reciprocal Rank (MRR). Comparisons with general LLMs reveal a 24% performance gain when using the Hybrid Fine-Tuned Generative LLM (HFM), the specialized response generation model integrated into the LQ-RAG framework. Furthermore, LQ-RAG achieves a 23% improvement in relevance score over naive configurations and a 14% improvement over RAG with Fine-Tuned LLMs (FTM). These findings underscore the potential of domain-specific fine-tuned LLMs, combined with advanced RAG modules and feedback mechanisms, to significantly enhance the reliability and performance of AI in legal practice. The reliance of this study on a proprietary model as the evaluation agent, combined with the lack of feedback from human experts, highlights the need for improvement. Future efforts should focus on developing a specialized legal evaluation agent and enhancing its performance by incorporating feedback from domain experts.},
  keywords={Law;Retrieval augmented generation;Accuracy;Tuning;Semantics;Hybrid power systems;Adaptation models;Training;Reliability;Mathematical models;Retrieval-augmented generation;legal query;LLM agent;information retrieval},
  doi={10.1109/ACCESS.2025.3542125},
  ISSN={2169-3536},
  month={},}@ARTICLE{9478893,
  author={Fuad, Md. Tahmid Hasan and Fime, Awal Ahmed and Sikder, Delowar and Iftee, Md. Akil Raihan and Rabbi, Jakaria and Al-Rakhami, Mabrook S. and Gumaei, Abdu and Sen, Ovishake and Fuad, Mohtasim and Islam, Md. Nazrul},
  journal={IEEE Access}, 
  title={Recent Advances in Deep Learning Techniques for Face Recognition}, 
  year={2021},
  volume={9},
  number={},
  pages={99112-99142},
  abstract={In recent years, researchers have proposed many deep learning (DL) methods for various tasks, and particularly face recognition (FR) made an enormous leap using these techniques. Deep FR systems benefit from the hierarchical architecture of the DL methods to learn discriminative face representation. Therefore, DL techniques significantly improve state-of-the-art performance on FR systems and encourage diverse and efficient real-world applications. In this paper, we present a comprehensive analysis of various FR systems that leverage the different types of DL techniques, and for the study, we summarize 171 recent contributions from this area. We discuss the papers related to different algorithms, architectures, loss functions, activation functions, datasets, challenges, improvement ideas, current and future trends of DL-based FR systems. We provide a detailed discussion of various DL methods to understand the current state-of-the-art, and then we discuss various activation and loss functions for the methods. Additionally, we summarize different datasets used widely for FR tasks and discuss challenges related to illumination, expression, pose variations, and occlusion. Finally, we discuss improvement ideas, current and future trends of FR tasks.},
  keywords={Face recognition;Feature extraction;Three-dimensional displays;Task analysis;Deep learning;Data mining;Videos;Deep learning;face recognition;artificial neural network;convolutional neural network;auto encoder;generative adversarial network;deep belief network;reinforcement learning},
  doi={10.1109/ACCESS.2021.3096136},
  ISSN={2169-3536},
  month={},}
