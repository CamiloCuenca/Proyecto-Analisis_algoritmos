@INPROCEEDINGS{10134683,
  author={Varshney, Neeraj and Kumar, Gaurav and Kumar, Ankit and Pandey, Saroj Kumar and Singh, Teekam and Singh, Kamred Udham},
  booktitle={2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={AI-Enable Generating Human Faces using Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={256-262},
  abstract={Lately, sensible Image processing using deep neural networks has become a fervently discussed issue in machine learning and computer vision. Image can be made at the pixel level by learning from a gigantic variety of pictures. Learning to make splendid movement pictures from highdifference draws is not only a captivating investigation issue yet also a reasonable application in innovative delight. In this research, we research the sketch-to-picture mix issue by using prohibitive generative poorly arranged networks. The model can normally deliver reasonable shadings for a sketch. The new model is not only prepared for painting hand-drawn sketches with real tones, yet also allows customers to exhibit supported tones. Test results on two sketch datasets show that the autopainter performs better contrasted with existing picture-topicture methodologies. With creating interest in the development of film, the interest in building a computerized structure to change over the authentic video into action is higher than at some other time. The edge-by-diagram modification of the action age measure is costly and dreary. To help with moving quickly and with no issue in a robotized collaboration we proposed a generative model that changes over genuine pictures into contrasting energy pictures without losing critical nuances of the source picture. We used an assortment of the generative hostile association as a fundamental plan with the custom incident ability to ensure the substance of the source picture, which changed over to an exuberance image.},
  keywords={Deep learning;Computer vision;Communication systems;Computational modeling;Image processing;Neural networks;Buildings;component;formatting;style;styling;insert (key words)},
  doi={10.1109/CSNT57126.2023.10134683},
  ISSN={2473-5655},
  month={April},}@ARTICLE{9181507,
  author={Yun, Jung Un and Jo, Byungho and Park, In Kyu},
  journal={IEEE Access}, 
  title={Joint Face Super-Resolution and Deblurring Using Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={159661-159671},
  abstract={Facial image super-resolution (SR) is an important aspect of facial analysis, and it can contribute significantly to tasks such as face alignment, face recognition, and image-based 3D reconstruction. Recent convolutional neural network (CNN) based models have exhibited significant advancements by learning mapping relations using pairs of low-resolution (LR) and high-resolution (HR) facial images. However, because these methods are conventionally aimed at increasing the PSNR and SSIM metrics, the reconstructed HR images might be blurry and have an overall unsatisfactory perceptual quality even when state-of-the-art quantitative results are achieved. In this study, we address this limitation by proposing an adversarial framework intended to reconstruct perceptually high-quality HR facial images while simultaneously removing blur. To this end, a simple five-layer CNN is employed to extract feature maps from LR facial images, and this feature information is provided to two-branch encoder-decoder networks that generate HR facial images with and without blur. In addition, local and global discriminators are combined to focus on the reconstruction of HR facial structures. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method for generating photorealistic HR facial images from a variety of LR inputs. Moreover, it was also verified, through a use case scenario that the proposed method can contribute more to the field of face recognition than existing approaches.},
  keywords={Face;Image reconstruction;Image resolution;Generative adversarial networks;Generators;Decoding;Feature extraction;Facial image super-resolution;deblurring;generative adversarial network;face recognition},
  doi={10.1109/ACCESS.2020.3020729},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10464680,
  author={Kittur, Priyanka and Pasha, Aaryan and Joshi, Shaunak and Kulkarni, Vaishali},
  booktitle={2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)}, 
  title={A Review on Anti-Spoofing : Face Manipulation and Liveness Detection}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The latest developments in deep generative networks have greatly improved the quality of generating life-like fake face videos. Face digital manipulation such as DeepFakes, FaceSwap, and Face2Face are critical issues for automated face recognition systems as it detrimentally affects their performance. To tackle this difficulty, face anti-spoofing is utilized which is a task to prevent unauthorized entry by breaching facial recognition systems using a mask, photo, video or another substitute for a legitimate person's face. The review paper examines the problems and solutions explored in the domain of digital face manipulation, as well as the notion of liveliness detection and numerous aspects and cues that can be used to determine the authenticity of a face. The review is organized into sections, discussing face manipulation techniques, liveness detection using eye blinking, and finally a hybrid system is proposed that considers both face manipulation and liveness detection to strengthen existing solutions. It aims to enhance the security and reliability of facial recognition technology in various applications.},
  keywords={Deepfakes;Computer vision;Reviews;Face recognition;Security;Reliability;Task analysis;Anti-Spoofing;Face Manipulation;Liveness;Detection;Deep Learning},
  doi={10.1109/CVMI59935.2023.10464680},
  ISSN={},
  month={Dec},}@ARTICLE{10365567,
  author={Qi, Xingqun and Sun, Muyi and Wang, Zijian and Liu, Jiaming and Li, Qi and Zhao, Fang and Zhang, Shanghang and Shan, Caifeng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network With Graph Representation Learning}, 
  year={2025},
  volume={36},
  number={2},
  pages={2182-2195},
  abstract={Biphasic face photo-sketch synthesis has significant practical value in wide-ranging fields such as digital entertainment and law enforcement. Previous approaches directly generate the photo-sketch in a global view, they always suffer from the low quality of sketches and complex photograph variations, leading to unnatural and low-fidelity results. In this article, we propose a novel semantic-driven generative adversarial network to address the above issues, cooperating with graph representation learning. Considering that human faces have distinct spatial structures, we first inject class-wise semantic layouts into the generator to provide style-based spatial information for synthesized face photographs and sketches. In addition, to enhance the authenticity of details in generated faces, we construct two types of representational graphs via semantic parsing maps upon input faces, dubbed the intraclass semantic graph (IASG) and the interclass structure graph (IRSG). Specifically, the IASG effectively models the intraclass semantic correlations of each facial semantic component, thus producing realistic facial details. To preserve the generated faces being more structure-coordinated, the IRSG models interclass structural relations among every facial component by graph representation learning. To further enhance the perceptual quality of synthesized images, we present a biphasic interactive cycle training strategy by fully taking advantage of the multilevel feature consistency between the photograph and sketch. Extensive experiments demonstrate that our method outperforms the state-of-the-art competitors on the CUHK Face Sketch (CUFS) and CUHK Face Sketch FERET (CUFSF) datasets.},
  keywords={Faces;Semantics;Task analysis;Layout;Representation learning;Training;Databases;Face photo-sketch synthesis;generative adversarial network;graph representation learning;intraclass and interclass;iterative cycle training (ICT)},
  doi={10.1109/TNNLS.2023.3341246},
  ISSN={2162-2388},
  month={Feb},}@INPROCEEDINGS{10270042,
  author={Yilmaz, Deniz},
  booktitle={2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)}, 
  title={Development and Evaluation of an Explainable Diagnostic AI for Alzheimer's Disease}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Alzheimer's Disease (AD) is a progressive neurode-generative disease that is estimated to affect 24.3 million people worldwide, and with an expected rise in cases, an accurate and efficient diagnosis method is necessary. Machine learning has been implemented for diagnosing AD in previous studies utilizing various modalities of data. More specifically, deep learning has been shown to perform at very high accuracy rates. However, many healthcare applications of deep learning paradigms lack trust due to their lack of interpretability. To provide evidence for the potential of deep learning to meet established standards of accurate diagnosis, we have designed an explainable diagnostic machine learning model for predicting AD severity levels. Using two open-source Magnetic Resonance Imaging (MRI) datasets, we develop and evaluate a Convolutional Neural Network (CNN). An accuracy rate of 99.9% was achieved using the CNN model, which outperforms other models trained on the same dataset. To improve model transparency, we leverage the explainable AI technique SHapley Additive exPlanations (SHAP) to interpret the predictions of the CNN. The implementation of explainable AI demonstrates that the predictions of the model are influenced by well-known pathological indicators of AD.},
  keywords={Deep learning;Support vector machines;Pathology;Magnetic resonance imaging;Transfer learning;Predictive models;Convolutional neural networks;Machine learning;Alzheimer's disease;healthcare;neuroimaging;deep learning},
  doi={10.1109/CAISAIS59399.2023.10270042},
  ISSN={},
  month={Sep.},}@INBOOK{10953501,
  author={Sangeetha, S.K.B. and Dhaya, R.},
  booktitle={Artificial Intelligent Techniques for Wireless Communication and Networking}, 
  title={Deep Learning Era for Future 6G Wireless Communications&#x2014;Theory, Applications, and Challenges}, 
  year={2022},
  volume={},
  number={},
  pages={105-119},
  abstract={Summary <p>Over hundreds of years have passed since wireless communication technology was first introduced. The developers have made remarkable strides since 1880, including setting up an LTE network. Industry experts, from Verizon to Qualcomm, together with various experts, have a range of ideas about how to accelerate the evolution of wireless communication into &#x2014; and even beyond &#x2014; the 21st century. The wireless network technology requires smooth connectivity, easy accessibility, and safe connections at worldwide supply. 5G wireless technology is designed to provide higher data peak speeds, very reduced latency, increased reliability, massive network bandwidth, improved availability, plus an enhanced UI6G. Mobile users with a higher speed, superb coverage and no latency can broaden their existing mobile ecosystems into new worlds. 6 G will impact all sectors to increase the safety of transport, distributed health, agricultural accuracy, digitised logistics and more. Therefore, efforts on 6 G networks from both industry and academia have already been put into the study. Recently deep learning has been used as a new paradigm for designing and optimizing high&#x2010;intelligence 6 G networks. With the realistic account of all challenges and opportunities of wireless technology from engineering perspectives, requires a complete study of available methods. In this chapter, huge potential efforts are made to study the background of 6G wireless communication with the detail of how deep learning made a contribution to 6G wireless technology. In the end, this chapter also highlights future research directions for deep learning&#x2010;driven wireless technology.</p>},
  keywords={Lesions;Melanoma;Skin;Transfer learning;Training data;Training;Boosting;Standards;Data mining;Sun},
  doi={10.1002/9781119821809.ch8},
  ISSN={},
  publisher={Wiley},
  isbn={9781119821793},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953501},}@INPROCEEDINGS{10963259,
  author={Han, Jiajia and Yang, Tao and Zhang, Cai and Jiang, Han},
  booktitle={2024 5th International Conference on Computers and Artificial Intelligence Technology (CAIT)}, 
  title={Fine-Tuning Large Language Models for the Electric Power Industry with Specialized Chinese Electric Power Corpora}, 
  year={2024},
  volume={},
  number={},
  pages={658-663},
  abstract={The advent of large language models has exerted a profound impact across various sectors. Despite significant strides in general applications and certain domain-specific areas, existing general language models exhibit limitations in addressing the specific requirements of the electric power industry. Especially when processing Chinese content, their performance often falls short compared to handling English content. In response to this challenge, our research team collected and organized a large volume of Chinese literature related to the electric power domain and established a specialized corpus. Leveraging this corpus, we combined manually edited question-answer pairs with the generative capabilities of ChatGPT to construct an instruction dataset for the electric power domain. Based on this dataset, we successfully trained a large language model focusing on the electric power industry. In addition, we developed an evaluation dataset to screen out large language model architectures that perform well in knowledge storage and presentation capabilities. After comprehensive evaluation, our large language model in electric power domain shows superior performance in question answering tasks within the professional field.},
  keywords={Industries;Computers;Large language models;Computational modeling;Focusing;Computer architecture;Chatbots;Question answering (information retrieval);Power systems;Domain specific languages;large language models;electric power corpora;domain-specific language models},
  doi={10.1109/CAIT64506.2024.10963259},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10774807,
  author={Kadam, Shalmali R. and Dhore, Mahendra P.},
  booktitle={2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)}, 
  title={Review of Deep Learning Methods}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Deep Learning falls within the realm of artificial intelligence as a subset of Machine Learning. It plays a crucial role in our everyday lives. The field of Deep learning has expanded significantly in recent years and finds application in wide array of contexts. Deep learning (DL) has made its impact in many areas predictive forecasting, voice generation and recognition, audio, video, image generation and recognition, multimedia, computer vision and language processing etc. Deep Learning primarily employs artificial neural network (ANN) to identify patterns and acquire decision making capabilities from them. These neural networks are designed to emulate the functionality of human brain, which possesses a deep architecture. Deep Learning excels in its capacity to learn from extensive datasets. Deep Learning employs Deep Learning Neural Networks (DNN) comprising multiple layers of units with optimized algorithms and architectures. This paper focuses on many generally used deep learning optimization methods in order to enhance accuracy. Additionally, deep convolution networks (CNN), Artificial Neural Networks, Supervised or Discriminative Learning, Feed-forward Neural Networks, Recurrent Neural Networks (RNN), Backpropagation, convolutional neural networks, Autoencoders, LSTM, and generative advertisement networks (GAN) are encompassed in the review.},
  keywords={Deep learning;Training;Recurrent neural networks;Reviews;Speech recognition;Generative adversarial networks;Feedforward neural networks;Convolutional neural networks;Long short term memory;Biological neural networks;Neural Network NN;Artificial Neural Network ANN;Generative Adversarial Network GAN;Complex Neural Network/Deep Neural Network(DNN);Utilizations of Advanced Learning;Supervised and Unsupervised Learning;Automated Encoders AE;Deep Neural Network DNN;Activation function;feature map;dataset},
  doi={10.1109/ICCUBEA61740.2024.10774807},
  ISSN={2771-1358},
  month={Aug},}@INPROCEEDINGS{10462836,
  author={Hidayaturrahman and Jeremy, Nicholaus Hendrik and Philip, Samuel and Lie, Filippo Jonathan and Prawira, Indra},
  booktitle={2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP)}, 
  title={Catastrophic Forgetting within AI-Generated Dataset}, 
  year={2023},
  volume={},
  number={},
  pages={101-105},
  abstract={Catastrophic forgetting is an unavoidable phenomenon that arises when a previously learned model undergoes further training on a target dataset. When an event occurs, there is a loss of knowledge from past training, whether it be a partial or complete loss. In this study, there are two interconnected contributions. This research presents the introduction of a novel dataset for the purpose of examining the occurrence of catastrophic forgetting. The dataset was generated by employing a generative language model using an existing sentiment analysis dataset known as Sentiment140. The experiment yielded findings indicating that the created dataset exhibits distribution shifts in comparison to the original dataset. The present study involves the examination of two distinct language models, namely BERT and DistilBERT, which possess varying numbers of parameters. Moreover, the utilization of this information lends itself to the examination of domain adaptation. The occurrence of this phenomenon can be attributed to the domain shift that takes place between the two datasets.},
  keywords={Training;Measurement;Adaptation models;Analytical models;Sentiment analysis;Computational modeling;Image processing;catastrophic forgetting;domain adaptation;BERT;DistilBERT;sentiment analysis},
  doi={10.1109/IWAIIP58158.2023.10462836},
  ISSN={},
  month={Dec},}@ARTICLE{9416669,
  author={Abdollahi, Arnick and Pradhan, Biswajeet and Sharma, Gaurav and Maulud, Khairul Nizam Abdul and Alamri, Abdullah},
  journal={IEEE Access}, 
  title={Improving Road Semantic Segmentation Using Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={64381-64392},
  abstract={Road network extraction from remotely sensed imagery has become a powerful tool for updating geospatial databases, owing to the success of convolutional neural network (CNN) based deep learning semantic segmentation techniques combined with the high-resolution imagery that modern remote sensing provides. However, most CNN approaches cannot obtain high precision segmentation maps with rich details when processing high-resolution remote sensing imagery. In this study, we propose a generative adversarial network (GAN)-based deep learning approach for road segmentation from high-resolution aerial imagery. In the generative part of the presented GAN approach, we use a modified UNet model (MUNet) to obtain a high-resolution segmentation map of the road network. In combination with simple pre-processing comprising edge-preserving filtering, the proposed approach offers a significant improvement in road network segmentation compared with prior approaches. In experiments conducted on the Massachusetts road image dataset, the proposed approach achieves 91.54% precision and 92.92% recall, which correspond to a Mathews correlation coefficient (MCC) of 91.13%, a Mean intersection over union (MIOU) of 87.43% and a F1-score of 92.20%. Comparisons demonstrate that the proposed GAN framework outperforms prior CNN-based approaches and is particularly effective in preserving edge information.},
  keywords={Roads;Image segmentation;Generative adversarial networks;Semantics;Remote sensing;Generators;Feature extraction;GAN;road segmentation;remote sensing;deep learning;U-Net},
  doi={10.1109/ACCESS.2021.3075951},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10557384,
  author={Nando, Yudi April and Chung, Wan-Young},
  booktitle={2024 IEEE Wireless Power Technology Conference and Expo (WPTCE)}, 
  title={Enhancing RF Energy Harvesting and Wireless Power Transfer with GAN-Optimized 3D Quasi-Yagi Antenna}, 
  year={2024},
  volume={},
  number={},
  pages={454-458},
  abstract={Radio Frequency Energy Harvesting (RFEH) and Wireless Power Transfer (WPT) are increasingly popular for powering low-power devices, with the design of receiving antennas being a key factor. This paper presents a cube-shaped 3D antenna, innovatively derived from the quasi-Yagi antenna (QYA) design and optimized for operation at 915 MHz. Unlike traditional 2D and directional antennas, this 3D design offers multidirectional reception, which is crucial for efficiently harvesting varying levels of dedicated and ambient energy. Using CST Studio tools for simulation, the antenna, based on an optimized 2D QYA model (Type D with 6.24 dBi gain and 43.87 dB return loss), achieved a high gain of 11.43 dBi, a return loss of 45.61 dB, and a bandwidth of 25 MHz, as verified by the N9912A FieldFox HandHeld RF Analyzer and ME1310 Antenna Training Kit (3D) Dream Catcher. Generative Adversarial Networks (GANs) were utilized for data augmentation and optimization, reducing the reliance on extensive data collection. Additionally, the integration of this 3D antenna with an impedance-matching network and subsequent experimental harvesting with a 1.5 kΩ resistor demonstrate its practicality. We thus propose a versatile and efficient RFEH solution suitable for a range of RF sources and applications.},
  keywords={Radio frequency;Wireless sensor networks;Three-dimensional displays;Receiving antennas;Wireless power transfer;Data augmentation;Energy harvesting;3D antenna design and optimization;generative adversarial networks (GANs);and RF energy harvesting (RFEH)},
  doi={10.1109/WPTCE59894.2024.10557384},
  ISSN={},
  month={May},}@ARTICLE{10488756,
  author={Kang, Yuhan and Zare, Samira and Lin, Alex Tong and Han, Zhu and Osher, Stanley and Nguyen, Hien Van},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Game Theory Meets Data Augmentation}, 
  year={2024},
  volume={5},
  number={12},
  pages={6080-6094},
  abstract={Data augmentation is a critical component in building modern deep-learning systems. In this article, we propose MFG Augment, a novel data augmentation method based on the mean-field game (MFG) theory that can synthesize a sequence of data between every two images or features. The central idea is to consider every image as a distribution over its pixel or feature space. Using MFG theory, we can generate a time-continuous “path” from one distribution to another so that the points along the “path” are augmented images or features. Empirically, the experiment results on MNIST, CIFAR-10, and ImageNet demonstrate that the proposed technology has better generalization ability and higher classification accuracy as compared to several benchmark methods. More importantly, our MFG Augment improves the test accuracy significantly when the dataset size is small. MFG Augment consistently shows better affinity and diversity scores, two important empirical metrics for evaluating the generalization of data augmentation techniques.},
  keywords={Data augmentation;Task analysis;Transforms;Training;Data models;Game theory;Artificial intelligence;Data augmentation;deep learning;game theory;image classification;mean-field game (MFG)},
  doi={10.1109/TAI.2024.3384129},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{10635279,
  author={Lai, Matteo and Marzi, Chiara and Mascalchi, Mario and Diciotti, Stefano},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Brain MRI Synthesis Using Stylegan2-ADA}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep learning (DL) applications in the medical field often face challenges related to limited data availability, resulting in issues like overfitting and imbalanced datasets. Synthetic data offers a promising solution to these problems by enabling data augmentation and enhancing the performance of DL models. In this study, we trained the state-of-the-art generative model StyleGAN2-ADA on 1412 images from the Alzheimer’s disease neuroimaging initiative (ADNI) dataset to generate synthetic slices of T1-weighted brain MRI of healthy subjects. The quality of the synthetic images has been evaluated through quantitative and qualitative assessments, including a visual Turing test conducted by an expert observer with 2000 images. The observer achieved an accuracy of 52.95%, indicative of a performance level comparable to random guessing. These results demonstrate the capability of StyleGAN2-ADA to generate anatomically relevant synthetic brain MRI data.},
  keywords={Neuroimaging;Visualization;Magnetic resonance imaging;Observers;Brain modeling;Data augmentation;Data models;Alzheimer’s Disease Neuroimaging Initiative (ADNI);Generative Adversarial Network (GAN);Deep Learning (DL)},
  doi={10.1109/ISBI56570.2024.10635279},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{9155781,
  author={Xi, Fei and Ruan, Xuejun},
  booktitle={2020 International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Influence of Intelligent Environmental Art based on Reinforcement Learning on the Regionality of Architectural Design}, 
  year={2020},
  volume={},
  number={},
  pages={51-54},
  abstract={Influence of intelligent environmental art based on reinforcement learning on the regionality of architectural design is studied in this paper. Database technology is a core technology of information system, an important part of modern information science and technology, and the core of the computer data processing and information management system. This paper will adopt the advantages of the intelligent technology to construct the efficient design framework. Reinforcement learning is the process of letting agents learn actions in the environment to then obtain the maximum reward value, while deep reinforcement learning is a learning method that combines the perceptual ability of deep learning with the decision-making ability of the enhanced learning. This paper integrates the novel deep learning model to improve the traditional framework. The experiment results have proven the effectiveness.},
  keywords={Art;Buildings;Learning (artificial intelligence);Machine learning;Databases;Sociology;Conferences;Reinforcement Learning;Data Mining;Intelligent Environmental;Computer Art;Computer Aided Design},
  doi={10.1109/ICESC48915.2020.9155781},
  ISSN={},
  month={July},}@INPROCEEDINGS{10973570,
  author={Hou, Kun and Sun, Shiqi and Jiang, Haiyang and Zhang, Haoliang and Li, Jingyuan and Liu, Yingying and Wang, and Haiyang},
  booktitle={2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={Expansion and Validation of Open-Domain Question and Answering: An Integrated Framework Based on Large Models}, 
  year={2024},
  volume={},
  number={},
  pages={848-853},
  abstract={Open-Domain Question Answering (ODQA) has emerged as a critical research area in the field of natural language processing (NLP) in artificial intelligence (AI). Existing approaches primarily follow two paradigms for evidence collection: retrieve-then-read methods often struggle to acquire comprehensive and diverse evidence, and generate-then-read approaches often produce documents that lack contextual accuracy and relevance. We introduce an innovative framework named Expansion Generation and Verification (EGV), derived from the core processes of generating, evaluating, and verifying evidence. EGV encompasses six key stages: generation expansion, expansion evaluation, document re-ranking, re-ranking evaluation, answer generation, and answer verification. This framework effectively integrates the strengths of both retrieval-based and generative evidence collection methodologies. Experimental evaluations on widely-used benchmarks, including NQ, WebQ, and TriviaQA, demonstrate that EGV achieves state-of-the-art performance in both answer accuracy and evidence quality. These results under-score EGV's potential to significantly advance ODQA research and its practical applications.},
  keywords={Accuracy;Large language models;Benchmark testing;Question answering (information retrieval);Generators;Intelligent agents;Open-Domain Question Answering;Large Language Models;Query Expansion;Document Re-ranking;Answer Verification},
  doi={10.1109/WI-IAT62293.2024.00138},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9382449,
  author={Fan, Tsai-Jyun and Lu, Chien-Yu and Chiu, Wei-Chen and Su, Li and Lee, Che-Rung},
  booktitle={2020 International Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, 
  title={Timbre-enhanced Multi-modal Music Style Transfer with Domain Balance Loss}, 
  year={2020},
  volume={},
  number={},
  pages={102-107},
  abstract={Style transfer of the polyphonic music recordings has always been a challenging task due to the difficulty of learning representations for both domain invariant (i.e. content) and domain-variant (i.e. style) features of the music. Although there exists prior works which employ the Multi-modal Unsupervised Image-to-Image Translation (MUNIT) framework to perform the music style transfer in an unsupervised manner and successfully provide the promising results, the gap between the transferred music recordings and the real ones is still noticeable. In order to reduce such gap, we propose and experiment several techniques for improving the transferred results, including the domain balanced loss, up-sampling, content discriminator, recycle loss, and the data scaling. We conduct extensive experiments on the tasks of bilateral style transfer among four different genres, namely: piano solo, guitar solo, string quartet, and chiptune. In evaluation, an objective testing scheme is proposed to investigate the pros and cons of all our proposed techniques, while we also design a subjective testing method for making comparison among different approaches and show that our proposed method is able to provide superior performance with respect to the prior works.},
  keywords={Music;Recycling;Task analysis;Artificial intelligence;Testing},
  doi={10.1109/TAAI51410.2020.00027},
  ISSN={2376-6824},
  month={Dec},}@INPROCEEDINGS{10605309,
  author={Wu, Feilong and Yin, Kejing and Cheung, William K.},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={An End-to-end Learning Approach for Counterfactual Generation and Individual Treatment Effect Estimation}, 
  year={2024},
  volume={},
  number={},
  pages={176-182},
  abstract={Estimating the causal effect due to an intervention is important for many applications, such as healthcare. Unobserved counterfactuals make unbiased treatment effect estimation non-trivial. Among existing approaches, counterfactual generation which augments observational data with generated pseudo counterfactuals has been found promising for reducing the bias. These methods typically take a two-stage approach for the counterfactual generation and treatment effect estimation. Therefore, the counterfactual generation could be sub-optimal. To this end, we propose to jointly optimize the auxiliary models for generating the counterfactuals and the outcome estimation models. In particular, we demonstrate the viability by first connecting a counterfactual outcome generator with a reparameterized VAE model, and then learning them in an end-to-end fashion using the EM algorithm. Our evaluation results based on synthetic and semi-synthetic datasets show that a simple causal effect VAE model learned together with the counterfactual outcome generator can outperform a number of SOTA models for treatment effect estimation1.},
  keywords={Accuracy;Estimation;Medical services;Generators;Data models;Joining processes;Artificial intelligence;treatment effect estimation;counterfactual generation;causal inference},
  doi={10.1109/CAI59869.2024.00040},
  ISSN={},
  month={June},}@INPROCEEDINGS{9809071,
  author={Lajkó, Márk and Csuvik, Viktor and Vidács, László},
  booktitle={2022 IEEE/ACM International Workshop on Automated Program Repair (APR)}, 
  title={Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)}, 
  year={2022},
  volume={},
  number={},
  pages={61-68},
  abstract={The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Gener-ate and Validate (G&V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now avail-able to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25%.},
  keywords={Training;Sentiment analysis;Codes;Computer bugs;Maintenance engineering;Transformers;Software;Automated Program Repair;Machine learning;JavaScript;Code Refinement;GPT},
  doi={10.1145/3524459.3527350},
  ISSN={},
  month={May},}@ARTICLE{10924165,
  author={Chandran, Saranya and Syam, Sreelakshmi R. and Sankaran, Sriram and Pandey, Tulika and Achuthan, Krishnashree},
  journal={IEEE Access}, 
  title={From Static to AI-Driven Detection: A Comprehensive Review of Obfuscated Malware Techniques}, 
  year={2025},
  volume={13},
  number={},
  pages={74335-74358},
  abstract={The frequency of cyber attacks targeting individuals, businesses, and organizations globally has escalated in recent years. The evolution of obfuscated malware, designed to evade detection, has been unprecedented, employing new and sophisticated mechanisms to breach systems, steal sensitive data, and disrupt operations. This work advances research on obfuscated malware detection by offering a comprehensive review of studies conducted over the past decade on multiple platforms. In addition, the diversity of obfuscation techniques and the effectiveness of detection methods, such as static, dynamic, hybrid, and AI, are presented in a comparative manner. Furthermore, memory forensics, an often underexplored area, is of paramount importance for real-time analysis and the detection of advanced obfuscated malware. Hybrid analysis, which amalgamates the strengths of various approaches, emerges as a robust solution against obfuscated malware detection. The role of AI in detecting advanced ransomware, spyware, and fileless malware by enabling real-time detection and adaptive defenses against these increasingly prevalent threats is presented. In addition, a novel framework is proposed, combining Generative AI and digital twins to simulate and predict malware behavior, offering enhanced detection capabilities. This study synthesizes the findings of 76 approaches for the detection of obfuscated malware, incorporates cutting-edge technologies, and identifies open research challenges, such as ensuring scalability, enhancing generalization across platforms, and reducing resource requirements for constrained environments to guide future advancements in obfuscated malware detection.},
  keywords={Malware;Codes;Reviews;Deep learning;Surveys;Forensics;Real-time systems;Ransomware;Encryption;Systematics;Android;artificial intelligence;deep learning;digital twins;generative AI;hybrid detection methods;machine learning;memory forensics;obfuscated malware;obfuscation techniques},
  doi={10.1109/ACCESS.2025.3550781},
  ISSN={2169-3536},
  month={},}@BOOK{10769254,
  author={Turner-Williams, Wendy},
  booktitle={Unleashing the Power of Data with Trusted AI: A guide for board members and executives},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Discover the transformative potential of AI for data-driven decision-making and fast-track your organization's growth journey with trusted AI implementationKey FeaturesGain comprehensive insights and analyses to make quick and accurate decisionsLearn to integrate trusted AI into your organizational workflows and decision-making processesExplore real-world case studies that showcase the transformative impact of AI in diverse industriesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWritten by a distinguished leader and innovator who has been instrumental in spearheading digital, cloud, and AI transformations across global brands, Unleashing the Power of Data with Trusted AI is an indispensable resource that will make you AI-ready. This comprehensive guide is designed to meet the urgent need for clarity and to give you actionable insights into today's rapidly evolving landscape of AI and its fundamental driver - data. You’ll delve into the exciting world of AI and its integration with data, uncover its significance, ethical considerations, and strategic applications with real-life success stories from industry giants like Starbucks, Netflix, and Siemens. You’ll also witness first-hand how the integration of data and AI has reshaped markets and elevated customer experiences, and discover the future of generative AI based on several surveys and case studies. You’ll gain an understanding of how AI has evolved across industries, empowering decision-making and fostering innovation. Tailored for board members, executives, innovators, and tech enthusiasts, this immersive guide will reshape your understanding of data and AI synergy. By the end of this guide, you’ll be able to lead your teams, customers, partners, and organizations confidently and responsibly in the era of AI.What you will learnNavigate ethical considerations and comply with data regulations effectivelyElevate data quality and enhance data literacy within your organizationCraft effective AI strategies for data analytics processesExplore real-world case studies showcasing the tangible benefits of trusted AIOptimize decision-making processes by harnessing AI-driven insightsWho this book is forThis report is for executives and board members of mid to large enterprises, such as CDAIOs, CTOs, CIOs, CISOs, CPOs, and CEOs, as well as AI, data, ethics, privacy, and security professionals. With this book, you’ll confidently develop your data and AI implementation strategy and navigate the complex landscape of emerging technologies with clarity.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835460368},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10769254},}@ARTICLE{10049475,
  author={Chou, Chuan-Bi and Lee, Ching-Hung},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Generative Neural Network-Based Online Domain Adaptation (GNN-ODA) Approach for Incomplete Target Domain Data}, 
  year={2023},
  volume={72},
  number={},
  pages={1-10},
  abstract={Recently, monitoring machine health with artificial intelligence (AI) models becomes more efficient using either vibration or audio signals. However, the vibration signals of machines with different tool materials or under different operating conditions are not consistent. Hence, the transfer learning algorithm used in this study is presented for domain adaptation to improve the inference accuracy. Herein, we introduce a domain adaptation technique to solve domain shift problems during machine health monitoring. Additionally, most existing articles are assumed to have obtained complete target data. However, these data are continuously acquired; that is, the target data are incomplete during the monitoring phase. Thus, a generative neural network-based online domain adaptation model (GNN-ODA) is proposed. This will improve the test accuracy by generating complete target data and further training the classifier using the complete source data. The experiments indicate that the proposed method outperforms other domain adaptation methods wherein target data were incomplete. The average accuracy on both the tool wear and bearing datasets exceeded 90% when the source and target domains were under similar operating conditions.},
  keywords={Monitoring;Adaptation models;Training;Artificial intelligence;Vibrations;Data models;Convolutional neural networks;Bearing fault diagnosis;domain adaptation;machine health monitoring;tool wear;transfer learning},
  doi={10.1109/TIM.2023.3246495},
  ISSN={1557-9662},
  month={},}@ARTICLE{10843801,
  author={Fan, Jian and Ren, Pengfei and Chen, Jianrui and Qian, Junhui and Wang, Jingjing and Jiang, Chunxiao},
  journal={IEEE Internet of Things Journal}, 
  title={Diffusion-Based Semantic-Communication-Assisted Low-Altitude Intelligent Service for IoT}, 
  year={2025},
  volume={12},
  number={10},
  pages={13568-13580},
  abstract={Autonomous aerial vehicles (AAVs), as the key Internet of Things (IoT) devices, play a dominant position in low-altitude environments. Semantic communication (SC), as the next-generation communication technology, serves as a bridge for surpassing the Shannon limit toward the 6G wireless network. Establishing air-ground SC to provide intelligent IoT services is a crucial initiative for building future smart cities. In this article, we propose an AAV-based SC framework, named diffusion joint source-channel coding (D-JSCC). Abandoning traditional convolutional neural networks, we use transformers as the backbone and innovatively incorporate the diffusion model (DM) for image enhancement, achieving an optimal balance between image distortion and human perception. To accurately capture the numerical and perceptual loss induced by wireless channels and seamlessly amalgamate the DM with SC, we integrate channel states as strong prior information to refine the sampling process. Furthermore, we employ the gradient guidance strategy, which counteracts the randomness of sampling ensuring high robustness in harsh communication conditions. Additionally, we strike a balance between performance and sampling steps, ensuring both efficient computation and high-quality image enhancement. Comprehensive experiments demonstrate the advantages of D-JSCC across different communication environments.},
  keywords={Internet of Things;Transformers;Image reconstruction;Image enhancement;Decoding;Autonomous aerial vehicles;Optimization;Generative adversarial networks;Robustness;Distortion;Diffusion model (DM);generative artificial intelligence (GAI);semantic communication (SC);wireless image transmission},
  doi={10.1109/JIOT.2025.3530462},
  ISSN={2327-4662},
  month={May},}@INPROCEEDINGS{10914305,
  author={Liu, Xiaohua and He, Weiling and Chang, Jiang and Chen, Qiuxia},
  booktitle={2024 4th International Conference on Smart Grid and Energy Internet (SGEI)}, 
  title={Unique Data Balancing Method for Defect Detection in Power Transmission and Transformation}, 
  year={2024},
  volume={},
  number={},
  pages={164-170},
  abstract={This paper presents a unique data balancing approach using power transmission and transformation defect detection as an example. The proposed method achieves a more precise balance compared to traditional oversampling methods. It takes into account specific dataset characteristics and constraints to ensure effective sample adjustment. The experimental results demonstrate the superiority of this approach in enhancing object detection and classification accuracy within the defect detection context, providing a promising solution for improving data quality and model performance in relevant fields.},
  keywords={Accuracy;Data integrity;Refining;Power transmission;Object detection;Generative adversarial networks;Smart grids;Safety;Maintenance;Defect detection;Data Balancing;Power Transmission and Transformation Defect Detection;Oversampling;Sample Adjustment},
  doi={10.1109/SGEI63936.2024.10914305},
  ISSN={},
  month={Dec},}@ARTICLE{10707656,
  author={Cho, Deun-Sol and Cho, Jae-Min and Kim, Won-Tae},
  journal={IEEE Transactions on Reliability}, 
  title={A Generative Digital Twin for Continually Enhancing the Intended Functional Safety of Cyber–Physical Systems}, 
  year={2025},
  volume={74},
  number={2},
  pages={2561-2575},
  abstract={Cyber–physical systems (CPSs) used in safety-critical applications must safely operate even in unpredictable situations. The functional safety of their physical components, which have relatively low complexity, has been significantly assured through the safety standards. However, it is still difficult to ensure the intended functional safety (IFS) of autonomous control components driven by artificial intelligence models, as only a limited number of scenarios can be realized by engineers. This issue may cause the fatal injuries and the material damages under the unconsidered scenarios. To address this issue, we propose a generative digital twin (gDT) that continually enhances the IFS of the CPSs using a deep reinforcement learning algorithm and a retraining scheme. The workflow of the gDT involves 1) generating unknown hazardous scenarios that engineers cannot consider during the development phase and 2) feeding the generated scenarios back to the autonomous control components. Experimental results show that the proposed digital twin can discover the unknown hazardous scenarios against prototypes of the CPSs and can enhance the infraction avoiding rate to 90% against the discovered ones while maintaining the ability to operate in the existing known safe ones.},
  keywords={Digital twins;Artificial intelligence;Monitoring;Iron;Synchronization;Accidents;Optimization;Markov decision processes;Training;Prototypes;Cyber–physical systems (CPSs);digital twin;retraining;safety of the intended functionality;unknown scenario generation},
  doi={10.1109/TR.2024.3434606},
  ISSN={1558-1721},
  month={June},}@INPROCEEDINGS{10398699,
  author={Casian, Marincaş and Gabriela, Czibula},
  booktitle={2023 IEEE 19th International Conference on Intelligent Computer Communication and Processing (ICCP)}, 
  title={A Generative Modelling-based Approach for Text-to-Speech Synthesis in Romanian Language}, 
  year={2023},
  volume={},
  number={},
  pages={323-330},
  abstract={In recent decades, technological advancements in the field of Artificial Intelligence (AI) have opened up new opportunities for generating audio content based on text. In particular, Text-to-Speech (TTS) transformation models have become the most popular method for generating speech from written text. These models can be utilised in applications such as audiobooks, various virtual assistants, and video games, and their performances continue to improve, with generated results becoming increasingly close to human voice. However, there are still many challenges regarding generating as natural voice as possible and appropriate expression of emotions. In this paper we will focus on TTS synthesis, understanding the state-of-the-art and exploring possibilities of training these models for languages other than English, diverse voices, and improving the quality of voice generation. We introduce an approach for Romanian language TTS synthesis using generative deep learning models. Our approach yielded promising results on Romanian language datasets, demonstrating the effectiveness of our methodology.},
  keywords={Training;Deep learning;Video games;Computational modeling;Virtual assistants;Human voice;Artificial intelligence},
  doi={10.1109/ICCP60212.2023.10398699},
  ISSN={2766-8495},
  month={Oct},}@INPROCEEDINGS{9731246,
  author={Ding, Hu and He, Shumeng and Wu, Yanwen and Jin, Yongli and Gan, Lin and Xu, Gaodi and Yang, Houqun},
  booktitle={2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={An efficient face recognition attack method based on generative adversarial networks and cosine metrics}, 
  year={2021},
  volume={},
  number={},
  pages={570-577},
  abstract={Deep neural networks are vulnerable to attacks on adversarial samples. These attacks are caused by adding small magnitude perturbations to the input samples, which may lead to misclassification of the deep neural network. Based on the study of the adversarial sample attack network model, we propose an attack sample based on the generative adversarial network HNUGAN, incorporating the cosine metric of disparity recognition, for the features of the face dataset, to construct an attack sample to attack the face recognition system. Using these adversarial samples can reduce the recognition accuracy of models such as GoogleNet and ResNet to a very low level, and thus complete the attack on the target model. Interestingly, we can obtain a batch of adversarial samples through adversarial training to expand the dataset and retrain the target model to improve its robustness and resistance to attacks.},
  keywords={Training;Measurement;Deep learning;Resistance;Target recognition;Face recognition;Perturbation methods;Face recognition;GAN;adversarial sample attack;migration learning;cosine metric},
  doi={10.1109/ACAIT53529.2021.9731246},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9212835,
  author={Wang, Kai and Li, Jianwei and Zhou, Bin},
  booktitle={2019 International Conference on Virtual Reality and Visualization (ICVRV)}, 
  title={Interactive Grayscale Image Colorization with Generative Adversarial Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Grayscale image colorization is a classical image editing problem. There are two different methods for colorization. The interaction-based colorization method can generate results based on user interaction. However, this method requires considerable artificial interaction to achieve the desired results. Another method is automatic colorization based on deep learning. However, in this case, the colorization result is unique and cannot be adjusted if the result is incorrect or if the user has additional requirements. In this paper, we combine deep learning with user interaction and propose a grayscale image colorization method based on generative adversarial networks. In this method, a full convolutional neural network is constructed based on the U-net structure as a generator that can process images of any size. The training data is automatically generated by randomly simulating the interactive strokes. The experimental results indicate that this approach can efficiently achieve good colorization results and is capable of generating results based on different user interactions.},
  keywords={Gray-scale;Image color analysis;Training;Color;Generators;Neural networks;Generative adversarial networks;Computing methodologies;Artificial intelligence;Computer vision;Image representations},
  doi={10.1109/ICVRV47840.2019.00009},
  ISSN={2375-141X},
  month={Nov},}@ARTICLE{9143474,
  author={Liu, Chao and Ma, Jingjing and Tang, Xu and Liu, Fang and Zhang, Xiangrong and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Deep Hash Learning for Remote Sensing Image Retrieval}, 
  year={2021},
  volume={59},
  number={4},
  pages={3420-3443},
  abstract={The content-based remote sensing image retrieval (CBRSIR) has attracted increasing attention with the number of remote sensing (RS) images growing explosively. Benefiting from the strong capacity of the deep convolutional neural network (DCNN), the performance of CBRSIR has been improved in recent years. Although great successes have been obtained, learning the RS images’ representative features and enhancing the retrieval efficiency for the large-scale CBRSIR tasks are still two challenging problems. In this article, we propose a new CBRSIR method named feature and hash (FAH) learning, which consists of a deep feature learning model (DFLM) and an adversarial hash learning model (AHLM). The DFLM aims at learning the RS images’ dense features to guarantee the retrieval precision. In the DFLM, the DCNN and the proposed feature aggregation are integrated to capture the multiscale features. Then, the discrimination of the obtained features can be highlighted by the attention map in the developed attention branch. The AHLM maps the dense features onto the compact hash codes so that the retrieval efficiency can be improved. The AHLM contains a hash learning submodel and an adversarial regularization submodel. In particular, the hash learning submodel learns the real-valued hash codes that are similarity preserved by semantic supervisions. The adversarial regularization submodel regularizes the real-valued hash codes to learn the discrete uniform distribution with possible values 0 and 1. In this way, the hash codes are coding-balanced and the quantization errors are reduced. Encouraging experimental results counted on three public benchmark data sets demonstrate that our FAH can achieve competitive performance in the CBRSIR task compared with many existing hash learning methods.},
  keywords={Image retrieval;Feature extraction;Learning systems;Remote sensing;Task analysis;Quantization (signal);Benchmark testing;Adversarial hash learning model (AHLM);content-based remote sensing image retrieval (CBRSIR);deep feature learning model (DFLM);hash learning},
  doi={10.1109/TGRS.2020.3007533},
  ISSN={1558-0644},
  month={April},}@ARTICLE{9374570,
  author={Ao, Yile and Lu, Wenkai and Xu, Pengcheng and Jiang, Bowu},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Seismic Dip Estimation With a Domain Knowledge Constrained Transfer Learning Approach}, 
  year={2022},
  volume={60},
  number={},
  pages={1-16},
  abstract={Accurate estimation of volumetric seismic dip is of great significance for subsequent seismic processing and interpretation works. Recently, with the development of deep learning techniques, convolutional networks are also applied for seismic dip estimation. Compared with traditional approaches, estimating dips with convolutional networks is not only more efficient but also shows great promise in accuracy and robustness. However, if we take dips estimated by traditional approaches as labels and train networks on the field seismic data directly, the accuracy and robustness of learned networks are influenced due to the error in dip labels. An alternative solution is synthesizing realistic seismic samples with accurate dip labels. However, we find that due to the differences in seismic responses and structural patterns between the synthetic and field seismic data, networks directly learned from synthetic samples cannot guarantee their generalization on the field seismic data. To overcome these drawbacks, we develop a transfer learning approach for improvement. The proposed approach pretrains the dip estimation network on synthetic seismic samples at first and then transfers it to the targeted field seismic data with a domain knowledge-inspired fine-tuning process. Moreover, the proposed approach also highlights the combination of deep learning techniques and domain knowledge in seismic processing—several subtle realizations, such as knowledge-driven sample augmentation, knowledge constrained loss function, and knowledge motivated transfer learning strategy, are introduced, which greatly enhance the learning of the seismic dip estimation network. The advantages of the proposed approach in accuracy, robustness, and resolution are validated by applying the estimated dips for structural filtering and curvature extraction on the Netherlands F3 and Kerry3D seismic data, which further confirms its practicality in the real-world application. We believe that the proposed approach has provided an effective improved way for further seismic dip estimation practices, and the present domain knowledge constrained deep learning case will also inspire researchers in the same discipline.},
  keywords={Estimation;Transfer learning;Task analysis;Robustness;Deep learning;Azimuth;Tensors;Convolutional neural network;domain knowledge constraint;seismic dip estimation;transfer learning},
  doi={10.1109/TGRS.2021.3061438},
  ISSN={1558-0644},
  month={},}@ARTICLE{9459439,
  author={Zhang, Yiyang and Liu, Feng and Fang, Zhen and Yuan, Bo and Zhang, Guangquan and Lu, Jie},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Learning From a Complementary-Label Source Domain: Theory and Algorithms}, 
  year={2022},
  volume={33},
  number={12},
  pages={7667-7681},
  abstract={In unsupervised domain adaptation (UDA), a classifier for the target domain is trained with massive true-label data from the source domain and unlabeled data from the target domain. However, collecting true-label data in the source domain can be expensive and sometimes impractical. Compared to the true label (TL), a complementary label (CL) specifies a class that a pattern does not belong to, and hence, collecting CLs would be less laborious than collecting TLs. In this article, we propose a novel setting where the source domain is composed of complementary-label data, and a theoretical bound of this setting is provided. We consider two cases of this setting: one is that the source domain only contains complementary-label data [completely complementary UDA (CC-UDA)] and the other is that the source domain has plenty of complementary-label data and a small amount of true-label data [partly complementary UDA (PC-UDA)]. To this end, a complementary label adversarial network (CLARINET) is proposed to solve CC-UDA and PC-UDA problems. CLARINET maintains two deep networks simultaneously, with one focusing on classifying the complementary-label source data and the other taking care of the source-to-target distributional adaptation. Experiments show that CLARINET significantly outperforms a series of competent baselines on handwritten digit-recognition and object-recognition tasks.},
  keywords={Learning systems;Labeling;Deep learning;Neural networks;Transfer learning;Complementary labels (CLs);deep learning;domain adaptation (DA);transfer learning},
  doi={10.1109/TNNLS.2021.3086093},
  ISSN={2162-2388},
  month={Dec},}@ARTICLE{10491242,
  author={Hu, Lei and Zhou, Dawei and Xu, Jiahua and Lu, Cheng and Han, Chu and Shi, Zhenwei and Zhu, Qikui and Gao, Xinbo and Wang, Nannan and Liu, Zaiyi},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Protecting Prostate Cancer Classification From Rectal Artifacts via Targeted Adversarial Training}, 
  year={2024},
  volume={28},
  number={7},
  pages={3997-4009},
  abstract={Magnetic resonance imaging (MRI)-based deep neural networks (DNN) have been widely developed to perform prostate cancer (PCa) classification. However, in real-world clinical situations, prostate MRIs can be easily impacted by rectal artifacts, which have been found to lead to incorrect PCa classification. Existing DNN-based methods typically do not consider the interference of rectal artifacts on PCa classification, and do not design specific strategy to address this problem. In this study, we proposed a novel Targeted adversarial training with Proprietary Adversarial Samples (TPAS) strategy to defend the PCa classification model against the influence of rectal artifacts. Specifically, based on clinical prior knowledge, we generated proprietary adversarial samples with rectal artifact-pattern adversarial noise, which can severely mislead PCa classification models optimized by the ordinary training strategy. We then jointly exploited the generated proprietary adversarial samples and original samples to train the models. To demonstrate the effectiveness of our strategy, we conducted analytical experiments on multiple PCa classification models. Compared with ordinary training strategy, TPAS can effectively improve the single- and multi-parametric PCa classification at patient, slice and lesion level, and bring substantial gains to recent advanced models. In conclusion, TPAS strategy can be identified as a valuable way to mitigate the influence of rectal artifacts on deep learning models for PCa classification.},
  keywords={Principal component analysis;Training;Noise;Magnetic resonance imaging;Medical diagnostic imaging;Interference;Biological system modeling;Magnetic resonance imaging;prostate cancer;rectal artifact;targeted adversarial training},
  doi={10.1109/JBHI.2024.3384970},
  ISSN={2168-2208},
  month={July},}@ARTICLE{10374266,
  author={Liu, Jinyang and Li, Shutao and Dian, Renwei and Song, Ze},
  journal={IEEE Transactions on Multimedia}, 
  title={Focus Relationship Perception for Unsupervised Multi-Focus Image Fusion}, 
  year={2024},
  volume={26},
  number={},
  pages={6155-6165},
  abstract={Multi-focus image fusion can extract the focus regions from different source images and combine them into a fully clear image. Existing unsupervised methods typically use gradient information to measure the focus regions in images and generate a fusion weight map, but ordinary gradient operators are difficult to measure information accurately in regions with weaker textures. In addition, using only gradient information as a constraint cannot make the model fully distinguish all the focus regions in the image, which seriously restricts the clarity of the fusion image. To address these issues, a novel unsupervised multi-focus image fusion method is proposed in this paper. Specifically, a neighborhood information fusion network is designed to generate an initial fusion weight map. It can capture features within different neighborhood ranges at once, which enhances the information association between different regions. In addition, to further improve the feature extraction ability of the model in the regions with low texture information, a local difference evaluation loss function is proposed. It is combined with the gradient measure loss function to constrain the network. Finally, a fusion weight optimization module is proposed to improve the clarity of the fusion image in the repeated defocusing regions and overexposed regions of different source images, which redistributes the weights of different source images. The proposed fusion method is compared with advanced methods on three public multi-focus datasets. Experimental results indicate that the proposed method has achieved better performance in qualitative and quantitative aspects.},
  keywords={Image fusion;Feature extraction;Loss measurement;Data mining;Visual perception;Tensors;Optimization;Multi-focus image fusion;unsupervised learning},
  doi={10.1109/TMM.2023.3347099},
  ISSN={1941-0077},
  month={},}@ARTICLE{11126161,
  author={Sun, Hao and Chen, Hanlizi and Chen, Wenjing and Wang, Chengji and Xie, Wei and Lu, Xiaoqiang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Learning Positive–Negative Prompts for Open-Set Remote Sensing Scene Classification}, 
  year={2025},
  volume={63},
  number={},
  pages={1-14},
  abstract={Most remote sensing scene classification (RSSC) methods are primarily built upon the closed-set assumption, which assumes that all test samples definitely belong to one of the categories seen during training. However, practical applications are usually open environments, where samples of other categories that have never been seen during training will appear, which is called open-set RSSC (OS-RSSC). These methods may mistakenly classify samples of unseen categories into those seen categories, resulting in a decrease in application potential. In this article, we propose a positive–negative prompt learning (PNPL) framework for OS-RSSC. PNPL aims to tune the powerful contrastive language-image pretraining (CLIP) model for OS-RSSC through learning positive and negative prompts. First, positive textual prompts and visual prompts are trained to provide the model with basic classification capabilities for known classes. Then, negative textual prompts are indirectly learned from positive prompts and images, enabling the model to capture the semantics of unknown classes. PNPL significantly increases the discriminative power between known and unknown classes, enhancing the model’s ability to accurately distinguish them. Extensive experiments on three RSSI datasets have shown that PNPL outperforms compared methods. Code is available at https://github.com/ChenHanlizi/PNPL.},
  keywords={Remote sensing;Feature extraction;Training;Visualization;Semantics;Scene classification;Prototypes;Image reconstruction;Vectors;Testing;Open-set classification (OSC);prompt learning;remote sensing imagery;scene classification},
  doi={10.1109/TGRS.2025.3599188},
  ISSN={1558-0644},
  month={},}@ARTICLE{10988609,
  author={Peng, Chunlei and Luo, Xiaoyi and Liu, Decheng and Wang, Nannan and Hu, Ruimin and Gao, Xinbo},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Semantic Token Transformer for Face Forgery Detection}, 
  year={2025},
  volume={20},
  number={},
  pages={4904-4914},
  abstract={In the era of digital media, the proliferation of forged images and videos poses a significant threat to societal stability. With the rapid advancement of deep learning, the generation of realistic fake images has become increasingly simple, presenting unprecedented challenges in discerning the authenticity of images. While some existing methods have shown promising results in forgery detection, they often underutilize facial semantic information. To address this issue, this paper introduces the Semantic Token Transformer for Face Forgery Detection. By incorporating facial semantic information with a transformer network, the input tokens of the transformer are transformed into tokens of varying shapes and sizes based on their importance, thereby enhancing the accuracy of the detector. To achieve this objective, we first employ an image processing stage to manipulate the image based on facial semantic information. Subsequently, we introduce a scoring network, guided by prior knowledge, which adaptively categorizes tokens into different clusters based on their importance and relevance to the results of the preprocessing stage. Finally, we merge the tokens within the clusters using an attention mechanism and input them into the detector for forgery detection. Through experiments conducted on multiple datasets and cross-dataset evaluations, we demonstrate that our approach outperforms state-of-the-art detection methods.},
  keywords={Forgery;Faces;Transformers;Semantics;Deepfakes;Shape;Frequency-domain analysis;Computer vision;Feature extraction;Training;Face forgery detection;transformer;semantic information;token cluster},
  doi={10.1109/TIFS.2025.3567110},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{11075057,
  author={Shaik, Anees Nashath and Villarini, Barbara and Argyriou, Vasileios},
  booktitle={2025 25th International Conference on Digital Signal Processing (DSP)}, 
  title={A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Surveillance systems play a critical role in security and reconnaissance, but their performance is often compromised by low-quality images and videos, leading to reduced accuracy in face recognition. Additionally, existing AI-based facial analysis models suffer from biases related to skin tone variations and partially occluded faces, further limiting their effectiveness in diverse real-world scenarios. These challenges are the results of data limitations and imbalances, where available training datasets lack sufficient diversity, resulting in unfair and unreliable facial recognition performance. To address these issues, we propose a data-driven platform that enhances surveillance capabilities by generating synthetic training data tailored to compensate for dataset biases. Our approach leverages deep learning-based facial attribute manipulation and reconstruction using autoencoders and Generative Adversarial Networks (GANs) to create diverse and high-quality facial datasets. Additionally, our system integrates an image enhancement module, improving the clarity of low-resolution or occluded faces in surveillance footage. We evaluate our approach using the CelebA dataset, demonstrating that the proposed platform enhances both training data diversity and model fairness. This work contributes to reducing bias in AI-based facial analysis and improving surveillance accuracy in challenging environments, leading to fairer and more reliable security applications.},
  keywords={Accuracy;Surveillance;Face recognition;Training data;Digital signal processing;Reconnaissance;Artificial intelligence;Facial features;Image enhancement;Image reconstruction},
  doi={10.1109/DSP65409.2025.11075057},
  ISSN={2165-3577},
  month={June},}@ARTICLE{10705427,
  author={Xiao, Bin and Kantarci, Burak and Kang, Jiawen and Niyato, Dusit and Guizani, Mohsen},
  journal={IEEE Internet of Things Journal}, 
  title={Efficient Prompting for LLM-Based Generative Internet of Things}, 
  year={2025},
  volume={12},
  number={1},
  pages={778-791},
  abstract={Large language models (LLMs) have demonstrated remarkable capacities on various tasks, and integrating the capacities of LLMs into the Internet of Things (IoT) applications has drawn much research attention recently. Due to security concerns, many institutions avoid accessing state-of-the-art commercial LLM services, requiring the deployment and utilization of open-source LLMs in a local network setting. However, open-source LLMs usually have more limitations regarding their performance, such as their arithmetic calculation and reasoning capacities, and practical systems of applying LLMs to IoT have yet to be well-explored. Therefore, we propose an LLM-based Generative IoT (GIoT) system deployed in the local network setting in this study. To alleviate the limitations of LLMs and provide service with competitive performance, we apply prompt engineering methods to enhance the capacities of the open-source LLMs, design a Prompt Management Module and a Postprocessing Module to manage the tailored prompts for different tasks and process the results generated by the LLMs. To demonstrate the effectiveness of the proposed system, we discuss a challenging table question answering (Table-QA) task as a case study of the proposed system, as tabular data is usually more challenging than plaintext because of their complex structures, heterogeneous data types and sometimes huge sizes. We conduct comprehensive experiments on the two popular Table-QA data sets, and the results show that our proposal can achieve competitive performance compared with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system can provide competitive performance with tailored prompting methods and is easily extensible to new tasks without training.},
  keywords={Internet of Things;Cognition;Python;Artificial intelligence;Structured Query Language;Training;Servers;Question answering (information retrieval);Security;Performance evaluation;Generative Internet of Things (GIoT);large language model (LLM);prompt engineering;table question answering (Table-QA)},
  doi={10.1109/JIOT.2024.3470210},
  ISSN={2327-4662},
  month={Jan},}@INPROCEEDINGS{10145741,
  author={Perera, N. N. and Ganegoda, G. U.},
  booktitle={2023 3rd International Conference on Advanced Research in Computing (ICARC)}, 
  title={A Comprehensive Review on Speech Synthesis Using Neural-Network Based Approaches}, 
  year={2023},
  volume={},
  number={},
  pages={214-219},
  abstract={Speech is a primary mode of communication between human beings. With that, the need of creating artificial speech became a dream of humankind from the beginning of the 1980s. As a result of those experiments, many Speech synthesis systems were developed throughout the past few decades and further many advancements were made to the existing systems as well. However, with the beginning of the Artificial Intelligence era, these advancements were rapidly improved with the aid of neural networks. These newly found advancement techniques and methods were led to reemerge the research area of "Speech synthesis using neural networks-based approaches". This paper reviews currently available speech synthesis techniques, techniques that use neural networks-based approaches, advantages, disadvantages, and limitations. Further this review paper intends to suggest a new hybrid approach and what are modifications can be done in the upcoming future as well.},
  keywords={Neural networks;Machine learning;Speech synthesis;Speech synthesis;Neural networks},
  doi={10.1109/ICARC57651.2023.10145741},
  ISSN={},
  month={Feb},}@ARTICLE{9729828,
  author={Devika, K. and Mahapatra, Dwarikanath and Subramanian, Ramanathan and Oruganti, Venkata Ramana Murthy},
  journal={IEEE Access}, 
  title={Outlier-Based Autism Detection Using Longitudinal Structural MRI}, 
  year={2022},
  volume={10},
  number={},
  pages={27794-27808},
  abstract={Diagnosis of Autism Spectrum Disorder (ASD) using clinical evaluation (cognitive tests) is challenging due to wide variations amongst individuals. Since no effective treatment exists, prompt and reliable ASD diagnosis can enable the effective preparation of treatment regimens. This paper proposes structural Magnetic Resonance Imaging (sMRI)-based ASD diagnosis via an outlier detection approach. To learn spatio-temporal patterns in structural brain connectivity, a Generative Adversarial Network (GAN) is trained exclusively with sMRI scans of healthy subjects. Given a stack of three adjacent slices as input, the GAN generator reconstructs the next three adjacent slices; the GAN discriminator then identifies ASD sMRI scan reconstructions as outliers. This model is compared against two other baselines– a simpler UNet and a sophisticated Self-Attention GAN. Axial, Coronal and Sagittal sMRI slices from the multi-site ABIDE II dataset are used for evaluation. Extensive experiments reveal that our ASD detection framework performs comparably with the state-of-the-art with far fewer training data. Furthermore, longitudinal data (two scans per subject over time) achieve 17-28% higher accuracy than cross-sectional data (one scan per subject). Among other findings, metrics employed for model training as well as reconstruction loss computation impact detection performance, and the coronal modality is found to best encode structural information for ASD detection.},
  keywords={Generative adversarial networks;Image reconstruction;Variable speed drives;Magnetic resonance imaging;Autism;Brain modeling;Training;Autism spectrum disorder;sMRI slice reconstruction;outlier detection;generative adversarial network;self-attention},
  doi={10.1109/ACCESS.2022.3157613},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9708011,
  author={Bhagwani, Hitesh and Agarwal, Sonali and Kodipalli, Ashwini and Martis, Roshan Joy},
  booktitle={2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)}, 
  title={Targeting class imbalance problem using GAN}, 
  year={2021},
  volume={},
  number={},
  pages={318-322},
  abstract={Imbalanced data categorization is inescapable, and it has an impact on the model’s classification problem, which can cause false results. The purpose of this paper is to propose a generative adversarial network (GAN) for restoring balance in imbalanced datasets. This is a challenge since the limited minority data may not be sufficient for GAN training. The proposed article overcomes this issue by adversarial training all available data of minority and majority classes. The data for the minority class is generated using the generative model, which learns all of the useful features from the majority class. The generator in the GAN generates realistic-looking minority class samples. To validate the given method’s classification performance, experiments are performed on a credit card fraud detection dataset. This paper uses a Generative Adversarial Network to give an appropriate solution for imbalanced data classification.},
  keywords={Training;Support vector machines;Generative adversarial networks;Credit cards;Data models;Generators;Optimization;Imbalanced;GAN;generator;high-dimensional},
  doi={10.1109/ICEECCOT52851.2021.9708011},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10619874,
  author={Choudhury, Atlanta and Sarma, Kandarpa Kumar and Mastorakis, Nikos},
  booktitle={2024 IEEE 18th International Symposium on Applied Computational Intelligence and Informatics (SACI)}, 
  title={AI-based Text to Scene Generation as Part of a Pandemic Compliant Infrastructure}, 
  year={2024},
  volume={},
  number={},
  pages={000395-000398},
  abstract={After the 2020–2022 COVID-19 pandemic, it is observed that a responsive medical infrastructure and damage control techniques, including application of technology, have become more important than ever before. Many such technologies, including the Internet of Things (IoT), and artificial intelligence (AI)-aided decision-making have become relevant. Within such a framework, text-to-scene generation through the incorporation of Artificial Intelligence (AI) becomes a critical element of a pandemic-compliant infrastructure design. Our suggested architecture uses AI algorithms to create sceneries that are dynamically adjusted for pandemic compliance. These sceneries cover a wide range of settings, including as public locations, medical institutions, virtual meeting rooms, and more. Our approach makes use of generator and discriminator blocks as part of a Generative Adversarial Network (GAN) which is further enhanced by several attention layers to generate scenes from test descriptions. The output of the proposed approach ensures that scene creation regarding safety precautions like mask wearing, social distance, and sanitization procedures are included in generated output.},
  keywords={COVID-19;Pandemics;Computer viruses;Transfer learning;Transforms;Web conferencing;Generative adversarial networks;Deep Learning (DL);Deep Transfer Learning (DTL);Cyber-attack;Scene Generation},
  doi={10.1109/SACI60582.2024.10619874},
  ISSN={2765-818X},
  month={May},}@INPROCEEDINGS{10648174,
  author={Chen, Honghui and Zhao, Baoquan and Yue, Guanghui and Liu, Weide and Lv, Chenlei and Wang, Ruomei and Zhou, Fan},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={Clip-Medfake: Synthetic Data Augmentation With AI-Generated Content for Improved Medical Image Classification}, 
  year={2024},
  volume={},
  number={},
  pages={3854-3860},
  abstract={Data augmentation is serving as a critical and fundamental technology to improve model generalization and performance in a wide spectrum of machine learning tasks. Despite the increasing interest in developing various pathways to artificially generate new data to reduce the overfitting issue during model training, enriching the diversity of training data in the field of medicine remains facing enormous challenges. By virtue of recent advancements in generative artificial intelligence, we present a novel data augmentation framework, CLIP-MedFake, to address the shortage of training data used in medical image classification. The proposed method first employs the Stable Diffusion model to generate new fake data based on a small amount of training data, and then adopts the paradigm of few-shot learning and uses the CLIP architecture as the backbone to pre-train the model with synthetic data and then fine-tune it with real medical images. Extensive experiment results on two publicly available datasets demonstrate the effectiveness of the proposed method in promoting medical image classification.},
  keywords={Training;Generative AI;Training data;Data augmentation;Data models;Task analysis;Few shot learning;Medical image classification;data augmentation;artificial intelligence-generated content},
  doi={10.1109/ICIP51287.2024.10648174},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10957704,
  author={Nasimova, Nigorakhon and Nasimov, Rashid and Sobirova, Guzal and Usmanxodjayeva, Adibaxon and Rakhimov, Mekhriddin and Javliev, Shakhzod},
  booktitle={2024 International Conference on Information Science and Communications Technologies (ICISCT)}, 
  title={Generating and Evaluating Synthetic Medical Images}, 
  year={2024},
  volume={},
  number={},
  pages={117-122},
  abstract={In recent years, deep learning models have been used in various fields. Nonetheless, adaptation is still required in delicate fields like medical imaging. Since the medical field has a time constraint that necessitates the use of technology, accuracy guarantees credibility. Medical data cannot be used in machine learning applications due to privacy concerns. The development of big data is closely related to the high accuracy of disease diagnosis, classification, and recommendation. Argumentation techniques are used to expand the size of medical image datasets because deep learning-based image analysis necessitates large amounts of data. This paper proposes to include an Fréchet MedicalNet distance (FMD) value with higher accuracy than the Fréchet inception distance (FID) value in evaluating medical images that are different from the methods used to generate synthetic medical images and to calculate the FID value quickly.},
  keywords={Training;Pathology;Information science;Accuracy;Nose;Production;Generative adversarial networks;Time factors;Medical diagnosis;Medical diagnostic imaging;Echocardiogram;Artificial intelligence;GAN network;Fréchet MedicalNet distance;Fréchet inception distance synthetic medical image},
  doi={10.1109/ICISCT64202.2024.10957704},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10849439,
  author={Guan, Haiyang and Wei, Xiaoyi and Long, Weifan and Yang, Dingkang and Zhai, Peng and Zhang, Lihua},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={MDRPC: Music-Driven Robot Primitives Choreography}, 
  year={2024},
  volume={},
  number={},
  pages={748-755},
  abstract={Dance has been an important art form and means of communication since the dawn of human civilization. Equipping humanoid robots with the ability to perform smooth dance movements to music is a key research priority in artificial intelligence, robotics and human-computer interaction. However, existing kinematics-based dance generation methods often violate real-world physical laws as they do not consider physical constraints, leading to unrealistic movements. Additionally, due to the diversity and dynamic variability of input music, most existing physics-based methods, which rely on task-specific reward functions, face significant challenges in effectively handling music-driven dance generation tasks. To address these issues, we introduce MDRPC, the first physics-based, music-driven dance generation method for humanoid robots. Inspired by human choreographic principles, MDRPC is defined as a two-phase framework. The initial phase utilizes adversarial imitation learning to acquire a rich set of reusable dance primitives from a music-dance dataset. In the subsequent phase, these dance primitives are orchestrated under the guidance of musical theory and choreographic rules to generate complex humanoid dance sequences. Specifically, we propose beat alignment and dance diversity reward functions to synchronize motion rhythms with music beats and enhance the diversity of dance movements. We implement MDRPC on a simulated humanoid robot, and the results confirm that our method effectively controls the humanoid, enabling it to perform dance movements harmoniously synchronized with the music.},
  keywords={Human computer interaction;Humanities;Art;Imitation learning;Humanoid robots;Rhythm;Synchronization;Artificial intelligence;Faces;robot control;music-driven;dance primitives;reinforcement learning;adversarial imitation learning},
  doi={10.1109/ICTAI62512.2024.00111},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{11068599,
  author={Ong, Melinda and Godshall, Nicholas and Crane, Jeremiah},
  booktitle={2025 IEEE Aerospace Conference}, 
  title={An Application for Model-Based Guided Engineering}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Model-based systems engineering (MBSE) deliverables are encumbered by time-intensive tasks such as manual model creation, documentation, and traceability analysis to ensure elements of the system model support tracking of requirements, design decisions, and system behaviors. To reach digital transformation milestones in condensed schedules, model-based guided engineering (MBGE) leverages generative artificial intelligence (AI) to accelerate MBSE development of complex systems using generative systems engineering (GSE) data. Booz Allen developed an AI software tool, the MBSE AI Platform for Productivity (MAPPy) which interfaces with Cameo MagicDraw, to harness MBGE and address existing bottlenecks in MBSE practices. The tool provides an interface for systems engineers to optimize workloads with advanced capabilities to support data literacy, MBSE frameworks, and vast program knowledge bases. This paper describes how MAPPy streamlines MBGE to develop GSE data for comprehensive MBSE technical baselines by integrating generative AI, retrieval-augmented generation (RAG), and agential workflows to remove redundant, timeconsuming tasks and accelerate overall model development.},
  keywords={Productivity;Analytical models;Technological innovation;Schedules;Generative AI;Retrieval augmented generation;Manuals;Documentation;Data models;Software tools},
  doi={10.1109/AERO63441.2025.11068599},
  ISSN={2996-2358},
  month={March},}@INPROCEEDINGS{10377923,
  author={Ma, Zongyang and Zhang, Ziqi and Chen, Yuxin and Qi, Zhongang and Luo, Yingmin and Li, Zekun and Yuan, Chunfeng and Li, Bing and Qie, Xiaohu and Shan, Ying and Hu, Weiming},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Order-Prompted Tag Sequence Generation for Video Tagging}, 
  year={2023},
  volume={},
  number={},
  pages={15635-15644},
  abstract={Video Tagging intends to infer multiple tags spanning relevant content for a given video. Typically, video tags are freely defined and uploaded by a variety of users, so they have two characteristics: abundant in quantity and disordered intra-video. It is difficult for the existing multilabel classification and generation methods to adapt directly to this task. This paper proposes a novel generative model, Order-Prompted Tag Sequence Generation (OP-TSG), according to the above characteristics. It regards video tagging as a tag sequence generation problem guided by sample-dependent order prompts. These prompts are semantically aligned with tags and enable to decouple tag generation order, making the model focus on modeling the tag dependencies. Moreover, the word-based generation strategy enables the model to generate novel tags. To verify the effectiveness and generalization of the proposed method, a Chinese video tagging benchmark CREATE-tagging, and an English image tagging benchmark Pexel-tagging are established. Extensive results show that OP-TSG is significantly superior to other methods, especially the results on rare tags improve by 3.3% and 3% over SOTA methods on CREATE-tagging and Pexel-tagging, and novel tags generated on CREATE-tagging exhibit a tag gain of 7.04%.},
  keywords={Computer vision;Adaptation models;Head;Image annotation;Tagging;Benchmark testing;Decoding},
  doi={10.1109/ICCV51070.2023.01437},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{11083573,
  author={Wang, Yilin and Yu, Yifei and Sun, Kong and Lei, Peixuan and Zhang, Yuxuan and Zio, Enrico and Xia, Aiguo and Li, Yuanxiang},
  journal={IEEE Internet of Things Journal}, 
  title={RmGPT: A Foundation Model With Generative Pre-trained Transformer for Fault Diagnosis and Prognosis in Rotating Machinery}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={In industry, the reliability of rotating machinery is critical for production efficiency and safety. Current methods of Prognostics and Health Management (PHM) often rely on task-specific models, which face significant challenges in handling diverse datasets with varying signal characteristics, fault modes and operating conditions. Inspired by advancements in generative pretrained models, we propose RmGPT, a unified model for diagnosis and prognosis tasks. RmGPT introduces a novel generative token-based framework, incorporating Signal Tokens, Prompt Tokens, Time-Frequency Task Tokens and Fault Tokens to handle heterogeneous data within a unified model architecture. We leverage self-supervised learning for robust feature extraction and introduce a next signal token prediction pretraining strategy, alongside efficient prompt learning for task-specific adaptation. Extensive experiments demonstrate that RmGPT significantly outperforms state-of-the-art algorithms, achieving near-perfect accuracy in diagnosis tasks and exceptionally low errors in prognosis tasks. Notably, RmGPT excels in few-shot learning scenarios, achieving 82% accuracy in 16-class one-shot experiments, highlighting its adaptability and robustness. This work establishes RmGPT as a powerful PHM foundation model for rotating machinery, advancing the scalability and generalizability of PHM solutions. Code is available at:https://github.com/Pandalin98/RmGPT.},
  keywords={Prognostics and health management;Adaptation models;Foundation models;Machinery;Data models;Self-supervised learning;Training;Predictive models;Internet of Things;Fault diagnosis;Rotating Machinery;Reliability;Prognostics and Health Management;Remaining Useful Life Prediction;Fault Diagnosis;Foundation Model;Self-supervised Learning},
  doi={10.1109/JIOT.2025.3580823},
  ISSN={2327-4662},
  month={},}@ARTICLE{11177583,
  author={Tang, Yingbo and Cao, Zhiqiang and Guan, Peiyu and Gong, Xurong and Wang, Mengyao and Yu, Junzhi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Diff-COPE: Diffusion-Based Category-Level 6D Object Pose Estimation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Category-level 6D object pose estimation has gained increasing attention in applications of robotic manipulation, augmented reality, and scene understanding, due to its ability to generalize to unseen instances within the same category. However, existing methods struggle with handling the intra-class shape variations as they either adopt mean shape as priors, or build the associations among different instances without explicit category-shared information. To address this problem, a novel category-level object pose estimation method based on diffusion model is proposed, which utilize the generative ability of diffusion model to refine a sparse categorical representation. In contrast to existing dense correspondence-based methods, our method employs a set of keypoints provided by learnable queries to represent object shape, enabling better categorical representation of different instances by focusing on the representative object components. The keypoints are then refined through a forward diffusion process and a reverse denoising process conditioned on category information. This allows the flexible adaption to various instances, especially for those that deviate from the mean shape within the same category. On this basis, a geometric-semantic feature fusion module is presented to enhance keypoint feature representation. By integrating the geometric information from point cloud with the high-level semantics from RGB image using a two-branch attention mechanism, the keypoint feature is enriched and deeply combined, which facilitates the subsequent pose estimation. Extensive experiments on the REAL275 dataset, the CAMERA25 dataset, and real-world complex scenarios demonstrated the effectiveness of proposed method.},
  keywords={Pose estimation;Shape;Diffusion models;Three-dimensional displays;Point cloud compression;Solid modeling;Semantics;Training;Noise reduction;Image reconstruction;Category-level 6D object pose estimation;diffusion model;keypoint refinement;feature fusion},
  doi={10.1109/TCSVT.2025.3613733},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{6896010,
  author={Han, Xiaolong and Chen, Chen},
  booktitle={Proceedings of the 33rd Chinese Control Conference}, 
  title={The design and optimization method of near space intelligent target generator}, 
  year={2014},
  volume={},
  number={},
  pages={6223-6228},
  abstract={This paper presents the design and optimization method of near space intelligent target generator to simulate the physical characteristics of the near space vehicle. Combined with High Level Architecture distributed simulation technology, a common, repeatable and verified platform for the near space vehicle has been provided. This method used 3D modeling software Creator and 3D visual rendering software Vega, two-dimensional map and three-dimensional vision were constructed to form a simulation environment, which enhanced the authenticity of the simulation. Based on particle swarm optimization, the intelligent path planning study of near space vehicle was conducted in this environment to make up for the inadequate intelligence of traditional target generators.},
  keywords={Solid modeling;Generators;Space vehicles;Load modeling;Path planning;Radar;Target Generator;Near space;HLA;Virtual reality;Artificial intelligence},
  doi={10.1109/ChiCC.2014.6896010},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{10894072,
  author={Gobinath, R. and Boopathi, R and S, Vijayalakshmi and Anu Disney, D. and Nirmala Priyadharshini, P. and Maheshwari, K.},
  booktitle={2025 International Conference on Multi-Agent Systems for Collaborative Intelligence (ICMSCI)}, 
  title={A Hybrid GAN-CNN Framework for Low-Light Image Enhancement Combining Structural Noise Reduction and Perceptual Quality}, 
  year={2025},
  volume={},
  number={},
  pages={1779-1786},
  abstract={Low light region is one of the difficult scenarios for surveillance system, especially noisy, low-quality images may lead to poor object detection and scene analysis. To tackle this problem, we introduce a hybrid GAN - CNN framework for low light image denoising and quality enhancement. We develop a framework that combines the strengths of Generative Adversarial Networks (GANs) for photorealistic image generation and Convolutional Neural Networks (CNNs) for robust noise reduction. First denoising and feature extraction is done by the CNN module, and the GAN part further refines the denoised output to improve the perceptual quality of the generated images. We train and validate the proposed model in standard low light image dataset and show superior PSNR, SSIM, and visual clarity compared to state of art methods. We show that the hybrid GAN-CNN model successfully trades between image denoising accuracy and visual realism, thereby providing a viable alternative for enhancing surveillance performance in lowlight conditions. Keywords: Image denoising, GAN-CNN hybrid, low-light enhancement, PSNR, SSIM, surveillance systems.},
  keywords={Visualization;Accuracy;Surveillance;Noise reduction;Noise;Generative adversarial networks;Feature extraction;Convolutional neural networks;Image enhancement;Image denoising},
  doi={10.1109/ICMSCI62561.2025.10894072},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11141284,
  author={Oluwafemi, Ejiga Peter Ojonugwa and Tunde, Bamidele Dayo and Hoque, Mahmudul and Briggs, Dapiriye and Rahman, Md Mahmudur and Khalifa, Fahmi},
  booktitle={2025 IEEE 4th International Conference on Computing and Machine Intelligence (ICMI)}, 
  title={Colonoscopy Image Synthesis: Transforming Medical Image Classification With Enhanced Quality and Computational Efficiency}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This work introduces a novel approach to generate synthetic medical images from textual prompts using fine-tuned DreamBooth and LoRa. The study focuses on image synthesis and image classification, evaluating the quality and diversity of generated images using Inception Scores and Fréchet Inception Distance. The results show consistent average Inception Scores of 2.36 across datasets, with FID scores (0.11 for single-center, 0.073 for multicenter, and 0.076 for combined) indicating that the generated images closely resemble real ones. In the combined data set, EfficientNet shows superior performance, achieving 89% training accuracy and 91 % validation accuracy, significantly outperforming other models. In particular, all models exhibit slightly higher validation accuracies compared to their training accuracies, Indicating a good generalization. The substantial performance gap between EfficientNet and the other models, with a difference of at least 19 percentage points, underscores its exceptional effectiveness on this dataset. This research highlights the promise of AI-generated medical images, while emphasizing the need for ethical considerations and strategies to use them in healthcare applications.},
  keywords={Training;Ethics;Accuracy;Image synthesis;Generative AI;Medical services;Data models;Machine intelligence;Biomedical imaging;Image classification;Generative AI;DreamBooth;Medical Data Augmentation;Synthetic Medical Imaging;LoRA},
  doi={10.1109/ICMI65310.2025.11141284},
  ISSN={},
  month={April},}@INBOOK{10880588,
  author={},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Front Matter}, 
  year={2025},
  volume={},
  number={},
  pages={i-xli},
  abstract={<p>The prelims comprise: <ul> <li>Half&#x2010;Title Page</li> <li>Series Page</li> <li>Title Page</li> <li>Copyright Page</li> <li>Dedication</li> <li>Contents</li> <li>About the Editors</li> <li>List of Contributors</li> <li>Preface</li> <li>Acknowledgments</li> </ul> </p>},
  keywords={},
  doi={10.1002/9781394280735.fmatter},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880588},}@INPROCEEDINGS{9323676,
  author={Rahnemoonfar, Maryam and Yari, Masoud and Paden, John},
  booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium}, 
  title={Radar Sensor Simulation with Generative Adversarial Network}, 
  year={2020},
  volume={},
  number={},
  pages={7001-7004},
  abstract={Significant resources have been spent in collecting and storing large and heterogeneous radar datasets during expensive Arctic and Antarctic fieldwork. The vast majority of data available is unlabeled, and the labeling process is both time-consuming and expensive. One possible alternative to the labeling process is the use of synthetically generated data with artificial intelligence. In this research, we evaluated the performance of synthetically generated snow radar images based on modified cycle-consistent adversarial networks. We conducted several experiments to test the quality of the generated radar imagery. Our experiments show a very good similarity between real and synthetic snow radar images.},
  keywords={Radar;Radar imaging;Snow;Ice;Gallium nitride;Measurement;Radar tracking;convolutional neural network;generative adversarial network;ice tracking;radar imagery},
  doi={10.1109/IGARSS39084.2020.9323676},
  ISSN={2153-7003},
  month={Sep.},}@ARTICLE{10258334,
  author={Li, Zhixuan and Ye, Weining and Jiang, Tingting and Huang, Tiejun},
  journal={IEEE Transactions on Multimedia}, 
  title={GIN: Generative INvariant Shape Prior for Amodal Instance Segmentation}, 
  year={2024},
  volume={26},
  number={},
  pages={3924-3936},
  abstract={Amodal instance segmentation (AIS) predicts the complete shape of the occluded object, including both visible and occluded regions. Because visual clues are lacking, the occluded region is difficult to segment accurately. In human amodal perception, shape-prior knowledge is helpful for AIS. The previous method uses a 2D shape prior by rote memorizing, establishing a shape dictionary and retrieving the closest mask to the segmentation result. However, this approach cannot obtain the shape prior, which is not prestored in the shape dictionary. In this article, to improve generalization ability, we propose a generative invariant shape-prior network (GIN), simulating the human perception process that learns the basic shape, which is invariant to transformations, including translation, rotation, and scaling. We design a novel framework that decouples the learning of shape priors from transformation. GIN is end-to-end trainable and needs no dictionary establishment, making the whole pipeline efficient. GIN outperforms state-of-the-art methods on three public datasets (D2SA, COCOA-cls, and KINS) with large margins.},
  keywords={Shape;Task analysis;Artificial intelligence;Three-dimensional displays;Dictionaries;Transformers;Knowledge engineering;Amodal perception;instance segmentation;shape prior},
  doi={10.1109/TMM.2023.3318075},
  ISSN={1941-0077},
  month={},}@ARTICLE{10225524,
  author={Pradhan, Nitesh and Dhaka, Vijaypal Singh and Rani, Geeta and Pradhan, Vivek and Vocaturo, Eugenio and Zumpano, Ester},
  journal={IEEE Access}, 
  title={Conditional Generative Adversarial Network Model for Conversion of 2 Dimensional Radiographs Into 3 Dimensional Views}, 
  year={2023},
  volume={11},
  number={},
  pages={96283-96296},
  abstract={The inefficacy of 2-Dimensional techniques in visualizing all perspectives of an organ may lead to inaccurate diagnosis of a disease or deformity. This raises a need for adopting 3-Dimensional medical images. But, the high expense, exposure to a high volume of harmful radiations, and limited availability of machinery for capturing images are limiting factors in implementing 3-Dimensional medical imaging for the whole populace. Thus, the conversion of 2-Dimensional images to 3-Dimensional images gained high popularity in the field of medical imaging. However, numerous research works offer the potential for the reconstruction of 3-Dimensional images. But, none of these provides the visualization of all angles of view from 0° to 360° for a 2-Dimensional input image such as X-ray and dual-energy X-ray absorptiometry. Also, these techniques fail to handle noisy and deformed input images. The purpose of this research is to propose a tailored Conditional Adversarial Network Model for the translation of 2-Dimensional images of bones into their corresponding 3-Dimensional view. The model is preceded by pre-processing techniques for dataset cleaning, noise removal, and converting the dataset to a uniform format. Further, the efficacy of the model is improved by determining the optimal values of model parameters, employing the customized activation function, and optimizers. Additionally, the visual quality of the generated 3-Dimensional images is evaluated to showcase the degree of quality degradation while translating. The experimental results obtained on the real-life datasets collected from hospitals across India prove the efficacy of the proposed model in generating 3-Dimensional images. The generated images are similar in quality to the input image and also effective in retaining the information available in an input image.},
  keywords={X-ray imaging;Imaging;Computed tomography;Bones;Visualization;Magnetic resonance imaging;Generative adversarial networks;Deep learning;Two dimensional displays;Conditional generative adversarial network;deep learning;X-ray;3-dimensional view;2-dimensional imaging},
  doi={10.1109/ACCESS.2023.3307198},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9898527,
  author={Zhang, Yuzhong and Zhang, Guixuan and Huang, Xinyuan},
  booktitle={2022 International Conference on Culture-Oriented Science and Technology (CoST)}, 
  title={A Survey of Procedural Content Generation for Games}, 
  year={2022},
  volume={},
  number={},
  pages={186-190},
  abstract={With the continuous development of video game, the complexity of video game is gradually increasing. Besides, the cost of manually creating video game content keeps increasing although it is already very expensive. Nowadays, the cost of Game development is gradually prohibitive. In the process of a 3A game development, tens of millions development budget is spent on creating game content. In order to curb the increasing consumption of time and money, Procedural Content Generation for Games (PCG-G) has become a popular way to solve these problems by automatic game content generation. This paper reviews the PCG-G research. Firstly, we introduce what PCG-G is and its development. Then we classify PCG-G into three groups which include traditional methods, search-based methods and machine learning methods. Finally, we will show cases of PCG-G application in video games, introduce what effect PCG-G has on the game player experience and some problem of PCG-G.},
  keywords={Deep learning;Costs;Uncertainty;Games;Evolutionary computation;Soil;Complexity theory;Procedural Content Generation for Games;video game;game content},
  doi={10.1109/CoST57098.2022.00046},
  ISSN={},
  month={Aug},}@ARTICLE{10769438,
  author={Song, Daeun and Lee, Nayoung and Kim, Jiwon and Choi, Eunjung},
  journal={IEEE Access}, 
  title={Anomaly Detection of Deepfake Audio Based on Real Audio Using Generative Adversarial Network Model}, 
  year={2024},
  volume={12},
  number={},
  pages={184311-184326},
  abstract={Deepfake audio causes damage not only to individuals and companies, but also to nations; therefore, research on deepfake audio detection technology is crucial. Most existing deepfake audio detection research has been conducted using supervised learning; however, when a new synthetic deepfake audio emerges, real-time detection becomes difficult because of the limitations of supervised learning. Therefore, this paper proposes a new anomaly detection technique for identifying deep-fake audio using unsupervised learning. This method involves learning the feature distribution of numerous real human voices and then calculating an anomaly score for each voice to determine whether it is deepfake. In this study, we imaged speech using mel-spectrogram and mel-frequency cepstral coefficient (MFCC), which are speech preprocessing methods. Subsequently, the parameters of the GANomaly and f-AnoGAN models, which are effective in detecting abnormalities in speech, were tuned and subjected to unsupervised training. The most effective result had an F1-score of 0.93 in and was obtained by combining imaging speech with Mel-Spectrogram with training using the GANomaly model.},
  keywords={Deepfakes;Anomaly detection;Mel frequency cepstral coefficient;Feature extraction;Training;Generative adversarial networks;Unsupervised learning;Spectrogram;Accuracy;Supervised learning;Anomaly detection;Deepfake;Deepfake audio;deep learning;f-AnoGAN;feature extraction;GANomaly;generative adversarial network model;mel-spectrogram;MFCC;unsupervised learning},
  doi={10.1109/ACCESS.2024.3506973},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10707467,
  author={Chatterjee, Pushpita and Das, Debashis and Rawat, Danda B},
  booktitle={2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)}, 
  title={A Generative AI Approach for Ensuring Data Integrity Security Resilience in Fintech Systems}, 
  year={2024},
  volume={},
  number={},
  pages={168-173},
  abstract={The Fintech industry represents the convergence of finance and technology through innovative digital solutions from mobile banking to cryptocurrency. However, it faces big challenges with keeping data safe and systems strong. Traditional methods struggle to keep pace with sophisticated threats and complexities inherent in modern Fintech ecosystems. This paper proposes an approach to address these challenges using Generative AI and blockchain integration to make Fintech systems more resilient. Advanced machine learning algorithms detect and prevent data tampering in the proposed systems. Generative AI is used for threat detection, anomaly recognition, and real-time monitoring in system security. Then, we integrate blockchain technology to enhance the overall resilience of systems. Blockchain technology enhances the reliability of financial services in secure transactions, validating blocks, and distributing control across decentralized networks. These combined methodologies address the critical challenges of data integrity, security, and system resilience in dynamic Fintech systems. The performance analysis demonstrates the efficacy of our proposed framework in enhancing data integrity, security measures, and system resilience within Fintech systems.},
  keywords={Fault tolerance;Generative AI;Data integrity;Smart contracts;Threat assessment;Real-time systems;Blockchains;Security;Faces;Resilience;Fintech;Data Integrity;Security;Resilience;Generative Artificial Intelligence;Machine Learning;Cybersecurity},
  doi={10.1109/CCGridW63211.2024.00027},
  ISSN={},
  month={May},}@ARTICLE{10937266,
  author={Chiaro, Diletta and Qi, Pian and Pescapè, Antonio and Piccialli, Francesco},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Generative AI-Empowered Digital Twin: A Comprehensive Survey With Taxonomy}, 
  year={2025},
  volume={21},
  number={6},
  pages={4287-4295},
  abstract={Generative artificial intelligence (GenAI) and digital twin (DT) technologies have individually demonstrated valuable capabilities across a range of fields. Their integration, however, offers a unique synergy with the potential to bring meaningful advancements in various sectors. In this survey, we explore the fusion of GenAI and DT, highlighting their combined ability to enhance insights, optimizations, and innovative solutions. We begin by clarifying the core principles of each technology and then outline their collaborative applications. Furthermore, we provide a detailed taxonomy of areas where GenAI has been leveraged within the realm of DT, and conversely, where DT technology has been enhanced through GenAI techniques. By systematically categorizing these applications, we aim to offer a clear perspective on the interplay between GenAI and DT across different sectors.},
  keywords={Medical services;Real-time systems;Taxonomy;Data models;Surveys;Digital twins;Solid modeling;Technological innovation;Synthetic data;Optimization;AI-enabled digital twins (DTs);DT;generative artificial intelligence (GenAI);simulation models;virtual modeling},
  doi={10.1109/TII.2025.3540473},
  ISSN={1941-0050},
  month={June},}@INPROCEEDINGS{11101058,
  author={Duran, Kubra and Cakir, Lal Verda and Ozdem, Mehmet and Gursu, Kerem and Canberk, Berk},
  booktitle={2024 IEEE Globecom Workshops (GC Wkshps)}, 
  title={Generative AI-enabled Digital Twins for 6G-enhanced Smart Cities}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={6G networks are envisioned to enable a wide range of applications, such as autonomous vehicles and smart cities. However, this rapid expansion of network topologies makes the management of 6G wireless networks more complex and leads to performance degradation. Even though state-of-the-art applications on network services are providing promising results, they also risk disrupting the network’s performance. To overcome this, the services have to leverage what-if implementations covering a variety of scenarios. At this point, traditional simulations fall short of encompassing the dynamism and complexity of a physical network. To overcome these challenges, we propose the Generative AI-based Digital Twins. For this, we derive an optimization formula to differentiate different network scenarios by considering the specific key performance indicators (KPIs) for wireless networks. Then, we fed this formula to the generative AI with the historical twins and real-time twins to start generating the desired topologies. To evaluate the performance, we implement network and smart-city-oriented services, namely massive connectivity, tiny instant communication, right-time synchronization, and truck path routes. The simulation results reveal that our approach can achieve 38% more stable network throughput in high device density scenarios. Furthermore, the generated scenario accuracy is able to reach up to 98% level, surpassing the baselines.},
  keywords={6G mobile communication;Accuracy;Smart cities;Network topology;Generative AI;Wireless networks;Throughput;Real-time systems;Digital twins;Synchronization;digital twin;generative artificial intelligence;6g;smart city},
  doi={10.1109/GCWkshp64532.2024.11101058},
  ISSN={2166-0077},
  month={Dec},}@INPROCEEDINGS{10944752,
  author={You, Jiahao and Jia, Ziye and Dong, Chao and Wu, Qihui and Han, Zhu},
  booktitle={2025 59th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios.},
  keywords={Adaptation models;Uncertainty;Heuristic algorithms;Computational modeling;Autonomous aerial vehicles;Prediction algorithms;Feature extraction;Trajectory;Vehicle dynamics;Optimization;Unmanned surface vehicles (USVs);unmanned aerial vehicles (UAVs);multi-access edge computing (MEC);multi-agent reinforcement learning (MARL);generative artificial intelligence (GAI)},
  doi={10.1109/CISS64860.2025.10944752},
  ISSN={2837-178X},
  month={March},}@INPROCEEDINGS{9409305,
  author={Botelho, Sergio and Joshi, Ameya and Khara, Biswajit and Rao, Vinay and Sarkar, Soumik and Hegde, Chinmay and Adavani, Santi and Ganapathysubramanian, Baskar},
  booktitle={2020 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC) and Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S)}, 
  title={Deep Generative Models that Solve PDEs: Distributed Computing for Training Large Data-Free Models}, 
  year={2020},
  volume={},
  number={},
  pages={50-63},
  abstract={Recent progress in scientific machine learning (SciML) has opened up the possibility of training novel neural network architectures that solve complex partial differential equations (PDEs). Several (nearly data free) approaches have been recently reported that successfully solve PDEs, with examples including deep feed forward networks, generative networks, and deep encoder-decoder networks. However, practical adoption of these approaches is limited by the difficulty in training these models, especially to make predictions at large output resolutions (≥ 1024 × 1024).Here we report on a software framework for data parallel distributed deep learning that resolves the twin challenges of training these large SciML models training in reasonable time as well as distributing the storage requirements. Our framework provides several out of the box functionality including (a) loss integrity independent of number of processes, (b) synchronized batch normalization, and (c) distributed higher-order optimization methods.We show excellent scalability of this framework on both cloud as well as HPC clusters, and report on the interplay between bandwidth, network topology and bare metal vs cloud. We deploy this approach to train generative models of sizes hitherto not possible, showing that neural PDE solvers can be viably trained for practical applications. We also demonstrate that distributed higher-order optimization methods are 2-3 × faster than stochastic gradient-based methods and provide minimal convergence drift with higher batch-size.},
  keywords={Training;Computational modeling;Neural networks;Optimization methods;Predictive models;Data models;Software;Deep generative models;Distributed training;PDEs;Loss functions;Cloud vs HPC;Higher-order optimization},
  doi={10.1109/MLHPCAI4S51975.2020.00013},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9217746,
  author={Piprek, Joachim},
  booktitle={2020 International Conference on Numerical Simulation of Optoelectronic Devices (NUSOD)}, 
  title={Connecting numerical simulation and machine learning: How to bridge the gap between theory and reality?}, 
  year={2020},
  volume={},
  number={},
  pages={105-106},
  abstract={Machine learning and numerical simulation represent opposite approaches to computational analysis of the real world, inductive vs. deductive. However, both methods suffer from various uncertainties and even their combination often fails to link theory and reality. This paper presents a critical review of such connections and proposes improvement options for optoelectronic devices.},
  keywords={Solid modeling;Numerical models;Computational modeling;Machine learning;Numerical simulation;Photonics;Optoelectronic devices;machine learning;deep learning;neural networks;artificial intelligence;numerical simulation;semiconductor;optoelectronic devices;photonics},
  doi={10.1109/NUSOD49422.2020.9217746},
  ISSN={2158-3242},
  month={Sep.},}@ARTICLE{9645169,
  author={Sun, Yuwei and Ochiai, Hideya and Esaki, Hiroshi},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness}, 
  year={2022},
  volume={3},
  number={6},
  pages={963-972},
  abstract={Wider coverage and a better solution to a latency reduction in 5G necessitate its combination with multi-access edge computing technology. Decentralized deep learning (DDL), such as federated learning and swarm learning, as a promising solution to privacy-preserving data processing for millions of smart edge devices leverages distributed computing of multilayer neural networks within the networking of local clients, without disclosing the original local training data. Notably, in industries such as finance and healthcare, where sensitive data of transactions and personal medical records are cautiously maintained, DDL can facilitate the collaboration among these institutes to improve the performance of trained models while protecting the data privacy of participating clients. In this survey article, we demonstrate the technical fundamentals of DDL that benefit many walks of society through decentralized learning. Furthermore, we offer a comprehensive overview of the current state of the art in the field by outlining the challenges of DDL and the most relevant solutions from novel perspectives of communication efficiency and trustworthiness.},
  keywords={Computational modeling;Information security;Edge computing;Deep learning;Artificial intelligence;Data privacy;Distributed computing;Collective intelligence;data privacy;distributed computing;edge computing;information security;multilayer neural network},
  doi={10.1109/TAI.2021.3133819},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{9093406,
  author={Kazemi, Hadi and Taherkhani, Fariborz and Nasrabadi, Nasser M.},
  booktitle={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Preference-Based Image Generation}, 
  year={2020},
  volume={},
  number={},
  pages={3393-3402},
  abstract={Deep generative models are a set of promising methods, that are able to model complex data and generate new samples. In principle, they learn to map a random latent code sampled from a prior distribution into a high dimensional data space, such as image space. However, these models have limited utilities as the user has minimal control over what the network produces. Despite the success of some recent work in learning an interpretable latent code, the field still lacks a coherent framework to learn a fully interpretable latent code, without any random part for sample diversity. Consequently, it is generally hard, if not impossible, for a non-expert user to produce a desired image by tuning the random and interpretable parts of the latent code. In this paper, we introduce the Preference-Based Image Generation (PbIG), a new method to retrieve the corresponding latent code of the user's mental image. We propose to adopt preference-based reinforcement learning, which learns from a user's judgment of the generated images by a pre-trained generative model. Since the proposed method is completely decoupled from the training stage of the underlying generative models, it can easily be adopted by any method, such as GANs and VAEs. We evaluate the effectiveness of PbIG framework using a set of experiments on baseline datasets using a pretraind StackGAN++.},
  keywords={Gallium nitride;Learning (artificial intelligence);Adaptation models;Image generation;Training;Generative adversarial networks;Data models},
  doi={10.1109/WACV45572.2020.9093406},
  ISSN={2642-9381},
  month={March},}@INBOOK{10954778,
  author={Orange, Erica},
  booktitle={AI + The New Human Frontier: Reimagining the Future of Time, Trust + Truth}, 
  title={Risks in Generative AI: Data Inbreeding}, 
  year={2024},
  volume={},
  number={},
  pages={55-59},
  abstract={Summary <p>In mythology, an ouroboros is a serpent&#x2010;like creature that consumes its own tail in a never&#x2010;ending loop. That is what may happen with generative AI&#x2014;generative models are increasingly consuming the outputs from other generative models. Data inbreeding is prone to the same mutations as when genetic inbreeding occurs. Data inbreeding will continue to pose threats, particularly as a large proportion of people paid to train AI models may be themselves outsourcing that work to AI. Data integrity is about maintaining the accuracy, reliability, consistency, and context of data throughout its entire life cycle. In addition to preserving the integrity of existing data, AI can also contribute to proactive measures for preventing breaches or unauthorized access. Some companies are starting to hire prompt engineers to help include the right references, hints, and keywords that can guide the AI towards the desired outcome.</p>},
  keywords={Artificial intelligence;Companies;Data models;Biological system modeling;Load modeling;Training;Prompt engineering;Leadership;Feeds;Chatbots},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394276998},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10954778},}@INPROCEEDINGS{9551043,
  author={Nneji, Grace Ugochi and Cai, Jingye and Jianhua, Deng and Monday, Happy Nkanta and Chikwendu, Ijeoma Amuche and Oluwasanmi, Ariyo and James, Edidiong Christopher and Mgbejime, Goodness Temofe},
  booktitle={2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Enhancing Low Quality in Radiograph Datasets Using Wavelet Transform Convolutional Neural Network and Generative Adversarial Network for COVID-19 Identification}, 
  year={2021},
  volume={},
  number={},
  pages={146-151},
  abstract={The coronavirus disease of 2019 (COVID-19) pandemic has caused a global public health epidemic since there is no 100% vaccine to cure or prevent the further spread of the virus. With the ever-increasing number of new infections, creating automated methods for COVID-19 identification of Chest X-ray images is critical to aiding clinical diagnosis and reducing the time-consumption for image interpretation. This paper proposes a novel joint framework for accurate COVID-19 identification by integrating an enhanced super-resolution generative adversarial network with a noise reduction filter bank of wavelet transform convolutional neural network on both Chest X-ray and Chest Tomography images for COVID-19 identification. The super-resolution utilized in this study is to enhance the image quality while the wavelet transform Convolutional Neural Network architecture is used to accurately identify COVID-19. Our proposed architecture is very robust to noise and vanishing gradient problem. We used public domain datasets of Chest x-ray images and Chest Tomography to train and check the performance of our COVID-19 identification task. This experiment shows that our system is consistently efficient by accuracy of 0.988, sensitivity of 0.994, and specificity of 0.987, AUC of 0.99, F1-score of 0.982 and 0.989 for precision using the Chest X-ray dataset while for Chest Tomography dataset, an accuracy of 0.978, sensitivity of 0.981, and specificity of 0.979, AUC of 0.985, F1-score of 0.961 and precision of 0.980. These performances have also outweighed other established state-of-the-art learning methods.},
  keywords={COVID-19;Wavelet transforms;Training;Sensitivity;Superresolution;Tomography;Generative adversarial networks;chest tomography;chest x-ray;COVID-19;deep learning;identification;super-resolution;wavelet transform},
  doi={10.1109/PRAI53619.2021.9551043},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10164925,
  author={Khan, Md Rahat Kader and Islam, Md Samiul and Al-Mukhtar, Mohammed and Porna, Shweta Bhattacharjee},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={SAGAN: Maximizing Fairness using Semantic Attention Based Generative Adversarial Network}, 
  year={2023},
  volume={3},
  number={},
  pages={721-726},
  abstract={In this work, we present an end-to-end framework with a predictor model that provides classifier outputs from given input features and an adversary that tries to predict protected or sensitive features in order to mitigate intrinsic biases with respect to sensitive features (e.g., race, sex). Our proposed model increases the predictor’s capacity to produce correct predictions, while decreasing the adversary’s ability to anticipate sensitive features. We include a novel Semantic Attention (SA) module to the framework and demonstrate that our SA based Generative Adversarial Network (SAGAN) is able to significantly minimize the adversary’s capability to predict sensitive features, while retaining the predictor’s predictive accuracy. UCI Adult (Census) dataset was used a benchmark dataset for the testing. Our results demonstrate that the predictive model does not lose much accuracy, while achieving a Disparate Impact (DI) score very close to 1. The flexibility of the method makes it fitting to be applicable to a broad spectrum of gradient-based learning models, including both regression and classification tasks as well as different definitions of fairness. The source code for the implementation is available on github [1].},
  keywords={Source coding;Semantics;Supervised learning;Pipelines;Predictive models;Generative adversarial networks;Natural language processing;SAGAN;Semantic Attention;GAN},
  doi={10.1109/ICIBA56860.2023.10164925},
  ISSN={},
  month={May},}@INPROCEEDINGS{9523078,
  author={Desai, Chaitra and Tabib, Ramesh Ashok and Reddy, Sai Sudheer and Patil, Ujwala and Mudenagudi, Uma},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={RUIG: Realistic Underwater Image Generation Towards Restoration}, 
  year={2021},
  volume={},
  number={},
  pages={2181-2189},
  abstract={In this paper, we present a novel method for generating synthetic underwater images considering revised image formation model. We propose to use the generated synthetic underwater images to train a conditional generative adversarial network (CGAN) towards restoration of degraded underwater images. Restoration of degraded underwater images using traditional dehazing models is challenging as they are insensitive to wavelength, depth, water type and treat backscattering and direct signal attenuation coefficients to be equal. However, learning based models for restoration perform well but sensitive to availability of ground truth information. Generating ground truth labels in underwater scenario demands in-situ measurements using expensive equipments and is infeasible due to varying underwater currents. Towards this, we propose to generate synthetic underwater images using revised image formation model. Revised image formation model is sensitive to different attenuation coefficients: 1) back scattering, 2) direct scattering and 3) veiling light. We propose to estimate these attenuation coefficients considering proven facts from the literature. We demonstrate restoration of real underwater images through restoration framework trained using rendered synthetic underwater images, and compare results of restoration with state-of-the-art techniques.},
  keywords={Computer vision;Image synthesis;Wavelength measurement;Current measurement;Conferences;Scattering;Attenuation},
  doi={10.1109/CVPRW53098.2021.00247},
  ISSN={2160-7516},
  month={June},}@ARTICLE{9760417,
  author={Zhou, Yaqian and Liu, Yu and Zhou, Heyu and Cheng, Zhiyong and Li, Xuanya and Liu, An-An},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Learning Transferable and Discriminative Representations for 2D Image-Based 3D Model Retrieval}, 
  year={2022},
  volume={32},
  number={10},
  pages={7147-7159},
  abstract={Existing research on the 2D image-based 3D model retrieval task focuses on learning transferable representations directly to narrow the domain discrepancy. However, it is not easy to achieve in practice due to the significant variations across two domains. In addition, some methods design a domain discriminator to distinguish the feature arising from source or target domains for transferable feature representations learning, which will lead to an unexpected deterioration of the feature discriminability. To settle these problems, we propose jointly learning transferable and discriminative representations for 2D image-based 3D model retrieval. Specifically, we extract features from the 2D images and 3D models (described as multiple views) by CNN. Considering the difficulty of directly narrowing the discrepancy of two domains, we are prone to connect 2D image and 3D model domains to an intermediate domain, where the domain gap aims to be eliminated. However, the feature transferability does not denote well discriminability. Based on the batch spectral penalization (BSP) theory, the feature transferability is dominated by feature vectors with higher singular values, while the feature discriminability depends on more eigenvectors with lower singular values to convey rich discriminative structures. Therefore, we penalize the largest singular values so that the feature vectors with lower singular values are appropriately enhanced, thereby strengthening feature discriminability. A series of experiments on two challenging datasets, MI3DOR and MI3DOR-2, indicate that our method can significantly improve performance.},
  keywords={Three-dimensional displays;Solid modeling;Adaptation models;Feature extraction;Error analysis;Data models;Generative adversarial networks;3D model retrieval;unsupervised domain adaptation;multi-view},
  doi={10.1109/TCSVT.2022.3168967},
  ISSN={1558-2205},
  month={Oct},}@INPROCEEDINGS{10823172,
  author={Arunkumar, Aluru and Pandiyarajan, Pandiselvam and Akshaya, S B L and Archana, Anthapu and Archana, Akula},
  booktitle={2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)}, 
  title={Renewable Energy Consumption on Solar and Wind Energy Prediction Using Deep Learning and Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={770-777},
  abstract={Solar and wind energy harvest electricity without producing harmful greenhouse gases or pollutants, considerably dipping their environmental impact when measured against fossil fuels. To forecast solar and wind energy production this paper suggests a method that chooses attributes with a strong contribution on the impact of prediction of energy production by ARIMA, Ridge and Lasso Regression. Time series data was obtained from open power systems for 37 European countries. Most importantly, improving the performance of the proposed model without regard to deep learning models is explored. In order to get insights into how the predictions are made by LSTM, the Explainable AI (XAI) method is used. The obtained local approximations to the model's decision boundary on certain predictions for specific instances. In addition, data from decision trees, random forests, and support vector machines (SVM) are used to validate it. 100% of the data was taken into consideration for validation after being split into 0.8% and 0.2 %, respectively, for training and testing. As the prediction values the amount of energy consumption of the year 2025 in solar is 72530 MW and wind energy is 45280 MW.},
  keywords={Deep learning;Support vector machines;Training;Renewable energy sources;Wind energy;Explainable AI;Production;Predictive models;Wind forecasting;Random forests;Storage of energy;Lime algorithm;solar;wind;renewable energy},
  doi={10.1109/ICICNIS64247.2024.10823172},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11069894,
  author={T R, Latha Yadav and R, Jagadeesha},
  booktitle={2025 3rd International Conference on Inventive Computing and Informatics (ICICI)}, 
  title={A Comprehensive Survey on Deep Learning Applications in Plant Disease Detection and Classification}, 
  year={2025},
  volume={},
  number={},
  pages={758-765},
  abstract={For agricultural production and food security to be guaranteed, plant leaf diseases must be accurately classified and detected. Conventional illness identification techniques are frequently timeconsuming, labor-intensive, and prone to human error. Plant disease detection has changed dramatically as a result of recent developments in artificial intelligence (AI), which have made automated, accurate, and scalable solutions possible. The most recent AI-driven approaches for classifying plant leaf diseases are examined in this review, including deep learning, machine learning, and hybrid models. In order to improve detection accuracy, it looks at different convolutional neural networks (CNNs), transfer learning strategies, and feature extraction algorithms. The report also highlights possible future research topics and addresses obstacles like data scarcity, model interpretability, and real-world deployment issues. By providing a comprehensive overview of AI-based approaches, this review aims to guide researchers and practitioners in developing more efficient and reliable plant disease classification systems.},
  keywords={Deep learning;Surveys;Precision agriculture;Plant diseases;Adaptation models;Reviews;Computational modeling;Transfer learning;Food security;Convolutional neural networks;Plant disease detection;artificial intelligence;deep learning;machine learning;convolutional neural networks;image classification;precision agriculture},
  doi={10.1109/ICICI65870.2025.11069894},
  ISSN={},
  month={June},}@ARTICLE{10138543,
  author={Lauenburg, Leander and Lin, Zudi and Zhang, Ruihan and Santos, Márcia dos and Huang, Siyu and Arganda-Carreras, Ignacio and Boyden, Edward S. and Pfister, Hanspeter and Wei, Donglai},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={3D Domain Adaptive Instance Segmentation via Cyclic Segmentation GANs}, 
  year={2023},
  volume={27},
  number={8},
  pages={4018-4027},
  abstract={3D instance segmentation for unlabeled imaging modalities is a challenging but essential task as collecting expert annotation can be expensive and time-consuming. Existing works segment a new modality by either deploying pre-trained models optimized on diverse training data or sequentially conducting image translation and segmentation with two relatively independent networks. In this work, we propose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN) that conducts image translation and instance segmentation simultaneously using a unified network with weight sharing. Since the image translation layer can be removed at inference time, our proposed model does not introduce additional computational cost upon a standard segmentation model. For optimizing CySGAN, besides the CycleGAN losses for image translation and supervised losses for the annotated source domain, we also utilize self-supervised and segmentation-based adversarial objectives to enhance the model performance by leveraging unlabeled target domain images. We benchmark our approach on the task of 3D neuronal nuclei segmentation with annotated electron microscopy (EM) images and unlabeled expansion microscopy (ExM) data. The proposed CySGAN outperforms pre-trained generalist models, feature-level domain adaptation models, and the baselines that conduct image translation and segmentation sequentially. Our implementation and the newly collected, densely annotated ExM zebrafish brain nuclei dataset, named NucExM, are publicly available at https://connectomics-bazaar.github.io/proj/CySGAN/index.html.},
  keywords={Image segmentation;Three-dimensional displays;Adaptation models;Solid modeling;Generative adversarial networks;Annotations;Computational modeling;3D instance segmentation;electron microscopy (EM);expansion microscopy (ExM);neuronal nuclei;unsupervised domain adaptation;zebrafish},
  doi={10.1109/JBHI.2023.3281332},
  ISSN={2168-2208},
  month={Aug},}@INPROCEEDINGS{10499754,
  author={Chamithri, Kosala and Asanka, P.P.G Dinesh},
  booktitle={2024 4th International Conference on Advanced Research in Computing (ICARC)}, 
  title={Identify the Current Demanded Knowledge, Skills, and Abilities (KSA) in IT through Analysis of Job Advertisements in Sri Lanka: A Framework Using Natural Language Processing Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={229-234},
  abstract={This research aims to develop a Natural Language Processing (NLP) framework to uncover the current demanded Knowledge, Skills, and Abilities (KSAs) in the Sri Lankan Information Technology (IT) sector. In the KSA, this paper focuses on professional qualifications and concepts, technical skills, and soft skills. Understanding the specific Knowledge, Skills, and Abilities demanded by industries is not only relevant to academics but also holds practical significance for job seekers, educational institutions, and employers. By employing Natural Language Processing techniques to analyze job advertisements, this research aims to bridge the skills gap, and support the relevant beneficiaries. The outcomes include the ability to use Generative AI for job advertisement annotation, an NLP framework to analyze job advertisements, identify the main areas in the IT sector according to ESCO (European Skills/Competences, Qualifications, and Occupations) classification-based customization and their specifically demanded KSAs for each domain. Importantly, the results also highlight that Knowledge and Skills vary across different domains, while Abilities remain common with slight changes.},
  keywords={Industries;Bridges;Generative AI;Annotations;Europe;Natural language processing;Information technology;artificial intelligence;generative artificial intelligence;information technology;knowledge-skills-abilities;natural language processing},
  doi={10.1109/ICARC61713.2024.10499754},
  ISSN={},
  month={Feb},}@ARTICLE{11157764,
  author={Bai, Shizhen and He, Hao and Han, Chunjia and Yang, Mu and Li, Zhifang and Fan, Weijia},
  journal={IEEE Transactions on Engineering Management}, 
  title={Light Trumps Shadow? How Generative AI Agent’s Language Arousal Influences Users’ Interactive Willingness: Evidence From Multimodal Analysis}, 
  year={2025},
  volume={72},
  number={},
  pages={3921-3936},
  abstract={This study investigates how language arousal in generative AI systems influences users’ interaction willingness, examining the roles of social identity and visual atmosphere. Drawing on the limited capacity model of motivated mediated message processing (LC4MP) and social identity theory, we constructed a theoretical model integrating language arousal, social identity, visual atmosphere, and interaction willingness, and analyzed 8809 interactions from Character. AI using multimodal methods combining linguistic analysis and visual processing. Our findings reveal that high-arousal language significantly increases interaction willingness, with social identity mediating this relationship. Most notably, we discovered a “psychological defense-curiosity paradox”: shadow visual atmospheres, despite triggering initial defensive reactions, enhance engagement more effectively than light atmospheres, challenging conventional “brighter is better” design assumptions. This research advances theory by repositioning language arousal as a direct causal variable in AI interaction, extending cognitive processing models to human–AI contexts, and demonstrating how visual elements strategically modulate psychological responses. These insights provide valuable direction for developing emotionally intelligent AI systems that effectively balance linguistic stimulation and visual atmosphere to create more engaging human–AI experiences.},
  keywords={Linguistics;Artificial intelligence;Visualization;Psychology;Atmospheric modeling;Generative AI;Atmospheric measurements;User experience;Social networking (online);Vocabulary;Human–AI interaction;language arousal of generative AI agent;multimodal analysis;psychological defense-curiosity paradox;social identity;visual atmosphere},
  doi={10.1109/TEM.2025.3608461},
  ISSN={1558-0040},
  month={},}@INPROCEEDINGS{11106547,
  author={Grauberger, Oleg and Wenz, Kai and Dörner, Marius and Schulz, Alexander},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Industrial Machine Data Generation and Artificial Optimisation for Blow Molding Extrusion Machines}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper introduces a novel framework developed under the AIDEAS project for automating the generation of simulation data and optimizing blow molding extrusion machine designs using evolutionary algorithms. The framework comprises three core components: a Machine Data Generator (MDG) for synthesizing extensive simulation datasets augmented with real or historical process data, a Machine Design Optimizer (MDO) leveraging AI-based techniques for design optimisation, and a CAx plugin enabling seamless integration with common CAD tools. By integrating digital twin technology, computational fluid dynamics (CFD) and evolutionary algorithms, the system enables iterative refinement of complex machine configurations, such as spiral distributors for blow molding, to achieve optimal performance. Engineers can efficiently define, simulate, and visualize machine designs in real-time using user-friendly front-end components and cloud-based services. Preliminary results demonstrate significant reductions in iteration cycles by up to 75%, material waste up to 66% and manual tuning efforts in manufacturer trials. Future developments will expand parametric capabilities and adapt the solution to additional polymer types and manufacturing processes, enhancing its applicability across the plastics industry.},
  keywords={Technological innovation;Spirals;Evolutionary computation;Manuals;Real-time systems;Generators;Fourth Industrial Revolution;Digital twins;Polymers;Optimization;Industry 4.0;Artificial Intelligence;CFDSimulation;Digital Twin;Automation;Design Optimisation;Evolutionary Algorithms;Machine Learning},
  doi={10.1109/ICE/ITMC65658.2025.11106547},
  ISSN={2693-8855},
  month={June},}@ARTICLE{11016754,
  author={Sönmez Sarikaya, Burcu and Bahtiyar, Şerif},
  journal={IEEE Access}, 
  title={GenAI-Based Jamming and Spoofing Attacks on UAVs}, 
  year={2025},
  volume={13},
  number={},
  pages={107596-107620},
  abstract={Recently, aerial vehicles have been more connected than ever, where there are many types of the vehicles. Uncrewed Aerial Vehicles (UAVs) operate on various environments with different technologies that are subject to many attacks. Creating effective intrusion detection systems against such attacks has been a significant challenge since there is a lack of sufficient attack data that can be used to design an intrusion detection system with advanced computing algorithms. In this research, we propose a novel framework to create attacks data for UAVs by using generative artificial intelligence algorithms. We use Variational Autoencoder, Gaussian Copula, Denoising Diffusion Probabilistic Model (DDPM), and Conditional Tabular Generative Adversarial Network to create synthetic attack data. Specifically, jamming and spoofing attacks on UAVs are generated to fool intrusion detection systems that may be implemented on UAVs. Experimental evaluations show that synthetically generated attack data reduces the accuracy of intrusion detections if the system was trained with inadequate attack data. Additionally, analysis results show that DDPM emerged as the most effective model for generating attack data, leading to F1 score reductions of 21% for jamming and 28% for spoofing attacks. This research highlights the need for more robust and adaptive intrusion detection systems that can be created with synthetic data. Thus, sustainable computing systems on UAVs will be achieved.},
  keywords={Autonomous aerial vehicles;Jamming;Global Positioning System;Security;Intrusion detection;Machine learning algorithms;Drones;Generative AI;Protocols;Data models;Cyber security;generative artificial intelligence;intrusion detection system;synthetic data;uncrewed aerial vehicles},
  doi={10.1109/ACCESS.2025.3574284},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10959369,
  author={Al-Barhamtoshy, Hassanin M. and Himdi, Tarik},
  booktitle={2025 2nd International Conference on Advanced Innovations in Smart Cities (ICAISC)}, 
  title={Data Generation in Healthcare Using Constraint Satisfaction Parameters (CSP)}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In this research, we examine the synthetic creation, alteration, and enhancement of data through Generative Artificial Intelligence (GAI). This inaugural study focused on generating healthy data via GAI for analysis and detection of chronic diseases (Depression, Stress, and Anxiety). Following the generation process using this deep learning approach, we analyze and visualize synthetic data through various metrics. The outcomes of this methodology are expected to yield novel insights into data generation concerning pressure-related diseases (depression, stress, and anxiety). Integrating high-quality synthetic data is anticipated to improve the production of financial transaction data. Finally, the proposed model is evaluated, and the results of the initial GAI model are generated. In this paper, we introduce a specialized generative Transformer language model that has been pre-trained on an extensive collection of medical constraint satisfaction parameters. We assess generation across medical natural language processing tasks and show that it surpasses existing models in most evaluations. Additionally, we offer a comprehensive analysis of the model's limitations by detailing numerous errors and challenges it faces. The mean scores for the produced descriptive data and shape pair trends in continuous columns were around 0.99 and 0.92, respectively. Conversely, the average scores for the categorical text generated in the initial region were notably high. This highlights the GAI model utilized in implementing each method. Our evaluation revealed that a majority of studies employ synthetic data generators to (i) reduce the costs and duration linked to clinical trials for rare diseases, (ii) enhance the predictive performance of AI models in personalized medicine, and (iii) provide researchers with access to high-quality, representative multimodal datasets while protecting sensitive patient information, among other advantages. Synthetic data generators based on deep learning are both effective and appropriate for use in descriptive contexts as well as under constraints involving AI.},
  keywords={Deep learning;Technological innovation;Generative AI;Anxiety disorders;Data collection;Depression;Transformers;Generators;Synthetic data;Diseases;Generative Artificial Intelligence;Synthetic data generators;Machine Learning;Deep Learning;Constraint Satisfaction Parameters)},
  doi={10.1109/ICAISC64594.2025.10959369},
  ISSN={},
  month={Feb},}@ARTICLE{11048518,
  author={Zhu, Botao and Wang, Xianbin and Zhang, Lei and Shen, Xuemin},
  journal={IEEE Network}, 
  title={Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI}, 
  year={2025},
  volume={39},
  number={5},
  pages={44-50},
  abstract={In collaborative systems with complex tasks relying on distributed resources, trust evaluation of potential collaborators has emerged as an effective mechanism for task completion. However, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment. In this paper, a novel progressive trust evaluation framework, namely chain-of-trust, is proposed to make better use of misaligned device attribute data. This framework, designed for effective task completion, divides the trust evaluation process into multiple chained stages based on task decomposition. At each stage, based on the task completion process, the framework only gathers the latest device attribute data relevant to that stage—leading to reduced trust evaluation complexity and overhead. By leveraging advanced incontext learning, few-shot learning, and reasoning capabilities, generative AI is then employed to analyze and interpret the collected data to produce correct evaluation results quickly. Only devices deemed trustworthy at this stage proceed to the next round of trust evaluation. The framework ultimately determines devices that remain trustworthy across all stages. Experimental results demonstrate that the proposed framework achieves high accuracy in trust evaluation.},
  keywords={Collaboration;Generative AI;Security;Cognition;Artificial intelligence;Accuracy;Training;Reliability;Complexity theory;Servers;Collaborative system;Generative AI;LLM;Trust evaluation},
  doi={10.1109/MNET.2025.3582407},
  ISSN={1558-156X},
  month={Sep.},}@INPROCEEDINGS{9213129,
  author={Wang, Mingxing},
  booktitle={2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Video Description with GAN}, 
  year={2020},
  volume={},
  number={},
  pages={10-13},
  abstract={Video description is to convert rich information of video data into text information, Which has been attracting broad research attention in the Artificial Intelligence Community. Deep learning has given computers a strong understanding of one-dimensional picture data and two-dimensional video data. However, In real application scenarios, it still faces the problem of insufficient robustness. For example, the generated text information is unreasonable, the scene information and semantic information rich in video data cannot be extracted effectively. GAN (Generative Adversarial Nets) is a model that generates data using countermeasures, in recent years, which is widely used in text generation, dialogue system, image synthesis, etc. However, there has not been much effort on exploring GAN for Video description. In this paper, we design a new discriminant network on the basis of the traditional text description. In addition, we add the long-short Time memory networks into the model to minimize the loss of information in the encoding or decoding process, so as to generate more reasonable sentences. Experimental results demonstrate that our proposed model exceeds most video description methods in public datasets.},
  keywords={Decoding;Visualization;Generators;Encoding;Convolutional neural networks;Feature extraction;Generative adversarial networks;long-short time memory networks;generative adversarial networks;video description},
  doi={10.1109/CCET50901.2020.9213129},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10595860,
  author={Gopi, Anitha and George, Elizabeth and RK, Rahul and James, Alex},
  booktitle={2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS)}, 
  title={AI for Antenna Design Re-engineering: Yes, Radiation Patterns Predict Antenna Structures!}, 
  year={2024},
  volume={},
  number={},
  pages={66-70},
  abstract={The identification of antenna type from radiation patterns with the view to re-engineer antenna designs using machine learning models is explored in this article. This technique can be deployed to identify the antenna type that is encapsulated or within a device without damaging the antenna. In this work, radiation patterns of different Patch, Monopole, and Dipole antenna are utilized and the dataset is increased using pixel samples with varying window shapes and sizes obtained through pixel sampling method. Different noise types including Gaussian, White, Pink, Speckle, and Salt and Pepper are added to this dataset to train and test the accuracy of different neural network models such as Convolutional Neural Network(CNN), YOLO, VG-19 Net and compared with Decision Tree, Naive Bayes,Random Forest, and K-Nearest Neighbours(KNN). The models can classify the antennas as dipole, monopole, and patch antenna with more than 98% accuracy.},
  keywords={YOLO;Accuracy;Dipole antennas;Patch antennas;Neural networks;Bayes methods;Decision trees;Antenna Pattern;Artificial Intelligence;Artificial Neural Network;Dipole Antenna;Monopole Antenna;Patch Antenna},
  doi={10.1109/AICAS59952.2024.10595860},
  ISSN={2834-9857},
  month={April},}@ARTICLE{10225523,
  author={Hsia, Shih-Chang and Wang, Szu-Hong and Yeh, Jen-Yu and Chang, Chuan-Yu},
  journal={IEEE Access}, 
  title={A Smart Leaf Blow Robot Based on Deep Learning Model}, 
  year={2023},
  volume={11},
  number={},
  pages={111956-111962},
  abstract={Although leaves are everywhere in the world, and they also play a vital role in our daily life, they tend to fall all over the ground in due course, thereby making it difficult for pedestrians and vehicles to move. In this paper, an automatic leaf blower was designed and based on the concept of convolutional neural network(CNN). This system can automatically collect leaves into a garbage bag. A four-wheel driving robot was implemented to drive a blow machine. The control sensors of this robot mainly include a camera, ultrasound, the electronic compass and acceleration. Besides, an ultra-wide band located module was used to obtain the position of the current robot during the working process. Also, the computer vision was employed to recognize whether the leaves are on the ground. For this, ResNet50 deep CNN was used as the training model to recognize the fallen leaves. Since there are many kinds of trees, their leaves are different shape. We collected the images of these leaves as dataset for training, and the recognition rate achieved 92.5%. The obtained result was sent to the controller to control the moving direction of the robot. For the real-time operation, the embedded system was used to sense the leaf data to decide the movement made by the machine based on a control algorithm. The CNN model was implemented with an accelerator on the embedded system for the real-time purpose, which the recognition speed can achieve 20 frames per second form the camera. The automatic leaf blow machine can be possibly used in an effective way instead of human power.},
  keywords={Robot kinematics;Navigation;Training;Mobile robots;Artificial intelligence;Robot vision systems;Vegetation;Convolutional neural networks;Leaf recognition;robot;blow machine;recognition;convolutional neural networks (CNNs)},
  doi={10.1109/ACCESS.2023.3307136},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9936810,
  author={Lee, Kin Wai and Yin Chin, Renee Ka},
  booktitle={2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={A Comparative Study of COVID-19 CT Image Synthesis using GAN and CycleGAN}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Generative adversarial networks (GANs) have been very successful in many applications of medical image synthesis, which hold great clinical value in diagnosis and analysis tasks, especially when data is scarce. This study compares the two most adopted generative modelling algorithms in recent medical image synthesis tasks, namely the traditional Generative Adversarial Network (GAN) and Cycle-consistency Generative Adversarial Network (CycleGAN) for COVID-19 CT image synthesis. Experiments show that very plausible synthetic COVID-19 images with a clear vision of artificially generated ground glass opacity (GGO) can be generated with CycleGAN when trained using an identity loss constant at 0.5. Moreover, it is found that the synthesis of the synthetic GGO features is generalized across images with different chest and lung structures, which suggests that diverse patterns of GGO can be synthesized using a conventional Image-to- Image translation setting without additional auxiliary conditions or visual annotations. In addition, similar experiment setting achieves encouraging perceptual quality with a Fréchet Inception Distance score of 0.347, which outperforms GAN at 0.383 and CycleGAN at 0.380 with an identity loss constant of 0.005. The experiment outcomes postulate a negative correlation between the strength of the identity loss and the significance of the synthetic instances manifested on the generated images, which highlights an interesting research path to improve the quality of generated images without compromising the significance of synthetic instances upon the image translation.},
  keywords={COVID-19;Adaptation models;Correlation;Image synthesis;Computed tomography;Generative adversarial networks;Loss measurement;generative adversarial networks;medical image synthesis;COVID-19;image-to-image translation;chest computerized tomography},
  doi={10.1109/IICAIET55139.2022.9936810},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10824403,
  author={Roufaida, Khebbache and Abdelhak, Merizig and Khaled, Rezeg},
  booktitle={2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)}, 
  title={Unveilling Soil Fertility Classification with Explainable AI}, 
  year={2024},
  volume={},
  number={},
  pages={501-507},
  abstract={Population growth necessitates the urgent enhancement of sustainable food production systems. However, the overuse of fertilizers significantly undermines soil fertility, posing a dual threat to agricultural productivity and environmental integrity. This paper introduces an innovative machine learning (ML) methodology integrated with interpretable artificial intelligence (IAI) aimed at promoting sustainable soil management practices. We conduct a thorough investigation into interpretable ML models specifically designed for the classification of soil fertility. Our approach meticulously analyzes model outcomes while pinpointing critical features that influence predictions regarding soil fertility. The results of this method exhibit remarkable promise, achieving high accuracy in predicting soil fertility.},
  keywords={Productivity;Technological innovation;Accuracy;Rural areas;Soil;Predictive models;Phosphorus;Nitrogen;Potassium;Random forests;Explainable AI;Smart farming;Machine learning;Soil fertility;Artificial intelligence;Agriculture 4.0},
  doi={10.1109/3ict64318.2024.10824403},
  ISSN={2770-7466},
  month={Nov},}@INPROCEEDINGS{9422042,
  author={Zhang, Wenbo},
  booktitle={2020 2nd International Conference on Information Technology and Computer Application (ITCA)}, 
  title={Sketch-to-Color Image with GANs}, 
  year={2020},
  volume={},
  number={},
  pages={322-325},
  abstract={Unsupervised Learning is a trending research field of artificial intelligence, which aims to interpret and understand the hidden structure of the data. However, the development of deep learning with Generative Adversarial Networks (GANs) creates more possibilities for unsupervised learning. GAN is a category of Neural Networks, which are mostly applied to generating images. In this paper, how GAN was implemented to help with sketch-to-color translation is illustrated. In order to achieve this goal, data-preprocessing is implemented first. Then, the model is trained for 65 epochs, and the performance of the model is improved by virtue of loss functions and optimizers. In the end, a proper User Interface (GUI) is designed to have a full application, and people could turn any sketch picture they want into a colored image.},
  keywords={Deep learning;Image synthesis;Computational modeling;Neural networks;Manuals;Computer applications;Generative adversarial networks;Colorization;Generative Adversarial Networks;Deep Learning},
  doi={10.1109/ITCA52113.2020.00075},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11071451,
  author={Wagner, Stefan and Barón, Marvin Muñoz and Falessi, Davide and Baltes, Sebastian},
  booktitle={2025 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE)}, 
  title={Towards Evaluation Guidelines for Empirical Studies Involving LLMs}, 
  year={2025},
  volume={},
  number={},
  pages={24-27},
  abstract={In the short period since the release of ChatGPT, large language models (LLMs) have changed the software engineering research landscape. While there are numerous opportunities to use LLMs for supporting research or software engineering tasks, solid science needs rigorous empirical evaluations. However, so far, there are no specific guidelines for conducting and assessing studies involving LLMs in software engineering research. Our focus is on empirical studies that either use LLMs as part of the research process or studies that evaluate existing or new tools that are based on LLMs. This paper contributes the first set of holistic guidelines for such studies. Our goal is to start a discussion in the software engineering research community to reach a common understanding of our standards for high-quality empirical studies involving LLMs.},
  keywords={Ethics;Reviews;Generative AI;Large language models;Focusing;Solids;Reproducibility of results;Standards;Guidelines;Software engineering;Large language models;generative artificial intelligence;empirical studies},
  doi={10.1109/WSESE66602.2025.00011},
  ISSN={},
  month={May},}@INPROCEEDINGS{10114244,
  author={Zheng, Jiaqi and Li, Yan},
  booktitle={2023 5th Asia Energy and Electrical Engineering Symposium (AEEES)}, 
  title={Research on Data Interpolation of Energy Storage Power Station Based on DC-GAN Network}, 
  year={2023},
  volume={},
  number={},
  pages={223-228},
  abstract={With the decline in the proportion of domestic traditional coal power generation, more and more lithium battery power stations have been put into use. There are thousands batteries in one station, then the safety of the battery in the power station become more and more important. Therefore, the power stations have stricter requirements on safety, economy, and reliability. How to ensure the safety and stability of power stations has become an urgent problem. However, due to various factors, the data from power station always have low sampling precision and long sampling intervals, which makes the data cannot meet the accuracy requirements of energy storage safety products. At the same time, Traditional machine learning algorithms and many artificial intelligence algorithms are difficult to forecast or monitor for the power station in this situation. This paper proposes a method of using a Deep Convolutional Generative Adversarial Network(DCGAN) to learn the existing high-sampling and high-precision data, and filling in the missing data caused by the long sampling interval in the power station. The traditional loss function of GAN has been improved to speed up the convergence of the network, and to solve collapse problem when traditional GAN Network training. Finally, the method proposed in this paper is verified by comparing the interpolated data generated by different models with the real data with high-sampling. Compared with the different traditional interpolation methods and traditional Linear GAN, in the case of different sampling time intervals, the method proposed in this paper is obvious more effective and it has more practical application value.},
  keywords={Training;Interpolation;Machine learning algorithms;Generative adversarial networks;Data models;Stability analysis;Safety;deep convolutional generative adversarial network(DCGAN);energy storage power station;large-capacity battery safety;safety monitoring;data interpolation},
  doi={10.1109/AEEES56888.2023.10114244},
  ISSN={},
  month={March},}@INPROCEEDINGS{11136352,
  author={B, Sastika. and Marappan, Raja},
  booktitle={2025 International Conference on Intelligent Computing and Knowledge Extraction (ICICKE)}, 
  title={AI-Driven Approaches for Enhancing Early Detection of Dental Caries}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Dental caries is a widespread oral health concern that requires early detection and prevention to minimize long-term complications. Despite progress in AI-driven diagnostics, challenges such as poor image quality, limited model generalizability, and lack of interpretability hinder clinical adoption. This review explores recent advances in applying artificial intelligence (AI) to dental caries detection and prevention, including deep learning, transfer learning, and self-supervised techniques. Methods like U-Net, ResNet, and ensemble CNNs have shown effectiveness in classification, segmentation, and progression prediction, achieving accuracies of 85–99%, precisions of 86–96%, and F1-scores of 87–94%. In prevention, AI has enabled risk assessment and personalized early intervention strategies. A novel framework that integrates digital twins with Generative AI, Reinforcement Learning, and Explainable AI for real-time, personalized diagnostics is proposed to address current limitations. This work lays the foundation for a comprehensive, interpretable, and adaptive AI-based dental caries management system that can transform future oral healthcare practices.},
  keywords={Generative AI;Explainable AI;Reviews;Prevention and mitigation;Transfer learning;Reinforcement learning;Transforms;Real-time systems;Dentistry;Digital twins;dental caries;personalized care;generative AI;reinforcement learning;explainable AI;digital twin},
  doi={10.1109/ICICKE65317.2025.11136352},
  ISSN={},
  month={June},}@INPROCEEDINGS{11112495,
  author={Gülüm, Semih and Kutal, Seçilay and Aptoula, Erchan and Yanıkoğlu, Berrin},
  booktitle={2025 33rd Signal Processing and Communications Applications Conference (SIU)}, 
  title={SSDV: An Offline Signature Detection and Verification Dataset}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Signatures have long been used as a basic means of authentication. With the increase in digitization and document forgery, signature verification has gained greater importance than ever before. This study introduces the SSDV (Sabanci Signature Detection and Verification) dataset, developed for signature detection and verification. SSDV provides a rich dataset that simulates real-world scenarios for signature verification and detection. This dataset, expanded with various types of noise and forged signature data, is designed to test and improve the robustness of signature verification systems. The scope of the dataset reflects the challenges encountered in current research and applications, providing a valuable resource for developing new models and algorithms in this field.},
  keywords={Phase noise;Handwriting recognition;Accuracy;Text analysis;Generative AI;Signal processing algorithms;Feature extraction;Digitization;Robustness;Forgery;signature detection;signature verification;document analysis;generative artificial intelligence},
  doi={10.1109/SIU66497.2025.11112495},
  ISSN={2165-0608},
  month={June},}@INPROCEEDINGS{10347899,
  author={Fan, Yannan and Wang, Bei},
  booktitle={2023 8th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)}, 
  title={A Filter-based GAN to Enhance Learning Frequency Information of EEG for Sleep Staging}, 
  year={2023},
  volume={8},
  number={},
  pages={236-241},
  abstract={In recent years, automatic sleep staging models based on artificial intelligence have been developed to provide physicians with more efficient diagnostic assistance support. However, the performance of automatic sleep staging models is limited by several problems such as insufficient available training data or imbalance among the classes. A common method is to manually synthesis the required data using generative adversarial networks(GANs). As an extremely informative signal in the frequency domain, existing electroencephalogram(EEG) synthesis methods pay little attention to this aspect. In this study, to enhance the learning of frequency domain information, a filter-based GAN is proposed for EEG signals synthesis. Rather than full-band learning, filtering allows the GAN to focus the learning on the frequency components containing the dominant rhythmic characteristics under different sleep stages. The results of several comparison experiments show that the filter-based GAN can synthesize higher quality EEG signals for sleep staging, with significantly improved scores on the three metrics compared to the baseline model.},
  keywords={Band-pass filters;Measurement;Sleep;Frequency-domain analysis;Training data;Generative adversarial networks;Brain modeling;EEG Signals Synthesis;Generative Adversarial Network;Filter;Dominant Rhythm;Sleep Staging},
  doi={10.1109/ICIIBMS60103.2023.10347899},
  ISSN={2189-8723},
  month={Nov},}@INPROCEEDINGS{11143466,
  author={Yuan, Weimin and Chen, Weilong and Nguyen, Hien and Zhu, Yifei and Wang, Dan and Han, Zhu},
  booktitle={2025 IEEE 26th International Workshop on Signal Processing and Artificial Intelligence for Wireless Communications (SPAWC)}, 
  title={Integrating Mean-Field Game Theory with Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Diffusion probabilistic models can be interpreted as a sequence of distribution transformations, wherein the inverse diffusion trajectory plays a pivotal role in applications ranging from image generation to wireless communication. Meanwhile, Mean Field Game (MFG) theory offers an effective framework for multi-agent control, enabling the regulation of distribution transitions while compressing the data and thereby reducing computational complexity. In this paper, we propose a novel framework, Mean Field Game Diffusion Model (MDM) that synergistically integrates MFG theory with diffusion models to improve both the efficiency and quality of image generation. Specifically, we formulate the reverse diffusion process as an MFG problem by treating the image as a data distribution and each pixel as an individual agent, thereby deriving governing equations to regulate their interactions. This approach optimizes the evolution trajectory of the data distribution by minimizing information-theoretic measures, such as the discrepancy between the generated and target distributions, thereby achieving high- quality generation while minimizing information loss. Comparative experiments across multiple datasets and diverse evaluation metrics demonstrate the effectiveness of MDM, providing a novel perspective that unifies generative modeling, game-theoretic optimization, and information theory.},
  keywords={Wireless communication;Image synthesis;Games;Diffusion models;Loss measurement;Data models;Trajectory;Computational complexity;Optimization;Information theory;Generative Artificial Intelligence;Diffusion Model;Mean-Field Game},
  doi={10.1109/SPAWC66079.2025.11143466},
  ISSN={1948-3252},
  month={July},}@INPROCEEDINGS{11019043,
  author={Krishna, Ch. Sai and Sai, S. Pavan and Roy, P. Dinesh and Sathya, V.},
  booktitle={2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)}, 
  title={AgroInsight Pro: Sustainable Agriculture Analytics & Decision Support System using GEN-AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Farmers today face massive issues. Weather conditions the future, the water level is low, and we have to generate more food for more people. Traditional farming practices which were appropriate for our grandparents aren’t adequate anymore. That’s why we developed AgroInsight Pro. This emerging technology utilizes AI to guide farmers to better decision-making. It collects valuable data like soil conditions, weather forecasting, crop yields, as well as evidence of infestation on plants. After that interprets all this information into simple, actionable advice that farmers can use immediately. We pilot-tested AgroInsight Pro with a farmer group, and the result was great. They used 25% less resources (such water, fertilizers, and pesticides and lost 30% less crop to diseases that would otherwise have been preventable. It was, according to this farmer, such as having farm and weather experts on standby all the time." What is interesting about AgroInsight Pro is that it is so easy to use. The farmers do not need to be tech-savvies. The system displays intricate farming information in a straightforward manner that anyone can comprehend. The more unpredictable the weather, such instruments like AgroInsight Pro increasingly come into play. By linking new technology with applied agricultural expertise, we’re helping create a more prosperous future for food production and agriculture.},
  keywords={Decision support systems;Generative AI;Weather forecasting;Transforms;Real-time systems;User experience;Crop yield;Water resources;Farming;Diseases;Agriculture;Artificial Intelligence;Decision Support Systems;Generative AI;Machine Learning;Precision Agriculture;Smart Farming;Sustainable Agriculture;Crop Management;Real-time Analytics},
  doi={10.1109/STCR62650.2025.11019043},
  ISSN={},
  month={May},}@INPROCEEDINGS{10981346,
  author={Morales-Chan, Miguel and Amado-Salvatierra, Hector R. and Hernandez-Rizzardini, Rocael and Román, Byron Linares},
  booktitle={2025 IEEE Engineering Education World Conference (EDUNINE)}, 
  title={Workshop: Transforming Student Interaction through Building Educational Chatbots in Engineering and Computing}, 
  year={2025},
  volume={},
  number={},
  pages={1-2},
  abstract={Educational chatbots offer a potential way to enhance student engagement and provide ongoing support throughout a course. This workshop is designed to empower educators with the skills to create educational chatbots tailored to classroom needs using the no-code platform Chatbase, alongside an introduction to OpenAI's API capabilities for non-programmers. Participants will explore the evolving capabilities of AI in creating chatbots that deliver local, course-specific context, such as assignment details, deadlines, and customized responses. The session emphasizes designing chatbot flows that support realistic educational interactions and integrate content specific to their courses. By the end of the workshop, educators will have a working prototype of a chatbot tailored to their specific class requirements, enhancing engagement in personalized learning. This hands-on experience offers an accessible path for educators to embrace AI-driven tools in education.},
  keywords={Generative AI;Conferences;Buildings;Prototypes;Chatbots;Engineering education;artificial intelligence;Chatbot;generative AI tools;LLMs},
  doi={10.1109/EDUNINE62377.2025.10981346},
  ISSN={},
  month={March},}@INPROCEEDINGS{9578611,
  author={Yang, Guoxing and Fei, Nanyi and Ding, Mingyu and Liu, Guangzhen and Lu, Zhiwu and Xiang, Tao},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={L2M-GAN: Learning to Manipulate Latent Space Semantics for Facial Attribute Editing}, 
  year={2021},
  volume={},
  number={},
  pages={2950-2959},
  abstract={A deep facial attribute editing model strives to meet two requirements: (1) attribute correctness – the target attribute should correctly appear on the edited face image; (2) irrelevance preservation – any irrelevant information (e.g., identity) should not be changed after editing. Meeting both requirements challenges the state-of-the-art works which resort to either spatial attention or latent space factorization. Specifically, the former assume that each attribute has well-defined local support regions; they are often more effective for editing a local attribute than a global one. The latter factorize the latent space of a fixed pretrained GAN into different attribute-relevant parts, but they cannot be trained end-to-end with the GAN, leading to sub-optimal solutions. To overcome these limitations, we propose a novel latent space factorization model, called L2M-GAN, which is learned end-to-end and effective for editing both local and global attributes. The key novel components are: (1) A latent space vector of the GAN is factorized into an attribute-relevant and irrelevant codes with an orthogonality constraint imposed to ensure disentanglement. (2) An attribute-relevant code transformer is learned to manipulate the attribute value; crucially, the transformed code are subject to the same orthogonality constraint. By forcing both the original attribute-relevant latent code and the edited code to be disentangled from any attribute-irrelevant code, our model strikes the perfect balance between attribute correctness and irrelevance preservation. Extensive experiments on CelebA-HQ show that our L2M-GAN achieves significant improvements over the state-of-the-arts.},
  keywords={Computer vision;Codes;Semantics;Transformers;Pattern recognition;Facial features;Faces},
  doi={10.1109/CVPR46437.2021.00297},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10639357,
  author={Meng, Tao and Shou, Yuntao and Ai, Wei and Yin, Nan and Li, Keqin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Deep Imbalanced Learning for Multimodal Emotion Recognition in Conversations}, 
  year={2024},
  volume={5},
  number={12},
  pages={6472-6487},
  abstract={The main task of multimodal emotion recognition in conversations (MERC) is to identify the emotions in modalities, e.g., text, audio, image, and video, which is a significant development direction for realizing machine intelligence. However, many data in MERC naturally exhibit an imbalanced distribution of emotion categories, and researchers ignore the negative impact of imbalanced data on emotion recognition. To tackle this problem, we systematically analyze it from three aspects: data augmentation, loss sensitivity, and sampling strategy, and propose the class boundary enhanced representation learning (CBERL) model. Concretely, we first design a multimodal generative adversarial network to address the imbalanced distribution of emotion categories in raw data. Second, a deep joint variational autoencoder is proposed to fuse complementary semantic information across modalities and obtain discriminative feature representations. Finally, we implement a multitask graph neural network with mask reconstruction and classification optimization to solve the problem of overfitting and underfitting in class boundary learning and achieve cross-modal emotion recognition. We have conducted extensive experiments on the interactive emotional dyadic motion capture (IEMOCAP) and multimodal emotion lines dataset (MELD) benchmark datasets, and the results show that CBERL has achieved a certain performance improvement in the effectiveness of emotion recognition. Especially on the minority class “fear” and “disgust” emotion labels, our model improves the accuracy and F1 value by 10% to 20%. Our code is publicly available at https://github.com/yuntaoshou/CBERL.},
  keywords={Emotion recognition;Data models;Semantics;Graph neural networks;Generative adversarial networks;Data augmentation;Feature extraction;Data augmentation;data imbalance;feature fusion;graph neural network;multimodal emotion recognition in conversations (MERC)},
  doi={10.1109/TAI.2024.3445325},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{9418417,
  author={Nikkath Bushra, S. and Uma Maheswari, K.},
  booktitle={2021 5th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={Crime Investigation using DCGAN by Forensic Sketch-to-Face Transformation (STF)- A Review}, 
  year={2021},
  volume={},
  number={},
  pages={1343-1348},
  abstract={This paper outlines about the most advanced technique of Artificial Intelligence for digitally ascertaining a criminal through facial recognition system by converting forensic sketch into a real photo using Deep Convolutional Generative Adversarial Network (DCGAN). Suppose a crime act is reported to a cop as an eyewitness by an individual by remembering certain set of facial features of an illicit and trying to imitate it in the form of hand drawn sketch based on the given information. The forensic sketch is pictured by an expert based on the verbal explanation given by a person after commitment of crime by a perpetrator. The rough sketch is given as an input to train the neural network and after several epochs the network quickly learns and generates a scrupulous realistic facial image of a suspect from the forensic sketch. This is very much helpful in crime investigations to obtain several such real photographs of a suspect from forensic sketches easily with precise details within a short period of time. This is an amazing practice that can produce real facial images of high resolution color photos from a low quality sketches which is incomplete or partial in nature with different pose variations like color, tone etc. It is very much useful in domains like forensics, law enforcement, Facial recognition system and security and authentication systems.},
  keywords={Image resolution;Image recognition;Image color analysis;Law enforcement;Forensics;Face recognition;Neural networks;Deep Convolutional Generative Adversarial Network (DCGAN);sketch-to-image;Crime Investigation;facial recognition},
  doi={10.1109/ICCMC51019.2021.9418417},
  ISSN={},
  month={April},}@INPROCEEDINGS{10757024,
  author={Kanca, Elif and Gulsoy, Tolgahan and Ayas, Selen and Kablan, Elif Baykal},
  booktitle={2024 Innovations in Intelligent Systems and Applications Conference (ASYU)}, 
  title={Generating Robust Adversarial Images in Medical Image Classification using GANs}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Rapid advancements in artificial intelligence and deep learning technologies have greatly improved the accurate classification of medical images, resulting in earlier diagnosis of disease. However, these advancements have also brought attention to the challenge of ensuring the resilience of models against ad- versarial medical images. In this study, we propose an adversarial attack algorithm that utilizes generative adversarial networks for medical image classification. This algorithm seamlessly integrates the target classifier into the attack generation framework, en- abling it to learn the distribution of adversarial examples closely resembling real data. Through extensive experiments across var- ious medical image classification datasets including fundoscopy, chest x-ray, skin lesion, and breast histology, the proposed attack algorithm achieves impressive attack success rates. Specifically, rates of 81.36%, 95.23%, 77.75%, and 51.67% are reported for each dataset, respectively. These findings underscore the critical importance of addressing adversarial vulnerabilities in medical image classification, ensuring the reliability and trustworthiness of diagnostic systems in clinical settings.},
  keywords={Technological innovation;Perturbation methods;Generators;Skin;Classification algorithms;Reliability;Medical diagnostic imaging;X-ray imaging;Image classification;Resilience;adversarial images;attack generation;generative adversarial networks;medical image classification},
  doi={10.1109/ASYU62119.2024.10757024},
  ISSN={2770-7946},
  month={Oct},}@ARTICLE{11087639,
  author={Li, Yang and Yang, Songlin and Wang, Wei and Dong, Jing},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Beyond Inserting: Learning Subject Embedding for Semantic-Fidelity Personalized Diffusion Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Text-to-Image (T2I) personalization based on advanced diffusion models (e.g., Stable Diffusion), which aims to generate images of target subjects given various prompts, has drawn huge attention. However, when users require personalized image generation for specific subjects such as themselves or their pet cat, the T2I models fail to accurately generate their subject-preserved images. The main problem is that pre-trained T2I models do not learn the T2I mapping between the target subjects and their corresponding visual contents. Even if multiple target subject images are provided, previous personalization methods either failed to accurately fit the subject region or lost the interactive generative ability with other existing concepts in T2I model space. For example, they are unable to generate T2I-aligned and semantic-fidelity images for the given prompts with other concepts such as scenes (“Eiffel Tower”), actions (“holding a basketball”), and facial attributes (“eyes closed”). In this paper, we focus on inserting accurate and interactive subject embedding into the Stable Diffusion Model for semantic-fidelity personalized generation using one image. We address this challenge from two perspectives: subject-wise attention loss and semantic-fidelity token optimization. Specifically, we propose a subject-wise attention loss to guide the subject embedding onto a manifold with high subject identity similarity and diverse interactive generative ability. Then, we optimize one subject representation as multiple per-stage tokens, and each token contains two disentangled features. This expansion of the textual conditioning space enhances the semantic control, thereby improving semantic-fidelity. We conduct extensive experiments on the most challenging subjects, face identities, to validate that our results exhibit superior subject accuracy and fine-grained manipulation ability. We further validate the generalization of our methods on various non-face subjects.},
  keywords={Faces;Face recognition;Accuracy;Diffusion models;Optimization;Circuits and systems;Transformers;Training;Text to image;Image synthesis;Generative Models;Text-to-Image Generation;Diffusion Models;Personalized Generation},
  doi={10.1109/TCSVT.2025.3588882},
  ISSN={1558-2205},
  month={},}@ARTICLE{10286330,
  author={Al Homssi, Bassel and Dakic, Kosta and Wang, Ke and Alpcan, Tansu and Allen, Ben and Boyce, Russell and Kandeepan, Sithamparanathan and Al-Hourani, Akram and Saad, Walid},
  journal={IEEE Communications Magazine}, 
  title={Artificial Intelligence Techniques for Next-Generation Massive Satellite Networks}, 
  year={2024},
  volume={62},
  number={4},
  pages={66-72},
  abstract={Space communications, particularly massive satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, massive satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models due to their dynamic and unique features, such as orbital speed, inter-satellite links, short pass time, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial ntelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly massive satellite network communications. It details the unique features of massive satellite networks, and the overarching challenges concomitant with their integration into the current communication infrastructure. Moreover, this article provides insights into state-of-the-art AI techniques across various layers of the communication link. This entails applying AI for forecasting the highly dynamic radio channel, spectrum sensing and classification, signal detection and demodulation, inter-satellite and satellite access network optimization, and network security. Moreover, future paradigms and the mapping of these mechanisms onto practical networks are outlined.},
  keywords={Satellite broadcasting;Artificial intelligence;Low earth orbit satellites;Satellites;Optimization;Forecasting;Next generation networking;Space communications},
  doi={10.1109/MCOM.004.2300277},
  ISSN={1558-1896},
  month={April},}@INPROCEEDINGS{9176782,
  author={Parthasarathy, Dhasarathy and Bäckstrom, Karl and Henriksson, Jens and Einarsdóttir, Sólrún},
  booktitle={2020 IEEE International Conference On Artificial Intelligence Testing (AITest)}, 
  title={Controlled time series generation for automotive software-in-the-loop testing using GANs}, 
  year={2020},
  volume={},
  number={},
  pages={39-46},
  abstract={Testing automotive mechatronic systems partly uses the software-in-the-loop approach, where systematically covering inputs of the system-under-test remains a major challenge. In current practice, there are two major techniques of input stimulation. One approach is to craft input sequences which eases control and feedback of the test process but falls short of exposing the system to realistic scenarios. The other is to replay sequences recorded from field operations which accounts for reality but requires collecting a well-labeled dataset of sufficient capacity for widespread use, which is expensive. This work applies the well-known unsupervised learning framework of Generative Adversarial Networks (GAN) to learn an unlabeled dataset of recorded in-vehicle signals and uses it for generation of synthetic input stimuli. Additionally, a metric-based linear interpolation algorithm is demonstrated, which guarantees that generated stimuli follow a customizable similarity relationship with specified references. This combination of techniques enables controlled generation of a rich range of meaningful and realistic input patterns, improving virtual test coverage and reducing the need for expensive field tests.},
  keywords={Gallium nitride;Time series analysis;Testing;Training;Measurement;Automotive engineering;Standards;software-in-the-loop;generative adversarial net-works;time series generation;latent space arithmetic},
  doi={10.1109/AITEST49225.2020.00013},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9543344,
  author={Liu, Yu},
  booktitle={2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering (CSAIEE)}, 
  title={Face Restoration Network with Feature Prior}, 
  year={2021},
  volume={},
  number={},
  pages={222-226},
  abstract={Recent works on blind face restoration mainly focus on reference-based methods, which made great progress in recovering high-frequency details and realistic texture from the real world low-quality (LQ) images. However, the multi-scale trait of LQ images is not fully utilized with these methods. Extra face reference also takes up much resources and brings redundant model parameters. In this paper, we introduce the face restoration network with feature prior (FP-FRN) consisting of an adversarial network with a multi-scale feature extraction network which utilizes the multi-scale facial feature to preserve low-level facial characteristics and predict high-level details. Compared to other state-of-the-art approaches, i.e., DFDNet, PSFR-GAN, out FP-FRN generates more realistic texture details and better preserved the low-level feature of the LQ images such as color and shape. As demonstrated by experiments on datasets of synthesized and real LQ images, FP-FRN is superior over other methods.},
  keywords={Training;Image segmentation;Shape;Image color analysis;Semantics;Feature extraction;Robustness;Blind Face Restoration;Feature Prior;Generative Adversarial Networks},
  doi={10.1109/CSAIEE54046.2021.9543344},
  ISSN={},
  month={Aug},}
