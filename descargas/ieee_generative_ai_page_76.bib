@INPROCEEDINGS{11140364,
  author={Shukla, Akshat and Sharma, Harsh and Prakash, Satya and Mehta, Kareena and Shivam, Shivam},
  booktitle={2025 6th International Conference for Emerging Technology (INCET)}, 
  title={Assessing the Long-Term Impact of AI Bias Mitigation Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This research works on assessing the long-term effects of AI bias mitigation techniques on different AI models and datasets like ResNet50 for image classification, BERT for sentiment analysis, and Logistic Regression for predictive policing, pre-, in- & post-processing techniques and fairness-aware deep learning models like FairGAN, Adversarial Debiasing & Fair Representation Learning. Results prove that initially there was a good reduction in bias but sustaining fairness over time is difficult because the bias re-appears with time, especially under dynamic environments. We underline that fairness is usually traded off for accuracy, and thus any technique should be evaluated according to multiple variables that consider various metrics and contextual factors. To tackle these challenges, we propose Dynamic Bias-Adaptive Learning (DBAL) a new framework which addresses these challenges by dynamically adjusting bias mitigation techniques based on observed fairness performance. This approach paves path for developing adaptive, scalable and responsible AI systems that are capable of sustaining fairness over time.},
  keywords={Deep learning;Training;Ethics;Analytical models;Accuracy;Prevention and mitigation;Computational modeling;Predictive models;Data models;Artificial intelligence;AI bias mitigation;fairness;long-term evaluation;Dynamic Bias-Adaptive Learning (DBAL);dynamic strategies;data drift;model updates;ethical AI},
  doi={10.1109/INCET64471.2025.11140364},
  ISSN={2996-4490},
  month={May},}@ARTICLE{9046288,
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Comprehensive Survey on Graph Neural Networks}, 
  year={2021},
  volume={32},
  number={1},
  pages={4-24},
  abstract={Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
  keywords={Deep learning;Neural networks;Task analysis;Kernel;Feature extraction;Data mining;Learning systems;Deep learning;graph autoencoder (GAE);graph convolutional networks (GCNs);graph neural networks (GNNs);graph representation learning;network embedding},
  doi={10.1109/TNNLS.2020.2978386},
  ISSN={2162-2388},
  month={Jan},}@INPROCEEDINGS{9156533,
  author={Li, Jingyuan and Wang, Ning and Zhang, Lefei and Du, Bo and Tao, Dacheng},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Recurrent Feature Reasoning for Image Inpainting}, 
  year={2020},
  volume={},
  number={},
  pages={7757-7765},
  abstract={Existing inpainting methods have achieved promising performance for recovering regular or small image defects. However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center. In this paper, we devise a Recurrent Feature Reasoning (RFR) network which is mainly constructed by a plug-and-play Recurrent Feature Reasoning module and a Knowledge Consistent Attention (KCA) module. Analogous to how humans solve puzzles (i.e., first solve the easier parts and then use the results as additional information to solve difficult parts), the RFR module recurrently infers the hole boundaries of the convolutional feature maps and then uses them as clues for further inference. The module progressively strengthens the constraints for the hole center and the results become explicit. To capture information from distant places in the feature map for RFR, we further develop KCA and incorporate it in RFR. Empirically, we first compare the proposed RFR-Net with existing backbones, demonstrating that RFR-Net is more efficient (e.g., a 4% SSIM improvement for the same model size). We then place the network in the context of the current state-of-the-art, where it exhibits improved performance. The corresponding source code is available at: https://github.com/jingyuanli001/RFR-Inpainting},
  keywords={Cognition;Convolution;Semantics;Merging;Convolutional codes;Correlation;Computational efficiency},
  doi={10.1109/CVPR42600.2020.00778},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9156566,
  author={Shao, Yuanjie and Li, Lerenhan and Ren, Wenqi and Gao, Changxin and Sang, Nong},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Domain Adaptation for Image Dehazing}, 
  year={2020},
  volume={},
  number={},
  pages={2805-2814},
  abstract={Image dehazing using learning-based methods has achieved state-of-the-art performance in recent years. However, most existing methods train a dehazing model on synthetic hazy images, which are less able to generalize well to real hazy images due to domain shift. To address this issue, we propose a domain adaptation paradigm, which consists of an image translation module and two image dehazing modules. Specifically, we first apply a bidirectional translation network to bridge the gap between the synthetic and real domains by translating images from one domain to another. And then, we use images before and after translation to train the proposed two image dehazing networks with a consistency constraint. In this phase, we incorporate the real hazy image into the dehazing training via exploiting the properties of the clear image (e.g., dark channel prior and image gradient smoothing) to further improve the domain adaptivity. By training image translation and dehazing network in an end-to-end manner, we can obtain better effects of both image translation and dehazing. Experimental results on both synthetic and real-world images demonstrate that our model performs favorably against the state-of-the-art dehazing algorithms.},
  keywords={Adaptation models;Training;Atmospheric modeling;Data models;Gallium nitride;Image color analysis;Bridges},
  doi={10.1109/CVPR42600.2020.00288},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9458279,
  author={Campello, Víctor M. and Gkontra, Polyxeni and Izquierdo, Cristian and Martín-Isla, Carlos and Sojoudi, Alireza and Full, Peter M. and Maier-Hein, Klaus and Zhang, Yao and He, Zhiqiang and Ma, Jun and Parreño, Mario and Albiol, Alberto and Kong, Fanwei and Shadden, Shawn C. and Acero, Jorge Corral and Sundaresan, Vaanathi and Saber, Mina and Elattar, Mustafa and Li, Hongwei and Menze, Bjoern and Khader, Firas and Haarburger, Christoph and Scannell, Cian M. and Veta, Mitko and Carscadden, Adam and Punithakumar, Kumaradevan and Liu, Xiao and Tsaftaris, Sotirios A. and Huang, Xiaoqiong and Yang, Xin and Li, Lei and Zhuang, Xiahai and Viladés, David and Descalzo, Martín L. and Guala, Andrea and Mura, Lucia La and Friedrich, Matthias G. and Garg, Ria and Lebel, Julie and Henriques, Filipe and Karakas, Mahir and Çavuş, Ersin and Petersen, Steffen E. and Escalera, Sergio and Seguí, Santi and Rodríguez-Palomares, José F. and Lekadir, Karim},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation: The M&Ms Challenge}, 
  year={2021},
  volume={40},
  number={12},
  pages={3543-3554},
  abstract={The emergence of deep learning has considerably advanced the state-of-the-art in cardiac magnetic resonance (CMR) segmentation. Many techniques have been proposed over the last few years, bringing the accuracy of automated segmentation close to human performance. However, these models have been all too often trained and validated using cardiac imaging samples from single clinical centres or homogeneous imaging protocols. This has prevented the development and validation of models that are generalizable across different clinical centres, imaging conditions or scanner vendors. To promote further research and scientific benchmarking in the field of generalizable deep learning for cardiac segmentation, this paper presents the results of the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge, which was recently organized as part of the MICCAI 2020 Conference. A total of 14 teams submitted different solutions to the problem, combining various baseline models, data augmentation strategies, and domain adaptation techniques. The obtained results indicate the importance of intensity-driven data augmentation, as well as the need for further research to improve generalizability towards unseen scanner vendors or new imaging protocols. Furthermore, we present a new resource of 375 heterogeneous CMR datasets acquired by using four different scanner vendors in six hospitals and three different countries (Spain, Canada and Germany), which we provide as open-access for the community to enable future research in the field.},
  keywords={Image segmentation;Heart;Training;Hospitals;Deep learning;Biomedical engineering;Protocols;Cardiovascular magnetic resonance;image segmentation;deep learning;generalizability;data augmentation;domain adaption;public dataset},
  doi={10.1109/TMI.2021.3090082},
  ISSN={1558-254X},
  month={Dec},}@ARTICLE{8822591,
  author={Pan, Shirui and Hu, Ruiqi and Fung, Sai-Fu and Long, Guodong and Jiang, Jing and Zhang, Chengqi},
  journal={IEEE Transactions on Cybernetics}, 
  title={Learning Graph Embedding With Adversarial Training Methods}, 
  year={2020},
  volume={50},
  number={6},
  pages={2475-2487},
  abstract={Graph embedding aims to transfer a graph into vectors to facilitate subsequent graph-analytics tasks like link prediction and graph clustering. Most approaches on graph embedding focus on preserving the graph structure or minimizing the reconstruction errors for graph data. They have mostly overlooked the embedding distribution of the latent codes, which unfortunately may lead to inferior representation in many cases. In this article, we present a novel adversarially regularized framework for graph embedding. By employing the graph convolutional network as an encoder, our framework embeds the topological information and node content into a vector representation, from which a graph decoder is further built to reconstruct the input graph. The adversarial training principle is applied to enforce our latent codes to match a prior Gaussian or uniform distribution. Based on this framework, we derive two variants of the adversarial models, the adversarially regularized graph autoencoder (ARGA) and its variational version, and adversarially regularized variational graph autoencoder (ARVGA), to learn the graph embedding effectively. We also exploit other potential variations of ARGA and ARVGA to get a deeper understanding of our designs. Experimental results that compared 12 algorithms for link prediction and 20 algorithms for graph clustering validate our solutions.},
  keywords={Task analysis;Training;Clustering algorithms;Generators;Convolutional codes;Decoding;Data models;Adversarial regularization;graph autoencoder;graph clustering;graph convolutional networks (GCNs);graph embedding;link prediction},
  doi={10.1109/TCYB.2019.2932096},
  ISSN={2168-2275},
  month={June},}@ARTICLE{8902220,
  author={Li, Lerenhan and Dong, Yunlong and Ren, Wenqi and Pan, Jinshan and Gao, Changxin and Sang, Nong and Yang, Ming-Hsuan},
  journal={IEEE Transactions on Image Processing}, 
  title={Semi-Supervised Image Dehazing}, 
  year={2020},
  volume={29},
  number={},
  pages={2766-2779},
  abstract={We present an effective semi-supervised learning algorithm for single image dehazing. The proposed algorithm applies a deep Convolutional Neural Network (CNN) containing a supervised learning branch and an unsupervised learning branch. In the supervised branch, the deep neural network is constrained by the supervised loss functions, which are mean squared, perceptual, and adversarial losses. In the unsupervised branch, we exploit the properties of clean images via sparsity of dark channel and gradient priors to constrain the network. We train the proposed network on both the synthetic data and real-world images in an end-to-end manner. Our analysis shows that the proposed semi-supervised learning algorithm is not limited to synthetic training datasets and can be generalized well to real-world images. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art single image dehazing algorithms on both benchmark datasets and real-world images.},
  keywords={Training;Semisupervised learning;Image color analysis;Image restoration;Convolution;Atmospheric modeling;Deep learning;Image dehazing;deep learning;semi-supervised learning},
  doi={10.1109/TIP.2019.2952690},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{9156294,
  author={Naseer, Muzammal and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Porikli, Fatih},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Self-supervised Approach for Adversarial Robustness}, 
  year={2020},
  volume={},
  number={},
  pages={259-268},
  abstract={Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model's parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the \textbf{unseen} adversarial attacks (\eg by reducing the success rate of translation-invariant \textbf{ensemble} attack from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection.},
  keywords={Perturbation methods;Task analysis;Distortion;Training;Robustness;Feature extraction;Neural networks},
  doi={10.1109/CVPR42600.2020.00034},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9745054,
  author={Wang, Kewei and Du, Shuaiyuan and Liu, Chengxin and Cao, Zhiguo},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Interior Attention-Aware Network for Infrared Small Target Detection}, 
  year={2022},
  volume={60},
  number={},
  pages={1-13},
  abstract={Infrared small target detection plays an important role in target warning, ground monitoring, and flight guidance. Existing methods typically utilize local-contrast information of each pixel to detect infrared small targets, neglecting the interior relation between target pixels or background pixels. The mere use of the local information of one pixel, however, is not sufficient for accurate detection, which may lead to missing detection and false alarms. As a harmonious whole, information between pixels are necessary to determine if a pixel belongs to the target or the background. Motivated by the fact that pixels from targets or backgrounds are correlated with each other, we propose a coarse-to-fine interior attention-aware network (IAANet) for infrared small target detection. Specifically, a region proposal network (RPN) is first applied to obtain coarse target regions and filter out backgrounds. Then, we leverage a transformer encoder to model the attention between pixels in coarse target regions, outputting attention-aware features. Finally, predictions are obtained by feeding attention-aware features to a classification head. Extensive experiments show that our approach is capable of detecting targets precisely, of suppressing a variety of false alarm sources, and works effectively in various background environments and target appearances. We show that our IAANet outperforms the state-of-the-art methods by a large margin. Code will be made available at: https://github.com/kwwcv/iaanet.},
  keywords={Semantics;Object detection;Visualization;Feature extraction;Convolutional neural networks;Proposals;Clutter;Attention-aware;coarse-to-fine;infrared small target detection},
  doi={10.1109/TGRS.2022.3163410},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9578572,
  author={Huang, Tao and Dong, Weisheng and Yuan, Xin and Wu, Jinjian and Shi, Guangming},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging}, 
  year={2021},
  volume={},
  number={},
  pages={16211-16220},
  abstract={In coded aperture snapshot spectral imaging (CASSI) system, the real-world hyperspectral image (HSI) can be reconstructed from the captured compressive image in a snapshot. Model-based HSI reconstruction methods employed hand-crafted priors to solve the reconstruction problem, but most of which achieved limited success due to the poor representation capability of these hand-crafted priors. Deep learning based methods learning the mappings between the compressive images and the HSIs directly achieved much better results. Yet, it is nontrivial to design a powerful deep network heuristically for achieving satisfied results. In this paper, we propose a novel HSI reconstruction method based on the Maximum a Posterior (MAP) estimation framework using learned Gaussian Scale Mixture (GSM) prior. Different from existing GSM models using hand-crafted scale priors (e.g., the Jeffrey’s prior), we propose to learn the scale prior through a deep convolutional neural network (DCNN). Furthermore, we also propose to estimate the local means of the GSM models by the DCNN. All the parameters of the MAP estimation algorithm and the DCNN parameters are jointly optimized through end-to-end training. Extensive experimental results on both synthetic and real datasets demonstrate that the proposed method outperforms existing state-of-the-art methods. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm.},
  keywords={GSM;Training;Image coding;Imaging;Estimation;Reconstruction algorithms;Apertures},
  doi={10.1109/CVPR46437.2021.01595},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9710802,
  author={Ghosh, Anindita and Cheema, Noshaba and Oguz, Cennet and Theobalt, Christian and Slusallek, Philipp},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Synthesis of Compositional Animations from Textual Descriptions}, 
  year={2021},
  volume={},
  number={},
  pages={1376-1386},
  abstract={"How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?" "How unstructured and complex can we make a sentence and still generate plausible movements from it?" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion, one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long complex sentences describing multiple sequential and compositional actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.},
  keywords={Manifolds;Solid modeling;Computer vision;Three-dimensional displays;Natural languages;Motion pictures;Animation;Vision + language;Gestures and body pose},
  doi={10.1109/ICCV48922.2021.00143},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{8692748,
  author={Guo, Haiyun and Zhu, Kuan and Tang, Ming and Wang, Jinqiao},
  journal={IEEE Transactions on Image Processing}, 
  title={Two-Level Attention Network With Multi-Grain Ranking Loss for Vehicle Re-Identification}, 
  year={2019},
  volume={28},
  number={9},
  pages={4328-4338},
  abstract={Vehicle re-identification (re-ID) aims to identify the same vehicle across multiple non-overlapping cameras, which is rather a challenging task. On the one hand, subtle changes in viewpoint and illumination condition can make the same vehicle look much different. On the other hand, different vehicles, even different vehicle models, may look quite similar. In this paper, we propose a novel Two-level Attention network supervised by a Multi-grain Ranking loss (TAMR) to learn an efficient feature embedding for the vehicle re-ID task. The two-level attention network consisting of hard part-level attention and soft pixel-level attention can adaptively extract discriminative features from the visual appearance of vehicles. The former one is designed to localize the salient vehicle parts, such as windscreen and car head. The latter one gives an additional attention refinement at pixel level to focus on the distinctive characteristics within each part. In addition, we present a multi-grain ranking loss to further enhance the discriminative ability of learned features. We creatively take the multi-grain relationship between vehicles into consideration. Thus, not only the discrimination between different vehicles but also the distinction between different vehicle models is constrained. Finally, the proposed network can learn a feature space, where both intra-class compactness and inter-class discrimination are well guaranteed. Extensive experiments demonstrate the effectiveness of our approach and we achieve state-of-the-art results on two challenging datasets, including VehicleID and Vehicle-1M.},
  keywords={Task analysis;Automotive components;Visualization;Feature extraction;Space vehicles;Cameras;Lighting;Two-level attention network;multi-grain ranking loss;vehicle re-identification;feature embedding},
  doi={10.1109/TIP.2019.2910408},
  ISSN={1941-0042},
  month={Sep.},}@INPROCEEDINGS{9423199,
  author={Zhao, An and Ding, Mingyu and Lu, Zhiwu and Xiang, Tao and Niu, Yulei and Guan, Jiechao and Wen, Ji-Rong},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Domain-Adaptive Few-Shot Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1389-1398},
  abstract={Existing few-shot learning (FSL) methods make the implicit assumption that the few target class samples are from the same domain as the source class samples. However, in practice, this assumption is often invalid –the target classes could come from a different domain. This poses an additional challenge of domain adaptation (DA) with few training samples. In this paper, the problem of domain-adaptive few-shot learning (DA-FSL) is tackled, which is expected to have wide use in real-world scenarios and requires solving FSL and DA in a unified framework. To this end, we propose a novel domain-adversarial prototypical network (DAPN) model. It is designed to address a specific challenge in DA-FSL: the DA objective means that the source and target data distributions need to be aligned, typically through a shared domain-adaptive feature embedding space; but the FSL objective dictates that the target domain per class distribution must be different from that of any source domain class, meaning aligning the distributions across domains may harm the FSL performance. How to achieve global domain distribution alignment whilst maintaining source/target per-class discriminativeness thus becomes the key. Our solution is to explicitly enhance the source/target per-class separation before domain-adaptive feature embedding learning, to alleviate the negative effect of domain alignment on FSL. Extensive experiments show that our DAPN outperforms the state-of-the-arts. The code is available at https://github.com/dingmyu/DAPN.},
  keywords={Training;Measurement;Bridges;Adaptation models;Computer vision;Conferences;Computational modeling},
  doi={10.1109/WACV48630.2021.00143},
  ISSN={2642-9381},
  month={Jan},}@ARTICLE{9612711,
  author={Gao, Junyu and Han, Tao and Yuan, Yuan and Wang, Qi},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Domain-Adaptive Crowd Counting via High-Quality Image Translation and Density Reconstruction}, 
  year={2023},
  volume={34},
  number={8},
  pages={4803-4815},
  abstract={Recently, crowd counting using supervised learning achieves a remarkable improvement. Nevertheless, most counters rely on a large amount of manually labeled data. With the release of synthetic crowd data, a potential alternative is transferring knowledge from them to real data without any manual label. However, there is no method to effectively suppress domain gaps and output elaborate density maps during the transferring. To remedy the above problems, this article proposes a domain-adaptive crowd counting (DACC) framework, which consists of a high-quality image translation and density map reconstruction. To be specific, the former focuses on translating synthetic data to realistic images, which prompts the translation quality by segregating domain-shared/independent features and designing content-aware consistency loss. The latter aims at generating pseudo labels on real scenes to improve the prediction quality. Next, we retrain a final counter using these pseudo labels. Adaptation experiments on six real-world datasets demonstrate that the proposed method outperforms the state-of-the-art methods.},
  keywords={Feature extraction;Image reconstruction;Decoding;Task analysis;Convolutional neural networks;Image segmentation;Head;Crowd counting;domain adaptation;image translation},
  doi={10.1109/TNNLS.2021.3124272},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{8918424,
  author={Wu, Jian and Hu, Changran and Wang, Yulong and Hu, Xiaolin and Zhu, Jun},
  journal={IEEE Transactions on Cybernetics}, 
  title={A Hierarchical Recurrent Neural Network for Symbolic Melody Generation}, 
  year={2020},
  volume={50},
  number={6},
  pages={2749-2757},
  abstract={In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.},
  keywords={Bars;Rhythm;Recurrent neural networks;Generators;Gallium nitride;Melody generation;recurrent neural network (RNN)},
  doi={10.1109/TCYB.2019.2953194},
  ISSN={2168-2275},
  month={June},}@ARTICLE{9399689,
  author={Li, Keqiuyin and Lu, Jie and Zuo, Hua and Zhang, Guangquan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multi-Source Contribution Learning for Domain Adaptation}, 
  year={2022},
  volume={33},
  number={10},
  pages={5293-5307},
  abstract={Transfer learning becomes an attractive technology to tackle a task from a target domain by leveraging previously acquired knowledge from a similar domain (source domain). Many existing transfer learning methods focus on learning one discriminator with single-source domain. Sometimes, knowledge from single-source domain might not be enough for predicting the target task. Thus, multiple source domains carrying richer transferable information are considered to complete the target task. Although there are some previous studies dealing with multi-source domain adaptation, these methods commonly combine source predictions by averaging source performances. Different source domains contain different transferable information; they may contribute differently to a target domain compared with each other. Hence, the source contribution should be taken into account when predicting a target task. In this article, we propose a novel multi-source contribution learning method for domain adaptation (MSCLDA). As proposed, the similarities and diversities of domains are learned simultaneously by extracting multi-view features. One view represents common features (similarities) among all domains. Other views represent different characteristics (diversities) in a target domain; each characteristic is expressed by features extracted in a source domain. Then multi-level distribution matching is employed to improve the transferability of latent features, aiming to reduce misclassification of boundary samples by maximizing discrepancy between different classes and minimizing discrepancy between the same classes. Concurrently, when completing a target task by combining source predictions, instead of averaging source predictions or weighting sources using normalized similarities, the original weights learned by normalizing similarities between source and target domains are adjusted using pseudo target labels to increase the disparities of weight values, which is desired to improve the performance of the final target predictor if the predictions of sources exist significant difference. Experiments on real-world visual data sets demonstrate the superiorities of our proposed method.},
  keywords={Feature extraction;Task analysis;Transfer learning;Learning systems;Training;Adaptation models;Visualization;Classification;deep learning;domain adaptation;transfer learning},
  doi={10.1109/TNNLS.2021.3069982},
  ISSN={2162-2388},
  month={Oct},}@ARTICLE{10049409,
  author={Hao, Xiaoyang and Feng, Zhixi and Yang, Shuyuan and Wang, Min and Jiao, Licheng},
  journal={IEEE Internet of Things Journal}, 
  title={Automatic Modulation Classification via Meta-Learning}, 
  year={2023},
  volume={10},
  number={14},
  pages={12276-12292},
  abstract={Internet of Things (IoT) networks are often subject to many malicious attacks in untrusted environments, and automatic modulation classification (AMC) is an effective way to combat IoT physical-layer threats. However, most existing AMC methods assume sufficient labeled signals and invariant signal distribution, which is often impossible in untrusted environments. In this article, a new meta-learning method is proposed for a few-shot AMC with distribution bias. First, a multi-frequency octave ResNet (MFOR) is constructed to learn coarse (low-frequency) and fine (high-frequency) features, which can efficiently identify the modulation type of the signal while saving computational resources. Second, a large number of classification-related meta-tasks are established for training MFOR to explore general knowledge in signal classification, and then transfer it to the AMC. Different with deep neural networks (DNNs) that learn a mapping by multiple instances, the MFOR with meta-learning (denoted as M-MFOR) can improve the generalization ability of new AMC tasks with very few instances and distribution bias. Furthermore, we find that the distribution bias between data can be reduced by adjusting the normalized distribution and propose a class-related mixup. Extensive experiments are taken on several datasets to investigate the effectiveness of M-MFOR. The results show its feasibility and superiority over existing methods.},
  keywords={Task analysis;Modulation;Internet of Things;Training;Convolutional neural networks;Frequency modulation;Feature extraction;Automatic modulation classification (AMC);distribution bias;few-shot;meta-learning;multi-frequency octave resnet (MFOR)},
  doi={10.1109/JIOT.2023.3247162},
  ISSN={2327-4662},
  month={July},}@ARTICLE{9677903,
  author={Wang, Wenhao and Zhao, Fang and Liao, Shengcai and Shao, Ling},
  journal={IEEE Transactions on Image Processing}, 
  title={Attentive WaveBlock: Complementarity-Enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-Identification and Beyond}, 
  year={2022},
  volume={31},
  number={},
  pages={1532-1544},
  abstract={Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause trouble in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameter-free module, the WaveBlock, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements on multiple UDA person re-identification tasks. We also prove the generality of the proposed method by applying it to vehicle re-identification and image classification tasks. Our codes and models are available at: AWB.},
  keywords={Task analysis;Neural networks;Clustering algorithms;Adaptation models;Training;Pipelines;Reliability;Person re-identification;unsupervised domain adaptation;attentive WaveBlock},
  doi={10.1109/TIP.2022.3140614},
  ISSN={1941-0042},
  month={},}@ARTICLE{9741755,
  author={Zhang, Hongyuan and Li, Pei and Zhang, Rui and Li, Xuelong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Embedding Graph Auto-Encoder for Graph Clustering}, 
  year={2023},
  volume={34},
  number={11},
  pages={9352-9362},
  abstract={Graph clustering, aiming to partition nodes of a graph into various groups via an unsupervised approach, is an attractive topic in recent years. To improve the representative ability, several graph auto-encoder (GAE) models, which are based on semisupervised graph convolution networks (GCN), have been developed and they have achieved impressive results compared with traditional clustering methods. However, all existing methods either fail to utilize the orthogonal property of the representations generated by GAE or separate the clustering and the training of neural networks. We first prove that the relaxed  $k$ -means will obtain an optimal partition in the inner-product distance used space. Driven by theoretical analysis about relaxed  $k$ -means, we design a specific GAE-based model for graph clustering to be consistent with the theory, namely Embedding GAE (EGAE). The learned representations are well explainable so that the representations can be also used for other tasks. To induce the neural network to produce deep features that are appropriate for the specific clustering model, the relaxed  $k$ -means and GAE are learned simultaneously. Meanwhile, the relaxed  $k$ -means can be equivalently regarded as a decoder that attempts to learn representations that can be linearly constructed by some centroid vectors. Accordingly, EGAE consists of one encoder and dual decoders. Extensive experiments are conducted to prove the superiority of EGAE and the corresponding theoretical analyses.},
  keywords={Convolution;Neural networks;Decoding;Task analysis;Laplace equations;Spectral analysis;Training;Graph auto-encoder (GAE);graph clustering;inner-product distance;relaxed k-means;unsupervised representation learning},
  doi={10.1109/TNNLS.2022.3158654},
  ISSN={2162-2388},
  month={Nov},}@ARTICLE{9337191,
  author={Wang, Qi and Han, Tao and Gao, Junyu and Yuan, Yuan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Neuron Linear Transformation: Modeling the Domain Shift for Crowd Counting}, 
  year={2022},
  volume={33},
  number={8},
  pages={3238-3250},
  abstract={Cross-domain crowd counting (CDCC) is a hot topic due to its importance in public safety. The purpose of CDCC is to alleviate the domain shift between the source and target domain. Recently, typical methods attempt to extract domain-invariant features via image translation and adversarial learning. When it comes to specific tasks, we find that the domain shifts are reflected in model parameters’ differences. To describe the domain gap directly at the parameter level, we propose a neuron linear transformation (NLT) method, exploiting domain factor and bias weights to learn the domain shift. Specifically, for a specific neuron of a source model, NLT exploits few labeled target data to learn domain shift parameters. Finally, the target neuron is generated via a linear transformation. Extensive experiments and analysis on six real-world data sets validate that NLT achieves top performance compared with other domain adaptation methods. An ablation study also shows that the NLT is robust and more effective than supervised and fine-tune training. Code is available at https://github.com/taohan10200/NLT.},
  keywords={Task analysis;Neurons;Feature extraction;Data models;Training;Supervised learning;Fuses;Crowd counting;domain adaptation (DA);few-shot learning;neuron linear transformation (NLT)},
  doi={10.1109/TNNLS.2021.3051371},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{10026339,
  author={Wang, Xiao and Tang, Ke and Dai, Xingyuan and Xu, Jintao and Xi, Jinhao and Ai, Rui and Wang, Yuxiao and Gu, Weihao and Sun, Changyin},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Safety-Balanced Driving-Style Aware Trajectory Planning in Intersection Scenarios With Uncertain Environment}, 
  year={2023},
  volume={8},
  number={4},
  pages={2888-2898},
  abstract={This paper proposes a two-stage trajectory planning method for self-driving vehicles (SDVs) in intersection scenarios with uncertain social circumstances while considering other traffic participants, which are human-driving vehicles (HDVs) with different driving styles. The mixture-of-experts approach is first utilized to learn from human-driving trajectory data to construct a multimodal motion planner, which uses a Transformer to model the interactions between vehicles by explicitly considering their driving styles to facilitate the integrated network to achieve scene-consistent multimodal trajectory prediction and candidate trajectory generation. Second, based on the generated trajectories for the SDV and the predicted trajectories for the other HDVs, each candidate planning trajectory is evaluated via a safety-balanced value function. After that, the trajectory with the highest value is selected for implementation. Such a method plans a safe and efficient driving trajectory in complex and uncertain scenarios. The experimental results demonstrate the efficiency and effectiveness of the designed method as well as the robustness and reasonableness of the SDVs' maneuver decisions at an intersection considering the behavioral dynamics of HDVs.},
  keywords={Trajectory;Planning;Trajectory planning;Safety;Predictive models;Training;Data models;Autonomous vehicles;trajectory planning;social interactions;uncertain circumstances},
  doi={10.1109/TIV.2023.3239903},
  ISSN={2379-8904},
  month={April},}@ARTICLE{9810916,
  author={Li, Shuo and Liu, Fang and Jiao, Licheng and Chen, Puhua and Li, Lingling},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Self-Supervised Self-Organizing Clustering Network: A Novel Unsupervised Representation Learning Method}, 
  year={2024},
  volume={35},
  number={2},
  pages={1857-1871},
  abstract={Deep learning-based clustering methods usually regard feature extraction and feature clustering as two independent steps. In this way, the features of all images need to be extracted before feature clustering, which consumes a lot of calculation. Inspired by the self-organizing map network, a self-supervised self-organizing clustering network ( $\text{S}^{3}$ OCNet) is proposed to jointly learn feature extraction and feature clustering, thus realizing a single-stage clustering method. In order to achieve joint learning, we propose a self-organizing clustering header (SOCH), which takes the weight of the self-organizing layer as the cluster centers, and the output of the self-organizing layer as the similarities between the feature and the cluster centers. In order to optimize our network, we first convert the similarities into probabilities which represents a soft cluster assignment, and then we obtain a target for self-supervised learning by transforming the soft cluster assignment into a hard cluster assignment, and finally we jointly optimize backbone and SOCH. By setting different feature dimensions, a Multilayer SOCHs strategy is further proposed by cascading SOCHs. This strategy achieves clustering features in multiple clustering spaces.  $\text{S}^{3}$ OCNet is evaluated on widely used image classification benchmarks such as Canadian Institute For Advanced Research (CIFAR)-10, CIFAR-100, Self-Taught Learning (STL)-10, and Tiny ImageNet. Experimental results show that our method significant improvement over other related methods. The visualization of features and images shows that our method can achieve good clustering results.},
  keywords={Feature extraction;Clustering algorithms;Representation learning;Training;Self-organizing feature maps;Clustering methods;Computer vision;Deep learning-based clustering;self-organizing map;self-supervised learning;unsupervised clustering algorithm;unsupervised representation learning},
  doi={10.1109/TNNLS.2022.3185638},
  ISSN={2162-2388},
  month={Feb},}@ARTICLE{9924606,
  author={Xu, Chenchu and Wang, Yifei and Zhang, Dong and Han, Longfei and Zhang, Yanping and Chen, Jie and Li, Shuo},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={BMAnet: Boundary Mining With Adversarial Learning for Semi-Supervised 2D Myocardial Infarction Segmentation}, 
  year={2023},
  volume={27},
  number={1},
  pages={87-96},
  abstract={Automatic segmentation of myocardial infarction (MI) regions in late gadolinium-enhanced cardiac magnetic resonance images is an essential step in the computed diagnosis of myocardial infarction. Most of the current myocardial infarction region segmentation methods are based on fully supervised deep learning. However, cardiologists' annotation of myocardial infarction regions in cardiac magnetic resonance images during the diagnosis process is time-consuming and expensive. This paper proposes a semi-supervised myocardial infarction segmentation. It consists of two models: 1) a boundary mining model and 2) an adversarial learning model. The boundary mining model can solve the boundary ambiguity problem by enlarging the gap between the foreground and background features, thus segmenting the myocardial infarction region accurately. The adversarial learning model can make the boundary mining model learn from additional unlabeled data by evaluating the segmentation performance and providing pseudo supervision, which significantly increases the robustness of the boundary mining model. We conduct extensive experiments on an in-house myocardial magnetic resonance dataset. The experimental results on six evaluation metrics demonstrate that our method achieves excellent results in myocardial infarction segmentation and outperforms the state-of-the-art semi-supervised methods.},
  keywords={Myocardium;Image segmentation;Magnetic resonance imaging;Data models;Adversarial machine learning;Training;Data mining;Semi-supervised learning;Myocardial infarction;Semantic segmentation;Adversarial learning},
  doi={10.1109/JBHI.2022.3215536},
  ISSN={2168-2208},
  month={Jan},}@INPROCEEDINGS{9710362,
  author={Guan, Dayan and Huang, Jiaxing and Xiao, Aoran and Lu, Shijian},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Domain Adaptive Video Segmentation via Temporal Consistency Regularization}, 
  year={2021},
  volume={},
  number={},
  pages={8033-8044},
  abstract={Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experience clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a domain adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regularization (TCR) for consecutive frames of target-domain videos. DA-VSN consists of two novel and complementary designs. The first is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconfident predictions of target frames to have similar temporal consistency as confident predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation network which outperforms multiple baselines consistently by large margins.},
  keywords={Computer vision;Adaptation models;Adaptive systems;Semantics;Data models;Adversarial machine learning;Task analysis;Video analysis and understanding;Segmentation;grouping and shape;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00795},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9186821,
  author={Gao, Junyu and Yang, Xiaoshan and Zhang, Yingying and Xu, Changsheng},
  journal={IEEE Transactions on Multimedia}, 
  title={Unsupervised Video Summarization via Relation-Aware Assignment Learning}, 
  year={2021},
  volume={23},
  number={},
  pages={3203-3214},
  abstract={We address the problem of unsupervised video summarization that automatically selects key video clips. Most state-of-the-art approaches suffer from two issues: (1) they model video clips without explicitly exploiting their relations, and (2) they learn soft importance scores over all the video clips to generate the summary representation. However, a meaningful video summary should be inferred by taking the relation-aware context of the original video into consideration, and directly selecting a subset of clips with a hard assignment. In this paper, we propose to exploit clip-clip relations to learn relation-aware hard assignments for selecting key clips in an unsupervised manner. First, we consider the clips as graph nodes to construct an assignment-learning graph. Then, we utilize the magnitude of the node features to generate hard assignments as the summary selection. Finally, we optimize the whole framework via a proposed multi-task loss including a reconstruction constraint, and a contrastive constraint. Extensive experimental results on three popular benchmarks demonstrate the favourable performance of our approach.},
  keywords={Feature extraction;Training;Optimization;Semantics;Recurrent neural networks;Task analysis;Graph neural network;unsupervised learning;video summarization},
  doi={10.1109/TMM.2020.3021980},
  ISSN={1941-0077},
  month={},}@ARTICLE{9457215,
  author={Liu, Haozhe and Zhang, Wentian and Liu, Feng and Wu, Haoqian and Shen, Linlin},
  journal={IEEE Transactions on Cybernetics}, 
  title={Fingerprint Presentation Attack Detector Using Global-Local Model}, 
  year={2022},
  volume={52},
  number={11},
  pages={12315-12328},
  abstract={The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This article thus proposes a global–local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by ~10% in terms of TDR (91.19% versus 80.74%).},
  keywords={Sensors;Ensemble learning;Self-supervised learning;Fingerprint recognition;Computer vision;Ensemble learning;presentation attack (PA) detection;rethinking strategy;self-supervised learning;weakly supervised learning},
  doi={10.1109/TCYB.2021.3081764},
  ISSN={2168-2275},
  month={Nov},}@INPROCEEDINGS{10203123,
  author={Chen, Guangyi and Chen, Zhenhao and Fan, Shunxing and Zhang, Kun},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={17874-17884},
  abstract={The indeterminate nature of human motion requires trajectory prediction systems to use a probabilistic model to formulate the multi-modality phenomenon and infer a finite set of future trajectories. However, the inference processes of most existing methods rely on Monte Carlo random sampling, which is insufficient to cover the realistic paths with finite samples, due to the long tail effect of the predicted distribution. To promote the sampling process of stochastic prediction, we propose a novel method, called BOsampler, to adaptively mine potential paths with Bayesian optimization in an unsupervised manner, as a sequential design strategy in which new prediction is dependent on the previously drawn samples. Specifically, we model the trajectory sampling as a Gaussian process and construct an acquisition function to measure the potential sampling value. This acquisition function applies the original distribution as prior and encourages exploring paths in the long-tail region. This sampling method can be integrated with existing stochastic predictive models without retraining. Experimental results on various baseline methods demonstrate the effectiveness of our method. The source code is released in this link.},
  keywords={Adaptation models;Source coding;Transportation;Gaussian processes;Tail;Predictive models;Sampling methods;Humans: Face;body;pose;gesture;movement},
  doi={10.1109/CVPR52729.2023.01714},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10063205,
  author={Hou, Yanrong and Liu, Ruixia and Shu, Minglei and Xie, Xiaoyun and Chen, Changfang},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Deep Neural Network Denoising Model Based on Sparse Representation Algorithm for ECG Signal}, 
  year={2023},
  volume={72},
  number={},
  pages={1-11},
  abstract={Electrocardiogram (ECG) denoising is very important for heart disease diagnosis. The traditional ECG denoising models have problems such as single noise type and poor interpretability of deep neural networks. The innovation of the proposed method is to incorporate the precious achievements of traditional methods into the design of neural networks and to build a bridge between them. Therefore, a novel interpretable deep denoising framework based on sparse representation is proposed in this study, and the half quadratic splitting (HQS) algorithm is applied to decompose the denoising method into sparse representations as an iterative solution process. In addition, a new weight distribution (WD) module is designed to extract adaptive hyperparameters based on ECG correlation instead of empirical values and greatly improves the efficiency of hyperparameter selection. To demonstrate the fairness and effectiveness of the proposed method, four different denoising models with different data preprocessing techniques are used for comparison. The extensive experimental validation and simulation studies demonstrated that the proposed framework has an excellent performance in quantitative and visual evaluation.},
  keywords={Noise reduction;Electrocardiography;Neural networks;Wavelet transforms;Signal processing algorithms;Deep learning;Adaptation models;Denoising;electrocardiogram (ECG);half quadratic splitting (HQS);neural network;sparse representation},
  doi={10.1109/TIM.2023.3251408},
  ISSN={1557-9662},
  month={},}@ARTICLE{9552010,
  author={Tang, Weixuan and Li, Bin and Barni, Mauro and Li, Jin and Huang, Jiwu},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Improving Cost Learning for JPEG Steganography by Exploiting JPEG Domain Knowledge}, 
  year={2022},
  volume={32},
  number={6},
  pages={4081-4095},
  abstract={Although significant progress has been achieved recently in automatic learning of steganographic cost, the existing methods designed for spatial images cannot be directly applied to JPEG images which are more common media in daily life. The difficulties of migration are mainly caused by the characteristics of the  $8\times 8$  DCT mode structure. To address the issue, in this paper we extend an existing automatic cost learning scheme to JPEG, where the proposed scheme called JEC-RL (JPEG Embedding Cost with Reinforcement Learning) is explicitly designed to tailor the JPEG DCT structure. It works with the embedding action sampling mechanism under reinforcement learning, where a policy network learns the optimal embedding policies via maximizing the rewards provided by an environment network. Following a domain-transition design paradigm, the policy network is composed of three modules, i.e., pixel-level texture complexity evaluation module, DCT feature extraction module, and mode-wise rearrangement module. These modules operate in serial, gradually extracting useful features from a decompressed JPEG image and converting them into embedding policies for DCT elements, while considering JPEG characteristics including inter-block and intra-block correlations simultaneously. The environment network is designed in a gradient-oriented way to provide stable reward values by using a wide architecture equipped with a fixed preprocessing layer with  $8\times 8$  DCT basis filters. Extensive experiments and ablation studies demonstrate that the proposed method can achieve good security performance for JPEG images against both advanced feature-based and modern CNN-based steganalyzers.},
  keywords={Costs;Discrete cosine transforms;Transform coding;Steganography;Correlation;Complexity theory;Additives;JPEG steganography;DCT coefficient;reinforcement learning;embedding policy;automatic cost learning},
  doi={10.1109/TCSVT.2021.3115600},
  ISSN={1558-2205},
  month={June},}@ARTICLE{10349694,
  author={Yang, Qi-Te and Zhan, Zhi-Hui and Liu, Xiao-Fang and Li, Jian-Yu and Zhang, Jun},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Grid Classification-Based Surrogate-Assisted Particle Swarm Optimization for Expensive Multiobjective Optimization}, 
  year={2024},
  volume={28},
  number={6},
  pages={1867-1881},
  abstract={SAEA, mainly including regression-based surrogate-assisted evolutionary algorithms (SAEAs) and classification-based SAEAs, are promising for solving expensive multiobjective optimization problems (EMOPs). Regression-based SAEAs usually use complex regression models to approximate the fitness evaluation, which will suffer from high-training costs to obtain a fine-accuracy surrogate. In contrast, classification-based SAEAs can achieve solution selection via coarse binary relations predicted by classifiers, thus avoiding high requirements in prediction accuracy and training costs. However, most of the binary relations in existing classification-based SAEAs mainly only involve convergence comparison whereas diversity maintenance is neglected. Considering the capacity of the grid technique in maintaining both convergence and diversity, we propose a new classification method called grid classification to discretize the objective space into grids and train a lightweight grid classification-based surrogate (GCS), for which low-training costs are needed. The GCS can evaluate the solution performance in terms of both convergence and diversity simultaneously according to the predicted grid locations, which opens up a new field for follow-up research on classification-based SAEAs. Following this, a GCS-assisted particle swarm optimization algorithm is proposed for tackling EMOPs. Experimental results on widely used benchmark problems (including high-dimensional EMOPs) and a 222-high-dimensional real-world application problem show its competitiveness in terms of both optimization performance and computational cost.},
  keywords={Training;Iron;Costs;Optimization;Convergence;Computational modeling;Classification algorithms;Evolutionary computation;expensive multiobjective optimization;grid classification;particle swarm optimization (PSO);surrogate-assisted evolutionary algorithm (SAEA)},
  doi={10.1109/TEVC.2023.3340678},
  ISSN={1941-0026},
  month={Dec},}@ARTICLE{10029939,
  author={Li, Keqiuyin and Lu, Jie and Zuo, Hua and Zhang, Guangquan},
  journal={IEEE Transactions on Cybernetics}, 
  title={Multidomain Adaptation With Sample and Source Distillation}, 
  year={2024},
  volume={54},
  number={4},
  pages={2193-2205},
  abstract={Unsupervised multidomain adaptation attracts increasing attention as it delivers richer information when tackling a target task from an unlabeled target domain by leveraging the knowledge attained from labeled source domains. However, it is the quality of training samples, not just the quantity, that influences transfer performance. In this article, we propose a multidomain adaptation method with sample and source distillation (SSD), which develops a two-step selective strategy to distill source samples and define the importance of source domains. To distill samples, the pseudo-labeled target domain is constructed to learn a series of category classifiers to identify transfer and inefficient source samples. To rank domains, the agreements of accepting a target sample as the insider of source domains are estimated by constructing a domain discriminator based on selected transfer source samples. Using the selected samples and ranked domains, transfer from source domains to the target domain is achieved by adapting multilevel distributions in a latent feature space. Furthermore, to explore more usable target information which is expected to enhance the performance across domains of source predictors, an enhancement mechanism is built by matching selected pseudo-labeled and unlabeled target samples. The degrees of acceptance learned by the domain discriminator are finally employed as source merging weights to predict the target task. Superiority of the proposed SSD is validated on real-world visual classification tasks.},
  keywords={Adaptation models;Training;Loss measurement;Transfer learning;Feature extraction;Cybernetics;Classification algorithms;Machine learning;Classification;domain adaptation;machine learning;transfer learning},
  doi={10.1109/TCYB.2023.3236008},
  ISSN={2168-2275},
  month={April},}@ARTICLE{9860082,
  author={Chen, Haoyu and Teng, Minggui and Shi, Boxin and Wang, Yizhou and Huang, Tiejun},
  journal={IEEE Transactions on Multimedia}, 
  title={A Residual Learning Approach to Deblur and Generate High Frame Rate Video With an Event Camera}, 
  year={2023},
  volume={25},
  number={},
  pages={5826-5839},
  abstract={Event cameras are bio-inspired cameras that can measure the intensity change asynchronously with high temporal resolution. One of the advantages of event cameras is that they suffer less from motion blur than traditional frame cameras when recording daily scenes with fast-moving objects. In this paper, we formulate the deblurring task on traditional cameras directed by events to be a residual learning one, and propose corresponding network architectures for effective learning of deblurring and high frame rate video generation tasks. We first train a modified U-Net network to restore a sharp image from a blurry image using the corresponding events. Then we train another similar network by replacing the downsampling blocks with blocks of the convolutional long short-term memory (Conv-LSTM) to recurrently generate high frame rate video using the restored sharp image and part of the events. Benefitting from the blur-free events and the proposed learning strategy, the experimental results show that the proposed method outperforms state-of-the-art methods for generating sharp images and high frame rate videos.},
  keywords={Cameras;Image restoration;Task analysis;Kernel;Interpolation;Feature extraction;Convolutional neural networks;Deblur;HFR video generation;event camera;residual learning},
  doi={10.1109/TMM.2022.3199556},
  ISSN={1941-0077},
  month={},}@ARTICLE{9874775,
  author={Liang, Ruxia and Zhang, Qian and Wang, Jianqiang and Lu, Jie},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Hierarchical Attention Network for Cross-Domain Group Recommendation}, 
  year={2024},
  volume={35},
  number={3},
  pages={3859-3873},
  abstract={Many online services allow users to participate in various group activities such as online meeting or group buying, and thus need to provide user groups with services that they are interested. The group recommender systems (GRSs) emerge as required and provide personalized services for various online user groups. Data sparsity is an important issue in GRSs, since even fewer group–item interactions are observed. Moreover, the group and the group members have complex and mutual relationships with each other, which exacerbates the difficulty in modeling the preferences of both a group and its members for recommendation. The cross-domain recommender system (CDRS) is a solution to alleviate data sparsity and assist preference modeling by transferring knowledge from a source domain which has relatively dense data to another. The existing CDRSs are usually developed for individual users and cannot be directly applied for group recommendation. To alleviate the data sparsity issue in GRSs, we first study the cross-domain group recommendation problem and propose a hierarchical attention network-based cross-domain group recommendation method, called HAN-CDGR. HAN-CDGR takes the advantage of data from a source domain to benefit recommendation generation for both the individual users and groups in the target domain which has data sparsity and cannot generate accurate recommendation. In HAN-CDGR, a hierarchical attention network is constructed to learn and model individual and group preferences, with consideration of both group members’ interactions and dynamic weights and the complex relationships between individuals and groups. Adversarial learning is used to effectively transfer knowledge from a source domain to the target domain. Extensive experiments, which demonstrate the effectiveness and superiority of our proposal, providing accurate recommendation for both individual users and groups, are conducted on three tasks.},
  keywords={Recommender systems;Data models;Task analysis;Neural networks;Adaptation models;Social networking (online);Knowledge engineering;Cross-domain recommender systems (CDRSs);group recommender systems (GRSs);hierarchical attention network;recommender systems},
  doi={10.1109/TNNLS.2022.3200480},
  ISSN={2162-2388},
  month={March},}@INPROCEEDINGS{9151077,
  author={Gao, Zhongpai and Zhang, Juyong and Guo, Yudong and Ma, Chao and Zhai, Guangtao and Yang, Xiaokang},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Semi-supervised 3D Face Representation Learning from Unconstrained Photo Collections}, 
  year={2020},
  volume={},
  number={},
  pages={1426-1435},
  abstract={Recovering 3D geometry shape, albedo, and lighting from a single image is a typical ill-posed problem. To address this challenging problem, we propose to utilize the joint constraints from unconstrained photo collections of one person to recover his or her identity shape and albedo. Unconstrained photo collections include one's photos captured under different times, backgrounds, and expressions, e.g., photos posted on Instagram. We train our model in a semi-supervised manner with adversarial loss to exploit large amounts of unconstrained facial images. A novel center loss is introduced to make sure that facial images from the same subject have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.},
  keywords={Face;Shape;Three-dimensional displays;Lighting;Image reconstruction;Solid modeling;Decoding},
  doi={10.1109/CVPRW50498.2020.00182},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{9812254,
  author={Huang, Xin and Rosman, Guy and Gilitschenski, Igor and Jasour, Ashkan and McGill, Stephen G. and Leonard, John J. and Williams, Brian C.},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)}, 
  title={HYPER: Learned Hybrid Trajectory Prediction via Factored Inference and Adaptive Sampling}, 
  year={2022},
  volume={},
  number={},
  pages={2906-2912},
  abstract={Modeling multi-modal high-level intent is important for ensuring diversity in trajectory prediction. Existing approaches explore the discrete nature of human intent before predicting continuous trajectories, to improve accuracy and support explainability. However, these approaches often assume the intent to remain fixed over the prediction horizon, which is problematic in practice, especially over longer horizons. To overcome this limitation, we introduce HYPER, a general and expressive hybrid prediction framework that models evolving human intent. By modeling traffic agents as a hybrid discrete-continuous system, our approach is capable of predicting discrete intent changes over time. We learn the probabilistic hybrid model via a maximum likelihood estimation problem and leverage neural proposal distributions to sample adaptively from the exponentially growing discrete space. The overall approach affords a better trade-off between accuracy and coverage. We train and validate our model on the Argoverse dataset, and demonstrate its effectiveness through comprehensive ablation studies and comparisons with state-of-the-art models.},
  keywords={Adaptation models;Maximum likelihood estimation;Automation;Predictive models;Probabilistic logic;Prediction algorithms;Inference algorithms},
  doi={10.1109/ICRA46639.2022.9812254},
  ISSN={},
  month={May},}@ARTICLE{9640532,
  author={Zhang, Qian and Liao, Wenhui and Zhang, Guangquan and Yuan, Bo and Lu, Jie},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Deep Dual Adversarial Network for Cross-Domain Recommendation}, 
  year={2023},
  volume={35},
  number={4},
  pages={3266-3278},
  abstract={Data sparsity is a common issue for most recommender systems and can severely degrade the usefulness of a system. One of the most successful solutions to this problem has been cross-domain recommender systems. These frameworks supplement the sparse data of the target domain with knowledge transferred from a source domain rich with data that is in some way related. However, there are three challenges that, if overcome, could significantly improve the quality and accuracy of cross-domain recommendation: 1) ensuring latent feature spaces of the users and items are both maximally matched; 2) taking consideration of user-item relationship and their interaction in modelling user preference; 3) enabling a two-way cross-domain recommendation that both the source and the target domains benefit from a knowledge exchange. Hence, in this paper, we propose a novel deep neural network called Dual Adversarial network for Cross-Domain Recommendation (DA-CDR). By training the shared encoders with a domain discriminator via dual adversarial learning, the latent feature spaces for both the users and items are maximally matched between the source and target domains. The domain-specific encoders are applied with an orthogonal constraint to ensure that any domain-specific features are properly extracted and work as supplement to the shared features. Allowing the two domains to collaboratively benefit from each other results in better recommendations for both domains. Extensive experiments with real-world datasets on six tasks demonstrate that DA-CDR significantly outperforms seven state-of-the-art baselines in terms of recommendation accuracy.},
  keywords={Feature extraction;Recommender systems;Knowledge transfer;Task analysis;Data mining;Knowledge engineering;Adversarial machine learning;Recommender systems;cross-domain recommendation;collaborative filtering;knowledge transfer},
  doi={10.1109/TKDE.2021.3132953},
  ISSN={1558-2191},
  month={April},}@ARTICLE{9918645,
  author={Li, Shuo and Liu, Fang and Jiao, Licheng and Liu, Xu and Chen, Puhua},
  journal={IEEE Transactions on Cybernetics}, 
  title={Learning Salient Feature for Salient Object Detection Without Labels}, 
  year={2023},
  volume={53},
  number={2},
  pages={1012-1025},
  abstract={Supervised salient object detection (SOD) methods achieve state-of-the-art performance by relying on human-annotated saliency maps, while unsupervised methods attempt to achieve SOD by not using any annotations. In unsupervised SOD, how to obtain saliency in a completely unsupervised manner is a huge challenge. Existing unsupervised methods usually gain saliency by introducing other handcrafted feature-based saliency methods. In general, the location information of salient objects is included in the feature maps. If the features belonging to salient objects are called salient features and the features that do not belong to salient objects, such as background, are called nonsalient features, by dividing the feature maps into salient features and nonsalient features in an unsupervised way, then the object at the location of the salient feature is the salient object. Based on the above motivation, a novel method called learning salient feature (LSF) is proposed, which achieves unsupervised SOD by LSF from the data itself. This method takes enhancing salient feature and suppressing nonsalient features as the objective. Furthermore, a salient object localization method is proposed to roughly locate objects where the salient feature is located, so as to obtain the salient activation map. Usually, the object in the salient activation map is incomplete and contains a lot of noise. To address this issue, a saliency map update strategy is introduced to gradually remove noise and strengthen boundaries. The visualization of images and their salient activation maps show that our method can effectively learn salient visual objects. Experiments show that we achieve superior unsupervised performance on a series of datasets.},
  keywords={Feature extraction;Visualization;Object detection;Training;Task analysis;Annotations;Semantics;Learning salient feature (LSF);saliency map refinement (SMR);salient object detection (SOD);salient object localization (SOL)},
  doi={10.1109/TCYB.2022.3209978},
  ISSN={2168-2275},
  month={Feb},}@ARTICLE{10472470,
  author={Liang, Tengfei and Jin, Yi and Liu, Wu and Wang, Tao and Feng, Songhe and Li, Yidong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Bridging the Gap: Multi-Level Cross-Modality Joint Alignment for Visible-Infrared Person Re-Identification}, 
  year={2024},
  volume={34},
  number={8},
  pages={7683-7698},
  abstract={Visible-Infrared person Re-IDentification (VI-ReID) is a challenging cross-modality image retrieval task that aims to match pedestrians’ images across visible and infrared cameras. To solve the modality gap, existing mainstream methods adopt a learning paradigm converting the image retrieval task into an image classification task with cross-entropy loss and auxiliary metric learning losses. These losses follow the strategy of adjusting the distribution of extracted embeddings to reduce the intra-class distance and increase the inter-class distance. However, such objectives do not precisely correspond to the final test setting of the retrieval task, resulting in a new gap at the optimization level. By rethinking these keys of VI-ReID, we propose a simple and effective method, the Multi-level Cross-modality Joint Alignment (MCJA), bridging both the modality and objective-level gap. For the former, we design the Visible-Infrared Modality Coordinator in the image space and propose the Modality Distribution Adapter in the feature space, effectively reducing modality discrepancy of the feature extraction process. For the latter, we introduce a new Cross-Modality Retrieval loss. It is the first work to constrain from the perspective of the ranking list in the VI-ReID, aligning with the goal of the testing stage. Moreover, to strengthen the robustness and cross-modality retrieval ability, we further introduce a Multi-Spectral Enhanced Ranking strategy for the testing phase. Based on the global feature only, our method outperforms existing methods by a large margin, achieving the remarkable rank-1 of 89.51% and mAP of 87.58% on the most challenging single-shot setting and all-search mode of the SYSU-MM01 dataset.},
  keywords={Training;Pedestrians;Feature extraction;Circuits and systems;Lighting;Identification of persons;Infrared imaging;Spectral analysis;Ranking (statistics);Person re-identification;cross-modality;visible-infrared;modality coordinator;distribution adapter;retrieval loss;multi-spectral enhanced ranking},
  doi={10.1109/TCSVT.2024.3377252},
  ISSN={1558-2205},
  month={Aug},}@ARTICLE{9732900,
  author={Liao, Wenhui and Zhang, Qian and Yuan, Bo and Zhang, Guangquan and Lu, Jie},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Heterogeneous Multidomain Recommender System Through Adversarial Learning}, 
  year={2023},
  volume={34},
  number={11},
  pages={8965-8977},
  abstract={To solve the user data sparsity problem, which is the main issue in generating user preference prediction, cross-domain recommender systems transfer knowledge from one source domain with dense data to assist recommendation tasks in the target domain with sparse data. However, data are usually sparsely scattered in multiple possible source domains, and in each domain (source/target) the data may be heterogeneous, thus it is difficult for existing cross-domain recommender systems to find one source domain with dense data from multiple domains. In this way, they fail to deal with data sparsity problems in the target domain and cannot provide an accurate recommendation. In this article, we propose a novel multidomain recommender system (called HMRec) to deal with two challenging issues: 1) how to exploit valuable information from multiple source domains when no single source domain is sufficient and 2) how to ensure positive transfer from heterogeneous data in source domains with different feature spaces. In HMRec, domain-shared and domain-specific features are extracted to enable the knowledge transfer between multiple heterogeneous source and target domains. To ensure positive transfer, the domain-shared subspaces from multiple domains are maximally matched by a multiclass domain discriminator in an adversarial learning process. The recommendation in the target domain is completed by a matrix factorization module with aligned latent features from both the user and the item side. Extensive experiments on four cross-domain recommendation tasks with real-world datasets demonstrate that HMRec can effectively transfer knowledge from multiple heterogeneous domains collaboratively to increase the rating prediction accuracy in the target domain and significantly outperforms six state-of-the-art non-transfer or cross-domain baselines.},
  keywords={Feature extraction;Recommender systems;Knowledge transfer;Task analysis;Transfer learning;Adversarial machine learning;Data mining;Adversarial learning;cross-domain recommendation recommender systems;knowledge transfer;recommender systems},
  doi={10.1109/TNNLS.2022.3154345},
  ISSN={2162-2388},
  month={Nov},}@INPROCEEDINGS{10377969,
  author={Pan, Wei and Zhu, Anna and Zhou, Xinyu and Iwana, Brian Kenji and Li, Shilin},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Few shot font generation via transferring similarity guided global style and quantization local style}, 
  year={2023},
  volume={},
  number={},
  pages={19449-19459},
  abstract={Automatic few-shot font generation (AFFG), aiming at generating new fonts with only a few glyph references, reduces the labor cost of manually designing fonts. However, the traditional AFFG paradigm of style-content disentanglement cannot capture the diverse local details of different fonts. So, many component-based approaches are proposed to tackle this problem. The issue with component-based approaches is that they usually require special pre-defined glyph components, e.g., strokes and radicals, which is infeasible for AFFG of different languages. In this paper, we present a novel font generation approach by aggregating styles from character similarity-guided global features and stylized component-level representations. We calculate the similarity scores of the target character and the referenced samples by measuring the distance along the corresponding channels from the content features, and assigning them as the weights for aggregating the global style features. To better capture the local styles, a cross-attention-based style transfer module is adopted to transfer the styles of reference glyphs to the components, where the components are self-learned discrete latent codes through vector quantization without manual definition. With these designs, our AFFG method could obtain a complete set of component-level style representations, and also control the global glyph characteristics. The experimental results reflect the effectiveness and generalization of the proposed method on different linguistic scripts, and also show its superiority when compared with other state-of-the-art methods. The source code can be found at https://github.com/awei669/VQ-Font.},
  keywords={Weight measurement;Computer vision;Costs;Codes;Vector quantization;Source coding;Design methodology},
  doi={10.1109/ICCV51070.2023.01787},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{10138342,
  author={Song, Ze and Wei, Xiaohui and Kang, Xudong and Li, Shutao and Liu, Jinyang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Toward Efficient Remote Sensing Image Change Detection via Cross-Temporal Context Learning}, 
  year={2023},
  volume={61},
  number={},
  pages={1-10},
  abstract={Change detection (CD) aims to find areas of specific changes in multitemporal remote sensing images. The existing methods fail to adequately explore the cross-temporal global context, making the establishment of spatial–temporal deep global associations insufficient and inefficient. As a result, their performance is vulnerable to complex and various objects in changing scenes. Hence, we propose a cross-temporal context learning network, termed as CCLNet, where the intratemporal and intertemporal long-range dependencies are mined and interactively fused, to fully exploit the cross-temporal context information. Specifically, a lightweight convolutional neural network (CNN) is first used to extract deep semantic features. Then, a well-designed cross-temporal fusion transformer (CFT) is proposed to locate the changing objects in the scene by establishing the long-range dependence across bitemporal images. Thanks to this, temporal-specific information extraction and cross-temporal information integration are seamlessly integrated into the same network, thereby significantly improving the discriminative features of changing objects. Furthermore, this allows us to use naive backbones with low computational costs to achieve reliable CD performance. Experiments on mainstream benchmarks show that our proposed method can handle CD tasks faster than state-of-the-art (SOTA) methods while maintaining better or comparable matching accuracy on a single RTX3090.},
  keywords={Feature extraction;Transformers;Semantics;Task analysis;Decoding;Context modeling;Convolutional neural networks;Attention mechanism;change detection (CD);high-resolution optical remote sensing image},
  doi={10.1109/TGRS.2023.3280902},
  ISSN={1558-0644},
  month={},}@ARTICLE{10746331,
  author={Kang, Xudong and Duan, Puhong and Li, Jier and Li, Shutao},
  journal={IEEE Transactions on Image Processing}, 
  title={Efficient Swin Transformer for Remote Sensing Image Super-Resolution}, 
  year={2024},
  volume={33},
  number={},
  pages={6367-6379},
  abstract={Remote sensing super-resolution (SR) technique, which aims to generate high-resolution image with rich spatial details from its low-resolution counterpart, play a vital role in many applications. Recently, more and more studies attempt to explore the application of Transformer in remote sensing field. However, they suffer from the high computational burden and memory consumption for remote sensing super-resolution. In this paper, we propose an efficient Swin Transformer (ESTNet) via channel attention for SR of remote sensing images, which is composed of three components. First, a three-layer convolutional operation is utilized to extract shallow features of the input low-resolution image. Then, a residual group-wise attention module is proposed to extract the deep features, which contains an efficient channel attention block (ECAB) and a group-wise attention block (GAB). Finally, the extracted deep features are reconstructed to generate high-resolution remote sensing images. Extensive experimental results proclaim that the proposed ESTNet can obtain better super-resolution results with low computational burden. Compared to the recently proposed Transformer-based remote sensing super-resolution method, the number of parameters is reduced by 82.68% while the computational cost is reduced by 87.84%. The code of the proposed ESTNet will be available at https://github.com/PuhongDuan/ESTNet for reproducibility.},
  keywords={Remote sensing;Superresolution;Feature extraction;Image reconstruction;Computational modeling;Current transformers;Deep learning;Sensors;Imaging;Spatial resolution;Remote sensing image;image super-resolution;Swin Transformer;channel attention},
  doi={10.1109/TIP.2024.3489228},
  ISSN={1941-0042},
  month={},}@ARTICLE{10654291,
  author={Chen, Bowen and Liu, Liqin and Liu, Chenyang and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Spectral-Cascaded Diffusion Model for Remote Sensing Image Spectral Super-Resolution}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={Hyperspectral remote sensing images (HSIs) have unique advantages in urban planning, precision agriculture, and ecology monitoring since they provide rich spectral information. However, hyperspectral imaging usually suffers from low spatial resolution and high cost, which limits the wide application of hyperspectral data. Spectral super-resolution provides a promising solution to acquire hyperspectral images with high spatial resolution and low cost, taking RGB images as input. Existing spectral super-resolution methods utilize neural networks following a single-shot framework, i.e., final results are obtained by one-stage spectral super-resolution, which struggles to capture and model the complex relationships between spectral bands. In this article, we propose a spectral-cascaded diffusion model (SCDM), a coarse-to-fine spectral super-resolution method based on the diffusion model. The diffusion model fits the real data distribution through stepwise denoising, which is naturally suitable for modeling rich spectral information. We cascade the diffusion model in the spectral dimension to gradually refine the spectral trends and enrich spectral information of the pixels. The cascade solves the highly ill-posed problem of spectral super-resolution step-by-step, mitigating the inaccuracies of previous single-shot approaches. To better utilize the potential of the diffusion model for spectral super-resolution, we design image condition mixture guidance (ICMG) to enhance the guidance of image conditions and progressive dynamic truncation (PDT) to limit cumulative errors in the sampling process. Experimental results demonstrate that our method achieves state-of-the-art performance in spectral super-resolution. Codes can be found at https://github.com/Mr-Bamboo/SCDM.},
  keywords={Superresolution;Diffusion models;Spatial resolution;Biological system modeling;Task analysis;Training;Image synthesis;Cascade-based methods;diffusion model;remote sensing;spectral super-resolution},
  doi={10.1109/TGRS.2024.3450874},
  ISSN={1558-0644},
  month={},}@ARTICLE{9869774,
  author={Cao, Shuning and Fang, Houzhang and Chen, Liqun and Zhang, Wei and Chang, Yi and Yan, Luxin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Robust Blind Deblurring Under Stripe Noise for Remote Sensing Images}, 
  year={2022},
  volume={60},
  number={},
  pages={1-17},
  abstract={The blind image deblurring methods have achieved great progress for Gaussian random noise. A few works have paid attention to image deblurring under structural noise, which is a very common degradation in multidetector imaging systems. This article considers the practical yet challenging problem of blind deblurring in the presence of the line-pattern stripe noise for remote sensing images. To overcome this issue, we explicitly formulate the structural noise into a novel and robust blind image deblurring framework. We observe that the structural line-pattern stripe noise would deteriorate both the kernel estimation and nonblind deblurring and propose a three-stage restoration framework to progressively estimate the blur kernel and clean image. Specifically, we first estimate an intermediate blur kernel by getting rid of the negative influence of the stripe noise in the unidirectional gradient domain. Next, a learning-based kernel refinement network is introduced to rectify the missing details of the inaccurate kernel. Finally, a low-rank decomposition-based nonblind deblurring model is proposed to simultaneously estimate the clean image and stripe noise. Experimental results on real and synthetic datasets demonstrate that the proposed robust blind image deblurring under stripe noise (RBDS) method outperforms the state-of-the-art blind deblurring methods.},
  keywords={Kernel;Image restoration;Estimation;Degradation;Task analysis;Imaging;Optimization;Blind deblurring;convolutional neural network (CNN);destriping;image restoration;low-rank representation},
  doi={10.1109/TGRS.2022.3202867},
  ISSN={1558-0644},
  month={},}@ARTICLE{10415256,
  author={Song, Ge and Huang, Kai and Su, Hanwen and Song, Fengyi and Yang, Ming},
  journal={IEEE Transactions on Multimedia}, 
  title={Deep Ranking Distribution Preserving Hashing for Robust Multi-Label Cross-Modal Retrieval}, 
  year={2024},
  volume={26},
  number={},
  pages={7027-7042},
  abstract={Deep supervised hashing techniques have exhibited remarkable efficiency in cross-modal retrieval tasks, because they enable the transformation of data from different modalities into compact binary codes that preserve semantic similarity structures. Nonetheless, existing methods often rely on pairwise or triplet relationships within known (or in-distribution) semantics during training, failing to capture the comprehensive ranking information inherent in web data that encompasses diverse concepts. In addition, these methods are vulnerable to out-of-distribution (OOD) semantic data when applied in realistic scenarios, resulting in suboptimal performance. In this paper, we propose ranking distribution preserving hashing (RDPH) to address these problems. We present a novel ranking loss, a differentiable surrogate that maximizes the NDCG metric for cross-modal retrieval. This loss incorporates two target ranking distributions derived from the ideal NDCG scores of samples and the cosine similarity of features. These distributions encourage RDPH to generate hash codes that approximate the desired inter-modal and intra-modal ranking distributions. To enhance the robustness of the hash codes against OOD data, RDPH leverages the CLIP paradigm to acquire OOD-resilient intermediate representations. Besides, we utilize the outlier exposure strategy to enhance the discriminative ability of OOD for hash codes under supervision by constructing auxiliary pseudo-OOD data from known data in feature space. Experiments on three datasets demonstrate that the proposed method achieves state-of-the-art performance on regular retrieval tasks and good results on simulated real-world retrieval tasks.},
  keywords={Codes;Semantics;Training;Correlation;Task analysis;Robustness;Hamming distances;Cross-modal retrieval;deep hashing;out-of-distribution;multi-label},
  doi={10.1109/TMM.2024.3358995},
  ISSN={1941-0077},
  month={},}@ARTICLE{9810896,
  author={Cong, Fuze and Xu, Shibiao and Guo, Li and Tian, Yinbing},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering}, 
  year={2022},
  volume={41},
  number={11},
  pages={3385-3397},
  abstract={Medical images contain various abnormal regions, most of which are closely related to the lesions or diseases. The abnormality or lesion is one of the major concerns during clinical practice and therefore becomes the key in answering questions about medical images. However, the recent efforts still focus on constructing a generic Visual Question Answering framework for medical-domain tasks, which is not adequate for practical medical requirements and applications. In this paper, we present two novel medical-specific modules named multiplication anomaly sensitive module and residual anomaly sensitive module to utilize weakly supervised anomaly localization information in medical Visual Question Answering. Firstly, the proposed multiplication anomaly sensitive module designed for anomaly-related questions can mask the feature of the whole image according to the anomaly location map. Secondly, the residual anomaly sensitive module could learn a flexible anomaly feature while preserving the information of the original questioned image, which is more helpful in answering anomaly-unrelated questions. Thirdly, the transformer decoder and multi-task learning strategy are combined to further enhance the question-reasoning ability and the model generalization performance. Finally, qualitative and quantitative experiments on a variety of medical datasets exhibit the superiority of the proposed approaches compared to the state-of-the-art methods.},
  keywords={Visualization;Medical diagnostic imaging;Feature extraction;Transformers;Question answering (information retrieval);Task analysis;Multitasking;Medical visual question answering;deep learning;weakly supervised anomaly localization},
  doi={10.1109/TMI.2022.3185113},
  ISSN={1558-254X},
  month={Nov},}@INPROCEEDINGS{10657098,
  author={Wang, Xiang and Zhang, Shiwei and Yuan, Hangjie and Qing, Zhiwu and Gong, Biao and Zhang, Yingya and Shen, Yujun and Gao, Changxin and Sang, Nong},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Recipe for Scaling up Text-to-Video Generation with Text-free Videos}, 
  year={2024},
  volume={},
  number={},
  pages={6572-6582},
  abstract={Diffusion-based text-to-video generation has witnessed impressive progress in the past year yet still falls behind text-to-image generation. One of the key reasons is the limited scale of publicly available data (e.g., 10M video-text pairs in WebVid10m vs. 5B image-text pairs in LAION), considering the high cost of video captioning. Instead, it could be far easier to collect unlabeled clips from video platforms like YouTube. Motivated by this, we come up with a novel text-to-video generation framework, termed TF-T2V, which can directly learn with text-free videos. The rationale behind is to separate the process of text decoding from that of temporal modeling. To this end, we employ a content branch and a motion branch, which are jointly optimized with weights shared. Following such a pipeline, we study the effect of doubling the scale of training set (i.e., video-only WebVid10M) with some randomly collected text-free videos and are encouraged to observe the performance improvement (FID from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of our approach. We also find that our model could enjoy sustainable performance gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some text labels for training. Finally, we validate the effectiveness and generalizability of our ideology on both native text-to-video generation and compositional video synthesis paradigms. Code and models will be publicly available at here.},
  keywords={Training;Video on demand;Scalability;Pipelines;Text to image;Performance gain;Market research},
  doi={10.1109/CVPR52733.2024.00628},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10681537,
  author={Hao, Xiaoyang and Feng, Zhixi and Peng, Tongqing and Yang, Shuyuan},
  journal={IEEE Internet of Things Journal}, 
  title={Meta-Learning Guided Label Noise Distillation for Robust Signal Modulation Classification}, 
  year={2025},
  volume={12},
  number={1},
  pages={402-418},
  abstract={Automatic modulation classification (AMC) has a wide range of applications in both civilian and military fields, such as industrial Internet of Things (IIoT) security, communication spectrum management, and military electronic countermeasures. However, label mislabeling often occurs in practical scenarios, significantly impacting the performance and robustness of deep neural networks (DNNs). In this article, we propose a meta-learning guided label noise distillation method to enhance the robustness of AMC models against label noise or errors. Specifically, we propose a teacher-student heterogeneous network (TSHN) to discriminate and distill label noise. Following the notion that labels represent information, a teacher network, utilizing trusted few-shot labeled samples, reevaluates and corrects labels for a considerable number of untrusted labeled samples through meta-learning. By dividing and conquering untrusted labeled samples according to their confidence levels, the student network learns more effectively. Additionally, we propose a multiview signal (MVS) method to further enhance the performance of hard-to-classify categories with few-shot trusted labeled samples. Extensive experiments on the RadioML2016 and HisarMod2019.1 data sets demonstrate that our methods significantly improve accuracy and robustness in signal AMC across diverse label noise scenarios, including symmetric, asymmetric, and mixed label noise. For example, compared to the baseline convolutional neural network with the cross-entropy loss, our proposed TSHN achieves a remarkable 1.26% to 36.84% accuracy improvement under symmetric label noise and 0.12% to 38.59% accuracy improvement under mixed label noise. Moreover, TSHN exhibits greater robustness to varying label noise rates compared to existing methods.},
  keywords={Noise;Robustness;Training;Noise measurement;Modulation;Industrial Internet of Things;Accuracy;Automatic modulation classification (AMC);few-shot trusted labeled samples;label noise;meta-learning;multiview signal (MVS)},
  doi={10.1109/JIOT.2024.3462544},
  ISSN={2327-4662},
  month={Jan},}@INPROCEEDINGS{9607803,
  author={Castillo, Angela and Escobar, María and Pérez, Juan C. and Romero, Andrés and Timofte, Radu and Van Gool, Luc and Arbelaez, Pablo},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Generalized Real-World Super-Resolution through Adversarial Robustness}, 
  year={2021},
  volume={},
  number={},
  pages={1855-1865},
  abstract={Real-world Super-Resolution (SR) has been traditionally tackled by first learning a specific degradation model that resembles the noise and corruption artifacts in low- resolution imagery. Thus, current methods lack generalization and lose their accuracy when tested on unseen types of corruption. In contrast to the traditional proposal, we present Robust Super-Resolution (RSR), a method that leverages the generalization capability of adversarial attacks to tackle real-world SR. Our novel framework poses a paradigm shift in the development of real-world SR methods. Instead of learning a dataset-specific degradation, we employ adversarial attacks to create difficult examples that target the model’s weaknesses. Afterward, we use these adversarial examples during training to improve our model’s capacity to process noisy inputs. We perform extensive experimentation on synthetic and real-world images and empirically demonstrate that our RSR method generalizes well across datasets without re-training for specific noise priors. By using a single robust model, we outperform state-of-the- art specialized methods on real-world benchmarks.},
  keywords={Degradation;Training;Adaptation models;Computational modeling;Conferences;Superresolution;Robustness},
  doi={10.1109/ICCVW54120.2021.00212},
  ISSN={2473-9944},
  month={Oct},}@ARTICLE{9798797,
  author={Chen, Zhuo and Yin, Fei and Yang, Qing and Liu, Cheng-Lin},
  journal={IEEE Transactions on Multimedia}, 
  title={Cross-Lingual Text Image Recognition via Multi-Hierarchy Cross-Modal Mimic}, 
  year={2023},
  volume={25},
  number={},
  pages={4830-4841},
  abstract={Optical character recognition and machine translation are usually studied and applied separately. In this paper, we consider a new problem named cross-lingual text image recognition (CLTIR) that integrates these two tasks together. The core of this problem is to recognize source language texts shown in images and transcribe them to the target language in an end-to-end manner. Traditional cascaded systems perform text image recognition and text translation sequentially. This can lead to error accumulation and parameter redundancy problems. To overcome these problems, we propose a multihierarchy cross-modal mimic (MHCMM) framework for end-to-end CLTIR, which can be trained with a massive bilingual text corpus and a small number of bilingual annotated text images. In this framework, a plug-in machine translation model is used as a teacher to guide the CLTIR model for learning representations compatible with image and text modes. Via adversarial learning and attention mechanisms, the proposed mimic method can integrate both global and local information in the semantic space. Experiments on a newly collected dataset demonstrate the superiority of the proposed framework. Our method outperforms other pipelines while containing fewer parameters. Additionally, the MHCMM framework can utilize a large-scale bilingual corpus to further improve the performance efficiently. The visualization of attention scores indicates that the proposed model can read text images in a fashion similar to the machine translation model reading text tokens.},
  keywords={Image recognition;Text recognition;Computational modeling;Task analysis;Machine translation;Hidden Markov models;Decoding;Cross-lingual text image recognition;cross-modal mimic;multihierarchy mimic},
  doi={10.1109/TMM.2022.3183386},
  ISSN={1941-0077},
  month={},}@ARTICLE{10460313,
  author={Yang, Chen and Liu, Huiling and Yang, Shuyuan and Feng, Zhixi and Tang, Xiaogang and Zhang, Feng},
  journal={IEEE Internet of Things Journal}, 
  title={Open-Set Radar Emitter Recognition via Deep Metric Autoencoder}, 
  year={2024},
  volume={11},
  number={10},
  pages={18281-18291},
  abstract={In the noncooperative electromagnetic environment, new radar emitters will emerge unexpectedly during the test phase, which brings the “open-set” radar emitter recognition (OS-RER). Conventional classifiers cannot identify new radar emitters that do not exist in the training data set. Therefore, in this article, a novel deep metric autoencoder (DMAE) is proposed for OS-RER. In DMAE, deep metric learning learns new nonlinear mappings in the metric space to measure the similarity between instances. The dual-path deep autoencoder is designed to reduce the open space risk by learning a low-dimensional manifold and a discriminative representation of known instances. Specifically, DMAE models known classes, and measures class belongingness through the reconstruction error of the AE and the entropy of the classifier. The deep metric network learns a more precise distance metric by minimizing the distance between the known class instances and the corresponding reconstruction. To accurately detect unknown instances, the classifier and the deep metric network are used together to preliminarily detect unknown instances. Finally, the detected unknown instances are used to further train the classifier to recognize the radar emitter in the open-set scenarios. The DMAE learns the discriminative representation through end-to-end learning. Extensive experiments conducted on real radar data sets and simulated radar data sets show that DMAE can identify unknown emitters and significantly outperforms existing open-set classification methods.},
  keywords={Radar;Measurement;Feature extraction;Radar detection;Training;Spaceborne radar;Manifolds;Dual-path deep autoencoder;dual-rejection strategy;open-set classification (OSC);radar emitter recognition (RER)},
  doi={10.1109/JIOT.2024.3361899},
  ISSN={2327-4662},
  month={May},}@ARTICLE{9085372,
  author={Xu, Lu and Hu, Chen and Tao, Ji’an and Xue, Jianru and Mei, Kuizhi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Improve Regression Network on Depth Hand Pose Estimation With Auxiliary Variable}, 
  year={2021},
  volume={31},
  number={3},
  pages={890-904},
  abstract={The regression based deep neural networks have achieved state-of-the-arts performance on depth 3D hand pose estimation task. This paper focuses on improving the regression mapping between features and pose joints. Inspired by the distribution modeling ability of Variational Autoencoders, we introduce an auxiliary variable into the regression network. During training, the auxiliary variable is modeled by an inference distribution that learns the underlying structural kinematics of human hand. Different with other regression methods on hand poses, our network estimates the pose joints from input depth features and the learned auxiliary variable as well. We show that by introducing the auxiliary variable, the regression is benefited from 1) regularization modeled by inference distribution; and 2) prior information carried by the auxiliary model. The effectiveness of the proposed regression method is evaluated with extensively self-comparative experiments and in comparison with other regression methods on hand pose datasets. The proposed network is easy to train in an end-to-end manner and can work with various feature extraction methods. We apply the proposed regression method to an existing hand pose estimation system, and improves the estimation accuracy by 18.35% and 16.65% on public hand pose datasets.},
  keywords={Pose estimation;Feature extraction;Kinematics;Three-dimensional displays;Task analysis;Training;Hand pose estimation;depth images;auxiliary variable;regression network;variational autoencoder},
  doi={10.1109/TCSVT.2020.2991987},
  ISSN={1558-2205},
  month={March},}@ARTICLE{10078434,
  author={Hu, Chang-Hui and Liu, Yu and Xu, Lin-Tao and Jing, Xiao-Yuan and Lu, Xiao-Bo and Yang, Wan-Kou and Liu, Pan},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Joint Image-to-Image Translation for Traffic Monitoring Driver Face Image Enhancement}, 
  year={2023},
  volume={24},
  number={8},
  pages={7961-7973},
  abstract={The real traffic monitoring driver face (TMDF) images are with complex multiple degradations, which decline face recognition accuracy in real intelligent transportation systems (ITS). This paper is the first to propose joint image-to-image (I2I) translation to enhance TMDF images of ITS. First, as TMDF images are without corresponding clear ones, identity preserving is critical for TMDF images under unpaired I2I translation. This paper proposes a fast diagonal symmetry pattern (FDSP) to preserve identity structure under unpaired I2I translation. Second, FDSP is introduced into CycleGAN to form FDSP-CG, which aims to learn the degradation mapping (i.e., FDSP-CG-d) from the clarity domain to the degradation domain. FDSP-CG-d can generate massive degradation/clarity image pairs for paired I2I translation training. Third, this paper proposes the dual residual block (DRB) to strengthen Pix2pix for rich face detail features learning (i.e., DRB-P2P), which learns the enhancement mapping from the degradation image to its clear version under paired I2I translation. Finally, the experiments on TMDF (i.e., the brevity name of the face database collected from real ITS) and Chinese famous face (CFF) databases, as well as CelebA and MegaFace databases, indicate that the proposed method can efficiently enhance TMDF images whose degradation variations are learned by FDSP-CG.},
  keywords={Degradation;Face recognition;Faces;Vehicles;Monitoring;Image edge detection;Training;Joint image-to-image translation;complex multiple degradations;fast diagonal symmetry pattern;traffic monitoring driver face image},
  doi={10.1109/TITS.2023.3258634},
  ISSN={1558-0016},
  month={Aug},}@ARTICLE{10105429,
  author={Sun, Xiao and Wang, Song and Yang, Jucheng and Wei, Feng and Wang, Yuan},
  journal={IEEE Internet of Things Journal}, 
  title={Two-Stage Deep Single-Image Super-Resolution With Multiple Blur Kernels for Internet of Things}, 
  year={2023},
  volume={10},
  number={18},
  pages={16440-16449},
  abstract={Single-image super-resolution (SISR) aims to reconstruct a high-resolution image from a single low-resolution (LR) image. Although convolutional neural network (CNN)-based SISR methods greatly enhance image restoration, they face critical challenges. First, SISR models using CNNs process image patches uniformly regardless of importance, causing spatial inefficiency in computation and representation. However, due to resource constraints, edge devices in the Internet of Things (IoT) cannot bear heavy computations or large memory storage. Second, most of the existing SISR methods are designed only for the widely adopted bicubic degradation and cannot handle LR images with arbitrary blur kernels, resulting in poor recovery performance. To address these issues, in this article, we propose a two-stage semantic and spatial deep super-resolution (SSDSR) model suitable for the IoT environment. The proposed SSDSR model is capable of handling a variety of blur kernels (e.g., isotropic Gaussian, motion, and disk blur) by effectively using their prior information. Moreover, the semantic feature extraction (SFE) module enables the proposed model to focus on key areas of LR images rather than treating all pixels equally, which significantly reduces the computational load. The semantic information from the SFE module and the spatial information from the spatial attention module are fused adaptively, allowing the proposed model to extract key information in LR images, thereby increasing the representation capacity of the CNN and improving image recovery. Compared with state-of-the-art SISR methods on benchmark datasets, the proposed SSDSR model demonstrates superior performance. When run in real time on an IoT edge device, our model exhibits high computational efficiency and excellent image quality.},
  keywords={Kernel;Internet of Things;Computational modeling;Semantics;Transformers;Feature extraction;Superresolution;Blur kernel;edge device;Internet of Things (IoT);single-image super-resolution (SISR);visual transformer (VT)},
  doi={10.1109/JIOT.2023.3268285},
  ISSN={2327-4662},
  month={Sep.},}@ARTICLE{10526355,
  author={Wang, Qi and Chi, Kaichen and Jing, Wei and Yuan, Yuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Recreating Brightness From Remote Sensing Shadow Appearance}, 
  year={2024},
  volume={62},
  number={},
  pages={1-11},
  abstract={Shadow removal from remote sensing images is still an open issue. Recently, deep network training on unpaired data is preferable since corresponding ground truths of shadow images are not available in practice. Nevertheless, unsupervised shadow removal research for remote sensing imagery is limited by the scarcity of publicly available benchmarks. This article proposes an unsupervised progressive network (UP-ShadowGAN) to jointly learn decoupled features for shadow removal and color transfer. UP-ShadowGAN explores the mapping between shadow and shadow-free domains through adversarial learning and cycle consistency constraint. In particular, we employ progressive learning to decompose the overall mapping process into more manageable shadow removal and color transfer steps. Specifically, the realistic illumination is restored by propagating spatial context between shadow and shadow-free nodes. Coupled with a multicolor space aggregation strategy, diverse color space representations alleviate color deviation caused by spatial inconsistency. More importantly, we contribute the first unpaired remote sensing shadow removal (URSSR) dataset, which encourages future exploration. Extensive experiments demonstrate that UP-ShadowGAN competes favorably with state-of-the-art methods. The dataset and code are available at https://github.com/chi-kaichen/UP-ShadowGAN.},
  keywords={Image color analysis;Clouds;Remote sensing;Lighting;Training;Physics;Image restoration;Graph reasoning;progressive learning;shadow removal;unpaired data},
  doi={10.1109/TGRS.2024.3398576},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9636359,
  author={Mania, Patrick and Kenfack, Franklin Kenghagho and Neumann, Michael and Beetz, Michael},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Imagination-enabled Robot Perception}, 
  year={2021},
  volume={},
  number={},
  pages={936-943},
  abstract={Many of today’s robot perception systems aim at accomplishing perception tasks that are too simplistic and too hard. They are too simplistic because they do not require the perception systems to provide all the information needed to accomplish manipulation tasks. Typically the perception results do not include information about the part structure of objects, articulation mechanisms and other attributes needed for adapting manipulation behavior. On the other hand, the perception problems stated are also too hard because — unlike humans— the perception systems cannot leverage the expectations about what they will see to their full potential. Therefore, we investigate a variation of robot perception tasks suitable for robots accomplishing everyday manipulation tasks, such as household robots or a robot in a retail store. In such settings it is reasonable to assume that robots know most objects and have detailed models of them. We propose a perception system that maintains its beliefs about its environment as a scene graph with physics simulation and visual rendering. When detecting objects, the perception system retrieves the model of the object and places it at the corresponding place in a VR-based environment model. The physics simulation ensures that object detections that are physically not possible are rejected and scenes can be rendered to generate expectations at the image level. The result is a perception system that can provide useful information for manipulation tasks.},
  keywords={Visualization;Object detection;Rendering (computer graphics);Task analysis;Robots;Physics;Intelligent robots},
  doi={10.1109/IROS51168.2021.9636359},
  ISSN={2153-0866},
  month={Sep.},}@ARTICLE{10462637,
  author={Wang, Yuchen and Xiong, Can and Ju, Changjiang and Yang, Genke and Chen, Yu-wang and Yu, Xiaotian},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Deep Transfer Operator Learning Method for Temperature Field Reconstruction in a Lithium-Ion Battery Pack}, 
  year={2024},
  volume={20},
  number={6},
  pages={8089-8101},
  abstract={Nonuniform thermal behavior in lithium-ion battery packs can accelerate aging, leading to inconsistent cell performance. If not adequately monitored and managed, this heating can give rise to unwanted side reactions, fires, and explosions, underscoring the criticality of temperature field reconstruction. In recent years, data-driven methods have gained popularity for addressing the temperature field reconstruction problem. However, many existing data-driven approaches require retraining when system parameters change, such as the initial temperature distribution or working conditions. This article presents a deep transfer operator learning method named physics-informed adversarial networks. The model architecture incorporates transformer blocks to capture comprehensive time and space features. Additionally, to enhance interpretability and generalization, the model introduces two effective mechanisms: 1) the integration of thermal partial differential equations to ensure compliance with physical laws; and 2) the application of domain adversarial mechanism in transfer learning to extract domain-invariant feature representations. These mechanisms enable the model to effectively reconstruct the temperature field, even in unencountered scenarios during training. The proposed method is validated under real-world energy storage working conditions, demonstrating superior performance compared to state-of-the-art deep learning methods. Notably, the approach exhibits excellent performance even when confronted with the limited availability of training data.},
  keywords={Batteries;Temperature sensors;Heating systems;Temperature distribution;Liquids;Training;Testing;Deep operator learning;lithium-ion battery (LiB) pack;temperature field reconstruction (TFR);transfer learning;transformer},
  doi={10.1109/TII.2024.3369708},
  ISSN={1941-0050},
  month={June},}@ARTICLE{9489277,
  author={Dong, Xuan and Liu, Chang and Li, Weixin and Hu, Xiaoyan and Wang, Xiaojie and Wang, Yunhong},
  journal={IEEE Transactions on Image Processing}, 
  title={Self-Supervised Colorization Towards Monochrome-Color Camera Systems Using Cycle CNN}, 
  year={2021},
  volume={30},
  number={},
  pages={6609-6622},
  abstract={Colorization in monochrome-color camera systems aims to colorize the gray image  ${{\mathrm{I}}_{\mathrm{G}}}$  from the monochrome camera using the color image  ${{\mathrm{R}}_{\mathrm{C}}}$  from the color camera as reference. Since monochrome cameras have better imaging quality than color cameras, the colorization can help obtain higher quality color images. Related learning based methods usually simulate the monochrome-color camera systems to generate the synthesized data for training, due to the lack of ground-truth color information of the gray image in the real data. However, the methods that are trained relying on the synthesized data may get poor results when colorizing real data, because the synthesized data may deviate from the real data. We present a self-supervised CNN model, named Cycle CNN, which can directly use the real data from monochrome-color camera systems for training. In detail, we use the Weighted Average Colorization (WAC) network to do the colorization twice. First, we colorize  ${{\mathrm{I}}_{\mathrm{G}}}$  using  ${{\mathrm{R}}_{\mathrm{C}}}$  as reference to obtain the first-time colorization result  ${{\mathrm{I}}_{\mathrm{C}}}$ . Second, we colorize the de-colored map of  ${{\mathrm{R}}_{\mathrm{C}}}$ , i.e.  ${{\mathrm{R}}_{\mathrm{G}}}$ , using the concatenated image of  ${{\mathrm{I}}_{\mathrm{G}}}$  and Cb/Cr channels of the first-time colorization result  ${{\mathrm{I}}_{\mathrm{C}}}$ , i.e.  ${{\mathrm{I}}_{\mathrm{C}}^{Cb}}$  and  ${{\mathrm{I}}_{\mathrm{C}}^{Cr}}$ , as reference to obtain the second-time colorization result  ${\mathrm{R}}_{\mathrm{C}}^{{ {'}}}$ . In this way, for the second-time colorization result  ${\mathrm{R}}_{\mathrm{C}}^{{ {'}}}$ , we use the Cb and Cr channels of the original color map  ${{\mathrm{R}}_{\mathrm{C}}}$  as ground-truth and introduce the cycle consistency loss to push  ${\mathrm{R}}_{\mathrm{C}}^{{ {'}}Cb/Cr} \approx {\mathrm{R}}_{\mathrm{C}}^{Cb/Cr}$ . Also, for the  $Y$  channel of the first-time colorization result  ${{\mathrm{I}}_{\mathrm{C}}^{Y}}$ , we propose the Global Curve Adjustment (GCA) network and the structure similarity loss to encourage the structure similarity between  ${{\mathrm{I}}_{\mathrm{C}}^{Y}}$  and  ${{\mathrm{I}}_{\mathrm{G}}}$ . In addition, we introduce a spatial smoothness loss within the WAC network to encourage spatial smoothness of the colorization result. Combining all these losses, we could train the Cycle CNN using the real data in the absence of the ground-truth color information of  ${{\mathrm{I}}_{\mathrm{G}}}$ . Experimental results show that we can outperform related methods largely for colorizing real data.},
  keywords={Cameras;Image color analysis;Color;Training;Loss measurement;Feature extraction;Deep learning;Weighted average colorization;global curve adjustment;cycle consistency;structure similarity;spatial smoothness},
  doi={10.1109/TIP.2021.3096385},
  ISSN={1941-0042},
  month={},}@ARTICLE{10741274,
  author={Li, Linfeng and Song, Yucheng and Tian, Tian and Tian, Jinwen},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Dense Condition-Driven Diffusion Network for Infrared Small Target Detection}, 
  year={2024},
  volume={73},
  number={},
  pages={1-13},
  abstract={Infrared small target detection (IRSTD) is important in military and civilian applications. In recent years, numerous methods based on convolutional neural networks (CNNs) have already been explored in the field of IRSTD. However, due to the mismatch between the network’s receptive field and the size of the target, conventional CNN-based methods struggle to fully differentiate between the background and the small target and are prone to losing the small target in deeper layers. A dense condition-driven diffusion network (DCDNet) based on the conditional diffusion model is proposed to address the IRSTD task. The diffusion model can easily fit the distribution of infrared background images, thereby isolating the small targets from the distribution. Extracted features from original images are used as conditions to guide the diffusion model in gradually transforming Gaussian noise into the target image. A dense conditioning module is introduced to provide richer guidance to the diffusion model. This module incorporates multiscale information from the conditional image into the diffusion model. Multiple samplings can reduce the amplitude of background noise to enhance the target. Comprehensive experiments performed on two public datasets demonstrate the proposed method’s effectiveness and superiority over other comparative methods in terms of probability of detection ( $P_{d}$ ), intersection over union (IoU), and signal-to-clutter ratio gain (SCRG).},
  keywords={Feature extraction;Diffusion models;Object detection;Transformers;Noise reduction;Background noise;Three-dimensional displays;Image segmentation;Deep learning;Visualization;Deep learning;denoising;dense condition;diffusion model;infrared small target detection (IRSTD)},
  doi={10.1109/TIM.2024.3488145},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10656893,
  author={Qing, Zhiwu and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Sang, Nong},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Hierarchical Spatio-temporal Decoupling for Text-to- Video Generation}, 
  year={2024},
  volume={},
  number={},
  pages={6635-6645},
  abstract={Despite diffusion models having shown powerful abilities to generate photorealistic images, generating videos that are realistic and diverse still remains in its infancy. One of the key reasons is that current methods intertwine spatial content and temporal dynamics together, leading to a notably increased complexity of text-to-video generation (T2V). In this work, we propose HiGen, a diffusion model-based method that improves performance by decoupling the spatial and temporal factors of videos from two perspectives, i.e., structure level and content level. At the structure level, we decompose the T2V task into two steps, including spatial reasoning and temporal reasoning, using a unified denoiser. Specifically, we generate spatially coherent priors using text during spatial reasoning and then generate temporally coherent motions from these priors during temporal reasoning. At the content level, we extract two subtle cues from the content of the input video that can express motion and appearance changes, respectively. These two cues then guide the model's training for generating videos, enabling flexible content variations and enhancing temporal stability. Through the decoupled paradigm, HiGen can effectively reduce the complexity of this task and generate realistic videos with semantics accuracy and motion stability. Extensive experiments demonstrate the superior performance of HiGen over the state-of-the-art T2V methods. We have released our source code and models.},
  keywords={Training;Source coding;Semantics;Spatial coherence;Cognition;Stability analysis;Complexity theory},
  doi={10.1109/CVPR52733.2024.00634},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10543170,
  author={She, Chunyan and Han, Fujun and Wang, Lidan and Duan, Shukai and Huang, Tingwen},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={MPC-Net: Multi-Prior Collaborative Network for Low-Light Image Enhancement}, 
  year={2024},
  volume={34},
  number={10},
  pages={10385-10398},
  abstract={Low-light image enhancement aims to obtain a normal-light image by adjusting the illumination of a low-light image. The existing methods do not fully explore the prior information hidden in low-light images, which raises the problems of detail loss and color distortion. To alleviate these issues, we propose a multi-prior collaborative network (MPC-Net) with transformer for low-light image enhancement. It extracts the indispensable prior information to facilitate high-quality image enhancement. Specifically, a pre-trained high-level vision model is employed to extract coarse texture and structure, which is then refined through a proposed self-distillation module to obtain compact representation for texture and structure. Furthermore, we design a color branch consisting of negative residual blocks and a pyramid structure to solve for noise-free color prior, aiming to provide the enhancer with a modeling mechanism for color information. Finally, a transformer-based multi-prior fusion module is developed to aggregate the content and prior information. Extensive experiments show that the proposed MPC-Net achieves superior performance on three referenced datasets and four no-referenced datasets. Our code is available at: https://github.com/Shecyy/MPC-Net.},
  keywords={Image color analysis;Transformers;Lighting;Image enhancement;Histograms;Colored noise;Image edge detection;Low-light image enhancement;multi-prior collaboration;transformer},
  doi={10.1109/TCSVT.2024.3408007},
  ISSN={1558-2205},
  month={Oct},}@ARTICLE{9905634,
  author={Jin, Ziqi and Xie, Jinheng and Wu, Bizhu and Shen, Linlin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Weakly Supervised Pedestrian Segmentation for Person Re-Identification}, 
  year={2023},
  volume={33},
  number={3},
  pages={1349-1362},
  abstract={Person re-identification (RelD) is an important problem in intelligent surveillance and public security. Among all the solutions to this problem, existing mask-based methods first use a well-pretrained segmentation model to generate a foreground mask, in order to exclude the background from ReID. Then they perform the RelD task directly on the segmented pedestrian image. However, such a process requires extra datasets with pixel-level semantic labels. In this paper, we propose a Weakly Supervised Pedestrian Segmentation (WSPS) framework to produce the foreground mask directly from the RelD datasets. In contrast, our WSPS only requires image-level subject ID labels. To better utilize the pedestrian mask, we also propose the Image Synthesis Augmentation (ISA) technique to further augment the dataset. Experiments show that the features learned from our proposed framework are robust and discriminative. Compared with the baseline, the mAP of our framework is about 4.4%, 11.7%, and 4.0% higher on three widely used datasets including Market-1501, CUHK03, and MSMT17. The code will be available soon.},
  keywords={Image segmentation;Feature extraction;Task analysis;Training;Semantics;Lips;Legged locomotion;Re-identification;weakly supervised segmentation;mask-based augmentation},
  doi={10.1109/TCSVT.2022.3210476},
  ISSN={1558-2205},
  month={March},}@ARTICLE{10559841,
  author={Shang, Ronghua and Xie, Yuhao and Zhang, Weitong and Feng, Jie and Xu, Songhua},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Joint Adversarial Network With Semantic and Topology Fusion for Cross-Scene Hyperspectral Image Classification}, 
  year={2024},
  volume={62},
  number={},
  pages={1-16},
  abstract={Hyperspectral image cross-scene classification (HSICC) poses a significant challenge due to distribution variations between source and target domains. Existing unsupervised domain adaptation methods primarily focus on local knowledge transfer, often neglecting the critical semantic information and sample topological structure inherent in hyperspectral images (HSIs). To address these limitations, this article introduces an end-to-end joint adversarial network with semantic and topology fusion (JAN-STF). This network liberates from the constraints of local perception by integrating semantic and topological information into both domain- and class-level adversarial learning processes. First, the network constructs a semantic-guided cross-domain graph structure to obtain cross-domain features. Subsequently, domain-level adversarial learning is conducted using these features to achieve domain-invariant representation with robust transferability. Moreover, to bolster stability in the ensuing class-level adversarial procedure, the network dynamically computes cross-domain category center distance loss utilizing an intra-domain topological semantic attention mechanism, thereby mapping features to proximate spaces. Finally, class-level adversarial learning is performed by leveraging the prediction discrepancy between the local classifier and the topological classifier, thus enhancing the discriminative performance of the domain-invariant representation. Extensive experiments on three broadly utilized HSICC datasets demonstrate JAN-STF’s superiority in accuracy and Kappa coefficient (KC) metrics over nine leading algorithms.},
  keywords={Semantics;Adversarial machine learning;Feature extraction;Hyperspectral imaging;Measurement;Training;Task analysis;Adversarial learning;domain adaptation;hyperspectral image (HSI);semantic attention;topological structure},
  doi={10.1109/TGRS.2024.3415965},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9856991,
  author={Schneider, Pascal and Rambach, Jason and Mirbach, Bruno and Stricker, Didier},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Unsupervised Anomaly Detection from Time-of-Flight Depth Images}, 
  year={2022},
  volume={},
  number={},
  pages={230-239},
  abstract={Video anomaly detection (VAD) addresses the problem of automatically finding anomalous events in video data. The primary data modalities on which current VAD systems work on are monochrome or RGB images. Using depth data in this context instead is still hardly explored in spite of depth images being a popular choice in many other computer vision research areas and the increasing availability of inexpensive depth camera hardware. We evaluate the application of existing autoencoder-based methods on depth video and propose how the advantages of using depth data can be leveraged by integration into the loss function. Training is done unsupervised using normal sequences without need for any additional annotations. We show that depth allows easy extraction of auxiliary information for scene analysis in the form of a foreground mask and demonstrate its beneficial effect on the anomaly detection performance through evaluation on a large public dataset, for which we are also the first ones to present results on.},
  keywords={Training;Optical losses;Computer vision;Cameras;Transformers;Sensors;Task analysis},
  doi={10.1109/CVPRW56347.2022.00037},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{9533636,
  author={Xu, Zhiwei and Li, Dapeng and Bai, Yunpeng and Fan, Guoliang},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={In the real world, many tasks require multiple agents to cooperate with each other under the condition of local observations. To solve such problems, many multi-agent reinforcement learning methods based on Centralized Training with Decentralized Execution have been proposed. One representative class of work is value decomposition, which decomposes the global joint Q-value Qjtinto individual Q-values Qa to guide individuals' behaviors, e.g. VDN (Value-Decomposition Networks) and QMIX. However, these baselines often ignore the randomness in the situation. We propose MMD-MIX, a method that combines distributional reinforcement learning and value decomposition to alleviate the above weaknesses. Besides, to improve data sampling efficiency, we were inspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to explicitly introduce randomness into the MMD-MIX. The experiments demonstrate that MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge (SMAC) environment.},
  keywords={Training;Neural networks;Collaboration;Reinforcement learning;Task analysis;Multi-Agent System;Distributional Reinforcement Learning;Coordination and Collaboration},
  doi={10.1109/IJCNN52387.2021.9533636},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10557536,
  author={Wang, Shuang and Xing, Hantong and Wang, Chenxu and Zhou, Huaji and Hou, Biao and Jiao, Licheng},
  journal={IEEE Transactions on Wireless Communications}, 
  title={SigDA: A Superimposed Domain Adaptation Framework for Automatic Modulation Classification}, 
  year={2024},
  volume={23},
  number={10},
  pages={13159-13172},
  abstract={Due to the uncertainty of non-cooperative communication channels, the received signals often contain various impairment factors, leading to a significant decline in the performance of existing deep learning (DL)-based automatic modulation classification (AMC) models. Several preliminary works utilize domain adaptation (DA) to alleviate this issue, however, they are constrained by singular domain difference factor, whereas in practice, these factors often manifest cumulatively. Therefore, this paper introduce a more realistic task named superimposed DA, where multiple domain difference factors are overlaid, reflecting the cumulative nature of them. We propose the SigDA as a solution framework, which adopts adversarial training to align the data distribution in different domains. Two technical modules, Multi-task based Masked Signal Feature Extractor (M2SFE) and Signal Feature Pyramid Aggregation (SFPA), are innovatively designed in SigDA. M2SFE utilizes mask and reconstruction task to enhance feature extraction and achieves discriminative feature selection through the design of feature mapping layers, while SFPA can solve the problem of inconsistent signal length in superimposed DA and can aggregate the features of signals into the same dimension. We consider and superimpose various typical signal domain difference factors, comprehensive experiments demonstrate that the proposed framework can achieve significant performance improvement in various communication channels.},
  keywords={Feature extraction;Task analysis;Modulation;Adaptation models;Convolution;Data models;Wireless communication;Automatic modulation classification;domain adaptation;multi-task learning;adversarial training},
  doi={10.1109/TWC.2024.3399067},
  ISSN={1558-2248},
  month={Oct},}@INPROCEEDINGS{10378269,
  author={Luo, Ling and Chowdhury, Pinaki Nath and Xiang, Tao and Song, Yi-Zhe and Gryaditskaya, Yulia},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={3D VR Sketch Guided 3D Shape Prototyping and Exploration}, 
  year={2023},
  volume={},
  number={},
  pages={9233-9242},
  abstract={3D shape modeling is labor-intensive, time-consuming, and requires years of expertise. To facilitate 3D shape modeling, we propose a 3D shape generation network that takes a 3D VR sketch as a condition. We assume that sketches are created by novices without art training and aim to reconstruct geometrically realistic 3D shapes of a given category. To handle potential sketch ambiguity, our method creates multiple 3D shapes that align with the original sketch’s structure. We carefully design our method, training the model step-by-step and leveraging multi-modal 3D shape representation to support training with limited training data. To guarantee the realism of generated 3D shapes we leverage the normalizing flow that models the distribution of the latent space of 3D shapes. To encourage the fidelity of the generated 3D shapes to an input sketch, we propose a dedicated loss that we deploy at different stages of the training process. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.},
  keywords={Training;Solid modeling;Computer vision;Three-dimensional displays;Codes;Art;Shape},
  doi={10.1109/ICCV51070.2023.00850},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{10378017,
  author={Wang, Yuxi and Liang, Jian and Xiao, Jun and Mei, Shuqi and Yang, Yuran and Zhang, Zhaoxiang},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Informative Data Mining for One-shot Cross-Domain Semantic Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1064-1074},
  abstract={Contemporary domain adaptation offers a practical solution for achieving cross-domain transfer of semantic segmentation between labelled source data and unlabeled target data. These solutions have gained significant popularity; however, they require the model to be retrained when the test environment changes. This can result in unbearable costs in certain applications due to the time-consuming training process and concerns regarding data privacy. One-shot domain adaptation methods attempt to overcome these challenges by transferring the pre-trained source model to the target domain using only one target data. Despite this, the referring style transfer module still faces issues with computation cost and over-fitting problems. To address this problem, we propose a novel framework called Informative Data Mining (IDM) that enables efficient one-shot domain adaptation for semantic segmentation. Specifically, IDM provides an uncertainty-based selection criterion to identify the most informative samples, which facilitates quick adaptation and reduces redundant training. We then perform a model adaptation method using these selected samples, which includes patch-wise mixing and prototype-based information maximization to update the model. This approach effectively enhances adaptation and mitigates the overfitting problem. In general, we provide empirical evidence of the effectiveness and efficiency of IDM. Our approach outperforms existing methods and achieves a new state-of-the-art one-shot performance of 56.7%/55.4% on the GTA5/SYNTHIA to Cityscapes adaptation tasks, respectively. The code will be released at https://github.com/yxiwang/IDM.},
  keywords={Training;Adaptation models;Costs;Semantic segmentation;Computational modeling;Minimization;Data models},
  doi={10.1109/ICCV51070.2023.00104},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{10243035,
  author={Zhu, Mengliang and Zeng, Xiangyu and Liu, Jie and Yang, Chaoying and Zhou, Kaibo},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Source-Free Cluster Adaptation for Privacy-Preserving Machinery Fault Diagnosis}, 
  year={2023},
  volume={72},
  number={},
  pages={1-10},
  abstract={Unsupervised domain adaptation (UDA) has been widely exploited for machinery fault diagnosis (MFD). However, existing UDA approaches always require direct access and sharing to the labeled source domain, raising privacy concerns. In this article, a practical and challenging scenario, source-free UDA (SFUDA), is considered for privacy-preserving MFD. In SFUDA, only a pretrained source model is provided for the unlabeled target domain, and the source data are inaccessible during adaptation. A novel SFUDA approach, namely source-free cluster adaptation (SF-CA), is proposed, which consists of source domain generalization (SDG) and target model adaptation (TMA). SDG aims to obtain a well-generalized source model for TMA. Specifically, the adaptive R-drop is proposed for SDG, where an entropy-aware weighted consistency training strategy is introduced to regularize dropout. TMA enforces cluster assumption for adaptation, where the structure exploration regularizations are proposed to learn structural information. Besides, selective self-training is introduced to alleviate model collapse during TMA. Finally, discriminative and tight-clustered target features can be obtained for SFUDA. Extensive experiments are conducted on three public datasets and one practical dataset. The experimental results show the effectiveness of SF-CA for privacy-preserving MFD and the feasibility of cluster assumption enforcement for SFUDA.},
  keywords={Adaptation models;Training;Privacy;Machinery;Feature extraction;Fault diagnosis;Data models;Cluster assumption enforcement (CAE);machinery fault diagnosis (MFD);privacy-preserving;source-free unsupervised domain adaptation (SFUDA)},
  doi={10.1109/TIM.2023.3312468},
  ISSN={1557-9662},
  month={},}@ARTICLE{10495337,
  author={Zhang, Lei and Li, Haisheng and Liu, Ruijun and Wang, Xiaochuan and Wu, Xiaoqun},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Quality Guided Metric Learning for Domain Adaptation Person Re-Identification}, 
  year={2024},
  volume={70},
  number={3},
  pages={6023-6030},
  abstract={Person re-identification is the task of identifying pedestrians across different cameras. Domain adaptation person re-identification involves transferring knowledge from labeled source domains to unlabeled target domains, with applications in security and surveillance. Challenges emerge due to variations in sample quality and disparities in distance distribution between positive and negative sample pairs. To address these challenges, this paper proposes a quality guided metric learning approach for domain adaptation person re-identification. We focus on improving appearance similarity metrics by evaluating sample quality based on local visibility, categorizing images as high or low quality. Besides, we introduce an adaptive weight triplet loss incorporating camera information to optimize triplets. This reduces the effects of invalid triplets and facilitating ongoing target domain learning.We have conducted comprehensive comparative evaluations to showcase the advantages and superiority of our proposed method. Our method has 2.6%, 1.9%, and 6.2% improved on Market-1501, DukeMTMC-reID, and MSMT17 datasets, respectively.},
  keywords={Adaptation models;Pedestrians;Training;Cameras;Noise;Data models;Surveillance;Person Re-identification;quality constraint;metric learning;triplet loss;adaptive weight},
  doi={10.1109/TCE.2024.3386657},
  ISSN={1558-4127},
  month={Aug},}@INPROCEEDINGS{10377824,
  author={Yu, Changfeng and Chen, Shiming and Chang, Yi and Song, Yibing and Yan, Luxin},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Both Diverse and Realism Matter: Physical Attribute and Style Alignment for Rainy Image Generation}, 
  year={2023},
  volume={},
  number={},
  pages={12353-12363},
  abstract={Although considerable progress has been made in image deraining under synthetic data, real rain removal is still a tough problem due to the huge domain gap between synthetic and real data. Besides, difficulties in collecting and labeling diverse real rain images hinder the progress of this field. Consequently, we attempt to promote real rain removal from rain image generation (RIG) perspective. Existing RIG methods mainly focus on diversity but miss realistic, or the realistic but neglect diversity of the generation. To solve this dilemma, we propose a physical alignment and controllable generation network (PCGNet) for diverse and realistic rain generation. Our key idea is to simultaneously utilize the controllability of attributes from synthetic and the realism of appearance from real data. Specifically, we devise a unified framework to disentangle background, rain attributes, and appearance style from synthetic and real data. Then we collaboratively align the factors with a novel semi-supervised weight moving strategy for attribute, an explicit distribution modeling method for real rain style. Furthermore, we pack these aligned factors into the generation model, achieving physical controllable mapping from the attributes to real rain with image-level and attribute-level consistency loss. Extensive experiments show that PCGNet can effectively generate appealing rainy results, which significantly improve the performance under synthetic and real scenes for all existing deraining methods.},
  keywords={Computer vision;Rain;Image synthesis;Aerospace electronics;Controllability;Labeling;Synthetic data},
  doi={10.1109/ICCV51070.2023.01138},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{8945893,
  author={Al-Naser, Mohammad and Siddiqui, Shoaib Ahmed and Ohashi, Hiroki and Ahmed, Sheraz and Katsuyki, Nakamura and Takuto, Sato and Dengel, Andreas},
  booktitle={2019 Digital Image Computing: Techniques and Applications (DICTA)}, 
  title={OGaze: Gaze Prediction in Egocentric Videos for Attentional Object Selection}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper proposes a novel gaze-estimation model for attentional object selection tasks. The key features of our model are two-fold: (i) usage of the deformable convolutional layers to better incorporate spatial dependencies of different shapes of objects and background, (ii) formulation of the gaze-estimation problem in two different ways, i.e. as a classification as well as a regression problem. We combine the two different formulations using a joint loss that incorporates both the cross-entropy as well as the mean-squared error in order to train our model. The experimental results on two publicly available datasets indicates that our model not only achieved real-time performance (13-18 FPS), but also outperformed the state-of-the-art models on the OSdataset along with comparable performance on GTEA-plus dataset.},
  keywords={Videos;Estimation;Predictive models;Computational modeling;Feature extraction;Solid modeling;Task analysis},
  doi={10.1109/DICTA47822.2019.8945893},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9669728,
  author={Zhao, Hanyu and Yuan, Sha and Xie, Niantao and Leng, Jiahong and Wang, Guoqiang},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Federated Adversarial Learning Method for Biomedical Named Entity Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={2962-2969},
  abstract={Identifying medical terms with specific meaning and information with semantic attribute is the prerequisite of conducting semantic analysis in medical field. However, the problem of medical data island restricts the development of entity recognition in a great extent. In addition to the ban of data sharing between different hospitals, different departments in the same hospital also can not exchange data due to privacy security concerns and ethical issues. To solve these problems, in the federated learning framework, the server trains a global model collaboratively through aggregating the encrypted or noised model parameters of the local participated clients without data leakage. In this paper, to well apply federated learning on biomedical named entity recognition (BioNER), we propose the federated adversarial learning (FAL) method with consideration of the training cost and model performance. FAL not only makes use of a modified structured pruning scheme to reduce the number of model parameters but also exploits an improved adversarial learning approach named protected fast gradient method (PFGM) to enhance the robustness and generalization of the model. In the experiment, we use the datasets of five departments in the same tumor hospital, such as gynecology department and gastric surgery department. Results show that the proposed FAL framework achieves expected effect with high efficiency.},
  keywords={Training;Ethics;Hospitals;Biological system modeling;Semantics;Collaborative work;Adversarial machine learning;named entity recognition;federated learning;BERT},
  doi={10.1109/BIBM52615.2021.9669728},
  ISSN={},
  month={Dec},}@ARTICLE{9762753,
  author={Xu, Chuanyun and Liu, Huan and Li, Tenghui and Zhang, Yang and Li, Tian and Li, Gang},
  journal={IEEE Open Journal of Intelligent Transportation Systems}, 
  title={Cascaded Feature-Mask Fusion for Foreground Segmentation}, 
  year={2022},
  volume={3},
  number={},
  pages={340-350},
  abstract={Foreground segmentation aims at extracting moving objects from the background in a robust manner under various challenging scenarios. The deep learning-based methods have achieved remarkable improvement in this field. These methods produce semantically correct predictions based on extracted rich semantic features yet perform poorly on segmentation of edge details. The main reason is that the high-level features extracted by the deep network lose the high-frequency information for the successful edge segmentation. On this basis, we propose a novel segmentation network with a cascade architecture to refine segmentation results step by step by introducing detailed information into high-level features. The network recorrects and optimizes the segmentation maps in each step so that more accurate segmentation results are obtained. Furthermore, we evaluate our approach on the challenging CDnet2014 dataset and achieve an F-measure of 0.9868. Our approach thus outperforms previous methods, such as FgSegNet_v2, FgSegNet, BSPVGan, Cascade CNN, IUTIS-5, WeSamBE, DeepBS, and GMM-Stauffer.},
  keywords={Feature extraction;Image segmentation;Semantics;Convolutional neural networks;Decoding;Image edge detection;Task analysis;Deep learning;feature-mask fusion;foreground segmentation;high-level features;video surveillance},
  doi={10.1109/OJITS.2022.3170075},
  ISSN={2687-7813},
  month={},}@ARTICLE{10504848,
  author={Zhang, Tao and Zhong, Sheng and Xu, Wenhui and Yan, Luxin and Zou, Xu},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Catenary Insulator Defect Detection: A Dataset and an Unsupervised Baseline}, 
  year={2024},
  volume={73},
  number={},
  pages={1-15},
  abstract={The high-speed railway, powered by the catenary, serves as a high-capacity and high-frequency transportation means due to its convenience. Insulators are integral components of the catenary. Once defects appear in insulators, they pose a risk of serious traffic accidents. Automatic detection of insulator defects proves to be an effective way to avoid further breakdowns and ensure the safety of the high-speed railway. To this end, many deep learning-based approaches have been developed. Though high performances have been achieved, they heavily rely on a large amount of high-quality annotated samples. Nevertheless, in practical scenarios, defect samples are difficult to collect. Moreover, annotating these samples is both labor-intensive and expert-requiring. In addition, to the best of our knowledge, there is currently no publicly available dataset specifically dedicated to insulator defect detection. To address the issue of data scarcity, in this work, we first collect catenary insulator images captured by the high-speed rail inspection device on real railway lines and construct the catenary insulator defect (CID) dataset. To break the dilemma of the lack of defect data and high-quality annotations, we further propose a simple but effective insulator defect detection framework in an unsupervised image reconstruction manner. Extensive experiments demonstrate that our proposed method demonstrates high accuracy in detecting various insulator defects without the need for manual annotations. The CID dataset and source codes have been made publicly available at https://github.com/LightZH/Insulator-Defect-Detection.},
  keywords={Insulators;Image reconstruction;Feature extraction;Defect detection;Anomaly detection;Rail transportation;Training;Anomaly detection;catenary insulator;defect detection;image reconstruction;unsupervised learning},
  doi={10.1109/TIM.2024.3390695},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{9484335,
  author={Sun, Zongcai and Fu, Chaoyou and Luo, Mandi and He, Ran},
  booktitle={2021 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={Self-Augmented Heterogeneous Face Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Heterogeneous face recognition (HFR) is quite challenging due to the large discrepancy introduced by cross-domain face images. The limited number of paired face images results in a severe overfitting problem in existing methods. To tackle this issue, we proposes a novel self-augmentation method named Mixed Adversarial Examples and Logits Replay (MAELR). Concretely, we first generate adversarial examples, and mix them with clean examples in an interpolating way for data augmentation. Simultaneously, we extend the definition of the adversarial examples according to cross-domain problems. Benefiting from this extension, we can reduce domain discrepancy to extract domain-invariant features. We further propose a diversity preserving loss via logits replay, which effectively uses the discriminative features obtained on the large-scale VIS dataset. In this way, we improve the feature diversity that can not be obtained from mixed adversarial examples methods. Extensive experiments demonstrate that our method alleviates the over-fitting problem, thus significantly improving the recognition performance of HFR.},
  keywords={Interpolation;Face recognition;Conferences;Feature extraction},
  doi={10.1109/IJCB52358.2021.9484335},
  ISSN={2474-9699},
  month={Aug},}@INPROCEEDINGS{10386055,
  author={Cai, Zhuotong and Xin, Jingmin and Dong, Siyuan and You, Chenyu and Shi, Peiwen and Zeng, Tianyi and Zhang, Jiazhen and Onofrey, John A. and Zheng, Nanning and Duncan, James S.},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Unsupervised Domain Adaptation by Cross-Prototype Contrastive Learning for Medical Image Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={819-824},
  abstract={Unsupervised Domain Adaptation (UDA), which aligns the labeled source distribution to the unlabeled target distribution, has shown remarkable achievement in the medical image segmentation task. Previous UDA methods unilaterally consider the global distribution alignment through explicit category-based loss while good separation and discrimination of class are insufficiently explored, resulting in the sub-aligned distribution across domains. In this paper, we propose cross-prototype contrastive learning method (CPCL) for UDA segmentation through class centroid alignment. Specifically, to reduce the intra-class distance and increase the inter-class distance, we first introduce prototype-feature contrastive learning to align the pixel-level features and the same-class global prototype across domains. Secondly, we further present prototype-prototype contrastive learning to align the same class prototypes between the source domain and target domain for compact category centroid and better global domain distribution alignment. Extensive experiments on two public cardiac datasets demonstrate that the proposed CPCL achieves superior domain adaptation performance as compared with the state-of-the-art.},
  keywords={Image segmentation;Adaptation models;Biological system modeling;Prototypes;Self-supervised learning;Performance gain;Task analysis;Unsupervised Domain Adaptation;Medical Image Segmentation;Cross-prototype Contrastive Learning},
  doi={10.1109/BIBM58861.2023.10386055},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{9762492,
  author={Zeng, Zhixiong and Xu, Nan and Mao, Wenji and Zeng, Daniel},
  journal={IEEE Intelligent Systems}, 
  title={An Orthogonal Subspace Decomposition Method for Cross-Modal Retrieval}, 
  year={2022},
  volume={37},
  number={3},
  pages={45-53},
  abstract={As a general characteristic observed in the real-world datasets, multimodal data are usually partially associated, which comprise the commonly shared information across modalities (i.e., modality-shared information) and the specific information only exists in a single modality (i.e., modality-specific information). Cross-modal retrieval methods typically use these information in multimodal data as a whole and project them into a common representation space to calculate the similarity measure. In fact, only modality-shared information can be well aligned in the learning of common representations, whereas modality-specific information usually brings about interference term and decreases the performance of cross-modal retrieval. The explicit distinction and utilization of these two kinds of multimodal information are important to cross-modal retrieval, but rarely studied in previous research. In this article, we explicitly distinguish and utilize modality-shared and modality-specific features for learning better common representations, and propose an orthogonal subspace decomposition method for cross-modal retrieval, named orthogonal subspace decomposition method. Specifically, we introduce a structure preservation loss to ensure modality-shared information to be well preserved, and optimize the intramodal discrimination loss and intermodal invariance loss to learn the semantic discriminative features for cross-modal retrieval. We conduct comprehensive experiments on four widely used benchmark datasets, and the experimental results demonstrate the effectiveness of our proposed method.},
  keywords={Semantics;Representation learning;Task analysis;Matrix decomposition;Automation;Interference;Intelligent systems;Cross-modal Retrieval;Representation Learning;Orthogonal Decomposition},
  doi={10.1109/MIS.2022.3169884},
  ISSN={1941-1294},
  month={May},}@ARTICLE{10418497,
  author={Li, Mengran and Zhang, Yong and Wang, Shaofan and Hu, Yongli and Yin, Baocai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Redundancy is Not What You Need: An Embedding Fusion Graph Auto-Encoder for Self-Supervised Graph Representation Learning}, 
  year={2025},
  volume={36},
  number={2},
  pages={3519-3533},
  abstract={Attribute graphs are a crucial data structure for graph communities. However, the presence of redundancy and noise in the attribute graph can impair the aggregation effect of integrating two different heterogeneous distributions of attribute and structural features, resulting in inconsistent and distorted data that ultimately compromises the accuracy and reliability of attribute graph learning. For instance, redundant or irrelevant attributes can result in overfitting, while noisy attributes can lead to underfitting. Similarly, redundant or noisy structural features can affect the accuracy of graph representations, making it challenging to distinguish between different nodes or communities. To address these issues, we propose the embedded fusion graph auto-encoder framework for self-supervised learning (SSL), which leverages multitask learning to fuse node features across different tasks to reduce redundancy. The embedding fusion graph auto-encoder (EFGAE) framework comprises two phases: pretraining (PT) and downstream task learning (DTL). During the PT phase, EFGAE uses a graph auto-encoder (GAE) based on adversarial contrastive learning to learn structural and attribute embeddings separately and then fuses these embeddings to obtain a representation of the entire graph. During the DTL phase, we introduce an adaptive graph convolutional network (AGCN), which is applied to graph neural network (GNN) classifiers to enhance recognition for downstream tasks. The experimental results demonstrate that our approach outperforms state-of-the-art (SOTA) techniques in terms of accuracy, generalization ability, and robustness.},
  keywords={Self-supervised learning;Task analysis;Redundancy;Termination of employment;Convolution;Representation learning;Graph neural networks;Adaptive graph convolution;attribute graphs;contrastive learning;graph auto-encoder (GAE);redundancy reduction;self-supervised learning (SSL)},
  doi={10.1109/TNNLS.2024.3357080},
  ISSN={2162-2388},
  month={Feb},}@INPROCEEDINGS{9561665,
  author={Xu, Jie and Chen, Xingyu and Lan, Xuguang and Zheng, Nanning},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Probabilistic Human Motion Prediction via A Bayesian Neural Network}, 
  year={2021},
  volume={},
  number={},
  pages={3190-3196},
  abstract={Human motion prediction is an important and challenging topic that has promising prospects in efficient and safe human-robot-interaction systems. Currently, the majority of the human motion prediction algorithms are based on deterministic models, which may lead to risky decisions for robots. To solve this problem, we propose a probabilistic model for human motion prediction in this paper. The key idea of our approach is to extend the conventional deterministic motion prediction neural network to a Bayesian one. On one hand, our model could generate several future motions when given an observed motion sequence. On the other hand, by calculating the Epistemic Uncertainty and the Heteroscedastic Aleatoric Uncertainty, our model could tell the robot if the observation has been seen before and also give the optimal result among all possible predictions. We extensively validate our approach on a large scale benchmark dataset Human3.6m. The experiments show that our approach performs better than deterministic methods. We further evaluate our approach in a Human-Robot-Interaction (HRI) scenario. The experimental results show that our approach makes the interaction more efficient and safer.},
  keywords={Uncertainty;Automation;Conferences;Neural networks;Predictive models;Benchmark testing;Probabilistic logic},
  doi={10.1109/ICRA48506.2021.9561665},
  ISSN={2577-087X},
  month={May},}@INPROCEEDINGS{10582015,
  author={Huang, Yubo and Zhu, Anran and Zeng, Cheng and Hu, Cong and Lai, Xin and Feng, Wenhao and Chen, Fan},
  booktitle={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={RFIS-FPI: Reversible Face Image Steganography Neural Network for Face Privacy Interactions}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Face information is vulnerable to theft and misappropriation in Internet face interactions, compromising user privacy. Image hiding can covertly embed confidential images in cover images while allowing seamless retrieval by recipients, providing new ideas for face privacy interactions. In this study, we present a novel framework, RFIS-FPI, which uses a target-protected hybrid policy combined with a reversible neural network to protect face image interactions. To address the character fidelity problem, this study employs a diffusion generation network that aims to generate cover images while preserving the essential properties of secret face images. To improve the covertness of the secret image, a low-frequency wavelet loss restriction is imposed on the information related to the secret image hidden in the high-frequency wavelet subbands, which significantly improves the security of the hidden image. Experimental evaluations show that RFIS-FPI excels in both face protection properties and hidden image detection, outperforming other state-of-the-art methods both qualitatively and quantitatively.},
  keywords={Steganography;Privacy;Visualization;Wavelet domain;Face recognition;Neural networks;Internet},
  doi={10.1109/FG59268.2024.10582015},
  ISSN={2770-8330},
  month={May},}@ARTICLE{10419371,
  author={Qi, Fan and Zhang, Huaiwen and Yang, Xiaoshan and Xu, Changsheng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={A Versatile Multimodal Learning Framework for Zero-Shot Emotion Recognition}, 
  year={2024},
  volume={34},
  number={7},
  pages={5728-5741},
  abstract={Multi-modal Emotion Recognition (MER) aims to identify various human emotions from heterogeneous modalities. With the development of emotional theories, there are more and more novel and fine-grained concepts to describe human emotional feelings. Real-world recognition systems often encounter unseen emotion labels. To address this challenge, we propose a versatile zero-shot MER framework to refine emotion label embeddings for capturing inter-label relationships and improving discrimination between labels. We integrate prior knowledge into a novel affective graph space that generates tailored label embeddings capturing inter-label relationships. To obtain multimodal representations, we disentangle the features of each modality into egocentric and altruistic components using adversarial learning. These components are then hierarchically fused using a hybrid co-attention mechanism. Furthermore, an emotion-guided decoder exploits label-modal dependencies to generate adaptive multimodal representations guided by emotion embeddings. We conduct extensive experiments with different multimodal combinations, including visual-acoustic and visual-textual inputs, on four datasets in both single-label and multi-label zero-shot settings. Results demonstrate the superiority of our proposed framework over state-of-the-art methods.},
  keywords={Emotion recognition;Task analysis;Semantics;Transformers;Feature extraction;Circuits and systems;Zero-shot learning;Multimodal emotion recognition;zero-shot learning;transformer},
  doi={10.1109/TCSVT.2024.3362270},
  ISSN={1558-2205},
  month={July},}@INPROCEEDINGS{10192125,
  author={Gojić, Gorana and Vincan, Vladimir and Kundačina, Ognjen and Mišković, Dragiša and Dragan, Dinu},
  booktitle={2023 10th International Conference on Electrical, Electronic and Computing Engineering (IcETRAN)}, 
  title={Non-adversarial Robustness of Deep Learning Methods for Computer Vision}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Non-adversarial robustness, also known as natural robustness, is a property of deep learning models that enables them to maintain performance even when faced with distribution shifts caused by natural variations in data. However, achieving this property is challenging because it is difficult to predict in advance the types of distribution shifts that may occur. To address this challenge, researchers have proposed various approaches, some of which anticipate potential distribution shifts, while others utilize knowledge about the shifts that have already occurred to enhance model generalizability. In this paper, we present a brief overview of the most recent techniques for improving the robustness of computer vision methods, as well as a summary of commonly used robustness benchmark datasets for evaluating the model’s performance under data distribution shifts. Finally, we examine the strengths and limitations of the approaches reviewed and identify general trends in deep learning robustness improvement for computer vision.},
  keywords={Deep learning;Training;Computer vision;Adaptation models;Computational modeling;Benchmark testing;Market research;non-adversarial;natural;out-of-distribution;robustness;domain adaptation;domain generalization;deep learning;computer vision},
  doi={10.1109/IcETRAN59631.2023.10192125},
  ISSN={},
  month={June},}@ARTICLE{10105934,
  author={Bian, Gui-Bin and Zhang, Li and Chen, He and Li, Zhen and Fu, Pan and Yue, Wen-Qian and Luo, Yu-Wen and Ge, Pei-Cong and Liu, Wei-Peng},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Motion Decoupling Network for Intra-Operative Motion Estimation Under Occlusion}, 
  year={2023},
  volume={42},
  number={10},
  pages={2924-2935},
  abstract={In recent intelligent-robot-assisted surgery studies, an urgent issue is how to detect the motion of instruments and soft tissue accurately from intra-operative images. Although optical flow technology from computer vision is a powerful solution to the motion-tracking problem, it has difficulty obtaining the pixel-wise optical flow ground truth of real surgery videos for supervised learning. Thus, unsupervised learning methods are critical. However, current unsupervised methods face the challenge of heavy occlusion in the surgical scene. This paper proposes a novel unsupervised learning framework to estimate the motion from surgical images under occlusion. The framework consists of a Motion Decoupling Network to estimate the tissue and the instrument motion with different constraints. Notably, the network integrates a segmentation subnet that estimates the segmentation map of instruments in an unsupervised manner to obtain the occlusion region and improve the dual motion estimation. Additionally, a hybrid self-supervised strategy with occlusion completion is introduced to recover realistic vision clues. Extensive experiments on two surgical datasets show that the proposed method achieves accurate motion estimation for intra-operative scenes and outperforms other unsupervised methods, with a margin of 15% in accuracy. The average estimation error for tissue is less than 2.2 pixels on average for both surgical datasets.},
  keywords={Optical flow;Surgery;Instruments;Estimation;Task analysis;Videos;Motion estimation;Computer-assisted surgery;motion estimation;optical flow;self-supervised learning;surgical images},
  doi={10.1109/TMI.2023.3268774},
  ISSN={1558-254X},
  month={Oct},}@ARTICLE{10795252,
  author={Wei, Xinjian and Qiu, Yu and Xu, Xiaoxuan and Xu, Jing and Mei, Jie and Zhang, Jun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={ECINFusion: A Novel Explicit Channel-Wise Interaction Network for Unified Multi-Modal Medical Image Fusion}, 
  year={2025},
  volume={35},
  number={5},
  pages={4011-4025},
  abstract={Multi-modal medical image fusion enhance the representation, aggregation and comprehension of functional and structural information, improving accuracy and efficiency for subsequent analysis. However, lacking explicit cross channel modeling and interaction among modalities results in the loss of details and artifacts. To this end, we propose a novel Explicit Channel-wise Interaction Network for unified multi-modal medical image Fusion, namely ECINFusion. ECINFusion encompasses two components: multi-scale adaptive feature modeling (MAFM) and explicit channel-wise interaction mechanism (ECIM). MAFM leverages adaptive parallel convolution and transformer in multi-scale manner to achieve the global context-aware feature representation. ECIM utilizes the designed multi-head channel-attention mechanism for explicit modeling in channel dimension to accomplish the cross-modal interaction. Besides, we introduce a novel adaptive L-Norm loss, preserving fine-grained details. Experiments demonstrate ECINFusion outperforms state-of-the-art approaches in various medical fusion sub-tasks on different metrics. Furthermore, extended experiments reveal the robust generalization of the proposed in different fusion tasks. In breif, the proposed explicit channel-wise interaction mechanism provides new insight for multi-modal interaction.},
  keywords={Biomedical imaging;Image fusion;Adaptation models;Feature extraction;Transformers;Magnetic resonance imaging;Convolution;Transforms;Single photon emission computed tomography;Semantics;Multi-modal medical image fusion;multi-modal image fusion;multi-modal interaction;feature fusion;transformer},
  doi={10.1109/TCSVT.2024.3516705},
  ISSN={1558-2205},
  month={May},}@ARTICLE{10508718,
  author={Chen, Puhua and Wang, Zhe and Mao, Shasha and Hui, Xinyue and Huyan, Ning},
  journal={IEEE Signal Processing Letters}, 
  title={Dual-Branch Residual Disentangled Adversarial Learning Network for Facial Expression Recognition}, 
  year={2024},
  volume={31},
  number={},
  pages={1840-1844},
  abstract={The facial expression recognition is very important for human-computer interaction. Therefore, a large number of researchers are focusing on this topic research and have acquired many valuable research achievements. However, there still exist many problems that need to be solved for practical applications, such as the impact of identity and appearance differences, posture change etc. In this work, a dual-branch residual disentangled adversarial learning network is proposed to learn more accurate expression features by disentangling the non-expression features from basic features through a novel combinatorial loss function. In the proposed method, dual-branch network structure is designed, one branch with a D-Net module is utilized to explore non-expression features and another branch just uses subtraction operation to obtain expression features. Based on the above network structure, a novel loss function is constructed to guide the two branches to learn different type features, which contains expression recognition loss, adversarial loss and cosine similarity loss. The main highlight of this work is that the proposed method could achieve the disentanglement of expression features and non-expression features just based on a low-complexity network and expression datasets without other auxiliary data. Finally, abundant experimental results on multiple expression datasets have confirmed the proposed method could obtain better expression recognition results than other state-of-the-art methods.},
  keywords={Feature extraction;Face recognition;Training;Facial features;Adversarial machine learning;Loss measurement;Testing;Facial expression recognition;feature disentanglement;adversarial training},
  doi={10.1109/LSP.2024.3390987},
  ISSN={1558-2361},
  month={},}@ARTICLE{10604846,
  author={Deng, Shiyu and Chen, Yinda and Huang, Wei and Zhang, Ruobing and Xiong, Zhiwei},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Unsupervised Domain Adaptation for EM Image Denoising With Invertible Networks}, 
  year={2025},
  volume={44},
  number={1},
  pages={92-105},
  abstract={Electron microscopy (EM) image denoising is critical for visualization and subsequent analysis. Despite the remarkable achievements of deep learning-based non-blind denoising methods, their performance drops significantly when domain shifts exist between the training and testing data. To address this issue, unpaired blind denoising methods have been proposed. However, these methods heavily rely on image-to-image translation and neglect the inherent characteristics of EM images, limiting their overall denoising performance. In this paper, we propose the first unsupervised domain adaptive EM image denoising method, which is grounded in the observation that EM images from similar samples share common content characteristics. Specifically, we first disentangle the content representations and the noise components from noisy images and establish a shared domain-agnostic content space via domain alignment to bridge the synthetic images (source domain) and the real images (target domain). To ensure precise domain alignment, we further incorporate domain regularization by enforcing that: the pseudo-noisy images, reconstructed using both content representations and noise components, accurately capture the characteristics of the noisy images from which the noise components originate, all while maintaining semantic consistency with the noisy images from which the content representations originate. To guarantee lossless representation decomposition and image reconstruction, we introduce disentanglement-reconstruction invertible networks. Finally, the reconstructed pseudo-noisy images, paired with their corresponding clean counterparts, serve as valuable training data for the denoising network. Extensive experiments on synthetic and real EM datasets demonstrate the superiority of our method in terms of image restoration quality and downstream neuron segmentation accuracy. Our code is publicly available at https://github.com/sydeng99/DADn.},
  keywords={Noise reduction;Noise measurement;Noise;Image denoising;Training;Image reconstruction;Biomedical imaging;Unsupervised domain adaptation;image denoising;electron microscopy},
  doi={10.1109/TMI.2024.3431192},
  ISSN={1558-254X},
  month={Jan},}@INPROCEEDINGS{10027797,
  author={Sun, Yifan and Zhao, Yunfeng and Yu, Guoxian and Yan, Zhongmin and Domeniconi, Carlotta},
  booktitle={2022 IEEE International Conference on Data Mining (ICDM)}, 
  title={Few-shot Partial Multi-label Learning with Data Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={478-487},
  abstract={Partial multi-label learning (PML) models the scenario where each training sample is annotated with a set of candidate labels, but only a subset of them corresponds to the ground-truths. The key challenge for PML is how to minimize the negative impact of incorrect labels concealed within the candidate ones. Most existing PML solutions require abundant samples to train a noise-robust multi-label predictor. However, due to privacy, safety or ethic issues, we more often have a handful of training samples for the target task. In this paper, we propose an approach named FsPML-DA (Few-shot Partial Multi-Label Learning with Data Augmentation) to simultaneously estimate label confidence, perform data augmentation and induce multilabel classifier. Specifically, FsPML-DA disambiguates the label confidence vector of each PML sample by jointly modeling the feature and semantic similarity, label credibility of other samples and label co-occurrence. Next, FsPML-DA introduces a synthetic feature network to generate more training samples from pairs of given samples with label confidence values. FsPML-DA then leverages original and generated samples to train a noise-tolerant multi-label classifier. Extensive experiments on benchmark datasets show that FsPML-DA performs better than recent competitive PML baselines and few-shot solutions. FsPML-DA can dislodge noisy labels by mining PML data in a sensible way and the proposed data augmentation strategy effectively combats with the scarcity of few-shot training samples.},
  keywords={Training;Privacy;Semantics;Benchmark testing;Feature extraction;Safety;Noise robustness;Multi-label Learning;Few-shot Learning;Noisy labels;Data augmentation},
  doi={10.1109/ICDM54844.2022.00058},
  ISSN={2374-8486},
  month={Nov},}@INPROCEEDINGS{9412141,
  author={Wu, Haoxue and Huang, Huaibo and Yu, Aijing and Cao, Jie and Lei, Zhen and He, Ran},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Exemplar Guided Cross-Spectral Face Hallucination via Mutual Information Disentanglement}, 
  year={2021},
  volume={},
  number={},
  pages={4206-4212},
  abstract={Recently, many Near infrared-visible (NIR-VIS) heterogeneous face recognition (HFR) methods have been proposed in the community. But it remains a challenging problem because of the sensing gap along with large pose variations. In this paper, we propose an Exemplar Guided Cross-Spectral Face Hallucination (EGCH) to reduce the domain discrepancy through disentangled representation learning. For each modality, EGCH contains a spectral encoder as well as a structure encoder to disentangle spectral and structure representation, respectively. It also contains a traditional generator that reconstructs the input from the above two representations, and a structure generator that predicts the facial parsing map from the structure representation. Besides, mutual information minimization and maximization are conducted to boost disentanglement and make representations adequately expressed. Then the translation is built on structure representations between two modalities. Provided with the transformed NIR structure representation and original VIS spectral representation, EGCH is capable to produce high-fidelity VIS images that preserve the topology structure of the input NIR while transfer the spectral information of an arbitrary VIS exemplar. Extensive experiments demonstrate that the proposed method achieves more promising results both qualitatively and quantitatively than the state-of-the-art NIR-VIS methods.},
  keywords={Network topology;Face recognition;Buildings;Minimization;Generators;Topology;Sensors},
  doi={10.1109/ICPR48806.2021.9412141},
  ISSN={1051-4651},
  month={Jan},}@INPROCEEDINGS{9517333,
  author={Wu, Zhiheng and Wu, Zhengxing and Lu, Yue and Wang, Jian and Yu, Junzhi},
  booktitle={2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)}, 
  title={A Novel Underwater Image Synthesis Method Based on a Pixel-Level Self-Supervised Training Strategy}, 
  year={2021},
  volume={},
  number={},
  pages={1254-1259},
  abstract={With the rapid development of deep neural networks, underwater vision plays an increasingly important role in the underwater robotic operation. However, the scarce underwater datasets greatly limit the performance of deep learning on underwater visual tasks, further hindering the applications of underwater operation. To solve this problem, we propose an underwater image synthesis method, which can directly convert the natural light image into the synthetic underwater image end-to-end. Particularly, a pixel-level self-supervised training strategy is designed to maximize the structural similarity between the synthesized and real images, through training the real underwater images. Finally, extensive experiments are carried out, and the obtained results demonstrate the effectiveness and superiority of our methods by quantitative and qualitative comparisons. The proposed underwater image synthesis method offers a valuable sight for underwater vision and manipulating control.},
  keywords={Training;Deep learning;Visualization;Image synthesis;Conferences;Real-time systems;Task analysis;Underwater image synthesis;self-supervised learning;underwater vision;CNNs},
  doi={10.1109/RCAR52367.2021.9517333},
  ISSN={},
  month={July},}@INPROCEEDINGS{9733464,
  author={Verwilst, Maxim and Žižakić, Nina and Gu, Lingchen and Pižurica, Aleksandra},
  booktitle={2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP)}, 
  title={Deep image hashing based on twin-bottleneck hashing with variational autoencoders}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={With the ever-increasing availability of data, the need for efficient and accurate image retrieval methods has become larger and larger. Deep hashing has proven to be a promising solution, by defining a hash function to convert the data into a manageable lower-dimensional representation. In this paper, we apply recent insights from the field of variational autoencoders to the field of deep image hashing, thus achieving an improvement over the current state of the art as shown by experimental evaluation. The code used in this paper is open-source and available on GitHub (https://github.com/maximverwilst/deepimagehashing-VAE).},
  keywords={Hash functions;Deep learning;Adaptation models;Codes;Conferences;Image retrieval;Signal processing;Image hashing;deep hashing;content-based image retrieval;variational autoencoders;unsupervised deep learning},
  doi={10.1109/MMSP53017.2021.9733464},
  ISSN={2473-3628},
  month={Oct},}@ARTICLE{10458950,
  author={Lin, Ling and Liu, Hao and Liang, Jinqiao and Li, Zhendong and Feng, Jiao and Han, Hu},
  journal={IEEE Transactions on Image Processing}, 
  title={Consensus-Agent Deep Reinforcement Learning for Face Aging}, 
  year={2024},
  volume={33},
  number={},
  pages={1795-1809},
  abstract={Face aging tasks aim to simulate changes in the appearance of faces over time. However, due to the lack of data on different ages under the same identity, existing models are commonly trained using mapping between age groups. This makes it difficult for most existing aging methods to accurately capture the correspondence between individual identities and aging features, leading to generating faces that do not match the real aging appearance. In this paper, we re-annotate the CACD2000 dataset and propose a consensus-agent deep reinforcement learning method to solve the aforementioned problem. Specifically, we define two agents, the aging process agent and the aging personalization agent, and model the task of matching aging features as a Markov decision process. The aging process agent simulates the aging process of an individual, while the aging personalization agent calculates the difference between the aging appearance of an individual and the average aging appearance. The two agents iteratively adjust the matching degree between the target aging feature and the current identity through a form of synergistic cooperation. Extensive experimental results on four face aging datasets show that our model achieves convincing performance compared to the current state-of-the-art methods.},
  keywords={Aging;Faces;Face recognition;Feature extraction;Hair;Deep reinforcement learning;Task analysis;Face aging;deep reinforcement learning;Markov decision process},
  doi={10.1109/TIP.2024.3364074},
  ISSN={1941-0042},
  month={},}@ARTICLE{10767603,
  author={Wu, Yang and Deng, Ye and Zhou, Sanping and Liu, Yuhan and Huang, Wenli and Wang, Jinjun},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={CR-former: Single-Image Cloud Removal With Focused Taylor Attention}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={Cloud removal aims to restore high-quality images from cloud-contaminated captures, which is essential in remote sensing applications. Effectively modeling the long-range relationships between image features is key to achieving high-quality cloud-free images. While self-attention mechanisms excel at modeling long-distance relationships, their computational complexity scales quadratically with image resolution, limiting their applicability to high-resolution remote sensing images. Current cloud removal methods have mitigated this issue by restricting the global receptive field to smaller regions or adopting channel attention to model long-range relationships. However, these methods either compromise pixel-level long-range dependencies or lose spatial information, potentially leading to structural inconsistencies in restored images. In this work, we propose the focused Taylor attention (FT-Attention), which captures pixel-level long-range relationships without limiting the spatial extent of attention and achieves the  $\mathcal {O}(N)$  computational complexity, where N represents the image resolution. Specifically, we utilize Taylor series expansions to reduce the computational complexity of the attention mechanism from  $\mathcal {O}(N^{2})$  to  $\mathcal {O}(N)$ , enabling efficient capture of pixel relationships directly in high-resolution images. Additionally, to fully leverage the informative pixel, we develop a new normalization function for the query and key, which produces more distinguishable attention weights, enhancing focus on important features. Building on FT-Attention, we design a U-net style network, termed the CR-former, specifically for cloud removal. Extensive experimental results on representative cloud removal datasets demonstrate the superior performance of our CR-former. The code is available at https://github.com/wuyang2691/CR-former.},
  keywords={Clouds;Image restoration;Computational modeling;Remote sensing;Atmospheric modeling;Image resolution;Attention mechanisms;Limiting;Transformers;Adaptation models;Cloud removal;focused Taylor attention (FT-Attention);remote-sensing image},
  doi={10.1109/TGRS.2024.3506780},
  ISSN={1558-0644},
  month={},}@ARTICLE{10770270,
  author={Liu, Jinyang and Li, Shutao and Dian, Renwei and Song, Ze and Tan, Lishan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Asymptotic Spectral Mapping for Hyperspectral Image Fusion}, 
  year={2025},
  volume={35},
  number={4},
  pages={3475-3485},
  abstract={The fusion of low-resolution hyperspectral images (LR HSI) and high-resolution multispectral images (HR MSI) is a crucial approach for generating hyperspectral images (HSI). However, existing hyperspectral image fusion methods often rely on a single feature mapping process, which makes it difficult to accommodate the significant differences in features between the source images and the real images. Consequently, the generated images frequently exhibit varying degrees of information loss across different spectral bands and limit the overall performance of the fusion. To address this issue, we propose a novel hyperspectral image fusion network. Specifically, an asymptotic spectral mapping module is designed to enhance the detail information fitting capabilities of the fusion network. This module transforms the fitting process of missing information into multiple sets of fitting processes with varying degrees, which can map features with different spectral fidelity to various scales and gradually fit spectral information, thereby reducing spectral distortion. Additionally, we introduce an adaptive defect optimization loss that guides the network to focus on reconstructing regions with substantial spectral differences between LR HSI and HSI, optimizing the network’s constraints regarding the similarity between predicted and real images. Experimental results demonstrate that the proposed fusion network outperforms existing state-of-the-art methods across diverse datasets.},
  keywords={Hyperspectral imaging;Image fusion;Feature extraction;Fitting;Image reconstruction;Transformers;Optimization;Data mining;Spatial resolution;Accuracy;Image fusion;hyperspectral image;spectral mapping;deep learning},
  doi={10.1109/TCSVT.2024.3507860},
  ISSN={1558-2205},
  month={April},}@ARTICLE{10704966,
  author={Zhang, Yuzhen and Su, Junning and Guo, Hang and Li, Chaochao and Lv, Pei and Xu, Mingliang},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={S-CVAE: Stacked CVAE for Trajectory Prediction With Incremental Greedy Region}, 
  year={2024},
  volume={25},
  number={12},
  pages={20351-20363},
  abstract={Predicting accurate future trajectories of agents is essential for autonomous navigation in complex scenarios. Although numerous work has made great progress on this goal, it is still challenging due to the uncertainty and continuity of behavioral intentions of agents, where uncertainty means the instantaneous multimodality of motion behavior, while the continuity refers to the consistency and stability of behavioral intention of an agent over a period of time constrained by its final destination. These factors easily affect the improvement of prediction accuracy. In this paper, we present a novel trajectory prediction method, Stacked Conditional VAE (S-CVAE) with Incremental Greedy Region (IGR). Specifically, the IGR is designed to enlarge the coverage of candidate waypoints/endpoints by reformulating the waypoints/endpoints prediction problem as candidate region generation, which can further encourage and model multimodality of behavioral intentions. Meanwhile, to exploit the inherent continuity between adjacent behavioral intentions of an agent, the S-CVAE architecture is constructed to transmit the behavioral intentions of one agent by inserting intermediate waypoints with IGR into the potential trajectories from the observed path to the final endpoint, and also enhances the reliability of the generated waypoints/endpoints in the next moment, further improve the accuracy of trajectory prediction. Our method is evaluated on several public datasets, including nuScenes, Apolloscape, SDD, INTERSECTION, Waymo, and VTPTL. The comprehensive experimental results demonstrate that our method achieves significant performance on these datasets. Especially in nuScenes and VTPTL, the accuracy is increased by at least 11.11% on average ADE and 2.40% on average FDE compared with state-of-the-arts.},
  keywords={Trajectory;Predictive models;Uncertainty;Accuracy;Turning;Shape;Reliability;Proposals;Navigation;Motion segmentation;Trajectory prediction;incremental greedy region;stacked conditional VAE;instantaneous multimodality;inherent continuity},
  doi={10.1109/TITS.2024.3465836},
  ISSN={1558-0016},
  month={Dec},}@INPROCEEDINGS{10802391,
  author={L’Erario, Giuseppe and Hanover, Drew and Romero, Ángel and Song, Yunlong and Nava, Gabriele and Viceconte, Paolo Maria and Pucci, Daniele and Scaramuzza, Davide},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Learning to Walk and Fly with Adversarial Motion Priors}, 
  year={2024},
  volume={},
  number={},
  pages={10370-10377},
  abstract={Robot multimodal locomotion encompasses the ability to transition between walking and flying, representing a significant challenge in robotics. This work presents an approach that enables automatic smooth transitions between legged and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our method allows the robot to imitate motion datasets and accomplish the desired task without the need for complex reward functions. The robot learns walking patterns from human-like gaits and aerial locomotion patterns from motions obtained using trajectory optimization. Through this process, the robot adapts the locomotion scheme based on environmental feedback using reinforcement learning, with the spontaneous emergence of mode-switching behavior. The results highlight the potential for achieving multimodal locomotion in aerial humanoid robotics through automatic control of walking and flying modes, paving the way for applications in diverse domains such as search and rescue, surveillance, and exploration missions. This research contributes to advancing the capabilities of aerial humanoid robots in terms of versatile locomotion in various environments. Video: https://youtu.be/mi6Do-x67CM},
  keywords={Legged locomotion;Navigation;Surveillance;Humanoid robots;Switches;Reinforcement learning;Reliability;Robots;Trajectory optimization;Intelligent robots},
  doi={10.1109/IROS58592.2024.10802391},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9736288,
  author={Wang, X. and Li, H. and Zimmermann, A. and Pinkwart, N. and Werde, S. and Van Rijn, L. and De Witt, C. and Baudach, B.},
  booktitle={2022 IEEE 16th International Conference on Semantic Computing (ICSC)}, 
  title={IFSE - Personalized Quiz Generator and Intelligent Knowledge Recommendation}, 
  year={2022},
  volume={},
  number={},
  pages={201-208},
  abstract={This paper has presented an AI-based quiz sub-system that customizes personalized exercises for individual learners together with accurate instant feedback and knowledge recommendation after taking quizzes, eventually aiming to assist learners with self-regulated learning. A knowledge-based quiz generation algorithm and a set of intelligent feedback recom-mendation algorithms are proposed, which are designed for a large number of exercise materials from various courses to numerous students' self-learning and, generically, for all kinds of knowledge domains. Intelligent feedback for student exercise (IFSE) application has been developed and integrated into the Moodle learning management system in order to conduct field experiments and to be evaluated by university students.},
  keywords={Learning management systems;Conferences;Semantics;Knowledge based systems;Generators;Personal Quiz;Intelligent Feedback;Ontology;Knowledge Graph;Knowledge Recommendation;Self-regulated Learning},
  doi={10.1109/ICSC52841.2022.00041},
  ISSN={2325-6516},
  month={Jan},}@ARTICLE{10620325,
  author={Li, Hanyang and Sun, Yuhang and Li, Jiahui and Li, Hang and Dong, Hongli},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={A Meta-Learning-Based Approach for Automatic First-Arrival Picking}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={Precise first-arrival picking holds pivotal importance in the realms of seismic data processing and microseismic monitoring. Recently, data-driven approaches have shown remarkable performance. However, these approaches rely on high-quality labeled datasets and involve a time-consuming and labor-intensive labeling process. In addition, data-driven picking methods often suffer from generalization problems in the face of varying noise characteristics and geological environments. To tackle the challenges head-on, this study introduces a novel training algorithm grounded in meta-learning. In contrast to traditional training methods, this innovative approach distinguishes itself by reducing the costs associated with dataset creation and requiring only a modest number of high-quality labeled samples to achieve superior performance. Furthermore, the proposed method can be seamlessly implemented with different types of deep neural networks (DNNs). Our extensive experimentation on two field datasets encompassing distinct geological zones demonstrates the method’s effectiveness in alleviating the dependence on high-quality training samples, enhancing first-arrival picking accuracy, and bolstering the model’s robustness against strong noise interference.},
  keywords={Training;Task analysis;Metalearning;Data models;Geology;Adaptation models;Accuracy;First-arrival picking;meta-learning;nonaccurate label;seismic data},
  doi={10.1109/TGRS.2024.3436817},
  ISSN={1558-0644},
  month={},}@ARTICLE{10934063,
  author={Dai, Penglin and Zhou, Junfei and Ma, Jialong and Zhang, Hao and Wu, Xiao},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Meta-Transfer Learning-Based Cross-Domain Gesture Recognition Using WiFi Channel State Information}, 
  year={2025},
  volume={71},
  number={2},
  pages={2530-2543},
  abstract={Gesture recognition plays a crucial role in a wide range of consumer electronics applications, including human-computer interaction and virtual reality, by enabling the identification and interpretation of human gestures. In recent times, WiFi-based gesture recognition has garnered significant attention due to its privacy protection and unobtrusive nature. However, this approach heavily depends on neural network-based models and is notably influenced by environmental conditions, such as specific location and orientation. To address these environmental impacts, we propose a network framework that leverages transfer learning and meta-learning. The primary focus is on utilizing transfer learning to train a convolutional neural network (ResNet) for feature extraction, enabling the extraction of domain-independent features specifically related to gestures, rather than being influenced by environmental factors. Additionally, we employ the meta-learning algorithm MAML to train the fully connected network for gesture classification. Following training, a small set of samples can be utilized for rapid adaptation to diverse domains, maintaining high accuracy across different domains to achieve cross-domain gesture recognition. Our performance evaluation is based on the Widar 3.0 dataset, encompassing both in-domain and cross-domain scenarios. The simulation results demonstrate that our proposed algorithm surpasses Widar3.0 and WiGRUNT by approximately 7.9% and 1.12% in gesture recognition accuracy across all scenarios.},
  keywords={Gesture recognition;Feature extraction;Wireless fidelity;Accuracy;Training;Metalearning;Transfer learning;Solid modeling;Convolutional neural networks;Consumer electronics;Channel state information;cross domain;gesture recognition;meta learning},
  doi={10.1109/TCE.2025.3552827},
  ISSN={1558-4127},
  month={May},}@ARTICLE{11005543,
  author={Xu, Hai and Zhou, Hui and Zhong, Sheng and Yan, Luxin and Zou, Xu},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Consistent Learning of Sparse Background Features for Infrared Small-Target Labeling}, 
  year={2025},
  volume={63},
  number={},
  pages={1-13},
  abstract={Recent years have witnessed many remarkable achievements in infrared small-target (IST) detection based on deep learning (DL). To achieve high performance in real-world applications, DL methods require a substantial number of accurate labels. However, IST annotating is labor-intensive as they are very small. Determining and annotating edge pixels demands considerable time and effort, which slows down data expansion and further research in IST detection. To mitigate the issue, we propose a pseudo-label (PL) generation method named consistent learning of sparse background feature (CLSBF). This approach models the generation of IST PLs as a domain transformation from local target maps to background ones. It can relax the supervision requirements of DL methods, transitioning from absolute pixel-level supervision to point supervision. This method employs an unsupervised approach primarily, supplemented by semi-simulated supervision, to achieve mutual conversion of local images from different domains and obtain the final target PLs through the differences between target images and transformed background ones. Experiments show that equipped with our method, models trained with the input of a coarsely accurate center label can achieve performance up to 99.94% compared to models trained with official accurate labels. Furthermore, when the official labels are not accurate enough, models trained with PLs generated by CLSBF consistently show a performance improvement of 1.11%–4.09% when they are evaluated based on re-labeled bounding boxes. Extensive experiments demonstrate the reliability of CLSBF in generating PLs and its potential to alleviate the labor-intensive process of manual labeling significantly. We have released an IST labeling tool with CLSBF as an assistant at https://github.com/SeaHifly/CLSBF_software.git},
  keywords={Annotations;Accuracy;Training;Labeling;Image segmentation;Image reconstruction;Image edge detection;Deep learning;Manuals;Object detection;Deep learning (DL);infrared small target (IST);pseudo-label (PL) generation},
  doi={10.1109/TGRS.2025.3570274},
  ISSN={1558-0644},
  month={},}
