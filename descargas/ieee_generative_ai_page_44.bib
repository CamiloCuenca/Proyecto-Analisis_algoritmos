@INPROCEEDINGS{11160586,
  author={Zhang, Sai and Liang, Xiao and Li, Xuxu and Qi, Xuanwei and Liu, Hao and Wang, Jian and Zhang, Linghao},
  booktitle={2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Research on Efficient Adaptation and Knowledge Enhancement Technology for Generative Models in Power Equipment Inspection Scenario}, 
  year={2025},
  volume={},
  number={},
  pages={97-103},
  abstract={The paper explores the adaptation and knowledge enhancement of generative models for power equipment maintenance and inspection. Given the complex, variable environments and high precision requirements in this field, traditional knowledge-based question-answering systems fall short in realtime decision-making. We propose a deep learning-based generative model adaptation framework (DLGA) that fine-tunes large-scale power equipment standards and regulations, improving question-answering accuracy to 85 %, a 5 % improvement over traditional systems. This en-hances the model's adaptability and reasoning capabilities, providing more efficient and intelligent decision-making support for power equipment maintenance and inspection.},
  keywords={Training;Adaptation models;Accuracy;Knowledge based systems;Decision making;Semantics;Inspection;Maintenance;Optimization;Standards;Deep learning;Generative models;GAN;Power inspection and maintenance;Replay pool},
  doi={10.1109/AIEA66061.2025.11160586},
  ISSN={},
  month={Aug},}@ARTICLE{10495156,
  author={Roy, Arunava and Dasgupta, Dipankar},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Distributed Conditional Wasserstein Deep Convolutional Relativistic Loss Generative Adversarial Network With Improved Convergence}, 
  year={2024},
  volume={5},
  number={9},
  pages={4344-4353},
  abstract={Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.},
  keywords={Servers;Generators;Training;Convergence;Computational modeling;Generative adversarial networks;Synchronization;Distance metrics;distributed-GANs (D-GANs);federated learning (FL);FL-GANs;standalone GANs},
  doi={10.1109/TAI.2024.3386500},
  ISSN={2691-4581},
  month={Sep.},}@INPROCEEDINGS{10462832,
  author={Asan, Jerico and Ekaputri, Ivana and Natalie, Catherine and Purwandari, Kartika},
  booktitle={2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP)}, 
  title={Exploring Generative Adversarial Networks (GANs) for Deepfake Detection: A Systematic Literature Review}, 
  year={2023},
  volume={},
  number={},
  pages={189-194},
  abstract={This paper explores the application of Generative Adversarial Networks, also known as GANs for deepfake detection. It investigates the use of various GAN models in deepfake detection, subjecting them through training against other models, and in many different datasets. Through that process, this paper aims to improve detection performance and adaptability to many evolving deepfake techniques. Evaluations are conducted on benchmark deepfake datasets, evaluation metrics, and comparation between the proposed GAN-based detection methods with the most prominent approaches. The results provide insights to the strengths and weaknesses of GANs for deepfake detection and help to develop more robust systems in the same field.},
  keywords={Training;Deepfakes;Steganography;Adaptation models;Systematics;Media;Generative adversarial networks;Deepfake;Deepfake Detection;Fake Images Generation;GAN;Deep Learning},
  doi={10.1109/IWAIIP58158.2023.10462832},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9288306,
  author={Zhu, Shixiong and Luo, Xiangfeng and Ma, Liyan and Xie, Shaorong and Zhang, Han},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Realistic Style-Transfer Generative Adversarial Network With a Weight-Sharing Strategy}, 
  year={2020},
  volume={},
  number={},
  pages={694-699},
  abstract={Style transfer aims to generate images by combining the style of one image and the content of another. Though valuable efforts have been made in generating high-quality style transferred images, the resulting images are far from the distribution of real images. This greatly limits the application of style transfer such as improving the diversity of training set in computer vision task. We find that the reason of style transfer failing to generate realistic images is lack of reference targets and neglect of preserving data distribution. To solve the problem, we propose a Style-transfer Generative Adversarial Network with a weight-sharing strategy to make the stylized images be resemblance to the real images. The experimental results demonstrate that the proposed method can generate images with satisfying style transfers and high visual quality. Moreover, we apply our stylized images to augment the training set of object detection task, and improve the average precision faithfully. We believe that our method can enhance the performance of style transfer on computer vision tasks.},
  keywords={Training;Computer vision;Visualization;Object detection;Tools;Generative adversarial networks;Task analysis;Style Transfer;Adversarial Loss;Weight-sharing Strategy},
  doi={10.1109/ICTAI50040.2020.00111},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10528538,
  author={Li, Peipei and Qiu, Zhao and Lin, Jiale and Chen, Huajing},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={DACGAN-TMS: A Diverse Conditional Image Synthesis Method Based on Generative Adversarial Networks}, 
  year={2023},
  volume={},
  number={},
  pages={438-444},
  abstract={Image synthesis based on generative adversarial networks (GANs) is a hot spot in current computer vision research. The diverse condition image synthesis tasks are necessary to ensure that the generated images still have high diversity while meeting the input conditions. However, conditional GANs (cGANs) excessively rely on the input conditional information and ignore the noise vectors, which can easily lead to the mode collapse problem. Although many methods have been proposed to solve this problem, these methods still have some limitations. In this work, we propose an innovative model called DACGAN-TMS, which is used to generate diverse images without increasing too much training overhead. In DACGAN-TMS, an additional regularization term is added to the loss function of the generator model to encourage the generator to detect more minor modes during training, that is, to stimulate the generator to generate different pictures in the course of training. It effectively improves the problem that the original ACGAN is easy to collapse during training and difficult to generate diversified pictures. In addition, the standard convolution of the generator is completely replaced by dynamic convolution, and the convolution parameters are adaptively adjusted to further enhance the expressive capability of the model. Extensive experiments and statistical comparison results manifest that DACGAN-TMS is superior to the relevant GAN-based models in generating different images.},
  keywords={Training;Adaptation models;Convolution;Image synthesis;Noise;Generative adversarial networks;Generators;image synthesis;ACGAN;diversity;mode collapse;dynamic convolution},
  doi={10.1109/ACAIT60137.2023.10528538},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11166734,
  author={Nirmala, M B and Swathi, Paapanna Gari and Mahaladkar, Shashank K and Tarun, N and Chethan Kumar, H and Deshmukh, Vinay Kumar A},
  booktitle={2025 5th International Conference on Intelligent Technologies (CONIT)}, 
  title={Smart Healthcare Monitoring: A Generative AI Approach for Real-Time Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The GenAI-Driven Real-Time Healthcare Monitoring System is a novel solution for transforming modern healthcare through the integration of wearable devices, Generative Artificial Intelligence (GenAI), and machine learning models. It addresses the most critical challenges of data scarcity, continuous health monitoring, and personalized care. Heart rate, blood pressure, and oxygen saturation can be monitored in real-time through wearable devices. Predictions of cardiovascular risks are enabled by LightGBM models. Synthetic health datasets mimicking real-world health data are generated using GANs. Such generated synthetic datasets allow for privacy while training robust models. The proposed work is available through a web interface, the system offers current metrics, trends, and suggestions in real-time using advanced LLM. GDPR and other privacy compliance standards are considered while developing this platform, multiple user support is also considered for chronic disease management and preventive care. Scalable and privacy-oriented, this system will close the gaps in existing healthcare solutions, demonstrating improved prediction accuracy and usability. Future developments may include IoT integration and mobile accessibility.},
  keywords={Performance evaluation;Accuracy;Generative AI;Medical services;Predictive models;Real-time systems;Wearable devices;Monitoring;Synthetic data;Diseases;component;formatting;style;styling;insert},
  doi={10.1109/CONIT65521.2025.11166734},
  ISSN={},
  month={June},}@INPROCEEDINGS{9179472,
  author={Caporusso, Nicholas and Zhang, Kelei and Carlson, Gordon},
  booktitle={2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)}, 
  title={Using Eye-tracking to Study the Authenticity of Images Produced by Generative Adversarial Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Nowadays, Machine Learning algorithms, such as Generative Adversarial Networks (GANs), enable generating content, and especially images, featuring people, objects, or landscapes, with unprecedented levels of accuracy and fidelity. As a result, it is becoming challenging for a viewer to distinguish a picture of a fake profile from one that has a real human in it. In this paper, we present the results of an experimental study in which we investigated the perception of images produced by GANs. Specifically, we focused on the individuals' ability to discriminate between fake and real profiles. Furthermore, we utilized eye-tracking technology to identify the presence of patterns in subjects' gaze, which, in turn, can be useful to optimize the output of GANs and, simultaneously, provide insight on the underlying cognitive dynamics.},
  keywords={Image recognition;Gallium nitride;Software;Artificial intelligence;Machine learning algorithms;Generative adversarial networks;Heuristic algorithms;Generative Adversarial Networks;Eye tracking;Cybersecurity;Faceforensics},
  doi={10.1109/ICECCE49384.2020.9179472},
  ISSN={},
  month={June},}@INPROCEEDINGS{9874614,
  author={Dubnov, Shlomo and Assayag, Gerard and Gokul, Vignesh},
  booktitle={2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR)}, 
  title={Creative Improvised Interaction with Generative Musical Systems}, 
  year={2022},
  volume={},
  number={},
  pages={121-126},
  abstract={In this paper we survey the methods for control and cre-ative interaction with pre-trained generative models for au-dio and music. By using reduced (lossy) encoding and sym-bolization steps we are able to examine the level of information that is passing between the environment (the musician) and the agent (machine improvisation). We further use the concept of music information dynamics to find an optimal symbolization in terms of predictive information measure. Methods and strategies for generative models are surveyed in this paper and their implications for creative interaction with the machine are discussed in the musical improvisation framework.},
  keywords={Multimedia systems;Music;Information processing;Encoding;Creativity;Creative Interaction;Music Synthesis;Generative Models;Distributed Co Creativity},
  doi={10.1109/MIPR54900.2022.00028},
  ISSN={2770-4319},
  month={Aug},}@INPROCEEDINGS{10862366,
  author={Gunawan, Ali and Wiputra, Richard},
  booktitle={2024 IEEE 12th Conference on Systems, Process & Control (ICSPC)}, 
  title={Exploring the Impact of Generative AI on Personalized Learning in Higher Education}, 
  year={2024},
  volume={},
  number={},
  pages={280-285},
  abstract={Generative AI (GAI) is revolutionizing higher education by personalizing learning experiences through the analysis of large student datasets. GAI adapts educational content, learning paths, and feedback to meet the unique needs of individual students, thereby enhancing engagement, motivation, and academic success. Despite these benefits, challenges such as the lack of technical expertise among educators, algorithmic bias, data privacy concerns, and financial constraints hinder the widespread adoption of AIdriven personalized learning systems. Ethical considerations, including transparency and equity, must also be addressed. Future research should explore the long-term effects of AI on critical thinking, teacher training, and the digital divide, as well as financial strategies to make AI integration more accessible.},
  keywords={Training;Learning systems;Ethics;Data privacy;Privacy;Generative AI;Learning (artificial intelligence);Digital divide;Generative AI;personalized learning;higher education;adaptive learning systems;student engagement;data privacy},
  doi={10.1109/ICSPC63060.2024.10862366},
  ISSN={2769-7916},
  month={Dec},}@INPROCEEDINGS{10743843,
  author={Gupta, Sachin and Inzimam and Manivasagam},
  booktitle={2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)}, 
  title={Investigating the use of Neural Networks in Enhancing Music Composition and Sound Quality}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={An exciting new area of study in the world of music technology is the use of neural networks to the improvement of sound quality and the automation of musical creation. The goal of this research is to evaluate the potential of neural networks in boosting musical creation and reception. To determine whether algorithms based on neural networks can help us create groundbreaking music, we do extensive research into the field. First, the study presents a recurrent neural network (RNN)-based approach to composing melodic sequences of notes. This method draws on the remarkable pattern-recognition skills of neural networks to motivate richer musical production. Listening to a big library of pieces, the RNN model is able to learn how to construct new pieces that are enjoyable to human hearing. Synthesizing high-quality audio waveforms is crucial in modern music composition, and this study investigates the use of Generative Adversarial Networks (GANs) to this problem. The AudioWaveGAN approach utilizes GANs to produce high-quality sound, filtering out unwanted noise while also composing immersive pieces. Our study compares the proposed method to various established rivals. The criteria for evaluation are based on genuine conventional methods. The findings illustrate the superiority of the suggested technique in terms of originality, sound quality, and a combined score, emphasizing its potential to change music creation and production. This study represents a major advance toward using neural networks in music creation and audio processing. The research shows that these fresh methods open up exciting opportunities for artists, composers, and music producers who want to release hitherto unattainable wellsprings of inspiration and provide their listeners with fully enveloping aural journeys.},
  keywords={Industries;Technological innovation;Recurrent neural networks;Noise;Music;Production;Writing;Generative adversarial networks;Libraries;Sparks;Artificial Intelligence;Creativity;Enhancement;Machine Learning;Music Composition;Neural Networks;Sound Quality;Technology;Innovation;Data Analysis},
  doi={10.1109/ACROSET62108.2024.10743843},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10959444,
  author={Das, Soma and Santra, Debabrata and Chhari, Trinanjan and Roy, Shubhasri and Mukherjee, Shreejita},
  booktitle={2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)}, 
  title={Prompt Driven Image Creation: A Comparative Evaluation of Generative AI Tools}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a comparative analysis of four generative AI models namely ChatGPT, Gemini, Copilot, and Stable Diffusion - evaluated on metrics such as visual quality, prompt adherence, creativity, usability, and processing time. Visual and quantitative results highlight Gemini and Copilot's superiority in artistic and imaginative tasks, while Stable Diffusion excels in customization for advanced users. ChatGPT demonstrates ease of use but is limited in complexity. We also provides the best practices in selecting and using such tools to improve creative work. This study emphasizes the importance of evaluating generative AI tools based on diverse requirements and practical use cases.},
  keywords={Measurement;Visualization;Image synthesis;Digital art;Chatbots;Complexity theory;Prompt engineering;Usability;Creativity;Best practices;Generative AI;Image Generation;Prompt Engineering;Digital Art;Visual Creativity;AI Tools Comparison},
  doi={10.1109/IEMENTech65115.2025.10959444},
  ISSN={2767-9934},
  month={Jan},}@INPROCEEDINGS{8834613,
  author={Oza, Manan and Vaghela, Himanshu and Bagul, Sudhir},
  booktitle={2019 International Conference of Artificial Intelligence and Information Technology (ICAIIT)}, 
  title={Semi-Supervised Image-to-Image Translation}, 
  year={2019},
  volume={},
  number={},
  pages={16-20},
  abstract={Image-to-image translation is a long-established and a difficult problem in computer vision. In this paper we propose an adversarial based model for image-to-image translation. The regular deep neural-network based methods perform the task of image-to-image translation by comparing gram matrices and using image segmentation which requires human intervention. Our generative adversarial network based model works on a conditional probability approach. This approach makes the image translation independent of any local, global and content or style features. In our approach we use a bidirectional reconstruction model appended with the affine transform factor that helps in conserving the content and photorealism as compared to other models. The advantage of using such an approach is that the image-to-image translation is semi-supervised, independent of image segmentation and inherits the properties of generative adversarial networks tending to produce realistic. This method has proven to produce better results than Multimodal Unsupervised Image-to-image translation.},
  keywords={Image reconstruction;Convolutional codes;Photorealism;Generative adversarial networks;Image segmentation;Generators;Mathematical model;GANs;image-to-image translation;style transfer},
  doi={10.1109/ICAIIT.2019.8834613},
  ISSN={},
  month={March},}@ARTICLE{10729227,
  author={Wu, Hongcheng and Lin, Guojun and Lin, Tong and Zhu, Yanmei and Wang, Zhisun and Diao, Haojie},
  journal={IEEE Access}, 
  title={CTRF-GAN: A Generative Network for Image Inpainting Using CSWin Transformer and Fast Fourier Convolution Residual}, 
  year={2024},
  volume={12},
  number={},
  pages={156327-156336},
  abstract={Image inpainting is an important task in computer vision, aiming at restoring missing or damaged areas in an image. The existing methods have certain problems such as texture blur and structure distortion, especially when the degenerated images exhibit complex structures and scenes. To address these issues, this paper proposes an image inpainting algorithm (called CTRF-GAN) based on the generative adversarial network (GAN) with CSWin Transformer module and Fast Fourier convolution residual module. The modified GAN framework replaces ordinary convolution in the encoder and decoder with gated convolution to reduce the training loss. The CSWin Transformer module is introduced into the generator to enhance the global dependency and to increase the receptive field. A fast Fourier convolution residual module (Res-AFFC) is proposed to extract high-frequency texture information. Finally, the HSV loss function is integrated to ensure that the color of the inpainting areas is consistent with the original image. Extensive experiments are conducted on the CelebA and Paris Street View datasets, verifying the superiority of the proposed algorithm.},
  keywords={Transformers;Generative adversarial networks;Generators;Feature extraction;Logic gates;Context modeling;Encoding;Vectors;Convolutional neural networks;Fast Fourier transforms;Image inpainting;CSWin transformer;fast Fourier convolution;gated convolution},
  doi={10.1109/ACCESS.2024.3484472},
  ISSN={2169-3536},
  month={},}@ARTICLE{10255116,
  author={Yao, Xiaofang and Zheng, Gang and Yu, Jiali and Shao, Jinliang and Zuo, Jialu},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Robust Prediction of Sea Surface Temperature Based on SSPGAN}, 
  year={2023},
  volume={16},
  number={},
  pages={9396-9405},
  abstract={Sea surface temperature (SST) is an important parameter for monitoring ocean phenomena. Driven by ocean satellite Big Data, deep neural networks have achieved state-of-the-art performance in forecasting fields of oceanic phenomena. However, when there are some changes in the data set, the performance of these models drops significantly. In order to address this issue, this article develops a robust strong stability-preserving generative adversarial network model (SSPGAN) to forecast SST. The main objective of this article is to continue to produce accurate SST predictions even in the presence of minor data perturbations. The SSPGAN consists of a generator of a deep learning model and a strong stability preserving discriminator. The generator aims to generate SST that are close to the future true SST using SST from the past. The discriminator simultaneously tries to tell the generated SST apart from the real SST. The robust of the discriminator and the overall model is enhanced by the strong stability preserving network. Experiments confirm the SSPGAN's accuracy and robust for predicting SST in the presence of small data perturbations. In this study, we employ the Wasserstein metric to quantify the gap between the actual SST distribution and the generated SST with a perturbation. In addition, this article also analyzes the robust conditions of the deep neural network when the data are disturbed.},
  keywords={Biological system modeling;Artificial neural networks;Predictive models;Generators;Data models;Thermal stability;Perturbation methods;Deep learning;Generative adversarial network;Lipschtiz;prediction;robust;sea surface temperature (SST)},
  doi={10.1109/JSTARS.2023.3316731},
  ISSN={2151-1535},
  month={},}@INPROCEEDINGS{8954076,
  author={Liu, Yunfan and Li, Qi and Sun, Zhenan},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Attribute-Aware Face Aging With Wavelet-Based Generative Adversarial Networks}, 
  year={2019},
  volume={},
  number={},
  pages={11869-11878},
  abstract={Since it is difficult to collect face images of the same subject over a long range of age span, most existing face aging methods resort to unpaired datasets to learn age mappings. However, the matching ambiguity between young and aged face images inherent to unpaired training data may lead to unnatural changes of facial attributes during the aging process, which could not be solved by only enforcing identity consistency like most existing studies do. In this paper, we propose an attribute-aware face aging model with wavelet based Generative Adversarial Networks (GANs) to address the above issues. To be specific, we embed facial attribute vectors into both the generator and discriminator of the model to encourage each synthesized elderly face image to be faithful to the attribute of its corresponding input. In addition, a wavelet packet transform (WPT) module is incorporated to improve the visual fidelity of generated images by capturing age-related texture details at multiple scales in the frequency space. Qualitative results demonstrate the ability of our model in synthesizing visually plausible face images, and extensive quantitative evaluation results show that the proposed method achieves state-of-the-art performance on existing datasets.},
  keywords={Visualization;Face recognition;Aging;Generative adversarial networks;Feature extraction;Wavelet packets;Vectors;Generators;Facial features;Faces;Face;Gesture;and Body Pose;Biometrics},
  doi={10.1109/CVPR.2019.01215},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{8545831,
  author={Gong, Nanxue and Yang, Yang and Liu, Yuehu and Liu, Dingdong},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
  title={Dynamic Facial Expression Synthesis Driven by Deformable Semantic Parts}, 
  year={2018},
  volume={},
  number={},
  pages={2929-2934},
  abstract={Dynamic facial expression synthesis has some wild applications in human-computer interaction and virtual reality. The popular data-driven synthesis method like generative adversarial network (GAN) has made a great progress in generating a single face image, but has not well performed for expression sequences. To solve this problem, we design a series of deformable semantic parts to represent facial geometrical movement. And we synthesize the facial appearance by the geometrical driven under the-state-of-art pix2pixHD framework. In order to maintain the person identity among image sequence, we utilize an encoder to constrain the attributes of target face. With the above efforts, our method is capable to synthesize satisfied dynamic facial expression sequences.},
  keywords={Face;Semantics;Generators;Gallium nitride;Image sequences;Generative adversarial networks;Mouth},
  doi={10.1109/ICPR.2018.8545831},
  ISSN={1051-4651},
  month={Aug},}@INPROCEEDINGS{10773969,
  author={Bui, Cao Vu and Vo, Quoc-Trinh and Vuong, Nguyen Luong},
  booktitle={2024 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)}, 
  title={GAN vs. Traditional Methods: A Multi-Scale Performance Evaluation in Satellite Image Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Effective data augmentation is essential for enhancing machine learning model performance in satellite image classification. This study presents a multi-scale evaluation of Generative Adversarial Networks (GANs) compared to traditional augmentation methods, examining their effectiveness across high, medium, and low-resolution satellite imagery using the WHU-RS19, RSD46-WHU, and EuroSAT datasets. Our findings reveal that GANs significantly boost classification accuracy, with improvements of up to 8.23% in high-resolution images, 6.49% in medium-resolution images, and a modest 0.61% in low-resolution scenarios compared to traditional methods. Traditional techniques maintain competitive performance at lower resolutions, where simpler operations and reduced computational demands are advantageous. This research highlights the resolution-dependent benefits and constraints of each method, offering practical guidelines for selecting optimal augmentation strategies to enhance accuracy and generalizability in remote sensing applications.},
  keywords={Performance evaluation;Image resolution;Accuracy;Urban planning;Data augmentation;Generative adversarial networks;Satellite images;Sensors;Remote sensing;Guidelines;GANs;Data Augmentation;Satellite Classification;Multi-scale Imaging;Remote Sensing;Deep Learning},
  doi={10.1109/ICCE-Asia63397.2024.10773969},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10351723,
  author={Bi, Xiaoyang and He, Enhao and Wang, Qipeng},
  booktitle={2023 IEEE 5th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)}, 
  title={Research on style transfer model based on improved CycleGAN algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={129-135},
  abstract={Style transfer is a heated computer vision topic that has been broadly mentioned in both academic research and practical applications, for example, the style transfer technique is behind the various filters in daily used image processing tools. However, it had been a major drawback in this field that its training set required strictly paired images, until the introduction of CycleGAN, a combination of style transfer technique and Generating Adversarial Networks (GAN), which has also been one of the most popular technologies in machine learning since its advent. CycleGAN is an image style transfer technique proposed by Jun-Yan Zhu et al. at the University of California, Berkeley. The paper proposed a way that can be performed without the strict pairs of the source image and style image so that a wider range of dataset options can be incorporated into this field. The generator in the CycleGAN network consists of an encoder, converter, and decoder, which can play the role of preserving the original image features and converting images. In our research, this study found that there is still certain scope for improvement in the original model, so this study conducted a further study on the basis of rebuilding the original CycleGAN. This study studied the CycleGAN framework, which is an unsupervised learning style transfer model based on generative adversarial networks, and proposed an improved nerual network model by modifying the adversarial loss, and compared it with the original CycleGAN network. The results showed that the modified CycleGAN can obtain more realistic images with better visual effects than original CycleGAN after training in fewer rounds.},
  keywords={Training;Image processing;Machine learning;Generative adversarial networks;Visual effects;Generators;Safety;CycleGAN;style transfer;loss function;deep learning},
  doi={10.1109/ICCASIT58768.2023.10351723},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9897380,
  author={Li, Zihang and An, Dongsheng and Feng, Yingjie and Gu, Xianfeng and Xu, Xiaoyin and Zhang, Min},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, 
  title={Image Compression Based on Importance Using Optimal Mass Transportation Map}, 
  year={2022},
  volume={},
  number={},
  pages={1191-1195},
  abstract={Demand for efficient image transmission and storage is in-creasing rapidly because of the continuing growth of multi-media technology and VR and AR applications. In this paper, we proposed an image compression method based on the recognition of importance of regions in images. As not all the information in an image is equally useful, we can identify important regions in an image for high fidelity compression and accept a comparatively more lossy compression about less important regions of the image. First, we segment images to two parts, namely, foreground and background, where the foreground represents the more important component and the background is of less importance. Second, we apply optimal mass transportation mapping in a GAN (generative adversarial network) framework to both the foreground and back-ground to magnify the foreground and shrink the background while keeping the shape and total image area unchanged. As a result, in the processed image, the ratio of foreground to background is larger than the corrresponding ratio in the original image. This ratio is controllable in our process, giving users the ability to control the degree of compression. The GAN-processed image is then used for compression. To restore the image, we apply a GAN model to the compressed image and recover the ratio of foreground and background using an optimal mass transportation map. Test results show that our method is highly effective in reconstructing detail of important components in compressed images while achieving a high compression ratio.},
  keywords={Image segmentation;Image coding;Image recognition;Shape;Transportation;Process control;Generative adversarial networks;optimal mass transport;GAN;image compression},
  doi={10.1109/ICIP46576.2022.9897380},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10649307,
  author={Liu, An and Tang, Chaoqing and Qiu, Haichao},
  booktitle={2024 8th International Conference on Robotics, Control and Automation (ICRCA)}, 
  title={A Computer Vision System for Defect Contrast Enhancement on Transparent Materials with Structured Light and Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={243-247},
  abstract={Transparent materials are widely used in industrial applications like screens and precision optical instruments. However, due to its special optical properties, current computer vision systems that use dark-field illumination schemes suffer from poor defect contrast. Many tiny defects are even invisible. To address this problem, this paper proposes a computer vision system for defect contrast enhancement on transparent materials. Firstly, based on physical modulation effects, structured light illumination is used to enhance regional defect contrast. Then an unsupervised generative adversarial network is designed to fuse regional high-contrast information and remove the uneven illumination background from sub-images. Experiments are carried out on specimens with different types of defects and material thickness, the results show that the contrast of our method has improved by 34% compared with the traditional dark-field illumination. This computer vision imaging system enhances defect contrast physically and computationally, which brings much more defect information than current solutions for subsequent inspection tasks like defect localization and segmentation.},
  keywords={Optical filters;Computer vision;Refining;Lighting;Optical computing;Optical imaging;Generative adversarial networks;defect contrast enhancement;structured light;image fusion},
  doi={10.1109/ICRCA60878.2024.10649307},
  ISSN={},
  month={Jan},}@ARTICLE{10972074,
  author={Zhou, Huaji and Hao, Xiaoyu and Liu, Xu and Sun, Xinyu and Li, Lingling and Liu, Fang and Jiao, Licheng},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Diffusion SigFormer for Interference Time-Series Signal Recognition}, 
  year={2025},
  volume={74},
  number={},
  pages={1-12},
  abstract={Various interferences in the actual environment make electromagnetic signal recognition challenging, and this topic has an extremely important application value. In this article, a novel interference signal recognition Transformer was proposed, named Diffusion SigFormer, to explore the mechanism of signal interference and improve the ability to identify interference signals. First, we explore the interference law of electromagnetic signals and design a signal interference mechanism for simulating real scenes. Second, the diffusion signal denoising module (DSDM) was proposed to denoise the input interference signal. We used different types of noise and various interference rates to simulate noise in real electromagnetic environments. Third, SigFormer was designed to extract and classify the denoised signal. For the characteristics of electromagnetic signals, SigFormer leveraged 1-D patch embedding and combined Transformer with convolution. Finally, we conducted experimental verification on RML2016.10a, RML2016.10b, and BT datasets. The experimental results have shown that the proposed method has excellent anti-interference ability.},
  keywords={Noise;Electromagnetics;Interference;Transformers;Feature extraction;Diffusion models;Signal denoising;Modulation;Convolutional neural networks;Training;Bluetooth;diffusion;interference signal recognition;modulation;SigFormer},
  doi={10.1109/TIM.2025.3563008},
  ISSN={1557-9662},
  month={},}@ARTICLE{10335724,
  author={Du, Yunhao and Lei, Cheng and Zhao, Zhicheng and Dong, Yuan and Su, Fei},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Video-Based Visible-Infrared Person Re-Identification With Auxiliary Samples}, 
  year={2024},
  volume={19},
  number={},
  pages={1313-1325},
  abstract={Visible-infrared person re-identification (VI-ReID) aims to match persons captured by visible and infrared cameras, allowing person retrieval and tracking in 24-hour surveillance systems. Previous methods focus on learning from cross-modality person images in different cameras. However, temporal information and single-camera samples tend to be neglected. To crack this nut, in this paper, we first contribute a large-scale VI-ReID dataset named BUPTCampus. Different from most existing VI-ReID datasets, it 1) collects tracklets instead of images to introduce rich temporal information, 2) contains pixel-aligned cross-modality sample pairs for better modality-invariant learning, 3) provides one auxiliary set to help enhance the optimization, in which each identity only appears in a single camera. Based on our constructed dataset, we present a two-stream framework as baseline and apply Generative Adversarial Network (GAN) to narrow the gap between the two modalities. To exploit the advantages introduced by the auxiliary set, we propose a curriculum learning based strategy to jointly learn from both primary and auxiliary sets. Moreover, we design a novel temporal k-reciprocal re-ranking method to refine the ranking list with fine-grained temporal correlation cues. Experimental results demonstrate the effectiveness of the proposed methods. We also reproduce 9 state-of-the-art image-based and video-based VI-ReID methods on BUPTCampus and our methods show substantial superiority to them. The codes and dataset are available at: https://github.com/dyhBUPT/BUPTCampus.},
  keywords={Task analysis;Cameras;Training;Optimization;Generative adversarial networks;Representation learning;Radar tracking;Visible-infrared person re-identification;curriculum learning;re-ranking},
  doi={10.1109/TIFS.2023.3337972},
  ISSN={1556-6021},
  month={},}@ARTICLE{10562281,
  author={Kokomoto, Kazuma and Okawa, Rena and Nakano, Kazuhiko and Nozaki, Kazunori},
  journal={IEEE Access}, 
  title={Tooth Development Prediction Using a Generative Machine Learning Approach}, 
  year={2024},
  volume={12},
  number={},
  pages={87645-87652},
  abstract={This study pioneers the use of generative deep learning in pediatric dentistry to predict dental growth using panoramic radiography, going beyond numerical analysis and providing dynamic representations of tooth development. We employed StyleGAN-XL, a state-of-the-art generative adversarial network (GAN), to generate realistic images of dental development stages in children. Our dataset consisted of 8,092 anonymized panoramic radiographs from Osaka University Dental Hospital containing various dentition stages and conditions. By interpolating latent vectors from primary or mixed dentition images with those from permanent dentition, we generated continuous transitioning images that visually represented the progression of dental development. The performance of the StyleGAN-XL model was evaluated using Fréchet inception distance scores. Pivotal tuning inversion was used to project real images onto the model’s latent space, allowing us to effectively interpolate between current and future dental states. The resulting images showed a smooth transition from primary to permanent dentition, closely resembling the actual stages of dental development. This method represents a significant advancement in dental imaging and predictive analytics, offering a novel approach for clinicians and patients to visualize and understand dental growth. Our findings suggest broader applications for generative models in medical imaging, extending beyond traditional enhancement and modeling tasks. Our study highlights the transformative potential of GANs in medical imaging and provides a foundation for future advancements in predictive dentistry.},
  keywords={Dentistry;Image resolution;Teeth;Image synthesis;Generators;Artificial intelligence;Machine learning;Informatics;Bioinformatics;Pediatrics;Artificial intelligence;machine learning;medical informatics applications;pediatric dentistry;orthodontics;dental informatics},
  doi={10.1109/ACCESS.2024.3416748},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10792854,
  author={Moreira, Francisco Willem R. and Hermes, Guilherme and de Lima, Jean Mário M.},
  booktitle={2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)}, 
  title={Development of a Cross Platform Mobile Application Using Gemini to Assist Visually Impaired Individuals}, 
  year={2024},
  volume={9},
  number={},
  pages={559-566},
  abstract={This article presents the development of a cross-platform mobile application using the Google’s Gemini Generative Artificial Intelligence in Flutter, aimed at providing support to people with visual impairments in understanding visual content. The application leverages mobile computing technologies to offer an accessible and intuitive experience, allowing users to obtain real-time image descriptions or from the gallery. Addition- ally, it integrates the Google Gemini artificial intelligence model, which utilizes computer vision techniques to generate contextual descriptions of images. The article describes the architecture of the application, the main components of its implementation, and configuration of the model, as well as the challenges faced during the development process. Ultimately, it seeks to understand the benefits of using Gemini and ways to configure and receive the new model from Google via API.(Abstract)},
  keywords={Visualization;Generative AI;Computational modeling;Visual impairment;Real-time systems;Mobile applications;Internet;Informatics;Mobile computing;Context modeling;mobile app;google gemini;assistive technologies;generative artificial intelligence},
  doi={10.1109/ICIIBMS62405.2024.10792854},
  ISSN={2189-8723},
  month={Nov},}@ARTICLE{10536627,
  author={Li, Qi and Wang, Weining and Xu, Chengzhong and Sun, Zhenan and Yang, Ming-Hsuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning Disentangled Representation for One-Shot Progressive Face Swapping}, 
  year={2024},
  volume={46},
  number={12},
  pages={8348-8364},
  abstract={Although face swapping has attracted much attention in recent years, it remains a challenging problem. Existing methods leverage a large number of data samples to explore the intrinsic properties of face swapping without considering the semantic information of face images. Moreover, the representation of the identity information tends to be fixed, leading to suboptimal face swapping. In this paper, we present a simple yet efficient method named FaceSwapper, for one-shot face swapping based on Generative Adversarial Networks. Our method consists of a disentangled representation module and a semantic-guided fusion module. The disentangled representation module comprises an attribute encoder and an identity encoder, which aims to achieve the disentanglement of the identity and attribute information. The identity encoder is more flexible, and the attribute encoder contains more attribute details than its competitors. Benefiting from the disentangled representation, FaceSwapper can swap face images progressively. In addition, semantic information is introduced into the semantic-guided fusion module to control the swapped region and model the pose and expression more accurately. Experimental results show that our method achieves state-of-the-art results on benchmark datasets with fewer training samples.},
  keywords={Faces;Three-dimensional displays;Solid modeling;Shape;Decoding;Training;Semantics;Generative adversarial networks;disentangled representation module;semantic-guided fusion module;one-shot;progressive face swapping},
  doi={10.1109/TPAMI.2024.3404334},
  ISSN={1939-3539},
  month={Dec},}@ARTICLE{10122914,
  author={Park, Hyun-Cheol and Hong, In-Pyo and Poudel, Sahadev and Choi, Chang},
  journal={IEEE Access}, 
  title={Data Augmentation Based on Generative Adversarial Networks for Endoscopic Image Classification}, 
  year={2023},
  volume={11},
  number={},
  pages={49216-49225},
  abstract={The incidence of cancer among modern people has recently increased due to various reasons such as eating habits, smoking, and drinking. Therefore, medical image analysis for effective disease diagnosis is considered an extremely important diagnostic tool. In particular, endoscopy is used as a representative screening method for diagnosing diseases of the digestive system. However, it is quite difficult to quickly and thoroughly analyze medical data by relying solely on human vision, such as with endoscopy. Therefore, the purpose of this study was to reduce the fatigue of medical staff through the use of automated disease classification of the digestive system. To automate disease classification, we trained a total of six models, ranging from relatively old deep-learning-based models to recently published approaches. Additionally, to increase the number of medical data, which is generally insufficient, we applied data augmentation using two adversarial generative neural network-based models. We utilized Kvasir version 2 data for the experiment and demonstrated that InceptionNet-V3 showed the best performance improvement when data augmentation based on a Star-GAN was applied experimentally. Furthermore, the approach also exhibited good performance in terms of the F1-Score, which was used to evaluate the safety of the model. Thus, we propose a disease classification automation model centered on safer performance.},
  keywords={Medical diagnostic imaging;Endoscopes;Cancer;Image classification;Data models;Image recognition;Generative adversarial networks;Endoscopic image classification;colon disease classification;data augmentation;generative adversarial network;digestive system image classification},
  doi={10.1109/ACCESS.2023.3275173},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9277382,
  author={Zhang, Yu and Dong, Yunlong},
  booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)}, 
  title={Single Image Dehazing via Reinforcement Learning}, 
  year={2020},
  volume={1},
  number={},
  pages={123-126},
  abstract={Single image dehazing aims at restoring the clean image from single hazy one. A novel Reinforcement Learning (RL) based image dehazing method is proposed in this paper to handle the dehazing task with interpretable ability and extensibility. We model the dehazing problem as a Markov Decision Process (MDP) with several existing simple traditional image processing operations and prior knowledge-based dehazing methods as actions. Furthermore, a deep Q-learning network is established to learn the value function for image dehazing. Finally the learned network can iteratively choose the correct action during the processing sequence to produce dehazing results. Extensive results on real hazy images have been conducted to verify the proposed method. Also the learned sequence of image dehazing can provide considerable guidance for human.},
  keywords={Feature extraction;Reinforcement learning;Image color analysis;Image restoration;Computer vision;Task analysis;Transforms;Image Dehazing;Markov Decision Process;Deep Learning;Reinforcement Learning},
  doi={10.1109/ICIBA50161.2020.9277382},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9422012,
  author={Ren, Wanghao and Li, Zhiming and Li, Hailing and Li, Yang and Zhang, Chunwei and Fu, Xiaoqian},
  booktitle={2020 2nd International Conference on Information Technology and Computer Application (ITCA)}, 
  title={Application of Quantum Generative Adversarial Learning in Quantum Image Processing}, 
  year={2020},
  volume={},
  number={},
  pages={467-470},
  abstract={Quantum machine learning is a hot topic in the quantum community recently. The combination of quantum machine learning and image processing is also one of the researchers' concerns. With the promotion of deep learning to process images, the field of image processing has also shown amazing potential. The combination of quantum computing and artificial intelligence can not only exert the computing power of quantum computing but also find more applications in the field of image processing. This article combines quantum generative adversarial learning with the field of image processing. A quantum generative adversarial network is designed to load and learn classical image data. Numerical simulations intuitively show that quantum algorithms can also effectively process images. In our scheme, using N qubits can load 2N classical bits of image data.},
  keywords={Quantum computing;Quantum algorithm;Image processing;Computational modeling;Qubit;Quantum mechanics;Numerical simulation;Quantum machine learning;Quantum generative adversarial network;Image processing;Deep learning},
  doi={10.1109/ITCA52113.2020.00104},
  ISSN={},
  month={Dec},}@ARTICLE{10458271,
  author={Gao, Song and Wang, Xiaoxuan and Song, Bingbing and Liu, Renyang and Yao, Shaowen and Zhou, Wei and Yu, Shui},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Exploiting Type I Adversarial Examples to Hide Data Information: A New Privacy-Preserving Approach}, 
  year={2024},
  volume={8},
  number={3},
  pages={2518-2528},
  abstract={Deep neural networks (DNNs) are sensitive to adversarial examples which are generated by corrupting benign examples with imperceptible perturbations, or have significant changes but can still achieve original prediction results. The latter case is termed as the Type I adversarial example which, however, has limited attention in the literature. In this paper, we introduce two methods, termed HRG and GAG, to generate Type I adversarial examples and attempt to apply them to the privacy-preserving Machine Learning as a Service (MLaaS). Existing methods for the privacy-preserving MLaaS are mostly based on cryptographic techniques, which often incur additional communication and computation overhead, while using Type I adversarial examples to hide users' privacy data is a brand-new exploration. Specifically, HRG utilizes the high-level representations of DNNs to guide generators, and GAG leverages the generative adversarial network to transform original images. Our solution does not involve any model modifications and allows DNNs to run directly on transformed data, thus arousing no additional communication and computation overhead. Extensive experiments on MNIST, CIFAR-10, and ImageNet show that HRG can perfectly hide images into noise and achieve similar accuracy as the original accuracy, and GAG can generate natural images that are completely different from the original images with a small loss of accuracy.},
  keywords={Training;Generators;Generative adversarial networks;Perturbation methods;Data models;Computational intelligence;Artificial neural networks;Data privacy;Type I adversarial examples;deep neural networks;privacy-preserving MLaaS},
  doi={10.1109/TETCI.2024.3367812},
  ISSN={2471-285X},
  month={June},}@INPROCEEDINGS{10575900,
  author={Barveen, A. and Mohamed Faizal, M.K. and Geetha, S.},
  booktitle={2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={A Multimodal Content Moderation System using Adversarial Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={638-642},
  abstract={Memes are a visual representation of images with text embedded in them that conveys the thoughts and feelings of a peculiar audience and is mainly intended to elicit humor. It is widely spread across social media platforms in the guise of sardonic images, humorous jokes, and several other viral sensations. After the profound success of the late fusion technique that combines multimodal features, researchers have used stacked LSTM to extract textual features, whereas VGG16 has been used to extract visual features. Apart from this, BiLSTM and CNN have been used to extract contextual features. This study suggests a unique deep learning-based method for classifying objectionable memes within a multimodal dataset to address this problem. The proposed system uses machine learning to train the classifier on labeled data, which allows it to categorize the information accurately. The proposed system uses both text and image data. To improve user experience and safety, offensive images are blurred, and offensive words are shuffled using adversarial training.},
  keywords={Training;Learning systems;Deep learning;Visualization;Technological innovation;Social networking (online);Feature extraction;Late Fusion Technique;Stacked LSTM;VGG16;BiLSTM -CNN;Adversarial Training},
  doi={10.1109/ICAAIC60222.2024.10575900},
  ISSN={},
  month={June},}@ARTICLE{10948496,
  author={Wang, Xianpeng and Zhang, Jingchuan and Tang, Lixin and Liu, Yaxue},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Direction Learning With Multivariate Gaussian Probabilistic Model for Multiobjective Optimization}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={In recent years, utilizing data from the evolutionary process of multiobjective evolutionary algorithms (MOEAs) to learn knowledge and guide evolutionary search has become a popular research topic. However, existing knowledge learning (KL) frameworks often suffer from the low quality of collected datasets and the inefficiency of model construction, which significantly limits their effectiveness. To address this issue, this paper proposes a novel evolutionary direction learning (EDL) framework, which aims to learn the evolutionary direction (ED) knowledge for each objective to enhance the population generation of MOEAs. The proposed EDL incorporates an effective data collection method based on objective improvement to generate high-quality datasets, based on which a multivariate Gaussian probabilistic model is employed to learn ED knowledge for each objective through a data fusion modeling approach. Besides, a knowledge assignment method is designed to select the most suitable ED knowledge to guide the evolution of solutions. Experimental results on both synthetic and real-world problems demonstrate that the proposed EDL framework can accelerate the convergence of MOEAs and significantly improve their performance. A comparison of the proposed EDL with three state-of-the-art KL frameworks indicates that EDL is a highly competitive learning framework, achieving superior performance with larger datasets and impressive efficiency.},
  keywords={Data models;Optimization;Computational modeling;Vectors;Data collection;Training;Probabilistic logic;Learning (artificial intelligence);Knowledge engineering;Evolutionary computation;Multiobjective evolutionary algorithm;knowledge learning;evolutionary direction;multivariate Gaussian probabilistic model},
  doi={10.1109/TEVC.2025.3557412},
  ISSN={1941-0026},
  month={},}@INPROCEEDINGS{9456474,
  author={Wang, Decheng and Chen, Yan},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)}, 
  title={Research on the Algorithm of Art Style Transfer of Xin'an Painting School}, 
  year={2021},
  volume={},
  number={},
  pages={120-124},
  abstract={Xin‘an Painting School plays an important role in the history of Chinese painting. It takes Huizhou landscape as the creative theme and has a unique artistic style. However, the current art style transfer field does not concern about this very regional characteristics of painting school. Therefore, we propose an improved CycleGAN to realize the transfer of Xin'an painting style. Firstly, DenseNet is introduced to alleviate the gradient vanishing problem and optimize the content and style features transfer between the layers of neural network. Secondly, group normalization is used to reduce the calculation error and keep the network training process stable. Finally, the least square loss is introduced in the adversarial losses, and the identity loss is introduced to obtain the feature of the target image as much as possible, which constrains the arbitrary transformation of the feature of the input image. The experiment shows that the generated pictures have a good artistic style of Xin’ an Painting School.},
  keywords={Training;Deep learning;Art;Conferences;Neural networks;Buildings;Data models;Xin'an Painting School;style transfer;group normalization;DenseNet;identity loss},
  doi={10.1109/AIID51893.2021.9456474},
  ISSN={},
  month={May},}@INPROCEEDINGS{9898919,
  author={Guo, Wu and Guo, Jian},
  booktitle={CIBDA 2022; 3rd International Conference on Computer Information and Big Data Applications}, 
  title={Stability analysis of power grid based on generative adversarial network}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={The stability of power grid is an important guarantee to ensure the safe operation of the power system. In view of the shortcomings of the traditional analysis and calculation methods of power grid stability, such as complex processing flow and unable to judge the large-scale whole quickly, combined with the key data feature extraction method and deep learning method, a power grid stability detection and recognition method based on generative adversarial network is proposed in this paper. Compared with the previous neural network models, the generated countermeasure network perfectly balances the recognition time and recognition accuracy of the system, and can realize efficient, lossless and fast diversified anomaly detection of large-scale power grid operation state. According to the experimental results of traditional learning methods and existing models, the proposed method has good real-time performance, robustness and recognition rate, and the recognition accuracy is 95%.},
  keywords={},
  doi={},
  ISSN={},
  month={March},}@ARTICLE{8471171,
  author={Kim, Kyukwang and Myung, Hyun},
  journal={IEEE Access}, 
  title={Autoencoder-Combined Generative Adversarial Networks for Synthetic Image Data Generation and Detection of Jellyfish Swarm}, 
  year={2018},
  volume={6},
  number={},
  pages={54207-54214},
  abstract={Image-based sensing of jellyfish is important as they can cause great damage to the fisheries and seaside facilities and need to be properly controlled. In this paper, we present a deep-learning-based technique to generate a synthetic image of the jellyfish easily with autoencoder-combined generative adversarial networks. The proposed system can easily generate simple images with a smaller number of data sets compared with other generative networks. The generated output showed high similarity with the real-image data set. The application using a fully convolutional network and regression network to estimate the size of the jellyfish swarm was also demonstrated, and showed high accuracy during the estimation test.},
  keywords={Gallium nitride;Training;Generative adversarial networks;Oceans;Robot kinematics;Generators;Autoencoder;generative adversarial networks;jellyfish swarm;fully convolutional network;regression},
  doi={10.1109/ACCESS.2018.2872025},
  ISSN={2169-3536},
  month={},}@ARTICLE{9006830,
  author={Jiang, Xue and Zhao, Jingjing and Qian, Wei and Song, Weichen and Lin, Guan Ning},
  journal={IEEE Access}, 
  title={A Generative Adversarial Network Model for Disease Gene Prediction With RNA-seq Data}, 
  year={2020},
  volume={8},
  number={},
  pages={37352-37360},
  abstract={Deep learning models often need large amounts of training samples (thousands of training samples) to effectively extract hidden patterns in the data, thus achieving better results. However, in the field of brain-related disease, the omics data obtained by using advanced sequencing technology typically have much fewer patient samples (tens to hundreds of samples). Due to the small sample problem, statistical methods and intelligent machine learning methods have been unable to obtain a convergent gene set when prioritizing biomarkers. Furthermore, mathematical models designed for prioritizing biomarkers perform differently on different datasets. However, the architecture of the generative adversarial network (GAN) can address this bottleneck problem. Through the game between the generator and the discriminator, samples with similar distributions to that of samples in the training set can be generated by the generator, and the prediction accuracy and robustness of the discriminator could be significantly improved. Therefore, in this study, we designed a new generative adversarial network model with a denoising auto-encoder (DAE) as the generator and a multilayer perceptron (MLP) as the discriminator. The prediction residual error was backpropagated to the decoder part of the DAE, modifying the captured probability distribution. Based on this model, we further designed a framework to predict disease genes with RNA-seq data. The deep learning model improves the identification accuracy of disease genes over the-state-of-the-art approaches. An analysis of the experimental results has uncovered new disease-related genes and disease-associated pathways in the brain, which in turn have provided insight into the molecular mechanisms underlying disease phenotypes.},
  keywords={Generative adversarial networks;Generators;Diseases;Data models;Training;Gallium nitride;Hidden Markov models;Denoising auto-encoder;multilayer perceptron;generative adversarial network;RNA-seq data},
  doi={10.1109/ACCESS.2020.2975585},
  ISSN={2169-3536},
  month={},}@ARTICLE{9344669,
  author={Wang, Yueyue and Guo, Xinchang and Liu, Peng and Wei, Bin},
  journal={IEEE Access}, 
  title={Up and Down Residual Blocks for Convolutional Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={26051-26058},
  abstract={Most recent existing image generation methods have made great progress in creating high-quality images, mainly focusing on improving the generator or discriminator of convolutional generative adversarial networks (GANs). In this paper, we propose up and down residual blocks for convolutional GANs, dubbed upResBlock and downResBlock respectively. This structure is based on deconvolutions, strided convolutions, and residual blocks. With the upResBlock module for the generator of convolutional GANs, our method can further enhance the generative power of the feature extraction while synthesizing image details for the specified size. With the downResBlock module for discriminator combined with upResBlock for generator, the proposed method can speed up the back propagation of gradient and doesn't suffer from the vanishing or exploding gradients problems, generating more realistic images as well. Extensive experiments demonstrate that the proposed up and down residual blocks can help convolutional GANs in generating photo-realistic images. In addition, our method shows its universality for the improvement of existing methods.},
  keywords={Generators;Generative adversarial networks;Task analysis;Gallium nitride;Image synthesis;Deconvolution;Training;Generative adversarial network;convolutional neural network;residual block;sampling},
  doi={10.1109/ACCESS.2021.3056572},
  ISSN={2169-3536},
  month={},}@ARTICLE{10568485,
  author={Collomosse, John and Parsons, Andy},
  journal={IEEE Computer Graphics and Applications}, 
  title={To Authenticity, and Beyond! Building Safe and Fair Generative AI Upon the Three Pillars of Provenance}, 
  year={2024},
  volume={44},
  number={3},
  pages={82-90},
  abstract={Provenance facts, such as who made an image and how, can provide valuable context for users to make trust decisions about visual content. Against a backdrop of inexorable progress in generative AI for computer graphics, over two billion people will vote in public elections this year. Emerging standards and provenance enhancing tools promise to play an important role in fighting fake news and the spread of misinformation. In this article, we contrast three provenance enhancing technologies—metadata, fingerprinting, and watermarking—and discuss how we can build upon the complementary strengths of these three pillars to provide robust trust signals to support stories told by real and generative images. Beyond authenticity, we describe how provenance can also underpin new models for value creation in the age of generative AI. In doing so, we address other risks arising with generative AI such as ensuring training consent, and the proper attribution of credit to creatives who contribute their work to train generative models. We show that provenance may be combined with distributed ledger technology to develop novel solutions for recognizing and rewarding creative endeavor in the age of generative AI.},
  keywords={Training;Visualization;Generative AI;Biological system modeling;Watermarking;Authentication;Trust management;Context awareness},
  doi={10.1109/MCG.2024.3380168},
  ISSN={1558-1756},
  month={May},}@ARTICLE{10677438,
  author={Megahed, Mohammed and Mohammed, Ammar},
  journal={IEEE Access}, 
  title={Collaborative-GAN: An Approach for Stabilizing the Training Process of Generative Adversarial Network}, 
  year={2024},
  volume={12},
  number={},
  pages={138716-138735},
  abstract={Generative Adversarial Network (GAN) outperforms its peers in the generative models’ family and is widely used to generate realistic samples in various domains. The basic idea of GAN is a competition between two networks called a generator and discriminator. Throughout the training process of GAN, the two networks face various challenges that affect the quality and diversity of the generated samples of GAN. These challenges include training instability and mode collapse problem. Training instability happens due to the variance of the performance between the generator and discriminator. The mode collapse, on the other hand, happens when the generator is stuck to generate diverse samples. One of the promising techniques that might overcome these issues and increase the networks’ performance is transfer learning between discriminators as same as generators. In this regard, the contribution of this paper is fourfold. First, it proposes a novel approach called Collaborative-GAN based on transfer learning to mitigate the training instability and tackle the mode collapse issues. In the proposed approach, the well-performer network transfers its learned weights to the low-performer ones based on a periodical evaluation during the training process. Second, the paper proposes a novel method to evaluate the discriminators’ performance based on a fuzzy inference system. Third, the paper proposes a method to evaluate the generators’ performance based on a series of detected FID scores that measure the diversity of the generated samples every certain intervals during the training process. We apply the proposed approach on two different architectures of GAN, which we called Single-GAN and Dual-GANs. In Single-GAN, the weights are transferred between the identical networks within the same GAN model. In Dual-GANs, on the other hand, the weights are transferred between identical networks across different GAN models. Thus, the paper introduces two types of transfer learning for GANs; inter and intra-transfer learning based on the paradigm of GAN architecture as a fourth contribution. We validate the proposed approach on three different benchmarks representing CelebA, Cifar-10, and Fashion-Mnist. The experimental results indicate that the proposed approach outperforms the state-of-the-art GAN models in terms of FID metric that measures the generated sample diversity. It is worth noting that the proposed approach achieved remarkable FID scores of 11.44, 24.19, and 11.21 on the Fashion-Mnist, Cifar-10, and CelebA datasets respectively.},
  keywords={Generators;Training;Generative adversarial networks;Transfer learning;Fuzzy logic;Propagation losses;Games;Generative adversarial network;transfer learning;training instability;mode collapse},
  doi={10.1109/ACCESS.2024.3457902},
  ISSN={2169-3536},
  month={},}@ARTICLE{10290875,
  author={Penava, Pascal and Buettner, Ricardo},
  journal={IEEE Access}, 
  title={A Novel Small-Data Based Approach for Decoding Yes/No-Decisions of Locked-In Patients Using Generative Adversarial Networks}, 
  year={2023},
  volume={11},
  number={},
  pages={118849-118864},
  abstract={We demonstrate how to use generative adversarial networks to improve the small data problem when training brain-computer-interfaces. The new approach is based on finely graded frequency bands, which are extracted from motor imagery electroencephalography data by using power spectral density method to synthetically generate electroencephalography data using generative adversarial networks. We evaluate our approach using one of the currently largest publicly available electroencephalography datasets, by first checking the synthetic and real data for statistical and visual similarity, and secondly, by training a random forest classifier, once using only the real data and then using the real data augmented with the synthetic data. With similarity scores of 95.72 % in the subject-dependent case and 83.51 % in the subject-independent case, and a predictive gain of 17.53 % in the subject-dependent case, and 7.51 % in the subject-independent case, we were able to achieve promising results. The results show that our approach can make it possible to research rare diseases for which there is too little patient data. Also, synthetic data can be a way for many electroencephalography-based brain-computer interface applications to obtain the required data more cost- and time-efficiently.},
  keywords={Electroencephalography;Generative adversarial networks;Diseases;Training;Synthetic data;Decoding;Classification tree analysis;Brain-computer interfaces;Prediction methods;Machine learning;Brain-computer-interface;decision prediction;generative adversarial networks;motor imagery tasks;electroencephalography;machine learning},
  doi={10.1109/ACCESS.2023.3326720},
  ISSN={2169-3536},
  month={},}@ARTICLE{9247227,
  author={Jin, Yeongbong and Ko, Bonggyun},
  journal={IEEE Access}, 
  title={Restoring Latent Vectors From Generative Adversarial Networks Using Genetic Algorithms}, 
  year={2020},
  volume={8},
  number={},
  pages={199673-199681},
  abstract={Rapid and massive advances in deep learning have made it possible to address with issues with computer vision. In recent years, one type of generative model has emerged, generative adversarial networks (GAN), that enables creating realistic and plausible images. GAN allows for building competition models based on game theory that allows for modeling data probability distributions. Since the introduction of GAN, researchers have conducted many follow-up studies to apply and improve these models. In this article, using a global optimization technique called a genetic algorithm, we suggest the methodology restoring latent vector of pre-trained GAN and measure its performance as hyper-parameters and fitness functions; specifically, we utilize the mating and mutation rate as hyper-parameters of the genetic algorithms and use the mean squared error and the structural similarity as the fitness functions and evaluate their impact. We obtain image reconstruction results through experiment using the MNIST, Fashion-MNIST, Cifar10, and CelebA dataset, and we compare our method with the gradient descent method. We discuss limitations of these experiments and future research topics.},
  keywords={Training;Computational modeling;Generative adversarial networks;Data models;Image restoration;Image reconstruction;Genetic algorithms;Generative adversarial networks;genetic algorithm;neural networks;image processing;latent vector},
  doi={10.1109/ACCESS.2020.3035674},
  ISSN={2169-3536},
  month={},}@ARTICLE{10634314,
  author={Ding, Ao and Li, Gaolei and Yi, Xiaoyu and Lin, Xi and Li, Jianhua and Zhang, Chaofeng},
  journal={IEEE Software}, 
  title={Generative AI for Software Security Analysis: Fundamentals, Applications, and Challenges}, 
  year={2024},
  volume={41},
  number={6},
  pages={46-54},
  abstract={This article delves into the potential opportunities and challenges that generative AI introduces to software security analysis based on its fundamentals and applications. In addition, a simple case is used to demonstrate the validity of our outlook.},
  keywords={Computer Security;Task analysis;Codes;Analytical models;Data models;Artificial intelligence;Generative AI;Binary codes;Software development management;Software engineering},
  doi={10.1109/MS.2024.3416036},
  ISSN={1937-4194},
  month={Nov},}@INPROCEEDINGS{9644261,
  author={Franchi, Valerio and Ntagiou, Evridiki},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={Planetary Rover Localization in Virtual Reality Environment via Orbital and Surface Imagery Learnt Embeddings}, 
  year={2021},
  volume={},
  number={},
  pages={114-118},
  abstract={The localization system is developed to localize a rover moving in a planetary terrain inside a bird-view map of the environment that it operates in, by utilizing image data from the vehicle's front-facing monocular camera. Since the system is destined to operate on a planetary terrain wherein the data is not readily available, an artificial model in the form of Virtual Reality (VR) is adopted, despite several limitations including a lack of features and textures compared to real-world data. A Unity Virtual Reality (VR) simulation environment loaded with artificially-built lunar terrains, is utilized to gather the training data and augmented using a Generative Adversarial Network (GAN). The localization system consists of a Monte Carlo Localization algorithm that employs visual odometry to propagate the particles in the environment and a Siamese Neural Network (SNN) to evaluate the estimate of each particle's location inside the map. The GAN was able to generate realistic images with high similarity to the VR lunar environment, while the rover localization system tested successfully with a positional error below 25 metres after a minimum of 4 iterations of the algorithm, inside a 600 metre by 600 metre map. Additionally, the error was observed to constantly decrease with every new time step.},
  keywords={Location awareness;Solid modeling;Moon;Neural networks;Virtual reality;Generative adversarial networks;Feature extraction;Visual localization;Convolutional neural network for feature extraction;GAN data augmentation;Virtual reality simulation environment;Mobile robotics;Planetary exploration},
  doi={10.1109/AIVR52153.2021.00027},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11025021,
  author={Barczak, Andre Chautard and Reyes, Napoleon and Sušnjak, Teo and Barczak, Erik Tabuchi and Pereira, Júlio César},
  booktitle={2024 International Conference on Sustainable Technology and Engineering (i-COSTE)}, 
  title={Are Benford's Law Based Detectors Effective for GAI Generated Images?}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Benford's Law (BL) has been proposed as a method to distinguish natural images from those that were altered in some way, through manipulation or distortion, after their acquisition. Another common use of BL is to detect artificially generated images. In this article, we evaluate the effectiveness of BL in the detection of Generative Artificial Intelligence (GAI) generated images. The images were generated in four different categories depending on the prompts to the GAI, namely original, artificial, cartoon-like and realistic. The results showed that only about 60% of the artificial images do not fit BL, while the other 40 % do. However, some very artificial looking images pass the threshold of BL. We hope that this article sheds some light on the topic of using BL for image forensics.},
  keywords={Image forensics;Generative AI;Image processing;Detectors;Distortion;Discrete cosine transforms;Benford's Law;Image Forensics. Generative Artificial Intelligence;Image Processing;DCT},
  doi={10.1109/i-COSTE63786.2024.11025021},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11020485,
  author={Jayanth, R. V. and Radhika, R. and Abhiram, T. and Neha, T. and Charan, G.},
  booktitle={2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)}, 
  title={Enhancing Skin Disease Classification Through GAN Generate Synthetic images for improved CNN Training and Generalization}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Every part of the world carries skin diseases which span from minor to potentially fatal conditions including skin cancer. Accurate early diagnosis of diseases depends on a time-intensive manual approach that shows errors frequently during the process. The proposed project develops an AI system to categorize skin conditions through deep learning using ResNet50 plus Random Forest as machine learning for image diagnosis alongside patient metadata examination. Explicitly the hybrid combined a CNN model to benchmark image classes while implementing Random Forest for metadata evaluation. The model benefits from enhanced generalization through GAN-generated synthetic images which help balance the dataset. The Streamlit-based web application enables users to upload images followed by diagnostic predictions output. The implemented system provides exceptional classification outcomes that allow medical staff to identify skin afflictions at the initial stage. The upcoming work plan consists of enlarging the present dataset while utilizing Explainable AI (XAI) methods and deploying the model on cloud services for simple access..},
  keywords={Training;Explainable AI;Streaming media;Metadata;Generative adversarial networks;Skin;Medical diagnosis;Random forests;Diseases;Residual neural networks;Skin Disease Classification;Deep Learning;Convolutional Neural Networks;ResNet50;Random Forest;Machine Learning;Generative Adversarial Networks;Synthetic Data;GAN;Image Classification;Patient Metadata;Data Augmentation;Skin Disease Diagnosis;Explainable AI;Streamlit;Web Application},
  doi={10.1109/STCR62650.2025.11020485},
  ISSN={},
  month={May},}@INPROCEEDINGS{8541173,
  author={Chen, Kun-Chih and Wang, Ting-Yi},
  booktitle={2018 11th International Workshop on Network on Chip Architectures (NoCArc)}, 
  title={NN-Noxim: High-Level Cycle-Accurate NoC-based Neural Networks Simulator}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Numerous advances have been made in developing intelligent system. The artificial neural network (ANN) was proposed, inspired by biological neural network, to solve the problems of pattern recognition, prediction and control optimization. While ANN provides the most advanced and accurate algorithms for many AI applications, it will consume much computing time and computing power. Hence, it is necessary to design a hardware efficient ANN accelerator for the future complex computing operation. However, due to the intensive computation and communication among each ANN neuron, the interconnection between each ANN neuron become complicated as the ANN size is scaling up. On the other hand, the network-on-chip (NoC) interconnection is proven as an efficient way to solve the problems of complicated multicore interconnection. Therefore, the NoC-based ANN design is an attractive way for the ANN hardware accelerator design. To facilitate the NoC-based ANN evaluation in the system level, we present a cycle-accurate NoC-based neural network simulator, NN-Noxim, in this paper. The classification precision of the simulation output is verified by the commercial high-level neural network simulator. Consequently, the proposed simulator can be used for the NoC-based neural network design, neural network computing design, and other related researches in the future.},
  keywords={Neurons;Artificial neural networks;Biological neural networks;Hardware;Computational modeling;Gallium nitride;Telecommunication traffic;NoC-based neural network simulator;NN-Noxim},
  doi={10.1109/NOCARC.2018.8541173},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10893017,
  author={Johri, Aditya and Hingle, Ashish and Schleiss, Johannes},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={Misconceptions, Pragmatism, and Value Tensions: Evaluating Students' Understanding and Perception of Generative AI for Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={In this research paper we examine undergraduate students' use of and perceptions of generative AI (GenAI). Although the initial hype around ChatGPT has subsided, GenAI applications continue to make inroads across learning activities. Like any other emerging technology, there is a lack of consensus around using GenAI within higher education. Students are early adopters of the technology, utilizing it in atypical ways and forming a range of perceptions and aspirations about it. To understand where and how students are using these tools and how they view them, we present findings from an open-ended survey response study with undergraduate students pursuing information technology degrees. Students were asked to describe 1) their understanding of GenAI; 2) their use of GenAI; 3) their opinions on the benefits, downsides, and ethical issues pertaining to its use in education; and 4) how they envision GenAI could ideally help them with their education. Thirty-seven students provided responses ranging in length from 20 to 300 words for each question. Responses were iteratively coded by researchers to uncover patterns in the data and then categorized thematically. Findings reveal that students' definitions of GenAI differed substantially and included many misconceptions - some highlight it as a technique, an application, or a tool, while others described it as a type of AI. There was a wide variation in the use of GenAI by students, with two common uses being writing and coding. They identified the ability of GenAI to summarize information and its potential to personalize learning as an advantage. Students identified two primary ethical concerns with using GenAI: plagiarism and dependency, which means that students do not learn independently. They also cautioned that responses from GenAI applications are often untrustworthy and need verification. Overall, they appreciated that they could do things quickly with GenAI but were cautious as using the technology was not necessarily in their best long-term as it interfered with the learning process. In terms of aspirations for GenAI, students expressed both practical advantages and idealistic and improbable visions. They said it could serve as a tutor or coach and allow them to understand the material better. We discuss the implications of the findings for student learning and instruction.},
  keywords={Surveys;Ethics;Generative AI;Plagiarism;Education;Writing;Encoding;Distance measurement;Stakeholders;Information technology;generative artificial intelligence (GenAI);survey study;thematic analysis;undergraduate students},
  doi={10.1109/FIE61694.2024.10893017},
  ISSN={2377-634X},
  month={Oct},}@ARTICLE{9440729,
  author={Liu, Si and Jiang, Wentao and Gao, Chen and He, Ran and Feng, Jiashi and Li, Bo and Yan, Shuicheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal}, 
  year={2022},
  volume={44},
  number={11},
  pages={8538-8551},
  abstract={In this paper, we address the makeup transfer and removal tasks simultaneously, which aim to transfer the makeup from a reference image to a source image and remove the makeup from the with-makeup image respectively. Existing methods have achieved much advancement in constrained scenarios, but it is still very challenging for them to transfer makeup between images with large pose and expression differences, or handle makeup details like blush on cheeks or highlight on the nose. In addition, they are hardly able to control the degree of makeup during transferring or to transfer a specified part in the input face. These defects limit the application of previous makeup transfer methods to real-world scenarios. In this work, we propose a Pose and expression robust Spatial-aware GAN (abbreviated as PSGAN++). PSGAN++ is capable of performing both detail-preserving makeup transfer and effective makeup removal. For makeup transfer, PSGAN++ uses a Makeup Distill Network (MDNet) to extract makeup information, which is embedded into spatial-aware makeup matrices. We also devise an Attentive Makeup Morphing (AMM) module that specifies how the makeup in the source image is morphed from the reference image, and a makeup detail loss to supervise the model within the selected makeup detail area. On the other hand, for makeup removal, PSGAN++ applies an Identity Distill Network (IDNet) to embed the identity information from with-makeup images into identity matrices. Finally, the obtained makeup/identity matrices are fed to a Style Transfer Network (STNet) that is able to edit the feature maps to achieve makeup transfer or removal. To evaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the Wild (MT-Wild) dataset that contains images with diverse poses and expressions and a Makeup Transfer High-Resolution (MT-HR) dataset that contains high-resolution images. Experiments demonstrate that PSGAN++ not only achieves state-of-the-art results with fine makeup details even in cases of large pose/expression differences but also can perform partial or degree-controllable makeup transfer. Both the code and the newly collected datasets will be released at https://github.com/wtjiang98/PSGAN.},
  keywords={Faces;Generative adversarial networks;Task analysis;Visualization;Nose;Image resolution;Skin;Makeup transfer;makeup removal;generative adversarial networks},
  doi={10.1109/TPAMI.2021.3083484},
  ISSN={1939-3539},
  month={Nov},}@ARTICLE{11037244,
  author={Luo, Zhongtao and Li, Jixuan and Gong, Yanru and Lu, Kun and Shi, Shengnan},
  journal={IEEE Aerospace and Electronic Systems Magazine}, 
  title={Range-Doppler Spectrum Anomaly Detection for Sky-wave Over-the-horizon Radar based on Image Recognition and Data Augmentation}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={Sky-wave over-the-horizon radar (OTHR) receives various kinds of environmental signals, such as sea-land clutter (SLC), external interference, and ionospheric clutter. OTHR detects targets in the range-Doppler spectrum (RDS) map, where SLC should be present, while others are considered unwanted. If SLC is absent or if unwanted interference is severe enough to obscure targets, we refer to this as an RDS anomaly regarding target detection. This paper addresses automatic detection of RDS anomalies and proposes an RDS anomaly detector (RDSAD) based on image recognition techniques. To develop the image classifier, real OTHR data are utilized to construct the database. However, the available data are imbalanced and scarce, thereby necessitating data augmentation. Noise and interference data are expanded via signal modeling, while SLC and clutter are synthesized using a generative adversarial network (GAN) designed in this paper. A lightweight ResNet18 model is developed as the RDS image classifier. The RDSAD evaluates the classification results to determine RDS anomalies. It can serve as an auxiliary module in the OTHR system, with the ability to issue warnings and offer suggestions to operators, thereby alleviating the burden of manual monitoring. Experimental results of real data and literature images demonstrate the effectiveness of our design.},
  keywords={Interference;Doppler effect;Object detection;Noise;Monitoring;Clutter;Radar;Ionosphere;Generative adversarial networks;Radar detection;Over-the-horizon radar (OTHR);range-Doppler spectrum (RDS);anomaly detection;image recognition;data augmentation;sea-land clutter;interference;generative adversarial network (GAN)},
  doi={10.1109/MAES.2025.3579767},
  ISSN={1557-959X},
  month={},}@ARTICLE{11139094,
  author={Chung, Ming-An and Lin, Chia-Wei and Ma, Yi-Xuan and Chen, Pu-Chun and Lin, Jia-Wei Lin Chun-Chia and Gao, Chen-You},
  journal={IEEE Sensors Journal}, 
  title={Enhancing Corrugation Detection via GAN-Based Data Augmentation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the rapid development of high-speed rail and rail transportation, the health of the rails is critical to operational safety. However, conventional manual inspection methods are inefficient and susceptible to subjective influences, which cannot meet the needs of modern railway systems. In this study, the generative adversarial network (GAN) technology was combined to generate rail corrugated wear images to solve the problem of insufficient data samples, and the improved YOLO algorithm was used for automatic defect detection. This paper compares the images generated by multiple GANs (such as WGAN, WGAN-GP, etc.) and conducts experimental analysis of the detection effects of different versions of the YOLO model. The results show that the proposed method not only improves the diversity of training data, but also significantly improves the accuracy and efficiency of rail defect detection. In this study, an efficient and reliable rail defect detection scheme is proposed, which provides technical support for the operation safety of rail transit.},
  keywords={Generative adversarial networks;Rails;Training;YOLO;Defect detection;Accuracy;Intelligent sensors;Data models;Data augmentation;Steel;Corrugation;object detection;Data augmentation;Generative Adversarial Networks;YOLO},
  doi={10.1109/JSEN.2025.3600280},
  ISSN={1558-1748},
  month={},}@ARTICLE{10122870,
  author={Rao, Dongyu and Xu, Tianyang and Wu, Xiao-Jun},
  journal={IEEE Transactions on Image Processing}, 
  title={TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  abstract={The end-to-end image fusion framework has achieved promising performance, with dedicated convolutional networks aggregating the multi-modal local appearance. However, long-range dependencies are directly neglected in existing CNN fusion approaches, impeding balancing the entire image-level perception for complex scenario fusion. In this paper, therefore, we propose an infrared and visible image fusion algorithm based on the transformer module and adversarial learning. Inspired by the global interaction power, we use the transformer technique to learn the effective global fusion relations. In particular, shallow features extracted by CNN are interacted in the proposed transformer fusion module to refine the fusion relationship within the spatial scope and across channels simultaneously. Besides, adversarial learning is designed in the training process to improve the output discrimination via imposing competitive consistency from the inputs, reflecting the specific characteristics in infrared and visible images. The experimental performance demonstrates the effectiveness of the proposed modules, with superior improvement against the state-of-the-art, generalising a novel paradigm via transformer and adversarial learning in the fusion task.},
  keywords={Transformers;Image fusion;Task analysis;Training;Feature extraction;Generative adversarial networks;Visualization;visual object tracking;RGBT tracking;temporal information;decision-level fusion},
  doi={10.1109/TIP.2023.3273451},
  ISSN={1941-0042},
  month={},}@ARTICLE{9256316,
  author={Liu, Shuo and Xu, Liwen},
  journal={IEEE Access}, 
  title={An Integrated Model Based on O-GAN and Density Estimation for Anomaly Detection}, 
  year={2020},
  volume={8},
  number={},
  pages={204471-204482},
  abstract={Anomaly detection is a classic and crucial problem in the field of artificial intelligence, which aims to find instances that deviate so much from the main distribution of data or differ from known instances. This paper explores how to combine advanced deep learning techniques with traditional probabilistic statistical methods for anomaly detection. We propose a very effective and concise semi-supervised anomaly detection method named “ORGAN-KDE” based on the orthogonal generative adversarial network (O-GAN) and kernel density estimation. In the training phase, we use the encoder of O-GAN to learn the latent representation of normal data, namely the code of normal data, and then use the kernel density estimation to solve the probability density function of code. The code of normal sample obtained through the trained encoder can get a larger probability value when passing through the trained kernel density estimator, while the code of anomalous sample can get a smaller probability value, so as to achieve the purpose of anomaly detection. Compared with other anomaly detection methods based on GAN, our method has a very simple network structure, and experiments have proved that it performs well on both structured datasets and image datasets.},
  keywords={Anomaly detection;Generators;Gallium nitride;Generative adversarial networks;Kernel;Estimation;Training;Anomaly detection;semi-supervised learning;orthogonal generative adversarial network;density estimation},
  doi={10.1109/ACCESS.2020.3037322},
  ISSN={2169-3536},
  month={},}@INBOOK{11164605,
  author={Choudhry, Mani Deepak and Sundarrajan, M. and Sundaram, Karthic and Rama, Abirami K.},
  booktitle={Generative AI and LLMs: Natural Language Processing and Generative Adversarial Networks}, 
  title={9 Bias and Fairness in Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={177-192},
  abstract={This chapter explores bias and fairness in generative AI, giving a comprehensive view that explores the deep relationship between artificial intelligence and human biases and what they provoke in the progress and implementation of generative prototypes. It is a thorough analysis of the biases in training data, algorithmic decisionmaking, and unintended consequences of AI applications, providing real-world examples. Exploration of the deep complications related to bias within generative AI systems and possible solutions will dominate this chapter. We shall try to relate those discussions to what are essentially ethical considerations and responsibilities embedded in their generation and use. This would provide us with a very clear foundation on which to discuss how one has to balance innovation and ethical issues in the field of AI. That humanist point of view will give way to the need to focus on broad societal implications regarding the development of AI.},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111425511},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11164605},}@INPROCEEDINGS{10465859,
  author={B, Ashwini and Sinha, Deepak Kumar and Chaudhary, Chetan},
  booktitle={2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)}, 
  title={Translational Deep Learning-Based Diagnostics: Enhancing Pathology With AI}, 
  year={2023},
  volume={},
  number={},
  pages={619-624},
  abstract={Transformative innovation in healthcare is led by the constantly changing field of deep learning techniques, especially designed for cellular imaging in diagnostic pathology. This paper provides a thorough examination of several deep learning approaches in the field of pathology, highlighting new uses and tackling important issues that are essential to the field's progress. The successful deployment of AI in clinical settings depends on the seamless integration of these apps into processes in digital pathology. This all-inclusive diagnostic method not only simplifies processes but also provides pathologists with cutting-edge instruments that accelerate work, improve diagnosis accuracy, and lower mistakes. The US has recently approved a limited number of manufacturers' digital pathology products for diagnostic usage, which offers a strong basis for the creation of useful AI solutions. Researchers, physicians, business executives, government agencies, and patient advocacy organizations working together will make it much easier for these cutting-edge technologies to spread and become widely used by medical professionals. These technologies usher in a new age in healthcare, distinguished by their exceptional performance, faster processing rates, cost effectiveness, increased accuracy, and strict safety regulations. AI and computational pathology will continue to develop in harmony, opening the door to a new age of improved healthcare delivery.},
  keywords={Deep learning;Pathology;Technological innovation;Government;Medical services;Streaming media;Turning;Deep Learning;Diagnostics;Pathology;Artificial Intelligence;Healthcare;Digital Pathology;Cellular Imaging;Diagnostic Imaging and Machine Intelligence},
  doi={10.1109/ICAICCIT60255.2023.10465859},
  ISSN={},
  month={Nov},}@ARTICLE{11029044,
  author={Li, Zhiyuan and Zhou, Yanhui and Wei, Hao and Ge, Chenyang and Mian, Ajmal},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={RDEIC: Accelerating Diffusion-Based Extreme Image Compression with Relay Residual Diffusion}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Diffusion-based extreme image compression methods have achieved impressive performance at extremely low bitrates. However, constrained by the iterative denoising process that starts from pure noise, these methods are limited in both fidelity and efficiency. To address these two issues, we present Relay Residual Diffusion Extreme Image Compression (RDEIC), which leverages compressed feature initialization and residual diffusion. Specifically, we first use the compressed latent features of the image with added noise, instead of pure noise, as the starting point to eliminate the unnecessary initial stages of the denoising process. Second, we directly derive a novel residual diffusion equation from Stable Diffusion’s original diffusion equation that reconstructs the raw image by iteratively removing the added noise and the residual between the compressed and target latent features. In this way, we effectively combine the efficiency of residual diffusion with the powerful generative capability of Stable Diffusion. Third, we propose a fixed-step fine-tuning strategy to eliminate the discrepancy between the training and inference phases, thereby further improving the reconstruction quality. Extensive experiments demonstrate that the proposed RDEIC achieves state-of-the-art visual quality and outperforms existing diffusion-based extreme image compression methods in both fidelity and efficiency. The source code and pre-trained models are available at https://github.com/huai-chang/RDEIC.},
  keywords={Image coding;Image reconstruction;Noise;Relays;Bit rate;Diffusion models;Training;Noise reduction;Mathematical models;Iterative methods;Image compression;compressed latent features;residual diffusion;extremely low bitrates},
  doi={10.1109/TCSVT.2025.3578127},
  ISSN={1558-2205},
  month={},}@ARTICLE{10244162,
  author={Singh, Vishakha and Sharma, Ritesh and Singh, Sanjay Kumar},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Designing New Blood-Brain Barrier Penetrating Molecules Using Novel Hybridized Gravitational Search Algorithm and Explainable AI}, 
  year={2024},
  volume={5},
  number={5},
  pages={2127-2138},
  abstract={Artificial intelligence (AI) has emerged as a powerful tool in computational biology, where it is being used to analyze large datasets to detect difficult biological patterns. This has enabled the design of new drug molecules. In this article, a novel method called hybridized gravitational search algorithm (HyGSA) has been proposed to design novel blood-brain barrier penetrating peptides (B3P2s) with desirable characteristics that enable them to cross the blood-brain barrier (BBB) and deliver neurological drugs directly to the brain. The HyGSA has two important modules in the form of an explainable machine learning classifier (with an accuracy, f1-score, and area under the ROC curve (AUROC) of 84%, 84%, and 91%, respectively) and an explainable deep learning-based B3P2 classifier (with an accuracy, f1-score, and AUROC of 89%, 91%, and 95%, respectively). The former was used to determine the crucial hand-engineered features, and the latter was designed to determine the critical amino acids that play an important role in the BBB penetrability of a peptide. Moreover, the population of particles was sampled at the beginning of each iteration to ensure a good mix of particles with low, average, and high fitness. This was achieved using a novel method that takes inspiration from the subset-sum problem and uses the mean and variance of the distribution of particles. For the pilot study, some B3P2s were discovered and optimized from a set of cell-penetrating peptides. Lastly, a free online tool has been deployed at https://b3p2design.anvil.app to help the scientific community discover and optimize B3P2s in protein sequences.},
  keywords={Peptides;Drugs;Search problems;Optimization;Explainable AI;Computational biology;Amino acids;Explainable artificial intelligence (AI);gravitational search algorithm (GSA);multiobjective optimization;nondominated fronts;subset-sum problem},
  doi={10.1109/TAI.2023.3313130},
  ISSN={2691-4581},
  month={May},}@INPROCEEDINGS{10868171,
  author={Liu, Wenping and Cui, Xin},
  booktitle={2024 4th International Conference on Educational Technology (ICET)}, 
  title={Generative AI-assisted Collaborative Argumentation: Implications for the Argumentation Process and Outcome}, 
  year={2024},
  volume={},
  number={},
  pages={220-225},
  abstract={With the rapid development of artificial intelligence technology, the field of education has undergone significant changes. In traditional collaborative argumentation, students often find it difficult to argue from multiple perspectives due to insufficient prior knowledge, and generative AI makes argumentation possible. The study invites university students to participate in online collaborative learning. A combination of quantitative and qualitative research methods is adopted, aiming to deeply analyze the impact of generative AI-assisted collaborative argumentation on the quality of argumentation and students' perceptions. The results of the study show that generative AI feedback not only promotes multi-perspective thinking, critical thinking, and organizational skills but also enhances students' confidence and motivation in argumentation. However, there is also the possibility of a lack of a sense of dialogue and an increase in student burden. The article verifies the effectiveness of generative AI in collaborative argumentation and provides new insights into its broader application in education.},
  keywords={Generative AI;Federated learning;Collaboration;Educational technology;Market research;generative AI feedback;collaborative argumentation;collaborative learning;dialog feedback},
  doi={10.1109/ICET62460.2024.10868171},
  ISSN={},
  month={Sep.},}@ARTICLE{10287183,
  author={Wang, Shuai and Zhou, Aimin and Zhang, Guixu and Fang, Faming},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Learning Regularity for Evolutionary Multiobjective Search: A Generative Model-Based Approach}, 
  year={2023},
  volume={18},
  number={4},
  pages={29-42},
  abstract={The prior domain knowledge, i.e., the regularity property of continuous multiobjective optimization problems (MOPs), could be learned to guide the search for evolutionary multiobjective optimization. This paper proposes a learning-to-guide strategy (LGS) for assisting the search for multiobjective optimization algorithms in dealing with MOPs. The main idea behind LGS is to capture the regularity via learning techniques to guide the evolutionary search to generate promising offspring solutions. To achieve this, a generative model called the generative topographic mapping (GTM) is adopted to capture the manifold distribution of a population. A set of regular grid points in the latent space are mapped into the decision space within some manifold structures to guide the search for mating with some parents for offspring generation. Following this idea, three alternative LGS-based generation operators are developed and investigated, which combine the local and global information in the offspring generation. To learn the regularity more efficiently in an algorithm, the proposed LGS is embedded in an efficient evolutionary algorithm (called LGSEA). The LGSEA includes an incremental training procedure aimed at reducing the computational cost of GTM training by reusing the built GTM model. The developed algorithm is compared with some newly developed or classical learning-based algorithms on several benchmark problems. The results demonstrate the advantages of LGSEA over other approaches, showcasing its potential for solving complex MOPs.},
  keywords={Manifolds;Training;Sensitivity;Scalability;Computational modeling;Social factors;Evolutionary computation;Artificial intelligence;Pareto optimization;Optimization methods},
  doi={10.1109/MCI.2023.3304080},
  ISSN={1556-6048},
  month={Nov},}@ARTICLE{10689629,
  author={Ghozatlou, Omid and Datcu, Mihai and Chapron, Bertrand},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={GAN-Generated Ocean SAR Vignettes Classification}, 
  year={2024},
  volume={21},
  number={},
  pages={1-5},
  abstract={The use of deep learning (DL) in Earth observation technology has become essential. Even though DL models do remarkably well in classifying high-resolution satellite images and extracting semantic information, they frequently need a lot of training data, which can be costly and time-consuming to obtain. With an emphasis on ocean synthetic aperture radar (SAR) image analysis, this research investigates the use of synthetically generated data using generative adversarial networks (GANs) for data augmentation. Relying on GAN-based generated images for remote sensing applications requires a thorough assessment of the quality and authenticity of the generated images, as well as validation of the model’s performance on real-world data. We assess the diversity and reliability of GAN-generated images by training a classification network on these images and evaluating their performance on real-world data. By comparing the classification accuracy in different experimental setups, we approximate the precision and recall for GANs performance.},
  keywords={Radar polarimetry;Oceans;Training;Accuracy;Noise;Generative adversarial networks;Synthetic aperture radar;Data augmentation;deep neural network;generative adversarial networks (GANs);ocean pattern classification;synthetic aperture radar (SAR);synthetic image generation},
  doi={10.1109/LGRS.2024.3466970},
  ISSN={1558-0571},
  month={},}@ARTICLE{11164467,
  author={Mao, Wenhao and Hou, Chengbin and Zhang, Tianyu and Lin, Xinyu and Tang, Ke and Lv, Hairong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Parse Trees Guided LLM Prompt Compression}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.},
  keywords={Computational efficiency;Linguistics;Information entropy;Computational modeling;Writing;Logic;Transformers;Measurement;Large language models;Analytical models;Large Language Models;Prompt Compression;Parse Trees;Prompt Structure;Text Pattern Analysis},
  doi={10.1109/TPAMI.2025.3609956},
  ISSN={1939-3539},
  month={},}@INPROCEEDINGS{11050639,
  author={Haider, Tayyba and Shafi, Numan and UI Haq, Nazeef},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1596-1602},
  abstract={Mental health issues, including stress, anxiety, and depression, have seen a significant rise globally, impacting millions of lives and creating an urgent need for innovative solutions. While traditional mental health services are beneficial, barriers such as social stigma, high costs, and limited availability hinder widespread access. AI-driven chatbots offer a promising avenue for scalable, accessible mental health support. However, a notable research gap exists in developing chatbots that can generate empathetic, contextually accurate responses grounded in reliable mental health literature. In this study, two Retrieval-Augmented Generation (RAG) models are proposed: one employs all-MiniLM-L6-v2 embeddings with FAISS for efficient retrieval and OpenAI GPT for response generation, while the other leverages GoogleGenerativeAIEmbeddings, Pinecone for vector storage, and BART for response generation. Both models were implemented in Google Colab, designed to deliver personalized mental health assistance based on trusted resources. Experimental evaluation reveals that the chatbot produces highly empathetic and context-aware responses, effectively addressing user needs. The results show that our model achieved a 0.85 cosine similarity score.},
  keywords={Costs;Generative AI;Retrieval augmented generation;Anxiety disorders;Mental health;Chatbots;Depression;Vectors;Internet;Reliability;Anxiety;Depression;Generative Models;Retrieval-Augmented Generation (RAG);Vector Search;Google Generative AI Embeddings},
  doi={10.1109/CAI64502.2025.00246},
  ISSN={},
  month={May},}@INPROCEEDINGS{11159888,
  author={Rai, Laxmisha and Liu, Fasheng},
  booktitle={2025 IEEE 8th International Conference on Electronic Information and Communication Technology (ICEICT)}, 
  title={Generating Visual Semiotics for Chinese Characters: Harnessing DeepSeek for Nonverbal Communication}, 
  year={2025},
  volume={},
  number={},
  pages={388-392},
  abstract={This study explores the potential of AI-mediated visual representation of Chinese characters through DeepSeek’s generative capabilities, addressing the intersection of computational linguistics, semiotics, and cross-cultural communication. We present a systematic analysis of over 1000 frequently used Chinese characters and their AI-generated visual correlates including symbols, emojis, and conceptual icons. We found that, DeepSeek is able to generate visual representations for all the 1000 simplified Chinese characters prompted. These visual representations highly relevant for the learners while learning Chinese characters. Most of these visual representations are in the form of symbols, emojis, and icons, rather than actual images. The visual representations are generated for characters are meaningful, and additional prompts are generated to translate example sentences to directly visual representations for further validation.},
  keywords={Visualization;Translation;Systematics;Generative AI;Chatbots;Semiotics;Internet;Information and communication technology;Emojis;Testing;DeepSeek;non-veral;Generative AI;Chinese language},
  doi={10.1109/ICEICT66683.2025.11159888},
  ISSN={2836-7782},
  month={July},}@ARTICLE{10637395,
  author={Wang, Hongqing and Chaw, Jun Kit and Goh, Sim Kuan and Shi, Liantao and Tin, Ting Tin and Huang, Nannan and Gan, Hong-Seng},
  journal={IEEE Access}, 
  title={Super-Resolution GAN and Global Aware Object Detection System for Vehicle Detection in Complex Traffic Environments}, 
  year={2024},
  volume={12},
  number={},
  pages={113442-113462},
  abstract={Intelligent vehicle detection systems have the potential to improve road safety and optimize traffic management. Despite the continuous advancements in AI technology, the detection of different types of vehicles in complex traffic environments remains a persistent challenge. In this paper, an end-to-end solution is proposed. The image enhancement part proposes a super-resolution synthetic image GAN (SSIGAN) to improve detection of small, distant objects in low-resolution (LR) images. An edge enhancer (EE) and a hierarchical self-attention module (HS) are applied to address the loss of high-frequency edge information and texture details in the super-resolved images. The output super-resolution (SR) image is fed into detection part. In the detection part, we introduce a global context-aware network (GCAFormer) for accurate vehicle detection. GCAFormer utilizes a cascade transformer backbone (CT) that enables internal information interaction and generates multi-scale feature maps. This approach effectively addresses the challenge of varying vehicle scales, ensuring robust detection performance. We also built in a cross-scale aggregation feature (CSAF) module inside GCAFormer, which fuses low- and high-dimensional semantic information and provides multi-resolution feature maps as input to the detection head, so as to make the network more adaptable to complex traffic environments and realize accurate detection. In addition, we validate the effectiveness of our proposed method on a large number of datasets, reaching 89.12% mAP on the KITTI dataset, 90.62% on the IITM-hetra, 86.83% on the Pascal VOC and 93.33% on the BDD-100k. The results were compared to SOTA and demonstrated the competitive advantages of our proposed method for Vehicle Detection in complex traffic environments.},
  keywords={Feature extraction;Vehicle detection;Accuracy;Image edge detection;Real-time systems;Superresolution;Generative adversarial networks;Intelligent vehicles;Generative adversarial networks;Intelligent vehicle detection;self-attention;multi-scale semantic feature;generative adversarial network;feature aggregation;transportation},
  doi={10.1109/ACCESS.2024.3442484},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10605261,
  author={Liu, Jiao and Gupta, Abhishek and Ong, Yew-Soon and Tan, Puay Siew},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Inverse Multiobjective Optimization by Generative Model Prompting}, 
  year={2024},
  volume={},
  number={},
  pages={737-740},
  abstract={The integration of multiobjective optimizers with inverse models—that map points on the Pareto front to corresponding nondominated solutions—has drawn attention. These inverse models serve a dual purpose, not only facilitating the generation of candidate solutions during the optimization process, but also offering insights for multiobjective decision-making upon completion of optimization. However, today’s inverse models mainly serve to capture one-to-one mapping relations, restricting them to learn only from nondominated solution samples. As a result, the information embedded in dominated samples is not fully utilized. In this paper, we introduce a novel approach of building conditional inverse generative models (invGMs) from optimization data, making the most of both nondominated and dominated solution samples during training. Different from standard inverse models, decision-makers can query such invGMs with prompts expressed in the form of any desired objective function values, leading them to produce a corresponding solution. Through iterative prompting, invGMs are shown to accelerate the creation of diverse sets of high-quality solutions even during the course of multiobjective optimization runs. Empirical studies on three industrial optimization problems highlight the proposed method’s faster convergence rate and improved inverse modeling accuracy.},
  keywords={Training;Accuracy;Inverse problems;Decision making;Linear programming;Data models;Iterative methods;Multiobjective optimization optimization;conditional generative models;inverse models},
  doi={10.1109/CAI59869.2024.00142},
  ISSN={},
  month={June},}@ARTICLE{11129698,
  author={Lu, Haixv and Liu, Debin and Yang, Laurence T. and Zhao, Ruonan and Lian, Shijie and Yuan, Songhe and Su, Junjie},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={A Zero-shot High-dimensional Feature Fusion with STF-GAN for Cross-domain Image Reconstruction}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={High-dimensional imaging techniques are rapidly evolving in the signal processing field, with applications spanning a wide range of domains such as autonomous driving, industrial manufacturing, healthcare, remote sensing, and robotics. Generative Adversarial Networks (GANs) have shown remarkable success in image generation, yet challenges persist in high-dimensional cross-domain reconstruction tasks—particularly when target-domain samples are unavailable. Existing zero-shot methods often suffer from expression distortion, mode collapse, and feature degradation due to inadequate fusion of multi-scale representations and poor semantic consistency. Inspired by the need for high-dimensional image recovery, we propose the Share Token Tensorized Attention Fusion Generative Adversarial Network (STF-GAN) framework. This is a new framework for multi-scale feature fusion of features from pre-trained generators using the Share Token Tensorized Attention Fusion approach. Our main contributions are the tensorized attention mechanism that fuses high-dimensional features across spatial and semantic domains while maintaining structural consistency, fast learning guided by CLIP, and bridging the source-target domain gap in the absence of paired data. Experimental results on FFHQ and AFHQ datasets demonstrate that STF-GAN achieves superior reconstruction fidelity, with an average improvement of 12.3% in Inception Score and 8.7% in Structural Consistency compared to IPL, while effectively mitigating distortion artifacts. These experiments demonstrate the ability of STF-GAN to process high-dimensional features, showing strong potential for applications in industrial imaging systems, medical diagnostics, and other fields where cross-domain image reconstruction must be performed with limited data availability.},
  keywords={Image reconstruction;Generative adversarial networks;Training;Generators;Zero shot learning;Semantics;Visualization;Solid modeling;Hidden Markov models;Image synthesis;Style Transfer;Generative Adversarial Network;Zero-shot Learning;High-dimensional Feature Fusion},
  doi={10.1109/JSTSP.2025.3600191},
  ISSN={1941-0484},
  month={},}@INPROCEEDINGS{11140344,
  author={Peng, Hong and Jin, Xiaoliang and Huang, Qiao and Liu, Supeng},
  booktitle={2025 4th International Symposium on Robotics, Artificial Intelligence and Information Engineering (RAIIE)}, 
  title={A Study on Enhancing the Reasoning Efficiency of Generative Recommender Systems Using Deep Model Compression}, 
  year={2025},
  volume={},
  number={},
  pages={364-367},
  abstract={In order to improve the inference efficiency of generative recommendation systems in practical applications, a deep model compression framework integrating structured pruning, dynamic quantization, and knowledge distillation is constructed to study the comprehensive impact of multi strategy synergy on model volume, response delay, and recommendation accuracy. Using publicly available datasets such as MovieLens-1M and Alibaba Tianchi as benchmarks, analyze the performance and system bottlenecks under different compression combinations. By constructing a hybrid parallel scheduling mechanism and multi-level cache optimization system, enhance the execution capability of the compression model in the inference stage. The results show that the joint compression of the three strategies can reduce the inference delay to 15.3ms while compressing the model parameters to 22.8% of the original, HR@10 Only a 0.4% decrease has been achieved, significantly improving operational efficiency and deployment adaptability.},
  keywords={Adaptation models;Solid modeling;Quantization (signal);Accuracy;Cognition;Delays;Model compression;Recommender systems;Robots;Optimization;generative recommender systems;model compression;inference acceleration;structured pruning},
  doi={10.1109/RAIIE65740.2025.11140344},
  ISSN={},
  month={June},}@ARTICLE{10713260,
  author={Agrawal, Arya and Sharma, Teena and Verma, Nishchal K.},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Comprehensive Exploration of Real-Time 3-D View Reconstruction Methods}, 
  year={2024},
  volume={5},
  number={12},
  pages={5915-5927},
  abstract={Real-time 3-D view reconstruction in an unfamiliar environment poses complexity for various applications due to varying conditions such as occlusion, latency, precision, etc. This article thoroughly examines and tests contemporary methodologies addressing challenges in 3-D view reconstruction. The methods being explored in this article are categorized into volumetric and mesh, generative adversarial network based, and open source library based methods. The exploration of these methods undergoes detailed discussions, encompassing methods, advantages, limitations, and empirical results. The real-time testing of each method is done on benchmarked datasets, including ShapeNet, Pascal 3D+, Pix3D, etc. The narrative highlights the crucial role of 3-D view reconstruction in domains such as robotics, virtual and augmented reality, medical imaging, cultural heritage preservation, etc. The article also anticipates future scopes by exploring generative models, unsupervised learning, and advanced sensor fusion to increase the robustness of the algorithms.},
  keywords={Three-dimensional displays;Image reconstruction;Real-time systems;Point cloud compression;Accuracy;Generative adversarial networks;Convolutional neural networks;3-D view reconstruction;3-D point cloud;convolutional neural networks;generative adversarial networks;volumetric and mesh reconstruction},
  doi={10.1109/TAI.2024.3477425},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{9905191,
  author={Dilhan, M. W. Sachin and Wagarachchi, N. Mihirini},
  booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={Stock Market Prediction using Artificial Intelligence}, 
  year={2022},
  volume={5},
  number={},
  pages={35-41},
  abstract={This research focuses on predicting stock closing prices for one day or the future in specific economic conditions. Today, Sri Lanka faces a financial crisis due to the COVID-19 pandemic. Therefore, lots of investors are bankrupt due to unpredictable stock prices. This work mainly focuses on predicting stock prices in banking sector shares such as Commercial Bank (COMB.N), Hatton National Bank (HNB.N), Seylan Bank (SEYB.N), and Sampath Bank (SAMP.N) on Colombo Stock Exchange (CSE) in Sri Lanka. According to the hypothesis, All Share Price Index (ASPI) and Banking Sector indices have been taken as a numerical sentiment parameter other than the historical prices from each bank. Since ASPI shows overall market performance and Banking sector indices show banking sector capitalization changed over time. There can be a positive and negative sentiment when the ASPI and Sector Indices increase and decrease, respectively. Finally, a dataset is divided into 70% for training and 30% for testing. This study has used Recurrent Neural Networks (RNNs) such as Long short-term memory (LSTM) and Gated Recurrent Unit (GRU) using 25, 50, 100, 150, and 200 epochs. LSTM model has given the lowest Mean Squared Error (MSE) and Root Mean Square Error (RMSE). According to the LSTM model, COMB.N, HNB.N, and SAMP.N were given the lowest MSE, and RMSE for 100 epochs, and SEYB.N was given the lowest MSE and RMSE value for the 150 epochs.},
  keywords={Training;Recurrent neural networks;Banking;Share prices;Predictive models;Logic gates;Indexes;Gated Recurrent Unit (GRU);Long Short-Term Memory (LSTM);Machine Learning;stock market prediction;Streamlit API},
  doi={10.1109/SCSE56529.2022.9905191},
  ISSN={2613-8662},
  month={Sep.},}@INPROCEEDINGS{10499002,
  author={Subaranjani, T and Jayaraj, N},
  booktitle={2024 11th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Advanced Neural Network Approaches for Distinguishing Real from Synthetic in GAN-generated Data Authenticity Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={297-302},
  abstract={Amid the transformative advancements of Generative Adversarial Networks (GANs) in machine learning, a pertinent challenge arises: discerning real instances from synthetic ones. This research introduces a novel neural network model meticulously tailored to differentiate between genuine tasks and those artfully crafted by GANs. The paper elaborates on the unique architectural design and optimization techniques employed, offering a comprehensive insight into the model’s development and testing phases. Empirical evaluations reveal an unparalleled accuracy rate, underscoring the model’s practicality and efficacy. Notwithstanding its high precision and recall balance, the study identifies potential areas of refinement, ensuring its adaptability to future GAN sophistications. As the realm of artificial data generation continues to evolve, this research stands as a beacon, advancing the understanding and tools essential for maintaining authenticity and integrity in data-driven domains.},
  keywords={Symbiosis;Adaptation models;Neural networks;Refining;Generative adversarial networks;Data models;Task analysis;Generative Adversarial Networks;Neural Network;Data Authenticity;Precision;Recall;Synthetic Data Detection},
  doi={10.23919/INDIACom61295.2024.10499002},
  ISSN={},
  month={Feb},}@ARTICLE{11175299,
  author={Gao, Lu and Jiang, Shaodong and Zhao, Yang and Zhang, Faxiang and Sun, Zhihui},
  journal={IEEE Sensors Journal}, 
  title={Data Augmentation and Application of DAS Based on Generative Adversarial Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Distributed fiber optic acoustic sensing system (DAS) shows great potential in perimeter security with its high accuracy, high sensitivity and wide range monitoring capability. Aiming at the data scarcity problem of DAS in perimeter security caused by the low probability of acquiring abnormal event samples and the interference of environmental noise, a data augmentation approach for LMA-DRAGAN utilizing generative adversarial network is proposed. The optimised LSKblock module and the improved MobileViT attention mechanism are effectively combined to achieve dynamic sampling of feature maps, which effectively retains the integrity of the original information. And an adaptive learning rate adjustment strategy is employed to balance the parameter update frequency of the generator and the discriminator, thereby helping to stabilize the training process. The study demonstrates that on the dataset containing five types of perimeter security events, the method proposed achieves MAE value of 0.1658, CS value of 0.9542, RMSE value of 0.2260, and IS value of 2.5112, which provides higher diversity while maintaining the realism compared to the conventional augmentation methods. Validated by AlexNet classification model, the recognition accuracy of the enhanced dataset reaches 97.97%, which is a 19.53% improvement over the unenhanced baseline. This study provides a reliable enhancement strategy for DAS perimeter security under small sample conditions, which is conducive to its wider engineering applications.},
  keywords={Data models;Training;Generators;Adaptation models;Security;Generative adversarial networks;Optical fiber amplifiers;Data augmentation;Intelligent sensors;Acoustics;DAS;Data augmentation;GAN;Data quality evaluation},
  doi={10.1109/JSEN.2025.3610509},
  ISSN={1558-1748},
  month={},}@INPROCEEDINGS{10892386,
  author={Agu, Nkiru L. and Haider, Syed M. and Pillai, Gobind G. and Bashir, Imran and Lacey, Gill},
  booktitle={2024 59th International Universities Power Engineering Conference (UPEC)}, 
  title={Power Transformer DGA Data Augmentation Using Conditional Tabular Generative Adversarial Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Dissolved gas analysis (DGA) serves as an effective method for diagnosing transformer faults, but the sample data obtained from the measurement of equipment failure are often insufficient, low in quality due to the presence of noise and are unevenly distributed. These characteristics of the DGA data, especially the imbalance in the fault classes make it a major challenge significantly impacting the accuracy of fault diagnosis. In this paper, conditional tabular generative adversarial network (CTGAN) is proposed to achieve an even probability distribution of the fault classes through generating realistic synthetic data to augment the minority fault classes. To evaluate the performance of the proposed technique descriptive statistics and evenness indices were used to compare the sample homogeneity of the generated synthetic data to the real DGA data. The results showed fidelity of the synthetic data generated by CTGAN in relation to the original DGA data and an increase in the probability of the minority fault classes.},
  keywords={Training;Power engineering;Noise;Distributed databases;Dissolved gas analysis;Generative adversarial networks;Probability distribution;Power transformers;Noise measurement;Synthetic data;GAN;CTGAN;data augmentation;dissolved gas analysis;fault diagnostics;adversarial training},
  doi={10.1109/UPEC61344.2024.10892386},
  ISSN={},
  month={Sep.},}@ARTICLE{10380501,
  author={Sun, Jianxin and Deng, Qiyao and Li, Qi and Sun, Muyi and Liu, Yunfan and Sun, Zhenan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={AnyFace++: A Unified Framework for Free-style Text-to-Face Synthesis and Manipulation}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Human faces contain rich semantic information that could hardly be described without a large vocabulary and complex sentence patterns. However, most existing text-to-image synthesis methods could only generate meaningful results based on limited sentence templates with words contained in the training set, which heavily impairs the generalization ability of these models. In this paper, we define a novel ‘free-style’ text-to-face generation and manipulation problem, and propose an effective solution, named AnyFace++, which is applicable to a much wider range of open-world scenarios. The CLIP model is involved in AnyFace++ for learning an aligned language-vision feature space, which also expands the range of acceptable vocabulary as it is trained on a large-scale dataset. To further improve the granularity of semantic alignment between text and images, a memory module is incorporated to convert the description with arbitrary length, format, and modality into regularized latent embeddings representing discriminative attributes of the target face. Moreover, the diversity and semantic consistency of generation results are improved by a novel semi-supervised training scheme and a series of newly proposed objective functions. Compared to state-of-the-art methods, AnyFace++ is capable of synthesizing and manipulating face images based on more flexible descriptions and producing realistic images with higher diversity.},
  keywords={Faces;Visualization;Training;Semantics;Sun;Feature extraction;Transformers;Generative Adversarial Networks;Text-to-image Generation},
  doi={10.1109/TPAMI.2023.3345866},
  ISSN={1939-3539},
  month={},}@INPROCEEDINGS{10545839,
  author={Oliveira, Paulo Moura},
  booktitle={2023 6th Experiment@ International Conference (exp.at'23)}, 
  title={ChatGPT: Assessing Impacts and Perspectives in Engineering Education Using a Genetic Algorithms Case Study}, 
  year={2023},
  volume={},
  number={},
  pages={181-186},
  abstract={The recent release of ChatGPT-3 by OpenAI may have been a major disruptive mark in terms of Artificial Intelligence based tools. The testing and rapid user adoption rate of ChatGPT-3 was massive with a worldwide impact. Despite its recent public release ChatGPT-3 is already eliciting a mix of positive reactions revealing outstanding positive aspects as well as some negative ones. A short evaluation of ChatGPT-3 is presented, using the context of genetic algorithms, a topic lectured in introductory artificial intelligence courses. Examples outlining potential advantages of adopting ChatGPT and disadvantages which raise ethical issues and may limit its use are presented.},
  keywords={Ethics;Analytical models;Generative AI;Image processing;Chatbots;Cognition;Engineering education;ChatGPT;ChatBots;Generative Language Models;Engineering Education;Genetic Algorithms},
  doi={10.1109/exp.at2358782.2023.10545839},
  ISSN={2376-6328},
  month={June},}@INPROCEEDINGS{8513727,
  author={Li, Jiajing and Zhang, Guoying and Yan, Hongfei and Yu, Longxue and Meng, Tao},
  booktitle={2018 IEEE International Conference on Smart Cloud (SmartCloud)}, 
  title={A Markov Logic Networks Based Method to Predict Judicial Decisions of Divorce Cases}, 
  year={2018},
  volume={},
  number={},
  pages={129-132},
  abstract={Prediction of the judicial decision of a case is a research issue of artificial intelligence in legal domain. Existing studies mainly focus on criminal cases and aim at charge prediction, moreover the results of these models are usually hard to interpret. In this paper we propose a Markov logic networks based method for this problem. We firstly describe and extract the semantic of legal factors in a formal way; then we build and train a Markov logic networks for the prediction. The experimental results of prediction for divorce cases show that, our method is insusceptible to different expression styles, at the same time its prediction outcomes are interpretable.},
  keywords={Markov processes;Law;Semantics;Predictive models;Grammar;Engines;artificial intelligence, Markov logic Networks, text mining, legal science, judicial decisions},
  doi={10.1109/SmartCloud.2018.00029},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10427633,
  author={Ranolo, Elmo and Sebial, Archival and Ilano, Anthony and Canillo, Angie Ceniza},
  booktitle={2023 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)}, 
  title={Image Detection of Seaweed, Seagrass, and Coral in Coastal and Underwater Marine Ecosystems}, 
  year={2023},
  volume={},
  number={},
  pages={931-935},
  abstract={Marine ecosystem monitoring is crucial for understanding and preserving the health of coastal and underwater environments. This research presents an innovative approach to automatically detect and classify key components of marine ecosystems, namely seaweed, seagrass, and coral, from the images captured in coastal and underwater environments. We leverage the You Only Look Once (YOLO) object detection framework for its high-accuracy object recognition capabilities. To enhance the detection accuracy and improve the quality of input images, a combination of Generative Adversarial Networks (GANs) and the Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithms is used during training. GANs generate synthetic images, expanding the diversity of training data. The CLAHE algorithm enhances the contrast and detail in both real and synthetic images, making the object detection process more robust. The integration of YOLO with GAN and CLAHE algorithms helps address challenges encountered in underwater imaging conditions, such as low visibility and changing lighting. Compared to the original YOLO-V7, it demonstrates an increase of 8% to 10% in detecting seaweed, seagrass, and coral using the new approach. This new approach showcases the potential for advancing the monitoring of coastal and underwater environments.},
  keywords={YOLO;Seaweed;Sea measurements;Generative adversarial networks;Marine ecosystems;Data models;Monitoring;Machine learning;GAN;Artificial intelligence;Yolo and CLAHE},
  doi={10.1109/ICAMIMIA60881.2023.10427633},
  ISSN={2832-8353},
  month={Nov},}@INPROCEEDINGS{10211622,
  author={Mohamed, Nadia Belghazi and Bogdanel, Georgiana and Moreno, Hilario Gómez},
  booktitle={2023 18th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Is Training Useful to Detect Deepfakes? : A Preliminary Study.}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative Adversarial Networks (GANs) constitute a significant breakthrough due to their ability to generate realistic synthetic data. Sometimes, the synthetic creation resembles a real person or their voice, and we call them deepfakes. Deepfakes are used for multiple purposes, including malicious ones. This makes them a major concern due to the widespread lack of information about their existence and because they are increasingly generated more realistically. It is becoming increasingly difficult to differentiate them from real content. In this study, we evaluate human ability to differentiate between a real and a synthetic face. The results indicate that people are unable to differentiate between real images and deepfakes, but that some training slightly improves these results. Therefore, our hypothesis is that human accuracy in identifying synthetic faces could be improved with thorough training on how to detect deepfake faces.},
  keywords={Training;Deepfakes;Generative adversarial networks;Faces;Synthetic data;Information systems;deepfakes;artificial intelligence;signal processing;StyleGan2;human ability;synthetic face},
  doi={10.23919/CISTI58278.2023.10211622},
  ISSN={2166-0727},
  month={June},}@INPROCEEDINGS{10245882,
  author={Ritharson P, Isaac and Vidhya, K and G, Madhavan and D, Barath and Sathish Kumar, K},
  booktitle={2023 International Conference on Circuit Power and Computing Technologies (ICCPCT)}, 
  title={GAN-Based Facial Feature Reconstruction for Improved Masked Face Recognition During Covid}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The use of face masks during is a major obstacle has been faced during the COVID-19 epidemic. Face recognition technologies, which rely on unobstructed facial features for accurate identification. In this paper, we make use of Generative Adversarial Networks (GANs) for face reconstruction from masked faces. We propose a GAN-based approach to learn the underlying relationships between masked and unmasked facial features and generate plausible reconstructions of the missing facial features. Our primary goal is to evaluate the effectiveness of GAN-based face reconstruction for masked faces using various metrics, including visual quality, accuracy, and robustness. The findings of this investigation show that the suggested strategy can produce high-quality and precise reconstructions of the missing facial features, and it can be used for face recognition tasks. Our findings suggest that GAN-based face reconstruction has the potential to overcome the limitations posed by face masks, providing a solution that preserves individual privacy and security while ensuring accurate face recognition. This research project has significant implications for the development of new face recognition technologies that can operate effectively in the presence of masks, contributing to the ongoing efforts to combat the COVID-19 pandemic.},
  keywords={Training;COVID-19;Face recognition;Generative adversarial networks;Generators;Robustness;Security;Face Recognition;Face Reconstruction;GAN's;Masks;Image Synthesis;Image Manipulation;Covid-19;Artificial Intelligence},
  doi={10.1109/ICCPCT58313.2023.10245882},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10879605,
  author={Khan, Rehan and Saeed, Umer and Koo, Insoo},
  booktitle={2025 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={A Conditional GAN and Dual-Channel Hybrid Deep Feature Framework for Robust Sensor Fault Detection in WSNs}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Sensor-generated data is vital to the operation of numerous systems and services in the rapidly growing field of the Internet of Things. Wireless Sensor Networks, as an essential setup for these systems, are frequently deployed in large, diverse, and often harsh environments. However, these networks are highly vulnerable to various faults, potentially leading to improper data transmission, reliability, and financial stability of the systems. To address these challenges, we propose a hybrid model for sensor fault detection that integrates a machine learning classifier with the deep learning (DL) model, specifically VGG-16 and ResNet-50. Synthetic samples are generated using a Conditional Generative Adversarial Network and common sensor faults, such as hardover, drift, spike, erratic, and stuck fault are introduced by leveraging a publicly available temperature sensor dataset. Time-series data is transformed into Gramian Angular Field images, from which deep features are extracted using VGG-16 and ResNet-50. These extracted features are then fused to form a hybrid feature pool. Our framework effectively addresses problems related to data imbalance and enhances accuracy. The proposed model outperforms the individual feature sets, VGG-16 (89.22%) and ResNet-50 (84.21%), achieving notable accuracy of 92.55% with the fused feature set, underscoring its potential for robust sensor fault detection.},
  keywords={Temperature sensors;Wireless sensor networks;Accuracy;Fault detection;Feature extraction;Generative adversarial networks;Data models;Stability analysis;Reliability;Thermal stability;Artificial Intelligence;Deep Learning;Con-ditional GAN;Wireless Sensor Networks;IoT;Sensor Fault Detection},
  doi={10.1109/ICEIC64972.2025.10879605},
  ISSN={2767-7699},
  month={Jan},}@INBOOK{11164768,
  author={Aathilakshmi, S. and Sivapriya, G. and Manikandan, T.},
  booktitle={Generative AI and LLMs: Natural Language Processing and Generative Adversarial Networks}, 
  title={6 LLM Fine-Tuning: Instruction and Parameter-Efficient Fine-Tuning (PEFT)}, 
  year={2024},
  volume={},
  number={},
  pages={117-134},
  abstract={In artificial intelligence (AI) generating function the large language model plays an important role in various long data communication. Even though the system has pretrained language model, this large language model (LLM) is going to help the model according to the trained data in various analyses. This system is very useful in many fields like natural language processing, question answering, and GPT to produce better performance. The collection of large data in pretrained model will give a better solution but it is not able to tune all the data which is suitable for multitask application. This LLM is used to train the data more efficiently according to different categorizations such as medical dataset prediction, language translation, and user support. To improve the accuracy level the trained fine-tuning method is used which is more suitable for any specific task. The pretrained model is used to train all the dataset which is available in specific task and analyzed using fine-tuned data transition. Even though this pretrained model produces large analyzation of data in language processing, language training and prediction with the help of LLM it produce good performance for any dataset. The fine-tuning model is used to understand data transmission, task prediction, and medical analyzation. For example, training the dataset using fine-tuning is used to produce large dataset transition with effective way of different applications like LLM and GPT. This large language model is one of the best datasets trained sequence in real-time application of generative AI. This LLM knows how to collect a dataset according to the user concern and how it will produce the outcome using fine-tuning method but this LLM required more storage area compared to the LLM fine-tuned model. For example, finetuning an LLM like GPT has some trained language according to the predefined model; by the way of this LLM the present trained dataset is going to analyze the question and answers, present situation-based, but it has some difficulties when increasing the number of dataset for high application. At this case the parameter-efficient fine-tuning method is used for good analysis, which considers as the storage capacity for any given task.},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111425511},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11164768},}@INPROCEEDINGS{10796824,
  author={Sabah, Hawraa Ali and Singh, Sandeep and Al-Farouni, Mohammed and Pareek, Shashank and Verma, Deeksha and Dey, Protyay},
  booktitle={2024 First International Conference on Software, Systems and Information Technology (SSITCON)}, 
  title={Style Transfer in 3D Object Generation using Generative Adversarial Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper uses style transfer techniques with GANs to improve the variety and quality of generated 3D objects. By outlining the numerous advantages and highlighting the key obstacles, we show that it is feasible to merge style transfer with 3D object generation. It has been shown that GANs, originally thought of in terms of 2D domains, can be easily extended to 3D settings; however, certain issues have been identified with the modeling of 3D shapes and the training of models. Mode collapse and high computational complexity are two of the main obstacles that prevent GANs from being used for 3D objects. Additionally, it clarifies why a distinct and more intricate architecture is necessary to address the issue of 3D gan and neural implicit representations in modeling. Additional emphasis was placed on enhancing training stability through the use of superior methods, such as the progressive growing and WassersteinGANs with gradient penalty method. The study does, however, detail the difficulties of putting these models into practice, as well as some environmentally friendly ways to improve computation, such as by trimming networks and making use of clouds. These are the research vulnerabilities that have been identified: Researchers should expand their focus in their current studies of the aforementioned model to include domains that adapt and transfer learning, and they should also take into account the reality of operating in VR and AR settings. If these holes are filled, we can remove the current limitations and use style transfer to create 3D objects to a far greater extent. So, it’s clear that style transfer intermittently into 3D GANs can work, but there are still a lot of obstacles to overcome and limits to these methods, so researchers are working hard to make them more effective and realistic.},
  keywords={Training;Solid modeling;Adaptation models;Three-dimensional displays;Costs;Computational modeling;Transfer learning;Computer architecture;Stability analysis;Computational efficiency;style transfer;3D object generation;generative adversarial networks (GANs);mode collapse;training stability},
  doi={10.1109/SSITCON62437.2024.10796824},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10297181,
  author={Das, Nilanjana and Kotal, Anantaa and Roseberry, Daniel and Joshi, Anupam},
  booktitle={2023 IEEE International Conference on Intelligence and Security Informatics (ISI)}, 
  title={Change Management using Generative Modeling on Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={A key challenge faced by small and medium-sized business entities is securely managing software updates and changes. Specifically, with rapidly evolving cybersecurity threats, changes/updates/patches to software systems are necessary to stay ahead of emerging threats and are often mandated by regulators or statutory authorities to counter these. However, security patches/updates require stress testing before they can be released in the production system. Stress testing in production environments is risky and poses security threats. Large businesses usually have a non-production environment where such changes can be made and tested before being released into production. Smaller businesses do not have such facilities. In this work, we show how “digital twins”, especially for a mix of IT and IoT environments, can be created on the cloud. These digital twins act as a non-production environment where changes can be applied, and the system can be securely tested before patch release. Additionally, the non-production digital twin can be used to collect system data and run stress tests on the environment, both manually and automatically. In this paper, we show how using a small sample of real data/interactions, Generative Artificial Intelligence (AI) models can be used to generate testing scenarios to check for points of failure.},
  keywords={Production systems;Regulators;Manuals;Software systems;Digital twins;Computer security;Task analysis;Digital Twin;Generative Modeling;Software Testing;Change Management},
  doi={10.1109/ISI58743.2023.10297181},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8243966,
  author={Liu, Guangzhong and Wang, Jiajie and Zhang, Chi and Liao, Song and Liu, Yuehu},
  booktitle={2017 Chinese Automation Congress (CAC)}, 
  title={Realistic view synthesis of a structured traffic environment via adversarial training}, 
  year={2017},
  volume={},
  number={},
  pages={6600-6605},
  abstract={Generating a realistic image from a novel viewpoint has always been a key problem in image-based rendering and other related domains. In this paper we utilize the state-of-the-art generative adversarial networks(GAN) to synthesize novel views of a structured scene. Based on our proposed representations for traffic scene, a realistic image of a certain viewpoint can be generated via conditional GANs, given the geometric layout of the corresponding position and pose. In order to preserve the geometric property of the input in the generated image, we propose a simple but effective constraint in the generator network. Qualitative and comparative results have validated our method as well as shown its effectiveness and efficiency.},
  keywords={Generators;Training;Roads;Layout;Solid modeling;Geometry;Gallium nitride;view synthesis;adversarial training;GAN;synthetic modeling},
  doi={10.1109/CAC.2017.8243966},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10510784,
  author={Chung, Allan and He, Yu-Chen and Lin, Ling-Fang and Liang, Yo-Wen},
  booktitle={2024 IEEE 7th Eurasian Conference on Educational Innovation (ECEI)}, 
  title={Importance of Different AI-Generated Journey Map Modules from Industrial Design Students’ Perspectives}, 
  year={2024},
  volume={},
  number={},
  pages={242-245},
  abstract={Generative Artificial Intelligence (GAI) plays a prominent role in assisting designers in creating Journey Maps (JMs) based on design thinking. Today, JMs is used to analyze user behavior and emotions in various patterns and establish a structured framework with reduced subjectivity through the integration of GAI. As user-friendly tools continue to gain traction and are investigated for the significance of journey map contents. However, it is important to develop future applications in education from students' perspective. Using the Analytic Hierarchy Process (AHP), we assessed the importance of different elements of JMs. As a result, student preferences based on their educational experiences were determined. Modules were assessed for advantages and disadvantages or the opportunities and threats associated with the journey user’s behaviors. Despite limitations such as sample size and regional focus, the study result underscored the potential of AI in design education to enhance the creation and evaluation of JMs. Consequently, it advocated for the integration of AI tools into design curricula to enhance design quality and efficiency.},
  keywords={Technological innovation;Generative AI;Education;Analytic hierarchy process;Journey Map;generative artificial intelligence;industrial design},
  doi={10.1109/ECEI60433.2024.10510784},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11012850,
  author={Lin, Yadanar and Ferdous Khan, M. Fahim and Sakamura, Ken},
  booktitle={2025 1st International Conference on Consumer Technology (ICCT-Pacific)}, 
  title={Athena: A GenAI-Powered Programming Tutor Based on Open-Source LLM}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={With the rapid growth of generative artificial intelligence (GenAI), it is important to find ways to utilize them for academic advantage. GenAI tools embody immense potential in providing personalized feedback to students any time anywhere, and hence can provide a reliable helping hand to teachers who often experience burnout in large classes and are burdened with administrative tasks. While current GenAI tools like ChatGPT are helpful, they occasionally offer misinformation - a phenomenon known as hallucination, undermine critical thinking by providing direct answers to questions, and, as paid services, can further widen the digital divide. Against the backdrop of these problems, this research introduces Athena, a GenAI programming mentor based on an open-source large language model (LLM), constructed to guide programming learners to think critically and provide reliable information leveraging retrieval augmented generation. Its impact on learning outcomes was measured by feedback from programming students. Most students have given a positive response, saying that their motivation to keep learning and their confidence in their abilities have increased. These results imply that having a reliable AI mentor that can guide students at all times can have a positive impact in self-directed learning process.},
  keywords={Hands;Generative AI;Large language models;Retrieval augmented generation;Education;Chatbots;Reliability;Digital divide;Fake news;Programming profession;Generative artificial intelligence (GenAI);programming education;large language model (LLM);retrieval augmented generation (RAG);educational chatbot},
  doi={10.1109/ICCT-Pacific63901.2025.11012850},
  ISSN={},
  month={March},}@INPROCEEDINGS{10800540,
  author={Kirane, Nikita and Choudhury, Rudra Roy},
  booktitle={2024 5th International Conference on Data Analytics for Business and Industry (ICDABI)}, 
  title={PMAssist: Scaling Product Efficiency using GenAI}, 
  year={2024},
  volume={},
  number={},
  pages={219-223},
  abstract={Software Product Management (SPM) is a broad discipline that spans technology and business domains, requiring extensive collaboration between development teams and client stakeholders. Product managers are integral throughout the product lifecycle, from ideation to product launch and development to ongoing monitoring and maintenance. Product managers frequently encounter challenging situations that impair their ability to make effective decisions in a data-driven environment. In this paper, we present PMAssist, a product management tool developed using Generative Artificial Intelligence (GenAI) to enhance product management efficiency. PMAssist offers an innovative AI-driven solution to the tedious tasks of document retrieval and report generation. This research provides a GenAI-based tool that product managers can leverage to solve complex, time-consuming, and labor-intensive manual processes involved in product management. Instead, they can focus more on strategic and tactical tasks.},
  keywords={Industries;Data analysis;Generative AI;Manuals;Software;Product development;Maintenance;Stakeholders;Monitoring;Business;artificial intelligence;product management;generative AI;data product development},
  doi={10.1109/ICDABI63787.2024.10800540},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10590006,
  author={Kim, Janghwan and Park, Chansol and Jang, Woosung and Kim, R. Young Chul},
  booktitle={2023 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Cartoon Extraction Mechanism via UML Model Based on Natural Language Requirement Specs}, 
  year={2023},
  volume={},
  number={},
  pages={1578-1582},
  abstract={Recently, research on the development of artificial intelligence, in particular generative AI, has been active around the current world. Recent research mainly focuses on generating various types of outputs (text, image, video, etc.) from natural language textual inputs. However, understanding the meaning of these prompts in AI remains a challenge. In this paper, we propose a mechanism for extracting Cartoon images via UML Models based on natural language-based specifications, mapping a cut image's cartoon elements with UML properties extracted through linguistic textual analysis in software engineering. We expect software engineers to automatically help toon writers generate a cartoon's image with linguistic mechanisms.},
  keywords={Analytical models;Generative AI;Scientific computing;Computational modeling;Unified modeling language;Semantics;Linguistics;Linguistic Analysis;Natural Language Processing;Toon Generation;UML Diagram;Software Engineering Design},
  doi={10.1109/CSCI62032.2023.00260},
  ISSN={2769-5654},
  month={Dec},}@INPROCEEDINGS{9836504,
  author={Nie, Wei and Gou, Peng and Liu, Yang and Zhou, Tianyu and Xu, Nuo and Wang, Peng and Du, QiQi},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={A semi-supervised image segmentation method based on generative adversarial network}, 
  year={2022},
  volume={10},
  number={},
  pages={1217-1223},
  abstract={Semantic segmentation based on deep learning is an important research direction in computer vision, and it has developed rapidly in intelligent interpretation of remote sensing images in recent years. However, semantic segmentation based on deep learning is also a long-standing challenge in remote sensing image applications. The main reason is that training a neural network requires a large amount of pixel-level labeled data, and these publicly available labeled data are usually few. It requires a lot of labor costs. To address this issue, this paper proposes a semi-supervised semantic segmentation method based on Generative adversarial networks (GAN). Compared with previous techniques, we extends typical GAN networks to pixel-level prediction and discrimination, and applies it to remote sensing image semantic segmentation. We introduce residual networks and dilated convolutions into the generator, and employ a flow alignment module (FAM) to learn the semantic flow between adjacent hierarchical feature maps. The connected discriminator formulates the training as the min-maximum optimization problem. Comprehensive quantitative and qualitative evaluations on multiple models show that our proposed method outperforms state-of-the-art semi-supervised image segmentation algorithms.},
  keywords={Training;Deep learning;Image segmentation;Semantics;Generative adversarial networks;Robustness;Sensors;semi-supervised;generative adversarial network;remote sensing;image segmentation},
  doi={10.1109/ITAIC54216.2022.9836504},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{9021276,
  author={Youssry, Nouran and Khattab, Ahmed},
  booktitle={2019 31st International Conference on Microelectronics (ICM)}, 
  title={Ameliorating IoT and WSNs via Machine Learning}, 
  year={2019},
  volume={},
  number={},
  pages={342-345},
  abstract={The dependence of everyday human life on smart devices is exponentially increasing. Wireless Sensor Networks (WSNs) and Internet of Things (IoT) are becoming of momentous importance every day. However, their challenges also increase with their evolution, and the urge for their improvement and continuous enhancement becomes more important. To adapt to such conditions, machine learning techniques are recently employed within WSNs and IoT systems to leverage their potential. This paper presents a survey of the use of machine learning-based algorithms in WSNs and IoT to deem numerous basic operation and performance constraints.},
  keywords={Wireless sensor networks;Fuzzy logic;Machine learning;Routing;Sensors;Artificial neural networks;Classification algorithms;Wireless Sensor Networks (WSNs);Internet of Things (IoT);Machine Learning;Artificial Intelligence},
  doi={10.1109/ICM48031.2019.9021276},
  ISSN={},
  month={Dec},}@ARTICLE{10606943,
  author={Liu, Jun and Feng, Zhixi and Yang, Shuyuan and Chen, Shuai and Ma, Yue},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Generative Model With Sinkhorn–Knopp Loss for Unsupervised Signal Modulation Clustering}, 
  year={2024},
  volume={20},
  number={11},
  pages={12860-12870},
  abstract={Modulation types clustering (MC) is crucial for adaptive high-frequency communication between devices in the Industrial Internet of Things. The strength of MC resides in its self-supervised framework, enabling it to extract modulation features efficiently without any manual labeling. However, the misalignment of proxy tasks and erroneous pseudolabeling constrain the performance of prevalent MC feature extraction techniques that utilize time series signals. In this article, we compare the saliency maps on time–frequency image (TFI) with that on time series signal, highlighting the consistency of TFI reconstruction with modulation feature extraction. Subsequently, in order to address the sensitivity of K-means to outliers, Sinkhorn–Knopp labeling (SKLb) is proposed to balance the scale of clusters and neighboring distances. Moreover, in consideration of the potential instability of the SKLb iteration result in backpropagation, the Sinkhorn–Knopp loss is proposed to ensure stable training of the model. Finally, two models, SK-IDC and SK-STDC, were tested on four datasets. Experimental results on these datasets present that our approach outperforms original signal representation and prevalent deep clustering methods, achieving State-of-the-Art performance.},
  keywords={Modulation;Feature extraction;Task analysis;Image reconstruction;Training;Labeling;Adaptation models;Generative neural network;Industrial Internet of Things (IIoT);Sinkhorn–Knopp (SK) algorithm;unsupervised modulation clustering},
  doi={10.1109/TII.2024.3424583},
  ISSN={1941-0050},
  month={Nov},}@INPROCEEDINGS{9544120,
  author={Han, Yunfei and Wang, Yi and Ma, Yupeng},
  booktitle={2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)}, 
  title={Generative Difference Image for Blind Image Quality Assessment}, 
  year={2021},
  volume={},
  number={},
  pages={108-115},
  abstract={Image quality usually refers to the degree of error of the distorted image relative to the reference image in the human visual perception system. Image quality assessment is to score the image quality objectively. No-reference image quality assessment is limited to distorted image information, which is more challenging in the field of computer vision. In this paper, we proposed an approach based on difference image generation to address this problem. First, by removing the up-sampling layer and batch normalization layer in the Super-Resolution Generative Adversarial Network (SRGAN) to build a difference image generation model, and applying the content loss function to optimize the model. Then, the regression network is constructed based on the convolutional neural network (CNN). The regression network contains 4 convolutional layers and 2 fully connected layers and learns the correlation between the generated difference image and the image quality score to predict the distorted image quality. Finally, comparative experiments were evaluated on three public datasets. Compared with the previous state-of-the-art methods, our method obtains similar results on the LIVE dataset and achieves significant improvement on the TID2013 and CSIQ datasets. The results demonstrate that our proposed approach achieves state-of-the-art image quality prediction.},
  keywords={Image quality;Correlation;Image synthesis;Superresolution;Generative adversarial networks;Distortion;Generators;image quality assessment;generative adversarial networks;difference image generation},
  doi={10.1109/ICCEAI52939.2021.00021},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10920678,
  author={Chang, Herng-Hua and Chen, Kuan-Yin},
  booktitle={2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={A Generative Adversarial Network with Attention Modules for Underwater Image Enhancement}, 
  year={2025},
  volume={},
  number={},
  pages={0914-0919},
  abstract={Underwater environments often alter the appearance of objects, which makes underwater image enhancement a challenging problem due to multiple distortion effects. Degradation in image information is primarily caused by the consequence of light scattering, wavelength-dependent color attenuation, and water turbulence. Deep learning has become a popular alternative to traditional methods in recent years. This study focuses on applying a generative adversarial network to enhance underwater images. Our model is constructed with a combination of real and synthetic underwater images as the training dataset and developed based on U-Net, convolution block attention modules, and pyramid pooling. A wide variety of underwater images with various scenes and color shifts were utilized to evaluate the proposed image enhancement model. Experimental results suggested that our proposed network outperformed many existing methods in terms of stable color shift correction and de-scattering capability, which demonstrated its potential in many underwater image enhancement applications.},
  keywords={Training;Degradation;Visualization;Image color analysis;Convolution;Light scattering;Imaging;Generative adversarial networks;Distortion;Image enhancement;generative a dversarial network;underwater image;image enhancement;convolution block attention module;pyramid pooling},
  doi={10.1109/ICAIIC64266.2025.10920678},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10574637,
  author={Pawar, Priyanka Pramod and Kumar, Deepak and Ananthan, Bhuvanesh and Christopher, Sj.Ben and Surya, R.},
  booktitle={2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)}, 
  title={An Advanced Wasserstein-Enabled Generative Adversarial Network Enabled Attack Detection for Blockchain-Assisted Intelligent Transportation System}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In day-to-day life, intelligent transport system (ITS) is playing an indispensable role in enhancing mobility and minimizing the carbon track. The security and enduring maintenance of the ITS from cyberattacks are essential. To achieve highly enhanced protection over attacks, a new blockchain (BC) technology with a deep learning (DL) model is introduced to secure the ITS system. Initially, the signature is created and verified by BC blocks, and hash values are chained to resist various attacks that try to falsify sensitive data along with the associated transactions. Moreover, the Advanced Wasserstein Generative Adversarial Network (Advanced-WGAN) technique is introduced to detect various distributed attacks that pause the immediate and confidential data of the shareholders. The simulation progression is carried out on the Python platform and the openly accessible CICDDoS2019 database is considered for the training process. The outcomes prove that the developed method attains an accuracy of 99%, F-score (FS) of 98.5%, precision of 99%, and recall of 99.2%. The experimental results show that the developed approach is highly secure against vulnerable cyberattacks.},
  keywords={Training;Accuracy;Smart contracts;Resists;Generative adversarial networks;Maintenance;Security;Intelligent Transport System;Blockchain;Advanced Wasserstein Generative Adversarial Network;Smart Contract;Attack Classification;Multilabel;secure data transactions},
  doi={10.1109/AIIoT58432.2024.10574637},
  ISSN={},
  month={May},}@INPROCEEDINGS{11166453,
  author={Hogan, Benjamin and Bourke, Brian and Ghadimi, Pezhman},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={A Proof-of-Concept Study Using Generative AI for Structured PGHD Extraction in Remote Patient Monitoring and Mobile Health}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a Generative AI approach to streamline patient self-reporting in Remote Patient Monitoring (RPM) and mHealth. Patients submit free-text or spoken health updates, which are automatically converted into structured data. To ensure privacy, synthetic narratives were generated using WizardLM-2. Eight LLMs were evaluated for data extraction accuracy. Claude 3.5 Sonnet, GPT-4o, and DeepSeek Chat outperformed others, showing promise for real-world use. This approach could reduce patient burden, improve data completeness, and enhance engagement. This work serves as a proof-of-concept to identify high-performing models for structured PGHD extraction, laying the groundwork for future validation on real-world patient narratives and eventual clinical integration.},
  keywords={Privacy;Patient monitoring;Accuracy;Costs;Generative AI;Data models;Robustness;Data mining;Testing;Synthetic data;Remote Patient Monitoring;mHealth;Generative AI;Patient-Generated Health Data;Synthetic Data;NLP;LLMs},
  doi={10.1109/ACDSA65407.2025.11166453},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10710874,
  author={Zeer, Ahmed and Dogan, Eren and Erdem, Yusuf and İnce, Elif and Shbib, Osama and Uzun, M. Egemen and Uz, Atahan and Yuce, M. Kaan and Kesgin, H. Toprak and Amasyali, M. Fatih},
  booktitle={2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={Cosmos-LLaVA: Chatting with the Visual}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In this study, a Turkish visual instruction model was developed and various model architectures and dataset combinations were analysed to improve the performance of this model. The Cosmos-LLaVA model, which is built by combining different large language models and image coders, is designed to overcome the deficiencies in the Turkish language. In the experiments, the effects of fine-tuning with various datasets on the model performance are analysed in detail. The results show that model architecture and dataset selection have a significant impact on performance.},
  keywords={Analytical models;Visualization;Large language models;Data processing;Data models;Question answering (information retrieval);artificial intelligence;natural language processing;multimodality;visual question answering;large language models;generative models;human evaluation;LLM-as-a-judge},
  doi={10.1109/IDAP64064.2024.10710874},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10331981,
  author={Wang, Ruijie and Feng, Peng and Qi, Wei and He, Peng and Liu, Yanan},
  booktitle={2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Research on Underwater Image Restoration Algorithm Based on Generative Adversarial Network}, 
  year={2023},
  volume={},
  number={},
  pages={687-695},
  abstract={Underwater optical imaging is prone to image distortion, blurring and other problems arising from thermal disturbance. To tackle down the above-mentioned problems, underwater images affected by thermal disturbance are analyzed in this study from two perspectives (i.e., image gray histogram distribution and similarity assessment). Subsequently, a deep learning-based underwater thermal disturbance image restoration algorithm is developed based on generative adversarial networks. The U-Net structure is employed in the generator network, and MultiRes blocks are adopted in the contracting and expanding paths for extracting image feature information on different scales maximally. Moreover, a Bottleneck Attention Module (BAM) is introduced in the contracting path to place a full focus on the distorted image information. High-precision image restoration is achieved using the Wasserstein adversarial loss function in the adversarial process between the generator and discriminator networks. To validate the algorithm, an underwater thermal imaging experimental platform is independently built, and a dataset is established. As indicated by the experimental results, the proposed algorithm is capable of effectively restoring images while preserving key edge features, such that a Structural Similarity (SSIM) index of 0.7749 and a Peak Signal to Noise Ratio (PSNR) index of 21.28 are generated. This study can lay a solid foundation for further processing in underwater image restoration.},
  keywords={Training;Histograms;PSNR;Image edge detection;Feature extraction;Generative adversarial networks;Distortion;Thermal disturbance;Underwater image restoration;Generative adversarial networks;Attention mechanism;MultiRes block;BAM},
  doi={10.1109/PRAI59366.2023.10331981},
  ISSN={},
  month={Aug},}@ARTICLE{10879482,
  author={Pu, Qiaolin and Yu, Wenjun and Lan, Xin and Zhou, Mu and Yang, Xiaolong},
  journal={IEEE Internet of Things Journal}, 
  title={A Novel RIS-Aided Indoor Localization in Single Access Point Scenarios via Generative AI}, 
  year={2025},
  volume={12},
  number={10},
  pages={13500-13510},
  abstract={With the continuous development of 6G communication technology and artificial intelligence (AI), reconfigurable intelligent surface (RIS) technology and generative AI (GAI) have received widespread attention. Hence, combining these two techniques to solve the problem of nonlocalizability in single access point (AP) scenarios is promising. This article proposes a novel RIS-aided localization scheme via generative artificial intelligence (GAI). Specifically, first, considering the presence of a certain amount of noise and the low discriminative features of the original collected received signal strength (RSS) brought by integrated reflective elements of RIS, we construct a generative adversarial network (GAN) model named variational autoencoder-convolutional neural network (VAE-CNN). It can effectively perform noise reduction in the data preprocessing stage to reduce unnecessary redundant information, and extract more discriminative features through convolutional networks to improve the differentiation between data. Second, the target’s location in the spatial domain is formulated as a sparse vector, and then a sparse recovery model is introduced to solve the location estimation problem in RIS-aided localization scenarios. Finally, considering the case that RIS has multiple reflective elements, which will lead to a high dimension of the measurement matrix in the sparse recovery model, we further apply the semi-tensor product (STP) sparse recovery theory on the model to reduce the storage space and high time consumption. Experimental results show that our proposed methods outperform the traditional approaches and reduce the computational complexity simultaneously.},
  keywords={Location awareness;Reconfigurable intelligent surfaces;Accuracy;Noise;Data models;Noise reduction;Internet of Things;Fingerprint recognition;Sparse matrices;Feature extraction;CNN;reconfigurable intelligent surface (RIS);reflective element;semi-tensor product (STP);sparse recovery;variational autoencoder (VAE)},
  doi={10.1109/JIOT.2025.3540854},
  ISSN={2327-4662},
  month={May},}@ARTICLE{10879083,
  author={Zhang, Kexin and Li, Lingling and Jiao, Licheng and Liu, Xu and Ma, Wenping and Liu, Fang and Yang, Shuyuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={CSCT: Channel–Spatial Coherent Transformer for Remote Sensing Image Super-Resolution}, 
  year={2025},
  volume={63},
  number={},
  pages={1-14},
  abstract={Remote sensing image super-resolution (RSISR) techniques are crucial in practice as an economical approach to enhancing the resolution of remote sensing images (RSIs). The scale of structural information and the richness of texture details in RSIs far exceed those in natural images. Therefore, accurately restoring and preserving edge and detail information are a critical challenge in the super-resolution (SR) process. Currently, convolutional neural network (CNN)-based methods primarily rely on local feature extraction, which fails to effectively capture and integrate global contextual information. Generative adversarial network (GAN)-based methods, while improving the visual quality, often suffer from artifacts and training instability, adversely affecting image quality. Moreover, these approaches struggle to accurately represent high-frequency features, leading to blurriness or distortion when reconstructing fine details and edges. To address these limitations, we introduce the channel–spatial coherent transformer (CSCT). The core of CSCT includes the channel–spatial coherent attention (CSCA) and the frequency-gated feed-forward network (FGFN), which work synergistically to enhance edge and detail preservation while significantly improving overall image clarity. CSCA efficiently aggregates channel and spatial information, while FGFN adaptively adjusts frequency information to enhance high-frequency details and suppress low-frequency noise. Moreover, this article leverages advanced data augmentation methods that markedly boost RSISR performance, offering new avenues for further exploration. The empirical analysis across several remote sensing SR benchmark datasets reveals that our approach excels in detail restoration, effectively reduces artifacts and noise, and significantly enhances the quality of SR images.},
  keywords={Superresolution;Remote sensing;Image edge detection;Transformers;Frequency-domain analysis;Spatial resolution;Sensors;Image restoration;Attention mechanisms;Training;Channel–spatial coherent attention (CSCA);frequency learnable filter;super resolution},
  doi={10.1109/TGRS.2025.3540260},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10561638,
  author={Li, Wenzheng and Song, Yuxuan and Wang, Shouyuan},
  booktitle={2024 IEEE 14th International Conference on Electronics Information and Emergency Communication (ICEIEC)}, 
  title={Intelligent Computing、Computational Power、 Computational Power Networks and Technology Ecosystems}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Entering 2022, AIGC, represented by foundation model such as ChatGPT, GPT4, Sora, and GPT-4o, is breaking through the wall rapidly. Generative artificial intelligence foundation model technology is rapidly iterating and continuously evolving, becoming a revolutionary tool for content generation, knowledge production, and human-computer interaction. With the increasing number of parameters in foundation model and the increasing complexity of deep learning algorithms, the demand for computing power in foundation model is further increasing. Massive data and large-scale parameters result in extremely large computational loads, limited storage on a single computer server, and limited computer capabilities. Training a foundation model with billions of parameters requires tens of thousands of GPU cards for synchronous computation, and high-performance computing power networks have become the main method and means to meet the demand for large computing power.This paper summarizes the development process of intelligent computing, explores the demand for computing power under the background of large models, and analyzes computing power, computing power networks, and technology ecology based on this, and analyzes their related technologies.},
  keywords={Training;Analytical models;Generative AI;Biological system modeling;Computational modeling;High performance computing;Graphics processing units;Computational Power;Computational Networks;Artificial Intelligence;Large Language Model},
  doi={10.1109/ICEIEC61773.2024.10561638},
  ISSN={2377-844X},
  month={May},}@INPROCEEDINGS{10254693,
  author={Moon, Seungjae and Kim, Junsoo and Kim, Jung-Hoon and Cha, Junseo and Choi, Gyubin and Hong, Seongmin and Kim, Joo-Young},
  booktitle={2023 IEEE Hot Chips 35 Symposium (HCS)}, 
  title={HyperAccel Latency Processing Unit (LPUTM) Accelerating Hyperscale Models for Generative AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  abstract={},
  keywords={Artificial intelligence},
  doi={10.1109/HCS59251.2023.10254693},
  ISSN={2573-2048},
  month={Aug},}@INPROCEEDINGS{10885312,
  author={Xu, Junhao and He, Dongbiao and Zhang, Chen and Zhou, Xu and Ming, Zhongxing and Cui, Laizhong},
  booktitle={2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA)}, 
  title={MonkeyGPT: Generative AI in Network Anomaly Detection of Video Conference Applications}, 
  year={2024},
  volume={},
  number={},
  pages={1273-1280},
  abstract={The rapid advancement of generative artificial intelligence (GAI) has led to the creation of transformative applications such as ChatGPT, which significantly boosts text processing efficiency and diversifies audio, image, and video content. Beyond digital content creation, GAI’s capability to analyze complex data distributions holds immense potential for next-generation networks and communications, especially given the swift rise of video conferencing applications (VCAs). This paper presents a dynamic, real-time method for detecting anomalous network links in video conferencing applications. The proposed tool, MonkeyGPT, generates tracing representations of network activity and trains a large language model from scratch to serve as a detection system based on network traffic data. Unlike traditional methods, MonkeyGPT provides an unrestricted search space and does not rely on predefined rules or patterns, enabling it to detect a wider range of anomalies. We demonstrate the effectiveness of MonkeyGPT as an anomaly detection tool in real-world VCAs. The results indicate that the model possesses strong detection capabilities, achieving an accuracy rate of over 97%. It is applicable to various platforms, including Zoom, Microsoft Teams, Tencent Meeting, and Feishu, showcasing its robust adaptability.},
  keywords={Accuracy;Generative AI;Large language models;Telecommunication traffic;Streaming media;Real-time systems;Data models;Anomaly detection;Videoconferences;Text processing;Generative AI;Network anomaly detection;Video Conference Application},
  doi={10.1109/ISPA63168.2024.00171},
  ISSN={2158-9208},
  month={Oct},}@INPROCEEDINGS{9026252,
  author={Ono, Hyuga and Suzuki, Satoshi},
  booktitle={2020 IEEE/SICE International Symposium on System Integration (SII)}, 
  title={Data Augmentation for GrossMotor-ActivityRecognition Using DCGAN}, 
  year={2020},
  volume={},
  number={},
  pages={440-443},
  abstract={Gross Motor Activity Recognition(GM-AR) AI has been studied to develop assessment AI that automatically evaluates child's GM skill. However, collecting and creating datasets takes a lot of effort and time. Therefore, the goal is to automatically generate a GM-AR data set. Since the data set used in this paper is a special time series image, DCGAN was used for data generation. Therefore, this paper aims to generate time-series images by GM-GAN using CNN for GMAR as Discriminator. Newly designed GM-AR could achieve 68.8 % accuracy. The generated image of GM-GAN could be converted into a skeleton with both hands and feet, and the skeleton was confirmed to work with 40 frames. Therefore, GM-GAN was able to generate PK time-series images.},
  keywords={Skeleton;Convolution;Generators;Generative adversarial networks;Gallium nitride;Artificial intelligence;Videos},
  doi={10.1109/SII46433.2020.9026252},
  ISSN={2474-2325},
  month={Jan},}
