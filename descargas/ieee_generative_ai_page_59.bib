@INPROCEEDINGS{8939828,
  author={Lee, Changha and Jeon, Minsu and Yang, Eunju and Kim, Seong-Hwan and Youn, Chan-Hyun},
  booktitle={2019 International Conference on Information and Communication Technology Convergence (ICTC)}, 
  title={Imbalanced-Free Memory Selection Scheme Based Continual Learning by Using K-means Clustering}, 
  year={2019},
  volume={},
  number={},
  pages={910-915},
  abstract={Recently, continual learning is a new training strategy based on replaying memory and allowing beneficial effects to previous tasks. To preserve old information, current continual learning scheme accumulates observed examples into limited buffer or repeatedly trains generative model. This idea of learning scheme is effective to reduce catastrophic forgetting which is deterioration in overall performance when training sequentially. However, there still exists the problem of imbalanced data distribution in limited buffer and it is hard to apply on real-time system due to too long time to train both generative model and classification model. In this paper, we propose sample selection algorithm based on iterative k-means algorithm to improve the memory based continual learning. This approach selects examples to store into a buffer in a unsupervised manner using k-means cluster information. Our experiments on a variant of MNIST and CIFAR-100 datasets show the effects on classification accuracy performance when compared to the state-of-the-art.},
  keywords={Task analysis;Clustering algorithms;Training;Multiplexing;Data models;Neural networks;Nickel;Continual Learning;Online Learning;Catastrophic Forgetting;Artificial Intelligence},
  doi={10.1109/ICTC46691.2019.8939828},
  ISSN={2162-1233},
  month={Oct},}@INPROCEEDINGS{11115733,
  author={Joshi, Vivek and Patel, Sanjay},
  booktitle={2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={GAN Based Unsupervised Learning for Rapid Fake News Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Rapid Increase of fake news across social media platforms has come as a significant problem to various societies around the world. This research aims to conduct a detailed examination on Generative Adversarial Networks as applied to fake news detection, including its effectiveness and advantages over other machine learning approaches. We deploy a novel algorithm using GAN and calculate bilingual evaluation understudy (BLEU) score. The BLEU score analysis of proposed methodology peaks at 0.58 which show significant improvement in efficacy of fake news detection using text-based methodology.},
  keywords={Machine learning algorithms;Social networking (online);Computational modeling;Machine learning;Generative adversarial networks;Generators;Fake news;Unsupervised learning;Long short term memory;Generative Adversarial Network (GAN)AN;NLP;SeqGAN;LaTeXtGAN;Generators;Language Modeling;LSTM},
  doi={10.23919/INDIACom66777.2025.11115733},
  ISSN={},
  month={April},}@ARTICLE{10638096,
  author={Ling, XuDong and Li, ChaoRong and Zhu, LiHong and Qin, FengQing and Zhu, Ping and Huang, Yuanyuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Spacetime Separable Latent Diffusion Model With Intensity Structure Information for Precipitation Nowcasting}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={The growing volume of meteorological data and advancements in computing performance have made the application of deep learning technology in short-term rainfall prediction crucial. However, existing learning approaches struggle to accurately predict detailed spatial location information, particularly obvious in predicting extreme rainfall events, leading to inadequate prediction accuracy and subpar performance in meteorological assessment indicators, limiting the effectiveness and applicability of deep learning models in rainfall prediction. To address these challenges, we propose a spacetime separable latent diffusion model with intensity structure information (SSLDM-ISI) to capture spatial and temporal information more efficiently. SSLDM-ISI incorporates two key strategies to solve the spatiotemporal information issue. First, a spatiotemporal conversion block (STC Block) within the backbone network effectively extracts and integrates spatiotemporal information. Second, our proposed latent space coding technique based on rainfall intensity structural information enhances the information representation ability of extreme rainfall. In addition, an examination of the impact of various conditions is conducted on the prediction results to enhance the model’s prediction accuracy and stability. Through comparative analysis of meteorological evaluation and image quality evaluation indicators on two datasets, our proposed approach outperforms existing advanced technologies in short-term rainfall prediction, achieving current state-of-the-art results. Our project is open source and available on GitHub at: https://github.com/ybu-lxd/SISLDM-ISI},
  keywords={Rain;Predictive models;Computational modeling;Spatiotemporal phenomena;Accuracy;Diffusion models;Weather forecasting;Diffusion model (DM);generative adversarial networks (GANs);nowcasting;precipitation},
  doi={10.1109/TGRS.2024.3444789},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10598794,
  author={Zou, Yuming and Piao, Xinglin and Zhang, Yong and Hu, Yongli},
  booktitle={2024 39th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={Spatial-Temporal Dual Graph Neural Network for Pedestrian Trajectory Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1205-1210},
  abstract={Pedestrian trajectory prediction has become a crucial task in the field of autonomous driving. To improve the accuracy of pedestrian trajectory prediction, researchers primarily concentrate on tackling two key challenges. One is to extract the intricate interactions between pedestrians, and the other is to simulate the diverse decision-making intentions displayed by pedestrians. However, most existing methods utilize the distance attribute to build the relationship of pedestrians only, but ignore other features such as the steering. Besides, some generation theory based methods would lead to substantial deviations in the generated trajectory distribution since they always refine the variational likelihood lower bound of observed data. In this paper, we adopt Graph theory and propose a Spatial-Temporal Dual Graph neural network for pedestrian trajectory prediction. In the proposed method, we construct a pedestrian graph structure by utilizing pedestrian distance and steering features to extract more comprehensive interaction information. Additionally, we introduces the flow-based Glow-PN module to predict multi-modal trajectories of pedestrians. Experimental results on two public benchmark datasets show that our model achieves superior prediction performance and operates effectively in diverse scenarios.},
  keywords={Pedestrians;Decision making;Predictive models;Benchmark testing;Feature extraction;Graph neural networks;Trajectory;graph neural network;trajectory prediction;generative flow;deep neural network},
  doi={10.1109/YAC63405.2024.10598794},
  ISSN={2837-8601},
  month={June},}@ARTICLE{10816165,
  author={Tian, Yu and Zhang, Kunbo and Huang, Yalin and Wang, Leyuan and Liu, Yue and Sun, Zhenan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Cross-Optical Property Image Translation for Face Anti-Spoofing: From Visible to Polarization}, 
  year={2025},
  volume={20},
  number={},
  pages={1192-1205},
  abstract={Despite the development of spectral sensors and spectral data-driven learning methods which have led to significant advances in face anti-spoofing (FAS), the singular dimensionality of spectral information often results in poor robustness and weak generalization. Polarization, another fundamental property of light, can reveal intrinsic differences between genuine and fake faces with advantaged performance in precision, robustness, and generalizability. In this paper, we propose a facial image translation method from visible light (VIS) to polarization (VPT), capable of generating valuable polarimetric optical characteristics for facial presentation attack detection using VIS spectrum information input only. Specifically, the VPT method adopts a multi-stream network structure, comprising a main network and two branch networks, to translate VIS images into degree of polarization (DoP) images and Stokes polarization parameters  ${S}_{1}$  and  ${S}_{2}$ . To further improve image translation quality, we introduce a frequency-domain consistency loss as a complement to the existing spatial losses to narrow the gap in the frequency domain. The physical mapping relations for the DoP and Stokes parameters are employed, and the Stokes loss is designed to ensure that the generated polarization modalities conform to objective physical laws. Extensive experiments on the CASIA-Polar and CASIA-SURF datasets demonstrate the superiority of VPT over other baseline methods in terms of polarization image quality and its remarkable performance in the FAS task. This work leverages the inherent physical advantages of polarization information in material discrimination tasks while addressing hardware limitations in polarization image collection, proposing a novel solution for face recognition system security control.},
  keywords={Face recognition;Translation;Feature extraction;Vibrations;Faces;Optical imaging;Frequency-domain analysis;Mathematical models;Optical reflection;Lighting;Cross-property translation;polarization;face anti-spoofing;generative model},
  doi={10.1109/TIFS.2024.3521323},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{11085916,
  author={Wei, Xinjian and Liu, Mingyang and Lu, Haotian and Xu, Jing},
  booktitle={2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL)}, 
  title={DEMFusion: A Diffusion-Enhanced Medical Image Fusion Network}, 
  year={2025},
  volume={},
  number={},
  pages={552-561},
  abstract={Multimodal medical image fusion technology integrates complementary information from diverse medical imaging modalities (e.g., CT, MRI) to overcome the physical limitations of single-modality imaging, enabling precise correlation and fusion of structural features with functional metabolic information in clinical practice. Existing multimodal medical image fusion methods exhibit limitations in preserving the unique information inherent to each modality, resulting in suboptimal fusion accuracy. To address these challenges, we proposes a diffusion-enhanced medical image fusion network (DEMFusion). The proposed directly learns the mapping relationships between input and fused outcomes while leveraging modality-specific information guidance to significantly enhance fusion accuracy. Within the proposed, one diffusion-enhanced module (TDModule) is designed to generate corresponding pseudo-medical images from real medical modalities, from which modality-unique information are extracted using both pseudomedical images and real medical images. Future more, the proposed method integrates surface-layer constraints and deeplayer constraints to objectively define unique information. The chromatic constraint is further designed to enhance chrominance retention in fused outcomes. Qualitative and quantitative experiments demonstrate that the proposed method effectively preserves modality-specific information while achieving superior fusion performance on various datasets.},
  keywords={Accuracy;Computed tomography;Magnetic resonance imaging;Surface morphology;Feature extraction;Surface texture;Data mining;Image fusion;Surface treatment;Biomedical imaging;multimodal medical image fusion;diffusion generative model;medical image analysis;saliency measurement;richness measurement},
  doi={10.1109/CVIDL65390.2025.11085916},
  ISSN={},
  month={May},}@INPROCEEDINGS{11004652,
  author={Sharma, Ruchi and Sharma, Tanishka and Arora, Shaveta and Verma, Seema},
  booktitle={2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)}, 
  title={A Comparative Analysis of Deep Neural Network Models for Deepfake Image Detection Generated using AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Deepfake consists of two words - “deep” and “fake,” where deep refers to the deep neural network and fake indicates the manipulation of data. The gradual increase in usage of AI-generated images poses serious concerns and threats as these kinds of images challenge the authenticity of images and videos. This manipulation of pictures should be addressed to reduce their misuse in spreading misinformation, identity theft, and other malicious activities. This study mainly focuses on developing a model that can help detect the use of deepfake images. We used convolutional neural networks (CNNs) together with advanced architectures ResNet50 and XceptionNet to classify whether an image is real or fake. We trained our model on two different datasets and compared their accuracies. These datasets contained authentic as well as deepfake images which are used for the model's training and testing. Comparing the three models, it is found that the XceptionNet architecture performed classification with an accuracy of 84%. To enhance detection accuracy, preprocessing techniques are also applied to extract meaningful features and reduce noise in the data.},
  keywords={Training;Deepfakes;Ethics;Analytical models;Accuracy;Identity theft;Feature extraction;Convolutional neural networks;Artificial intelligence;Residual neural networks;Deepfake;Stable Diffusion Model;CNN;ResNet50;XceptionNet},
  doi={10.1109/ITIKD63574.2025.11004652},
  ISSN={},
  month={April},}@INPROCEEDINGS{10887923,
  author={Zou, Mian and Yu, Baosheng and Zhan, Yibing and Ma, Kede},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Self-Supervised Learning for Detecting AI-Generated Faces as Anomalies}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The detection of AI-generated faces is commonly approached as a binary classification task. Nevertheless, the resulting detectors frequently struggle to adapt to novel AI face generators, which evolve rapidly. In this paper, we describe an anomaly detection method for AI-generated faces by leveraging self-supervised learning of camera-intrinsic and face-specific features purely from photographic face images. The success of our method lies in designing a pretext task that trains a feature extractor to rank four ordinal exchangeable image file format (EXIF) tags and classify artificially manipulated face images. Subsequently, we model the learned feature distribution of photographic face images using a Gaussian mixture model. Faces with low likelihoods are flagged as AI-generated. Both quantitative and qualitative experiments validate the effectiveness of our method. Our code is available at https://github.com/MZMMSEC/AIGFD_EXIF.git.},
  keywords={Codes;Self-supervised learning;Detectors;Signal processing;Feature extraction;Generators;Speech processing;Artificial intelligence;Faces;Gaussian mixture model;AI-generated face detection;anomaly detection;self-supervised learning},
  doi={10.1109/ICASSP49660.2025.10887923},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10544170,
  author={K K, Kumar and Gupta B, Kruthik and Kallur, Mahesh R and Kulkarni, Akash and Mallibhat, Kaushik},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={Digitalising Handwritten Hindi Documents Using Learning Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The character classification for handwritten documents has many challenges since each individual write with different font, has a different style of writing and writes with fontsize. This unconventional style poses a challenge to traditional techniques like Optical Character Recognition (OCR) and the solutions exists at the level of recognizing the character and a word. Through this project, author extends it for entire document digitalization thus helping to preserve the important documents written in Hindi. The authors used Convolution Neural Network models to address the challenge and were able to achieve 99.19% accuracy in classifying the characters.},
  keywords={Deep learning;Convolution;Optical character recognition;Neural networks;Learning (artificial intelligence);Writing;Feature extraction;Devanagari Script;Shabd Dataset;CNN;image processing;Feature Extraction},
  doi={10.1109/I2CT61223.2024.10544170},
  ISSN={},
  month={April},}@ARTICLE{10242064,
  author={Toofanee, Mohammud Shaad Ally and Dowlut, Sabeena and Hamroun, Mohamed and Tamine, Karim and Petit, Vincent and Duong, Anh Kiet and Sauveron, Damien},
  journal={IEEE Access}, 
  title={DFU-SIAM a Novel Diabetic Foot Ulcer Classification With Deep Learning}, 
  year={2023},
  volume={11},
  number={},
  pages={98315-98332},
  abstract={Diabetes affects roughly 537 million people in the world, and it is predicted to reach 783 million by 2045. Diabetic Foot Ulcer (DFU) is a major issue with diabetes that may lead to lower limb amputation. The rapid evolution of DFU demands immediate intervention to prevent the terrible consequences of amputation and related complications.This research introduces a novel approach utilizing deep neural networks and machine learning for the accurate classification of diabetic foot ulcer (DFU) images. The proposed method harnesses the cutting-edge capabilities of Convolutional Neural Networks (CNN) and Vision Image Transformers (ViT) within a Siamese Neural Network (SNN) Architecture. By employing similarity learning, the model efficiently categorizes DFU images into four distinct classes: None, Infection, Ischemia, or Both. The training process involves the use of the DFU2021 dataset, with all ethical clearances duly obtained. Notably, the model exhibits remarkable performance on both the validation and test data, indicating a significant breakthrough in the field of DFU disease image classification. The potential of this innovative model extends beyond classification; it holds promise as an integral component of a comprehensive detection tool and longitudinal treatment protocol validation for DFU disease.},
  keywords={Diabetes;Convolutional neural networks;Artificial intelligence;Neurons;Medical services;Feature extraction;Ethics;Classification algorithms;DFU;deep learning;CNN;vision transformers;Siamese network;similarity detection;DFU classification},
  doi={10.1109/ACCESS.2023.3312531},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9701723,
  author={Zhang, Fenglu and Yan, Chunman},
  booktitle={2021 17th International Conference on Computational Intelligence and Security (CIS)}, 
  title={Development and Application of Facial Makeup Transfer}, 
  year={2021},
  volume={},
  number={},
  pages={304-308},
  abstract={Facial makeup transfer is a technology that uses image processing technology to transfer the known makeup to the target face. The makeup transfer based on image editing has widespread applications in entertainment, social contact, identity authentication, and other fields. Firstly, this article briefly introduced the basic principles and practical application of makeup transfer, summarized the representative algorithms and models of facial makeup transfer in recent years, and then analyzed qualitative and quantitative evaluation criteria. Finally, the current situation in the technology at this stage are discussed, and look forward the future development direction of facial makeup transfer.},
  keywords={Training;Image quality;Databases;Image processing;Entertainment industry;Learning (artificial intelligence);Security;facial makeup transfer;face image editing;deep learning},
  doi={10.1109/CIS54983.2021.00070},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10847386,
  author={Tirupathi, P. and Ramana, N. and Sathwika, Gade},
  booktitle={2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Mood Based Music Composition with Transformers and Fuzzy Logic}, 
  year={2024},
  volume={},
  number={},
  pages={1225-1229},
  abstract={Music is a Universal Language that influences human emotions, mood and even physiological responses, making it a powerful tool in media, therapy, and entertainment. The increasing interest in AI-generated music that targets specific emotions faces challenges due to current technology's inability to control emotional expression effectively. As a result, AI struggles to create music that truly resonates with listeners on an emotional level. This research addresses this gap by introducing a method that combines transformer models with fuzzy logic to enhance AI's emotional expressiveness in music composition. This approach involves training a transformer model on mood-labeled music to capture correlations between musical features and emotions. Integrating fuzzy logic enables fine-tuning of elements like tempo and dynamics, allowing for precise adjustments that enhance the models ability for composing music that exactly relates the given mood. The result is a system that generates music with given mood. The outcomes show that this model successfully produces emotionally resonant music, refining mood accuracy with each iteration. This project combines automated music generation with mood-based composition allowing for personalized, emotionally attuned music experiences. This project demonstrates how AI can support musicians and creators by helping produce music that matches specific moods, that relates to individual preferences. Beyond art, it makes personalized music possible in areas like entertainment, and therapy, offering people music that fits their emotional needs.},
  keywords={Fuzzy logic;Training;Translation;Mood;Refining;Music;Medical treatment;Entertainment industry;Transformers;Artificial intelligence;AI music;mood music;transformers;fuzzy logic;emotion control;music generation},
  doi={10.1109/CICN63059.2024.10847386},
  ISSN={2472-7555},
  month={Dec},}@INPROCEEDINGS{10825288,
  author={Choi, Jiho and Byun, Sung-Woo and Chae, Najeong and Lim, Ji Hoon and Lim, Taehoon and Lee, Hye In and Seon Shin, Hwa},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={A study on phenotype prediction using an artificial intelligence-based data augmentation approach}, 
  year={2024},
  volume={},
  number={},
  pages={4679-4684},
  abstract={Global food security is increasingly at risk due to factors like climate change and population growth, necessitating advancements in agricultural technology. Digital breeding, a method centered around genotype-phenotype selection using next-generation sequencing (NGS), offers a solution by enabling the identification of genetic mutations and predicting crop traits with greater speed and precision compared to traditional approaches. This automated breeding process efficiently gathers and analyzes genotype and phenotype data, improving key traits such as growth, yield, and tolerance while minimizing human intervention. Despite advancements in sequencing technologies, challenges remain due to the high cost and impracticality of acquiring extensive genomic datasets. To address these limitations, this study explores data augmentation strategies using deep learning techniques, focusing on their success in other fields like computer vision. Unlike conventional Generative Adversarial Networks (GANs), which face stability issues, we present a novel approach using a stacked convolutional and LSTM architecture. This model leverages SNP position information to capture correlations within genomic regions and introduces a specialized metric to evaluate the quality of augmented data. The effectiveness of the proposed phenotype prediction model is demonstrated through real-world testing on a collection of 192 tomato varieties, highlighting its potential to revolutionize breeding processes and improve agricultural outcomes.},
  keywords={Measurement;Sequential analysis;Phenotypes;Genomics;Predictive models;Data augmentation;Stability analysis;Bioinformatics;Next generation networking;Testing;Digital Breeding;Genomic Selection;Genotype-Phenotype Prediction;Data Augmentation;Deep Learning in Agriculture},
  doi={10.1109/BigData62323.2024.10825288},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{11059049,
  author={Ellahi, Ehsan and R, Krishna Priya and Talha, Muhammad and Hariprabhu, M. and Aragani, Venu Madhav and Tiwari, Mohit},
  booktitle={2024 International Conference on Advances in Computing, Communication and Materials (ICACCM)}, 
  title={Optimizing the Performance of Generative Artificial Intelligence ,Recent Approaches to Engineering Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the ever-growing field of Generative Artificial Intelligence, quick engineering has become a revolutionary method in the field of NLP for big language versions. This technique entails the manipulation of input queries to improve the quality and relation of the textual outputs of these models. New findings point to the fact that through the enhancement of the aspect of prompt engineering it could indeed be a way of significantly boosting LLM performance through a change of the input query format. This paper highlights the practical use of the prompt engineering concept to Tamil-based LLMs with an explicit purpose of developing a well-structured process capable of generating accurate and contextually relevant conversational responses. Thus, with subtle changes to how prompts are offered, this research aims to enhance the efficacy of Tamil LLMs that demand trivial data entry. The research uses the Query Transformation Module (QTM), which is an elaborate method developed to systematically change the input prompts into three forms of inquiries. Therefore, each format is built with objectives and key points in mind to enhance the understanding of the models as well as the results. To measure the effectiveness of this strategy, the QTM was employed with leading Tamil LLMs SKT GPT-2 and Tamil ChatGPT. We employed four different query methods in our experiments: all the initially employed unaltered request and three modified requests according to the QTM. The evaluation was made using Google SSA to fit the evaluation criteria of naturalness and specificity for the sentences generated by the models. Our experimental findings show that overall, there is a reasonable improvement in the performance by an average of 11 percent. Identified that there is an increase of 46% in the quality of generated sentences when using the transformed queries as compared to the unmodified prompts. This improvement also depicts how the QTM has an effectiveness of enhancing the performance of Tamil LLMs, resulting into its usability as a tool in the corresponding field of prompt engineering.},
  keywords={Training;Measurement;Adaptation models;Large language models;Training data;Chatbots;Real-time systems;Internet;Prompt engineering;Usability;AI;LLMS;generative AI;prompt engineering;Tamil AI models;QTM;performance assessment of LLMs;AI chatbots},
  doi={10.1109/ICACCM61117.2024.11059049},
  ISSN={2642-7354},
  month={Nov},}@INPROCEEDINGS{10675599,
  author={Nikhil, Gunda and Yeligatla, Durga Revanth and Chaparala, Tejendra Chowdary and Chalavadi, Vishnu Thejaa and Kaur, Hardarshan and Singh, Vinay Kumar},
  booktitle={2024 Second International Conference on Inventive Computing and Informatics (ICICI)}, 
  title={An Analysis on Conversational AI: The Multimodal Frontier in Chatbot System Advancements}, 
  year={2024},
  volume={},
  number={},
  pages={383-389},
  abstract={This research study analyses the evolution of conversational AI, focusing on the integration of multi-modal systems that incorporate text, audio, and visual data to create more dynamic dialogue systems. By considering the significance of visual information in enhancing Automatic Speech Recognition (ASR), this study explores early developments in the field. This study discusses about the optimization of these models by using techniques such as reinforcement learning and multi-task learning along with the use of data fusion methods to effectively integrate multimodal inputs. Key research gaps are identified, particularly the need for improved emotional intelligence and handling of diverse input modalities. This research study concludes by highlighting the potential of advanced approaches such as adversarial machine learning and transfer learning in addressing current limitations and shaping the future trajectory of conversational AI.},
  keywords={Visualization;Privacy;Natural language generation;Transfer learning;Reinforcement learning;Multitasking;Trajectory;Conversational AI;Machine Learning;Multimodal;Natural Language Understanding (NLU);Natural Language Generation (NLG);Natural Language Processing (NLP);Emotional intelligence;Intent identification;Entity recognition;Deep learning models},
  doi={10.1109/ICICI62254.2024.00069},
  ISSN={},
  month={June},}@INPROCEEDINGS{10895437,
  author={Crumrine, Garrett and Alsmadi, Izzat and Guerrero, Jesus and Munian, Yuvaraj and Al-Abdullah, Muhammad},
  booktitle={2024 4th Intelligent Cybersecurity Conference (ICSC)}, 
  title={Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={39-47},
  abstract={Large language models (LLMs) have achieved groundbreaking advancements in natural language processing (NLP) that hold the promise of revolutionizing the relationship between humans and technology. However, this technological advancement has been joined by the emergence of “Mallas” (a term coined by Lin et al. [4]). These services facilitate the creation of malware, phishing attacks, deceptive websites, and most concerning, exploit code. This paper delves into the proliferation of Mallas by examining the use of various pretrained language models and their efficiency at generating vulnerabilities and exploits when being misused. Leveraging a comprehensive dataset from the Common Vulnerabilities and Exposures (CVE) program, it explores dataset creation, prompt engineering and fine-tuning methodologies needed to generate code and explanatory text related to vulnerabilities identified in the CVE database. Furthermore, this research aims to shed light on the strategies and exploitation techniques of Mallas; ultimately assisting the development of more secure and trustworthy AI applications. The paper concludes by emphasizing the critical need for further research into LLM-related cyber threat intelligence and advocating for the development of enhanced safeguards and ethical guidelines to mitigate the risks associated with these malicious applications of LLMs. We propose a novel Dynamic Ethical Boundary Reinforcement (DEBR) system as a proactive measure against LLM exploitation.},
  keywords={Ethics;Technological innovation;Codes;Protocols;Large language models;Phishing;Natural language processing;Malware;Prompt engineering;Computer security;Computer security;cyber threat intelligence;human computer interaction;natural language processing;question answering(information gathering)},
  doi={10.1109/ICSC63108.2024.10895437},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10872055,
  author={Reddy, Vignesh and R, Sunitha and Anusha, M. and Chaitra, S and Kumar, Abhilasha P},
  booktitle={2024 4th International Conference on Mobile Networks and Wireless Communications (ICMNWC)}, 
  title={Artificial Intelligence Based Intrusion Detection Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Breakthroughs in deep learning (DL) and machine learning (ML) have revolutionized cybersecurity by remarkably improving detection performances and achieving Turing completeness to rapidly changing attack scenario. These developments have made AI-based intrusion detection systems (IDS) one of the most important technologies for networked and Internet of Things (IoT) security. This work presents an in-depth overview of state-of-the-art IDS approaches such as Bi-Directional Long Short-Term Memory (Bi-LSTM) networks, Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs) and ensembles. Particular attention is given to dealing with the specific challenges that arise with integration of IoT devices, whose safety is often in jeopardy due to sophisticated cyber-attacks, as well as with the heterogeneity of modern network structures. Analysis reveals the advantages and disadvantages of each method and demonstrates that the Bi-LSTM, and GANs methods perform better in dynamic, complex data streams, while ensemble methods enhance the robustness and threat detection performance of the statistical classifier. Using these sophisticated AI approaches, IDS can be improved to provide security enhancements, delivering crucial information for securing IoT ecosystem and addressing the threats posed by complex network architectures.},
  keywords={Deep learning;Wireless communication;Training;Turing completeness;Intrusion detection;Robustness;Threat assessment;Data models;Internet of Things;Streams;deep learning;machine learning;iot;network security;GAN;Bi-LSTM;ensemble;intrusion detection system},
  doi={10.1109/ICMNWC63764.2024.10872055},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350998,
  author={Hamadene, Assia and Ouahabi, Abdeldjalil and Hadid, Abdenour},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Deepfakes Signatures Detection in the Handcrafted Features Space}, 
  year={2023},
  volume={},
  number={},
  pages={460-466},
  abstract={In the Handwritten Signature Verification (HSV) literature, several synthetic databases have been developed for data-augmentation purposes, where new specimens and new identities were generated using bio-inspired algorithms, neuromotor synthesizers, Generative Adversarial Networks (GANs) as well as several deep learning methods. These synthetic databases contain synthetic genuine and forgeries specimens which are used to train and build signature verification systems. Researches on generative data assume that synthetic data are as close as possible to real data, this is why, they are either used for training systems when used for data augmentation tasks or are used to fake systems as synthetic attacks. It is worth, however, to point out the existence of a relationship between the handwritten signature authenticity and human behavior and brain. Indeed, a genuine signature is characterised by specific features that are related to the owner’s personality. The fact which makes signature verification and authentication achievable. Handcrafted features had demonstrated a high capacity to capture personal traits for authenticating real static signatures. We, therefore, Propose in this paper, a handcrafted feature based Writer-Independent (WI) signature verification system to detect synthetic writers and signatures through handcrafted features. We also aim to assess how realistic are synthetic signatures as well as their impact on HSV system’s performances. Obtained results using 4000 synthetic writers of GPDS synthetic database show that the proposed handcrafted features have considerable ability to detect synthetic signatures vs. two widely used real individuals signatures databases, namely CEDAR and GPDS-300, which reach 98.67% and 94.05% of successful synthetic detection rates respectively.},
  keywords={Training;Handwriting recognition;Deepfakes;Databases;Synthesizers;Feature extraction;Generative adversarial networks},
  doi={10.1109/ICCVW60793.2023.00052},
  ISSN={2473-9944},
  month={Oct},}@INPROCEEDINGS{10865068,
  author={Zhang, Yuting and Yang, Boyuan},
  booktitle={2024 China Automation Congress (CAC)}, 
  title={BFD-DDPM: Denoising Diffusion Probabilistic Models for Bearing Fault Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={2760-2765},
  abstract={Rotating machinery fault diagnosis is of paramount importance in industrial production. In this field, the proposed generation model breaks through the restriction of machine learning on the completeness of sample set in the process of model training. While Generative Adversarial Networks (GANs) have shown potential in this domain, their susceptibility to training collapse due to stringent parameter selection remains a concern. In addressing these challenges, we propose BFD-DDPM, a novel bearing fault diagnosis grounded in the principles of unsupervised learning via the diffusion model. Utilizing the Denoising Diffusion Probabilistic Models (DDPM) - an enhanced generative model, normal samples of mechanical vibration signals are studied in an unsupervised manner to capture their prevalent distribution in a latent space. Subsequently, equipment fault diagnosis is achieved by locating the nearest generated data of test samples within this space via denoising diffusion, utilizing the Manhattan distance metric. Compared to unsupervised learning-based fault diagnosis methods, this paper demonstrates the advantages of DDPM in terms of accuracy and the robustness of parameter selection.},
  keywords={Fault diagnosis;Training;Vibrations;Accuracy;Noise reduction;Rolling bearings;Diffusion models;Generative adversarial networks;Robustness;Unsupervised learning;diffusion model;intelligent fault diagnosis;reverse denoising;unsupervised learning},
  doi={10.1109/CAC63892.2024.10865068},
  ISSN={2688-0938},
  month={Nov},}@ARTICLE{9658503,
  author={Zhang, Haibo and Sakurai, Kouichi},
  journal={IEEE Access}, 
  title={Conditional Generative Adversarial Network-Based Image Denoising for Defending Against Adversarial Attack}, 
  year={2021},
  volume={9},
  number={},
  pages={169031-169043},
  abstract={Deep learning has become one of the most popular research topics today. Researchers have developed cutting-edge learning algorithms and frameworks around deep learning, applying them to a wide range of fields to solve real-world problems. However, we are more concerned about the security risks associated with deep learning models, such as adversarial attacks, which this article will discuss. Attackers can use the deep learning model to create the conditions for an attack, maliciously manipulating the input images to deceive the classification model and produce false positives. This paper proposes a method of pre-denoising all input images to prevent adversarial attacks by adding a purification layer before the classification model. The method in this paper is proposed based on the basic architecture of Conditional Generative Adversarial Networks. It adds the image perception loss to the original algorithm Pix2pix to achieve more efficient image recovery. Our method can recover noise-attacked images to a level close to the actual image to ensure the correctness of the classification results. Experimental results show that our approach can quickly recover noisy images, and the recovery accuracy is 20.22% higher than the previous state-of-the-art.},
  keywords={Training;Neural networks;Deep learning;Robustness;Data models;Perturbation methods;Noise reduction;Adversarial attack;conditional generative adversarial network;image denoising},
  doi={10.1109/ACCESS.2021.3137637},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10591600,
  author={Mancini, Adriano and Galdelli, Alessandro and Capello, Elisa and Du Mutel de Pierrepont Franzetti, Iris David and Primatesta, Stefano and Fasano, Giancarmine and Opromolla, Roberto and Causa, Flavia and Vitiello, Federica},
  booktitle={2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)}, 
  title={4IPLAY - Improving Intelligent Infrastructure Inspection by Pushing UAS Level of Autonomy in Challenging Environments}, 
  year={2024},
  volume={},
  number={},
  pages={278-283},
  abstract={The 4IPLAY project wants to develop innovative solution to demonstrate in relevant environments the concept of repeatable automated visual inspection of infrastructures as bridges based on autonomous drones. 4IPLAY builds on the know-how experience of a multi-disciplinary research team and is focus on three main pillars: (i) automated acquisition and processing of multi-spectral data using artificial intelligence approaches; (ii) advanced flight control logics which are tailored for the conditions encountered in the proximity of large infrastructures; (iii) distributed multi-drone architectures to support real time cooperative navigation and offline trajectory reconstruction. The analysis of images using artificial intelligence approaches will be one of the pillar, with a focus on the detection of defects also using generative techniques to augments dataset. Two key aspects are analyzed for the control system: the definition of a data-driven model for ceiling and wall effects and a disturbance-rejection control system for an Unmanned Aerial Vehicle. The research on cooperative multi-drone systems includes 1-to-N planning and control, as well as tight integration of cooperative navigation and distributed sensing. The paper introduces the main research areas of the project and provides an overview of current progress.},
  keywords={Bridges;Visualization;Navigation;Process control;Inspection;Control systems;Logic;Unmanned Aircraft Systems;Autonomy;Vision based Inspection;Multi-UAV Cooperation;Intelligent Control},
  doi={10.1109/MetroAeroSpace61015.2024.10591600},
  ISSN={2575-7490},
  month={June},}@BOOK{10280481,
  author={Chollet, Francois},
  booktitle={Deep Learning with Python, Second Edition},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Printed in full color! Unlock the groundbreaking advances of deep learning with this extensively revised new edition of the bestselling original. Learn directly from the creator of Keras and master practical Python deep learning techniques that are easy to apply in the real world. In Deep Learning with Python, Second Edition you will learn:  Deep learning from first principles Image classification and image segmentation Timeseries forecasting Text classification and machine translation Text generation, neural style transfer, and image generation Printed in full color throughout  Deep Learning with Python has taught thousands of readers how to put the full capabilities of deep learning into action. This extensively revised full color second edition introduces deep learning using Python and Keras, and is loaded with insights for both novice and experienced ML practitioners. You’ll learn practical techniques that are easy to apply in the real world, and important theory for perfecting neural networks.},
  keywords={deep learning;python;machine learning;Keras;TensorFlow;computer vision;natural language processing;generative models},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617296864},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10280481},}@INPROCEEDINGS{10854069,
  author={Wang, Yikun},
  booktitle={6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024)}, 
  title={Research on the construction of intelligent aided design tool for automobile shape based on stable diffusion}, 
  year={2024},
  volume={2024},
  number={},
  pages={427-437},
  abstract={With the wide application of AI technology, AIGC is gradually establishing its core position in the field of intelligent design. Its powerful data processing and pattern recognition capabilities provide more reliable and diverse decision support for the design process. In the field of industrial equipment design, Stable Diffusion has significant advantages as an AIGC tool. In order to further explore its application potential in automobile appearance design and optimize the design process, this paper proposes a set of innovative intelligent generative design methods based on Stable Diffusion assistance from the aspects of data set construction and model training, and tests its design generation ability. Aiming at the process of building intelligent aided design tools, this study provides a reproducible realization path for intelligent design, which has important reference.},
  keywords={},
  doi={10.1049/icp.2024.4261},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9522984,
  author={van de Ven, Gido M. and Li, Zhe and Tolias, Andreas S.},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Class-Incremental Learning with Generative Classifiers}, 
  year={2021},
  volume={},
  number={},
  pages={3606-3615},
  abstract={Incrementally training deep neural networks to recognize new classes is a challenging problem. Most existing class-incremental learning methods store data or use generative replay, both of which have drawbacks, while ‘rehearsal-free’ alternatives such as parameter regularization or bias-correction methods do not consistently achieve high performance. Here, we put forward a new strategy for class-incremental learning: generative classification. Rather than directly learning the conditional distribution p(y|x), our proposal is to learn the joint distribution p(x, y), factorized as p(x|y)p(y), and to perform classification using Bayes’ rule. As a proof-of-principle, here we implement this strategy by training a variational autoencoder for each class to be learned and by using importance sampling to estimate the likelihoods p(x|y). This simple approach performs very well on a diverse set of continual learning benchmarks, outperforming generative replay and other existing baselines that do not store data.},
  keywords={Training;Learning systems;Deep learning;Computer vision;Monte Carlo methods;Conferences;Benchmark testing},
  doi={10.1109/CVPRW53098.2021.00400},
  ISSN={2160-7516},
  month={June},}@ARTICLE{9915446,
  author={Wang, Qi and Wang, Yuqing and Ao, Yile and Lu, Wenkai},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Seismic Inversion Based on 2D-CNNs and Domain Adaption}, 
  year={2022},
  volume={60},
  number={},
  pages={1-12},
  abstract={Deep learning has been applied to tackle the seismic inversion problem, bringing more efficiency, and accuracy. However, bad spatial continuity and poor generalizability limit the practical application. To solve these problems, we propose a 2D end-to-end seismic inversion method based on domain adaption. First, the proposed 2D network learns the inversion mapping of seismic data under the constraint of the domain adaption layer, which can reduce the difference between the features of real seismic data and synthetic seismic data, improving the generalization ability on real seismic data. Then, the trained model is fine-tuned with well-logging data. In the first process, the spatial continuity of the inversion result is guaranteed by the 2D training scheme. Meanwhile, due to the constraint of the domain adaption layer, our model not only performs well on the synthetic data but also has good generalization ability on the real seismic data. We carefully discuss the mechanism of the domain adaption layer. In the second process, fine-tuning introduces well-logging information, which can further improve the ability to invert details. Moreover, in order to improve the inversion accuracy on real seismic data, we develop a new training data generation method that can generate the synthetic samples close to the real samples, and a 2.5D training strategy is adopted to improve the continuity of the 3D data. The experiments on both synthetic and real seismic data show that our method performs better than both the recursive inversion method and the 1D closed-loop CNN methods.},
  keywords={Data models;Deep learning;Adaptation models;Task analysis;Training;Transfer learning;Impedance;2D-convolutional neural network (CNN);domain adaption;seismic inversion;transfer learning},
  doi={10.1109/TGRS.2022.3213337},
  ISSN={1558-0644},
  month={},}@ARTICLE{8903228,
  author={Chen, Xiang and Xu, Lijun and Wei, Hua and Shang, Zhongan and Zhang, Tingyu and Zhang, Linghao},
  journal={IEEE Access}, 
  title={Emotion Interaction Recognition Based on Deep Adversarial Network in Interactive Design for Intelligent Robot}, 
  year={2019},
  volume={7},
  number={},
  pages={166860-166868},
  abstract={Augmented Reality devices (AR), virtual reality devices (VR), are changing our lives and it is critical to provide intelligent interaction and improve the user’s intelligent interactive experience. When artificial intelligence is introduced into Intelligent interaction emotion classification or semantic segmentation and other tasks, it requires professional knowledge to manually label images sample. To address the problem of scarcity of labeled data in emotion classification, an improved classification method based on semi-supervised generative adversarial networks (GAN) is proposed in this paper. Firstly, the output layer of the traditional unsupervised GAN is replaced with Softmax layer to obtain the semi-supervised GAN. Secondly, additional labels are defined for generated samples to guiding the training process. Finally, we employ a semi-supervised training strategy to optimize the parameters of GAN and use the trained network to process videos. Experiments on existing public datasets show that our method has a certain improvement in compared with the classic methods based on deep learning, and has a higher recognition efficiency, which is more suitable for dimension emotion recognition of large-scale data.},
  keywords={Generative adversarial networks;Feature extraction;Emotion recognition;Training;Generators;Speech recognition;Intelligent robots;Emotional interaction;adversarial network;deep learning;softmax layer;artificial intelligence;interaction robot;semantic feature},
  doi={10.1109/ACCESS.2019.2953882},
  ISSN={2169-3536},
  month={},}@ARTICLE{9769934,
  author={Yang, Wen and Wu, Jinjian and Tian, Shiwei and Li, Leida and Dong, Weisheng and Shi, Guangming},
  journal={IEEE Transactions on Image Processing}, 
  title={Fine-Grained Image Quality Caption With Hierarchical Semantics Degradation}, 
  year={2022},
  volume={31},
  number={},
  pages={3578-3590},
  abstract={Blind image quality assessment (BIQA), which is capable of precisely and automatically estimating human perceived image quality with no pristine image for comparison, attracts extensive attention and is of wide applications. Recently, many existing BIQA methods commonly represent image quality with a quantitative value, which is inconsistent with human cognition. Generally, human beings are good at perceiving image quality in terms of semantic description rather than quantitative value. Moreover, cognition is a needs-oriented task where humans are able to extract image contents with local to global semantics as they need. The mediocre quality value represents coarse or holistic image quality and fails to reflect degradation on hierarchical semantics. In this paper, to comply with human cognition, a novel quality caption model is inventively proposed to measure fine-grained image quality with hierarchical semantics degradation. Research on human visual system indicates there are hierarchy and reverse hierarchy correlations between hierarchical semantics. Meanwhile, empirical evidence shows that there are also bi-directional degradation dependencies between them. Thus, a novel bi-directional relationship-based network (BDRNet) is proposed for semantics degradation description, through adaptively exploring those correlations and degradation dependencies in a bi-directional manner. Extensive experiments demonstrate that our method outperforms the state-of-the-arts in terms of both evaluation performance and generalization ability.},
  keywords={Semantics;Degradation;Image quality;Feature extraction;Distortion;Databases;Bidirectional control;Image quality assessment;quality caption;hierarchical semantics degradation;deep neural network},
  doi={10.1109/TIP.2022.3171445},
  ISSN={1941-0042},
  month={},}@ARTICLE{10195891,
  author={Wu, Ruiqi and Chao, Fei and Zhou, Changle and Chang, Xiang and Yang, Longzhi and Shang, Changjing and Zhang, Zihao and Shen, Qiang},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Internal Model Control Structure Inspired Robotic Calligraphy System}, 
  year={2024},
  volume={20},
  number={2},
  pages={2600-2610},
  abstract={Learning calligraphy writing skills in robots is regarded as a sophisticated task. Current robotic researchers have proposed many methods to implement various robotic calligraphy systems. However, several limitations of these methods, such as high computational costs and few diversities of generated results constrain the development of calligraphy robots. This article proposes a robotic writing framework based on a robotic hand–eye coordination method to solve these limitations. Inspired by the internal model control (IMC) system, a vision-motor network and a motor-vision network are built to simulate the direct and reverse models, respectively, in the IMC system of a robotic manipulator. The vision-motor network works as an action generator to convert image inputs to robotic actions, and the motor-vision network assists in the training of the vision-motor network. Thus, a pretraining of the motor-vision network is established by random writing movements of a robotic manipulator. Experimental results demonstrate that the proposed method can successfully write strokes of Chinese characters by inputting target stroke images. Although the proposed method is applied to robotic calligraphy, the underpinning research is readily applicable to many other applications, such as human–robot motion mimicking.},
  keywords={Robot kinematics;Writing;Training data;Informatics;Visualization;Monte Carlo methods;Motion planning;Robot control;Handwriting recognition;Writing;Motion planning;robotic calligraphy;robotic control;robotic hand–eye coordination},
  doi={10.1109/TII.2023.3295415},
  ISSN={1941-0050},
  month={Feb},}@ARTICLE{10985868,
  author={Zhang, Yun and Yu, Zhenhua and Yin, Zheng and Ye, Ou and Cong, Xuya and Song, Houbing Herbert},
  journal={IEEE Internet of Things Journal}, 
  title={Closed-Box Adversarial Attack Method for Object Detection Under Multiview Conditions}, 
  year={2025},
  volume={12},
  number={14},
  pages={27886-27900},
  abstract={Deep learning-based object detection has become an important application in industrial IoT. However, studies have shown that adversarial attacks may cause object detection to output incorrect detection results. Such vulnerabilities can threaten the robustness of object detection systems and lead to security problems. To address the issue of low attack effectiveness on target detection from different perspectives using the existing adversarial attack methods, this article proposes an adversarial attack method with multiview adaptive weight-balancing. First, a multiview channel is constructed for training, and the target features under different viewpoints are comprehensively considered to enhance the robustness of the attack method. Then, the model is optimized by combining the model shake drop and patch cut-out algorithms during the training process, so that the attack method no longer relies on a single model, thus enhancing its generalization ability. Finally, by dynamically adjusting the weights of each viewpoint, a weight-balancing strategy is constructed, which adaptively adjusts the preference of different perspectives during the training process to enhance the attack effect of the attack method in each viewpoint. To verify the performance of the method, experiments are conducted on multiple benchmarks, specifically the PKU-Reid dataset. Compared with the mainstream methods, the proposed method improves the attack success rate by 3.78% and 19.26% under glass-box and closed-box conditions, respectively, while reducing the mean average precision of the object detection model by 2.18% and 11.12%, respectively. The experimental results demonstrate that the proposed method effectively enhances attack performance on targets from different viewpoints and exhibits better viewpoint robustness.},
  keywords={Object detection;Training;Closed box;Robustness;Perturbation methods;Adaptation models;Glass box;Industrial Internet of Things;Detectors;Noise;Adversarial attack;Industrial Internet of Things (IoT);multiview channel training;object detection},
  doi={10.1109/JIOT.2025.3566950},
  ISSN={2327-4662},
  month={July},}@ARTICLE{10915753,
  author={Zhang, Chao and Wen, Qifei and Sangaiah, Arun Kumar and Alenazi, Mohammed J. F. and Aborokbah, Majed},
  journal={IEEE Consumer Electronics Magazine}, 
  title={Managing Consensus Behaviors Via Sentiment Analysis for GenAI-Driven Consumer Services}, 
  year={2025},
  volume={14},
  number={5},
  pages={123-132},
  abstract={With the rapid advancement of artificial intelligence/generative AI (AI/GenAI), various industries have undergone significant transformations, particularly in the realm of consumer devices and services. AI/GenAI plays a pivotal role in product purchase decisions by analyzing vast amounts of user-generated information, such as reviews. This article aims to provide new solutions for new energy vehicle (NEV) purchase decisions through AI/GenAI to better serve consumers. First, a genetic algorithm-guided long short-term memory and gated recurrent unit are integrated for sentiment analysis to accurately express the sentiment tendencies through AI/GenAI, with the results being converted into probabilistic linguistic term sets. Second, an enhanced K-means++ and an adaptive three-way clustering algorithm are utilized to ensure the accuracy of clustering results. Third, a model based on confidence and conflict levels to identify the non-cooperative behaviors is established to facilitate consensus among decision-makers. Fourth, during the selection phase, the three-way decision is combined with multiattributive border approximation area comparison methods to obtain the optimal alternatives. Finally, using NEV purchase examples, the proposed methods demonstrate robustness and reliability, showcasing their effectiveness in enabling GenAI-driven consumer services.},
  keywords={Consumer electronics;Logic gates;Adaptation models;Long short term memory;Decision making;Artificial intelligence;Training data;Generative AI;Artificial intelligence;Consensus control;Clustering algorithms},
  doi={10.1109/MCE.2025.3548514},
  ISSN={2162-2256},
  month={Sep.},}@ARTICLE{11165150,
  author={Chen, Xinyu and Han, Liuyan and Liu, Zixi and He, Wei and Ou, Xuegang and Zhang, Dechao and Li, Han},
  journal={Journal of Lightwave Technology}, 
  title={AI-Embedded Optical Modules With Millisecond-Granularity Power Analysis for Autonomous Metro Transport Network and Field Trial}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The rapid expansion of 5G-A/6 G and AI-driven data centers demands robust optical networks with real-time fault detection capabilities. To address this need, we propose an intelligent optical module for edge deployment featuring millisecond-granularity power sampling and AI-driven analytics for high-precision monitoring of optical failures. The system integrates a hardware-software co-design enabling 10 ms resolution with a dual-attentive optic-diagnose network (DAOD-Net), which combines conditional GAN-based data augmentation and a TimeTransformer architecture for enhanced fault classification. Real-time inference for multiple failure modes is achieved, including device failure, fiber break, pigtail detachment, connector loosening, and fiber bend. Field trials in the deployed network demonstrate an overall fault identification accuracy of 92.6% with the consistent >95% precision across all fault categories, outperforming traditional neural network methods by 14.7% in accuracy. This solution is promising for reducing optical fault diagnostic time from hours to seconds, advancing the state-of-the-art in network automation capabilities while providing a scalable framework for proactive maintenance in next-generation 5G-A/6 G and hyperscale data center networks.},
  keywords={Optical fibers;Optical fiber sensors;Artificial intelligence;Training;Optical buffering;Adaptive optics;Feature extraction;Optical reflection;Optical network units;Optical attenuators;Fault diagnose;neural network application;optical data processing;optical network},
  doi={10.1109/JLT.2025.3610489},
  ISSN={1558-2213},
  month={},}@ARTICLE{8663987,
  author={Yang, Yang and Li, Yang and Zhang, Wuxiong and Qin, Fei and Zhu, Pengcheng and Wang, Cheng-Xiang},
  journal={IEEE Communications Magazine}, 
  title={Generative-Adversarial-Network-Based Wireless Channel Modeling: Challenges and Opportunities}, 
  year={2019},
  volume={57},
  number={3},
  pages={22-27},
  abstract={In modern wireless communication systems, wireless channel modeling has always been a fundamental task in system design and performance optimization. Traditional channel modeling methods, such as ray-tracing and geometry- based stochastic channel models, require in-depth domain-specific knowledge and technical expertise in radio signal propagations across electromagnetic fields. To avoid these difficulties and complexities, a novel generative adversarial network (GAN) framework is proposed for the first time to address the problem of autonomous wireless channel modeling without complex theoretical analysis or data processing. Specifically, the GAN is trained by raw measurement data to reach the Nash equilibrium of a MinMax game between a channel data generator and a channel data discriminator. Once this process converges, the resulting channel data generator is extracted as the target channel model for a specific application scenario. To demonstrate, the distribution of a typical additive white Gaussian noise channel is successfully approximated by using the proposed GAN-based channel modeling framework, thus verifying its good performance and effectiveness.},
  keywords={Wireless communication;Generators;Channel models;Artificial intelligence;Analytical models;Data models;MIMO communication},
  doi={10.1109/MCOM.2019.1800635},
  ISSN={1558-1896},
  month={March},}@INBOOK{10951733,
  author={Kumari, V. Sheeja and Balu, Renjith},
  booktitle={Artificial Intelligence-Based System Models in Healthcare}, 
  title={Deep Learning Applications in Healthcare Systems}, 
  year={2024},
  volume={},
  number={},
  pages={179-204},
  abstract={Summary <p>Deep learning (DL) has gained significant popularity owing to rapid advancements in computer technologies and the explosive expansion of large datasets. As a subset of Artificial Intelligence (AI), DL represents a departure from the earlier rule&#x2010;based systems that characterized the early stages of AI. The current state of AI is deeply rooted in machine learning principles. In the initial phases of AI, rule&#x2010;based systems were prevalent, but the contemporary AI landscape is dominated by machine learning approaches. Specifically, in the context of tasks, such as classification, models or networks are trained using labeled data, where inputs are paired with corresponding labels. Following this training, the model can then predict outputs for unlabeled testing inputs based on its acquired knowledge. Unlike conventional machine learning methods that rely on handcrafted low&#x2010;level or mid&#x2010;level features for classification, DL employs neural networks with multiple layers, forming a deep structure between input and output layers, commonly known as hidden layers. DL's key advantage lies in its capacity to autonomously learn data&#x2010;driven and task&#x2010;specific features that are both highly representative and hierarchical. This integrated framework handles feature extraction and classification within a single network. The success of deep learning techniques is particularly evident in computer vision tasks, such as image classification, detection, and segmentation. More recently, DL has gained substantial traction in medical applications, including anatomical modelling (e.g., anatomical structure segmentation), tumor detection, disease classification, computer&#x2010;aided diagnosis, and surgical planning. This chapter aims to present recent advancements and potential future prospects of DL in the field of medicine and healthcare.</p>},
  keywords={Medical services;Medical diagnostic imaging;Deep learning;Artificial neural networks;Training;Feature extraction;Machine learning;Computational modeling;Biological system modeling;Neurons},
  doi={10.1002/9781394242528.ch8},
  ISSN={},
  publisher={Wiley},
  isbn={9781394242511},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951733},}@ARTICLE{9105006,
  author={Abdelmaksoud, Mohamed and Nabil, Emad and Farag, Ibrahim and Hameed, Hala Abdel},
  journal={IEEE Access}, 
  title={A Novel Neural Network Method for Face Recognition With a Single Sample Per Person}, 
  year={2020},
  volume={8},
  number={},
  pages={102212-102221},
  abstract={Face Recognition (FR) problem is one of the significant fields in computer vision. FR is used to identify the faces that appear over distributed cameras over the network. The problem of face recognition can be divided into two categories, the first is recognition with more than one sample per person, which can be called traditional face recognition problem. The second is the recognition of faces using only a Single Sample Per Person (SSPP). The efficiency of face recognition systems decreases because of limited references especially (SSPP) and faces taken in the Operational Domain (OD) different from faces in the Enrollment Domain (ED) in illumination, pose, low-resolution, and blurriness. This paper proposed a method that deals with all problems related to face recognition with SSPP. 3D face reconstruction is used to increase the reference gallery set with different poses and generate a design domain dictionary to overcome the problem of limited reference. Besides, the design domain dictionary is used to feed different deep learning models. Face illumination transfer techniques are utilized to overcome the illumination problem. Labeled Faces in the Wild (LFW) dataset is used to train Super-Resolution Generative Adversarial Network (SRGAN) to overcome the low-resolution problem. Deblur Generative Adversarial Network (DeblurGAN) is trained on the LFW dataset to overcome the problem of blurriness. The proposed method is evaluated using the Chokepoint dataset and COX-S2V dataset. The final results confirm an overall enhancement in accuracy compared to techniques that use SSPP for face recognition (generic learning and face synthesizing approaches). Also, the proposed method outperforms of Traditional and Deep Learning (TDL) method accuracy, which uses SSPP for face recognition.},
  keywords={Face;Face recognition;Three-dimensional displays;Lighting;Deep learning;Dictionaries;Training;Face recognition;generative adversarial network (GAN);single sample per person (SSPP);illumination transferring;deep learning},
  doi={10.1109/ACCESS.2020.2999030},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10415934,
  author={Güzay, Çağrı and Özdemir, Ege and Kara, Yahya},
  booktitle={2023 14th International Conference on Electrical and Electronics Engineering (ELECO)}, 
  title={A Generative AI-driven Application: Use of Large Language Models for Traffic Scenario Generation}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificial Intelligence (AI) has revolutionized the way we interact with technology. One of its forefronts is Natural Language Processing (NLP), an essential sub-field that focuses on human language understanding and generation. Large Language Models (LLMs) such as PaLM, BERT, GPT have emerged as the powerhouses of NLPs with remarkable capabilities. Today, we also utilize AI in other industries like automotive. While using AI-backed powerful instruments in autonomous vehicle technologies, we are still suffering to create virtual traffic scenarios for special purposes such as safety testing and validation, autonomous driver training and regulatory compliance. In this study, we introduce a solution which harnesses the power of LLMs to overcome scenario creation difficulties. Thus, our approach provides creation of the traffic scenarios in linguistic way rather than using user interfaces of the editors or writing special formatted structured scenario files.},
  keywords={Training;Writing;User interfaces;Natural language processing;Artificial intelligence;Autonomous vehicles;Testing},
  doi={10.1109/ELECO60389.2023.10415934},
  ISSN={},
  month={Nov},}@ARTICLE{9546645,
  author={Ma, Cheng and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Structure-Preserving Image Super-Resolution}, 
  year={2022},
  volume={44},
  number={11},
  pages={7898-7911},
  abstract={Structures matter in single image super-resolution (SISR). Benefiting from generative adversarial networks (GANs), recent studies have promoted the development of SISR by recovering photo-realistic images. However, there are still undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super-resolution (SPSR) method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. First, we propose SPSR with gradient guidance (SPSR-G) by exploiting gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss to impose a second-order restriction on the super-resolved images, which helps generative networks concentrate more on geometric structures. Second, since the gradient maps are handcrafted and may only be able to capture limited aspects of structural information, we further extend SPSR-G by introducing a learnable neural structure extractor (NSE) to unearth richer local structures and provide stronger supervision for SR. We propose two self-supervised structure learning methods, contrastive prediction and solving jigsaw puzzles, to train the NSEs. Our methods are model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results on five benchmark datasets show that the proposed methods outperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and SSIM metrics. Visual results demonstrate the superiority of our methods in restoring structures while generating natural SR images. Code is available at https://github.com/Maclory/SPSR.},
  keywords={Feature extraction;Image edge detection;Superresolution;Distortion;Task analysis;Generative adversarial networks;Image restoration;Super-resolution;image enhancement;self-supervised learning;generative adversarial network},
  doi={10.1109/TPAMI.2021.3114428},
  ISSN={1939-3539},
  month={Nov},}@ARTICLE{9749880,
  author={Zhao, Yuzhi and Po, Lai-Man and Wang, Xuehui and Yan, Qiong and Shen, Wei and Zhang, Yujia and Liu, Wei and Wong, Chun-Kit and Pang, Chiu-Sing and Ou, Weifeng and Yu, Wing-Yin and Liu, Buhua},
  journal={IEEE Transactions on Multimedia}, 
  title={ChildPredictor: A Child Face Prediction Framework With Disentangled Learning}, 
  year={2023},
  volume={25},
  number={},
  pages={3737-3752},
  abstract={The appearances of children are inherited from their parents, which makes it feasible to predict them. Predicting realistic children's faces may help settle many social problems, such as age-invariant face recognition, kinship verification, and missing child identification. It can be regarded as an image-to-image translation task. Existing approaches usually assume domain information in the image-to-image translation can be interpreted by “style”, i.e., the separation of image content and style. However, such separation is improper for the child face prediction, because the facial contours between children and parents are not the same. To address this issue, we propose a new disentangled learning strategy for children's face prediction. We assume that children's faces are determined by genetic factors (compact family features, e.g., face contour), external factors (facial attributes irrelevant to prediction, such as moustaches and glasses), and variety factors (individual properties for each child). On this basis, we formulate predictions as a mapping from parents’ genetic factors to children's genetic factors, and disentangle them from external and variety factors. In order to obtain accurate genetic factors and perform the mapping, we propose a ChildPredictor framework. It transfers human faces to genetic factors by encoders and back by generators. Then, it learns the relationship between the genetic factors of parents and children through a mapping function. To ensure the generated faces are realistic, we collect a large Family Face Database to train ChildPredictor and evaluate it on the FF-Database validation set. Experimental results demonstrate that ChildPredictor is superior to other well-known image-to-image translation methods in predicting realistic and diverse child faces. Implementation codes can be found at https://github.com/zhaoyuzhi/ChildPredictor.},
  keywords={Face recognition;Faces;Genetics;Generative adversarial networks;Training;Glass;Skin;Child face prediction;disentangled learning;generative adversarial network;image-to-image translation},
  doi={10.1109/TMM.2022.3164785},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10889237,
  author={Hwang, Suntae and Kang, Seonghyeon and Kim, Kyungsu and Ahn, Semin and Lee, Kyogu},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={DOSE: Drum One-Shot Extraction from Music Mixture}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Drum one-shot samples are crucial for music production, particularly in sound design and electronic music. This paper introduces Drum One-Shot Extraction, a task in which the goal is to extract drum one-shots that are present in the music mixture. To facilitate this, we propose the Random Mixture One-shot Dataset (RMOD), comprising large-scale, randomly arranged music mixtures paired with corresponding drum one-shot samples. Our proposed model, Drum One-Shot Extractor (DOSE), leverages neural audio codec language models for end-to-end extraction, bypassing traditional source separation steps. Additionally, we introduce a novel onset loss, designed to encourage accurate prediction of the initial transient of drum one-shots, which is essential for capturing timbral characteristics. We compare this approach against a source separation-based extraction method as a baseline. The results, evaluated using Fréchet Audio Distance (FAD) and Multi-Scale Spectral loss (MSS), demonstrate that DOSE, enhanced with onset loss, outperforms the baseline, providing more accurate and higher-quality drum one-shots from music mixtures. The code, model checkpoint, and audio examples are available at https://github.com/HSUNEH/DOSE},
  keywords={Training;Measurement;Accuracy;Source separation;Instruments;Production;Data models;Electronic music;Transient analysis;Speech processing;Drum;One-Shot;Music Source Separation;Neural Audio-Codec;Generative Model},
  doi={10.1109/ICASSP49660.2025.10889237},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10897888,
  author={Zhao, Lei and Li, Mengwei and Li, Bo and Wei, Xingxing},
  journal={IEEE Transactions on Multimedia}, 
  title={Diverse Visible-to-Thermal Image Translation via Controllable Temperature Encoding}, 
  year={2025},
  volume={27},
  number={},
  pages={5685-5695},
  abstract={Translating readily available visible (VIS) images into thermal infrared (TIR) images effectively alleviates the shortage of TIR data. While current methods have yielded commendable results, they fall short in generating diverse and realistic thermal infrared images, primarily due to insufficient consideration of temperature variations. In this paper, we propose a Thermally Controlled GAN (TC-GAN) that leverages VIS images to generate diverse TIR images, with the ability to control the relative temperatures of multiple objects, particularly those with temperature variations. Firstly, we introduce the physical coding module, which employs a conditional variational autoencoder GAN to learn the distributions of relative temperature information for the objects and environmental state information. Then, the physical information can be obtained by sampling the distribution. When this information is fused with the visible image, it facilitates the generation of diverse TIR images. To ensure authenticity and strengthen the physical constraints across different regions of the image, we introduce a self-attention mechanism in the generator that prioritizes the relative temperature relationships within the image. Additionally, we utilize a local discriminator that focuses on objects with actively changing temperatures and their interactions with the surrounding environment, thereby reducing the discontinuity between the target and the background. Experiments on the Drone Vehicle and AVIID datasets show that our approach outperforms mainstream diversity generation methods in terms of authenticity and diversity.},
  keywords={Translation;Temperature distribution;Training;Image coding;Data models;Diffusion models;Temperature sensors;Generative adversarial networks;Testing;Roads;Diversity generation;generative adversarial networks;self-attention;temperature coding;visible to thermal translation},
  doi={10.1109/TMM.2025.3543053},
  ISSN={1941-0077},
  month={},}@ARTICLE{10177892,
  author={Wang, Ning and Feng, Panpan and Ge, Zhaoyang and Zhou, Yanjie and Zhou, Bing and Wang, Zongmin},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Adversarial Spatiotemporal Contrastive Learning for Electrocardiogram Signals}, 
  year={2024},
  volume={35},
  number={10},
  pages={13845-13859},
  abstract={Extracting invariant representations in unlabeled electrocardiogram (ECG) signals is a challenge for deep neural networks (DNNs). Contrastive learning is a promising method for unsupervised learning. However, it should improve its robustness to noise and learn the spatiotemporal and semantic representations of categories, just like cardiologists. This article proposes a patient-level adversarial spatiotemporal contrastive learning (ASTCL) framework, which includes ECG augmentations, an adversarial module, and a spatiotemporal contrastive module. Based on the ECG noise attributes, two distinct but effective ECG augmentations, ECG noise enhancement, and ECG noise denoising, are introduced. These methods are beneficial for ASTCL to enhance the robustness of the DNN to noise. This article proposes a self-supervised task to increase the antiperturbation ability. This task is represented as a game between the discriminator and encoder in the adversarial module, which pulls the extracted representations into the shared distribution between the positive pairs to discard the perturbation representations and learn the invariant representations. The spatiotemporal contrastive module combines spatiotemporal prediction and patient discrimination to learn the spatiotemporal and semantic representations of categories. To learn category representations effectively, this article only uses patient-level positive pairs and alternately uses the predictor and the stop-gradient to avoid model collapse. To verify the effectiveness of the proposed method, various groups of experiments are conducted on four ECG benchmark datasets and one clinical dataset compared with the state-of-the-art methods. Experimental results showed that the proposed method outperforms the state-of-the-art methods.},
  keywords={Electrocardiography;Spatiotemporal phenomena;Task analysis;Semantics;Self-supervised learning;Robustness;Artificial neural networks;Adversarial learning;contrastive learning;data augmentation;electrocardiogram (ECG)},
  doi={10.1109/TNNLS.2023.3272153},
  ISSN={2162-2388},
  month={Oct},}@ARTICLE{10570491,
  author={Sabeel, Ulya and Heydari, Shahram Shah and El-Khatib, Khalil and Elgazzar, Khalid},
  journal={IEEE Transactions on Machine Learning in Communications and Networking}, 
  title={Incremental Adversarial Learning for Polymorphic Attack Detection}, 
  year={2024},
  volume={2},
  number={},
  pages={869-887},
  abstract={AI-based Network Intrusion Detection Systems (NIDS) provide effective mechanisms for cybersecurity analysts to gain insights and thwart several network attacks. Although current IDS can identify known/typical attacks with high accuracy, current research shows that such systems perform poorly when facing atypical and dynamically changing (polymorphic) attacks. In this paper, we focus on improving detection capability of the IDS for atypical and polymorphic network attacks. Our system generates adversarial polymorphic attacks against the IDS to examine its performance and incrementally retrains it to strengthen its detection of new attacks, specifically for minority attack samples in the input data. The employed attack quality analysis ensures that the adversarial atypical/polymorphic attacks generated through our system resemble original network attacks. We showcase the high performance of the IDS that we have proposed by training it using the CICIDS2017 and CICIoT2023 benchmark datasets and evaluating its performance against several atypical/polymorphic attack flows. The results indicate that the proposed technique, through adaptive training, learns the pattern of dynamically changing atypical/polymorphic attacks, identifies such attacks with approximately 90% balanced accuracy for most of the cases, and surpasses various state-of-the-art detection and class balancing techniques.},
  keywords={Training;Feature extraction;Data models;Computer security;Artificial intelligence;Benchmark testing;Accuracy;Artificial intelligence;atypical attacks;class imbalance;feature profile;intrusion detection system;polymorphic attacks},
  doi={10.1109/TMLCN.2024.3418756},
  ISSN={2831-316X},
  month={},}@ARTICLE{9512286,
  author={Huang, Xiaohan and Wang, Xuesong and Yu, Qiang and Cheng, Yuhu},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Minimum Adversarial Distribution Discrepancy for Domain Adaptation}, 
  year={2022},
  volume={14},
  number={4},
  pages={1440-1448},
  abstract={Domain adaptation (DA) refers to generalize a learning technique across the source domain and target domain under different distributions. Therefore, the essential problem in DA is how to reduce the distribution discrepancy between the source and target domains. Typical methods are to embed the adversarial learning technique into deep networks to learn transferable feature representations. However, existing adversarial related DA methods may not sufficiently minimize the distribution discrepancy. In this article, a DA method minimum adversarial distribution discrepancy (MADD) is proposed by combining feature distribution with adversarial learning. Specifically, we design a novel divergence metric loss, named maximum mean discrepancy based on conditional entropy (MMD-CE), and embed it in the adversarial DA network. The proposed MMD-CE loss can address two problems: 1) the misalignment from different class distributions between domains and 2) the equilibrium challenge issue in adversarial DA. Comparative experiments on Office-31, ImageCLEF-DA, and Office-Home data sets with state-of-the-art methods show that our method has some advantageous performances.},
  keywords={Entropy;Feature extraction;Artificial neural networks;Training data;Adversarial machine learning;Deep learning;Data mining;Adversarial learning;conditional entropy;domain adaptation (DA);feature distribution},
  doi={10.1109/TCDS.2021.3104231},
  ISSN={2379-8939},
  month={Dec},}@ARTICLE{10272354,
  author={Sun, Xiaochuan and Niu, Xiaoyu and Wang, Yutong and Li, Yingqi},
  journal={Journal of Communications and Information Networks}, 
  title={User Satisfaction-Aware Edge Computation Offloading in 5G Multi-Scenario}, 
  year={2023},
  volume={8},
  number={3},
  pages={271-282},
  abstract={Edge computation offloading has made some progress in the fifth generation mobile network (5G). However, load balancing in edge computation offloading is still a challenging problem. Meanwhile, with the continuous pursuit of low execution latency in 5G multi-scenario, the functional requirements of edge computation offloading are further exacerbated. Given the above challenges, we raise a unique edge computation offloading method in 5G multi-scenario, and consider user satisfaction. The method consists of three functional parts: offloading strategy generation, offloading strategy update, and offloading strategy optimization. First, the offloading strategy is generated by means of a deep neural network (DNN), then update the offloading strategy by updating the DNN parameters. Finally, we optimize the offloading strategy based on changes in user satisfaction. In summary, compared to existing optimization methods, our proposal can achieve performance close to the optimum. Massive simulation results indicate the latency of the execution of our method on the CPU is under 0.1 seconds while improving the average computation rate by about 10%.},
  keywords={Task analysis;5G mobile communication;Optimization;Servers;Ultra reliable low latency communication;Artificial neural networks;Wireless communication;DNN;user satisfaction;5G multi-scenario;edge computation offloading},
  doi={10.23919/JCIN.2023.10272354},
  ISSN={2509-3312},
  month={Sep.},}@INPROCEEDINGS{11042344,
  author={Santhosh, Parvathy and S, Shailesh.},
  booktitle={2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)}, 
  title={Digital Cultural Heritage Preservation:A Literature Analysis}, 
  year={2025},
  volume={1},
  number={},
  pages={1-7},
  abstract={A significant portion of our cultural legacy comprises artifacts such as historical buildings, monuments, paintings, sculptures, and archaeological sites. These artifacts face threats from human activities, natural disasters, and environmental factors, leading to degradation over time. Preserving such artwork is essential for understanding historical and cultural contexts. Restoration and classification processes, enabled by advanced machine learning algorithms, offer efficient solutions to safeguard and rejuvenate these invaluable assets for future generations.},
  keywords={Deep learning;Accuracy;Three-dimensional displays;Systematics;Biological system modeling;Superresolution;Image restoration;Cultural differences;Artificial intelligence;Sustainable development;Cultural Heritage;Artificial Intelligence;Digitalization;Virtual Restoration;In-Painting;Classification},
  doi={10.1109/ICTEST64710.2025.11042344},
  ISSN={},
  month={April},}@ARTICLE{10975772,
  author={Peng, Ru and Chen, Xingyu and Lan, Xuguang},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Adversarial Mixup-Based Contrast Learning for Data-Driven Predictive Maintenance in Long-Tailed Recognition}, 
  year={2025},
  volume={71},
  number={2},
  pages={5249-5258},
  abstract={Deep neural networks have achieved remarkable success in various computer vision tasks. However, in real-world applications, such as the Internet of Things (IoT), these models often struggle due to the long-tailed data distributions. For instance, in scenarios such as Holographic Counterpart Integration in IoT-based predictive maintenance for home systems or smart repair services, common operational states are prevalent in the dataset. In contrast, rare failures, such as hardware malfunctions or system breakdowns, are represented by only a few samples. This imbalance severely impacts models, making it difficult to accurately predict rare failures, leading to costly downtime or unanticipated equipment failure. Current contrastive learning-based methods are effective at optimizing feature distributions but often overlook inter-class relationships and are highly sensitive to class imbalance, which limits their generalization ability. To address these challenges, we propose the Adversarial Mixup-based supervised contrast learning (AMCL) framework, which integrates Mixup-based data augmentation with contrastive learning and incorporates an adversarial-inspired sample policy generator. AMCL generates boundary samples via a dynamically optimized Mixup strategy to enhance inter-class relationship modeling and improve predictions on ambiguous boundaries. Furthermore, we introduce a new MixCo loss function to account for the non-one-hot distribution of Mixup-generated targets, ensuring better alignment with augmented data and improving optimization efficiency. AMCL is easy to implement and achieves a performance superior to recent approaches for long-tailed recognition across various datasets such as ImageNet-LT, iNaturalist18, CIFAR-10-LT, and CIFAR-100-LT.},
  keywords={Heavily-tailed distribution;Tail;Internet of Things;Contrastive learning;Training;Semantics;Representation learning;Predictive maintenance;Optimization;Data augmentation;Deep long-tailed learning;data augmentation;supervised contrast learning;and data analysis and predictive maintenance},
  doi={10.1109/TCE.2025.3563895},
  ISSN={1558-4127},
  month={May},}@INPROCEEDINGS{9823491,
  author={G, Geetha and Thimmiaraja, J and Shelke, Chetan Jagannath and Pavithra, G. and Sharma, Vinay Kumar and Verma, Devvret},
  booktitle={2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Deep Learning with Unsupervised and Supervised Approaches in Medical Image Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1580-1584},
  abstract={“Medical imaging” is implemented in a wide range of therapeutic trials, including approaches for early detection, diagnosis, monitoring, and therapy assessment of a wide range of medical issues. Understanding “artificial neural networks” principles and “deep learning”, as well as their theories and applications in medical image processing, is essential for computer vision professionals. Medical image processing for the assessment and treatment of complicated illnesses from various ranges and diverse data continues to be a difficulty in providing better care. Both unsupervised and “supervised deep learning” has shown promise in the field of medical image analysis in recent years. There have been several evaluations of supervised deep learning, but there has been little research on unsupervised deep learning for medical picture processing. Over the next 15 years, “deep learning techniques” applied to “Medical Image analysis” might be a game-changing technique. Uses of deep learning in medical services will address a wide range of issues, including cancer screening and illness controlling to individualized therapy recommendations. In healthcare, “deep learning” approaches are sometimes referred to as “black boxes”. Because responsibility is now more important and can have serious legal implications, a decent predictive model is frequently insufficient. In order to gather theory-based data related to deep learning algorithms to help to analyse medical images, this research has selected a secondary qualitative method for detailed discussion.},
  keywords={Deep learning;Image analysis;Medical treatment;Predictive models;Prediction algorithms;Biomedical image processing;Task analysis;Deep learning or DL;Medical image;“Convolutional neural networks” or CNN;“Machine Learning” or ML;“Artificial Intelligence” or AI;detection;classification},
  doi={10.1109/ICACITE53722.2022.9823491},
  ISSN={},
  month={April},}@INPROCEEDINGS{10650688,
  author={Ali, Abbas Raza and Kumar, Kei and Siddiqui, Muhammad Ajmal and Zahid, Moona},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={An Open-source Cross-Industry and Cloud-agnostic Generative AI Platform}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Generative AI (GenAI) has recently gained immense popularity within the Machine Learning (ML) field due to its potential for enabling a wide array of applications. Businesses globally are eager to leverage this transformative technology; however, they encounter several challenges in utilizing the technology, including initial and running costs, data protection, infrastructure, adaptability across use-cases, accuracy, ethical implications, and the burgeoning carbon impact of GenAI, which act as barriers to entry. A low-cost, low-carbon impact GenAI platform that exclusively uses open-source components and smaller task-specific Large Language Models (LLMs) is necessary.This paper presents a GenAI platform that employs open-source components and models orchestrated for multiple cross-industry use-cases. The platform is designed to be cloud-provider agnostic, highly secure, scalable, and requires low infrastructure to deploy, thereby using comparatively less energy and having a lower carbon impact. These features enable several advantages, including domain transference, localization in different languages using prompt tuning, and easy deployment of complex multimodal, cross-industry, cross-domain use-cases. We have tested our platform on numerous use-cases across several key patterns, including text-to-text, text-to-code, text-to-analytics and text-to-image. All these configurations provide a semantic interface to unstructured text, tabular data, images, videos, and speech. The platform, the methodology, and ideas presented in this paper will provide enterprises with a blueprint of how to orchestrate GenAI using open-source components.Our experience building a multi-modal open GenAI platform and our investigation and analyses of primary patterns in enterprise use-cases for GenAI suggest that the proposed low-cost, low-carbon, model-to-the-data platform has the potential to revolutionize the adoption of GenAI technologies within business enterprises. We believe that the proposed platform can be transformative in this space, and we anticipate its future applications with excitement.},
  keywords={Adaptation models;Accuracy;Generative AI;Data security;Computational modeling;Data models;Computational efficiency;Generative AI;Large Language Models;Transformers;Natural Language Processing;Text Generation;Code Generation;Image Generation;Low Carbon Emission},
  doi={10.1109/IJCNN60899.2024.10650688},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11103526,
  author={Belludi, Suraj and Kopackova, Hana},
  booktitle={2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC)}, 
  title={Beyond AI-Powered Shopping: Can Gen Z Apply Generative AI for Business Decision?}, 
  year={2025},
  volume={},
  number={},
  pages={119-124},
  abstract={This study explores the role of generative AI (GenAI) in business decision-making among Generation Z (Gen Z), transitioning from AI-driven consumer behaviours to strategic professional applications. While Gen Z is highly proficient in using AI for personalized recommendations in consumer settings, their ability to critically engage with AI for complex decision-making in workplace environments remains a challenge. Through an experimental study, this research evaluates how Gen Z interacts with AI tools when selecting enterprise software solutions. Findings indicate that Full-Process Assistance approaches yield the most effective results, whereas Identification and Comparison approaches exhibit gaps in justification and iterative refinement. Additionally, AI's evaluation mechanisms sometimes misclassify decision success, highlighting the need for structured AI literacy programs and enhanced AI assessment frameworks. The study underscores the necessity of integrating AI literacy into educational curricula and refining AI tools to support Gen Z professionals in making informed, strategic business decisions. Future research should focus on AI-human hybrid decision models to ensure AI serves as a collaborative tool rather than an unquestioned authority in professional environments.},
  keywords={Technological innovation;Consumer behavior;Generative AI;Decision making;Refining;Employment;Software;Iterative methods;Zinc;Business;AI-assisted decision-making;Generative AI;Generation Z},
  doi={10.1109/ZINC65316.2025.11103526},
  ISSN={2995-2689},
  month={May},}@INPROCEEDINGS{10825944,
  author={Aftan, Sulaiman and Zhuang, Yu and Aseeri, Ahmad O. and Shah, Habib},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Steering a Standard Arab Language Processing Model Towards Accurate Saudi Dialect Sentiment Analysis Using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={5891-5900},
  abstract={Sentiment analysis (SA) is crucial for many NLP applications across various domains. While Arabic is one of the world’s major languages, high-quality NLP models developed for standard Arabic often underperform on regional dialects like the Saudi Dialect (SD) due to a lack of SD-specific training data. This paper presents a novel approach to adapting a high-resource language model, AraBERT, for low-resource dialect sentiment analysis by combining minimal SD data collection with generative AI. In the absence of openly accessible SD datasets, we augmented a small amount of collected SD data with GPT-generated SD data to fine-tune AraBERT for sentiment analysis in SD. Our contributions include (1) demonstrating the feasibility of low-effort data collection of a low-resource dialect for adapting existing high-resource NLP models and (2) leveraging GPT-generated data to augment collected data to enhance a high-resource language model for sentiment classification in a low-resource dialect, achieving significant improvements over the pre-trained high-resource model. These two contributions imply a potentially replicable approach that can serve as a template for future research in other low-resource NLP tasks. This paper presents a promising solution for enhancing model performance in low-resource dialects and has implications for similar under-resourced languages.},
  keywords={Analytical models;Adaptation models;Sentiment analysis;Visualization;Accuracy;Generative AI;Training data;Predictive models;Data models;Standards;Generative AI;Sentiment Analysis;Saudi Dialect;NLP},
  doi={10.1109/BigData62323.2024.10825944},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10450958,
  author={Li, Yanlong and Ye, Zhenhao and Wang, Yang and Zhang, Chi and Liu, Yuehu},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Unsupervised Video Anomaly Detection by Learning Consistency of Appearance and Motion}, 
  year={2023},
  volume={},
  number={},
  pages={9431-9436},
  abstract={Unsupervised video anomaly detection aims to train the model on unlabeled videos containing only normal behavior and identify data instances with abnormal objects or behaviors as anomalies. Abnormal events are typically detected due to their unique appearance and motion features. Existing methods mainly rely on various appearance-motion feature fusion techniques to model the information from both modalities for anomaly detection. However, they overlook the semantic consistency between motion and appearance information, resulting in constrained model performance. To address this limitation, we directly model the relationship of semantic consistency between appearance and motion. Given the varying importance of different pixels, manually designing feature fusion methods to represent their consistency is impractical. So, we designed a two-stream feature fusion module based on cross-attention. This module constructs information fusion between appearance and motion semantics in normal videos, dynamically capturing the inherent appearance-motion semantic consistency. We further believe that, regardless of the perspective, the appearance-motion semantic consistency should be similar. Therefore, this paper introduces a new consistency loss, urging the model to fully model the semantic representation of consistency between appearance and motion in normal data, thereby identifying anomalies with lower consistency. Experimental results on various standard datasets and ablation studies have demonstrated the effectiveness of the proposed method.},
  keywords={Automation;Semantics;Feature extraction;Data models;Behavioral sciences;Object recognition;Anomaly detection;video anomaly detection;unsupervised learning;AutoEncoder;feature fusion},
  doi={10.1109/CAC59555.2023.10450958},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{11092391,
  author={Bai, Tongyuan and Bai, Wangyuanfan and Chen, Dong and Wu, Tieru and Li, Manyi and Ma, Rui},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts}, 
  year={2025},
  volume={},
  number={},
  pages={5893-5903},
  abstract={Controllability plays a crucial role in the practical applications of 3D indoor scene synthesis. Existing works either allow rough language-based control, that is convenient but lacks fine-grained scene customization, or employ graph-based control, which offers better controllability but demands considerable knowledge for the cumbersome graph design process. To address these challenges, we present FreeScene, a user-friendly framework that enables both convenient and effective control for indoor scene synthesis. Specifically, FreeScene supports free-form user inputs including text description and/or reference images, allowing users to express versatile design intentions. The user inputs are adequately analyzed and integrated into a graph representation by a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion Transformer, which performs graph-aware denoising to enhance scene generation. Our MG-DiT not only excels at preserving graph structure but also offers broad applicability to various tasks, including, but not limited to, text-to-scene, graph-to-scene, and rearrangement, all within a single model. Extensive experiments demonstrate that FreeScene provides an efficient and user-friendly solution that unifies text-based and graph-based scene synthesis, outperforming state-of-the-art methods in terms of both generation quality and controllability in a range of applications.},
  keywords={Solid modeling;Three-dimensional displays;Retrieval augmented generation;Noise reduction;Process control;Controllability;Transformers;Pattern recognition;Periodic structures;Overfitting;diffusion model;indoor scene synthesis;layout generation},
  doi={10.1109/CVPR52734.2025.00553},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{11076643,
  author={Puvvadi, Meghana and Arava, Sai Kumar and Santoria, Adarsh and Chennupati, Sesha Sai Prasanna and Puvvadi, Harsha Vardhan},
  booktitle={2025 Global Conference in Emerging Technology (GINOTECH)}, 
  title={Survey of Marketing Agents, Agent-Based Models, and Generative AI in Marketing}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Generative AI and Agent-Based Models (ABMs) are transforming marketing with the ability to develop accurate, adaptive, and consumer-centered strategies. ABMs model interactions among agents such as consumers, firms, and influencers to reveal behavioral patterns and campaign optimization. At the same time, Large Language Models (LLMs) are elevating content creation, customer experience, and campaign management. This survey addresses where these technologies overlap to power contemporary marketing innovations, resolving for privacy, scalability, and ethics. Drawing on empirical case studies and current advances in multimodal and reinforcement learning, the book targets hybrid frameworks as well as future paths in ethical AI, green algorithms, and additional personalization.},
  keywords={Surveys;Ethics;Technological innovation;Adaptation models;Consumer behavior;Accuracy;Generative AI;Biological system modeling;Large language models;Sustainable development;Agent-Based Models;Generative AI;Marketing Ecosystems;Personalized Campaigns;Ethical AI;Consumer Behavior Modeling;Hybrid Frameworks},
  doi={10.1109/GINOTECH63460.2025.11076643},
  ISSN={},
  month={May},}@INPROCEEDINGS{10651441,
  author={Jiang, Hongyang and Zhao, Yonghe and Huang, Qiang and Cao, Yangkun and Sun, Huiyan and Chang, Yi},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Estimating Individual Causal Treatment Effect by Variable Decomposition}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Estimating individual-level causal effects is crucial for decision-making in various domains, such as personalized healthcare, social marketing, and public policy. Addressing confounding bias is a critical step in accurately estimating the causal effects of treatments on outcomes. However, many current causal inference approaches consider all observed variables as confounders without distinguishing them from colliders or indirect (two-order) colliders. This may lead to M-bias when improperly eliminating confounding bias. In this study, we propose a new framework to accurately estimate individual-level treatment effects by considering a causal structure that includes both confounding variables and indirect colliders. Specifically, we first perform a sample reweighting to approximately eliminate confounding bias. Then, we restore the covariate’ potential latent parents and extract the modules solely related to the outcome. Finally, we take both these modules with the treatment variables to infer counterfactuals for causal inference. To validate the effectiveness of our proposed approach, we conduct extensive experiments on synthetic and commonly used semi-synthetic benchmark datasets. The experimental results demonstrate that our method outperforms current state-of-the-art methods.},
  keywords={Neural networks;Decision making;Estimation;Medical services;Benchmark testing;Public policy;Synthetic data;Causal inference;causal effect;treatment effect;spurious association;M-bias},
  doi={10.1109/IJCNN60899.2024.10651441},
  ISSN={2161-4407},
  month={June},}@ARTICLE{9693190,
  author={Yan, Han and Zhang, Haijun and Liu, Linlin and Zhou, Dongliang and Xu, Xiaofei and Zhang, Zhao and Yan, Shuicheng},
  journal={IEEE Transactions on Multimedia}, 
  title={Toward Intelligent Design: An AI-Based Fashion Designer Using Generative Adversarial Networks Aided by Sketch and Rendering Generators}, 
  year={2023},
  volume={25},
  number={},
  pages={2323-2338},
  abstract={The traditional fashion industry is heavily dependent on designers whose talent and vision have a significant impact on their innovative designs. Through taking advantage of recent advances in image-to-image translation by generative adversarial networks (GANs), marked improvement in designers’ efficiency is now possible. Considering both randomness and controllability in the design process, this article presents a novel artificial intelligence (AI)-based framework for fashion design. Under this framework, a sketch-generation module which is based on latent space is firstly introduced for designing various sketches. Secondly, a rendering-generation module is proposed to learn mapping between textures and sketches to complete the task of fashion design. In order to achieve effectiveness in synthesizing semantic-aware textures on sketches, a multi-conditional feature interaction module is developed in the rendering-generation model. Moreover, two different training schemes are introduced to optimize both the sketch-generation module and the rendering-generation module. In order to evaluate the performance of our proposed models, we built a large-scale dataset which consists of 115,584 pairs of fashion item images. Experimental results demonstrate the effectiveness of our proposed method, and indicate that our model can facilitate designers’ design process by taking full advantage of the controllability of different conditions (e.g., sketch and texture) and the randomness of latent space.},
  keywords={Rendering (computer graphics);Training;Clothing;Solid modeling;Image synthesis;Aerospace electronics;Process control;Fashion design;generative adversarial network;image translation;fashion data},
  doi={10.1109/TMM.2022.3146010},
  ISSN={1941-0077},
  month={},}@ARTICLE{9543528,
  author={Gao, Zan and Zhao, Yibo and Zhang, Hua and Chen, Da and Liu, An-An and Chen, Shengyong},
  journal={IEEE Transactions on Cybernetics}, 
  title={A Novel Multiple-View Adversarial Learning Network for Unsupervised Domain Adaptation Action Recognition}, 
  year={2022},
  volume={52},
  number={12},
  pages={13197-13211},
  abstract={Abstract-domain adaptation action recognition is a hot research topic in machine learning and some effective approaches have been proposed. However, samples in the target domain with label information are often required by these approaches. Moreover, domain-invariant discriminative feature learning, feature fusion, and classifier module learning have not been explored in an end-to-end framework. Thus, in this study, we propose a novel end-to-end multiple-view adversarial learning network (MAN) for unsupervised domain adaptation action recognition in which the fusion of RGB and optical-flow features, domain-invariant discrimination feature learning, and action recognition is conducted in a unified framework. Specifically, a robust spatiotemporal feature extraction network, including a spatial transform network and an adaptive intrachannel weight network, is proposed to improve the scale invariance and robustness of the method. Then, a self-attention mechanism fusion module is designed to adaptively fuse the RGB and optical-flow features. Moreover, a multiview adversarial learning loss is developed to obtain domain-invariant discriminative features. In addition, three benchmark datasets are constructed for unsupervised domain adaptation action recognition, for which all actions and samples are carefully collected from public action datasets, and their action categories are hierarchically augmented, which can guide how to extend existing action datasets. We conduct extensive experiments on four benchmark datasets, and the experimental results demonstrate that our proposed MAN can outperform several state-of-the-art unsupervised domain adaptation action recognition approaches. When the SDAI Action II-6 and SDAI Action II-11 datasets are used, MAN can achieve 3.7% ( $H \to U$ ) and 6.1% ( $H \to U$ ) improvements over the temporal attentive adversarial adaptation network (published in ICCV 2019) module, respectively. As an added contribution, the SDAI Action II-6, SDAI Action II-11, and SDAI Action II-16 datasets will be released to facilitate future research on domain adaptation action recognition.},
  keywords={Feature extraction;Convolution;Training;Adversarial machine learning;Target recognition;Robustness;Multiple-view adversarial learning;robust spatiotemporal feature extraction;self-attention mechanism fusion;unsupervised domain adaptation action recognition},
  doi={10.1109/TCYB.2021.3105637},
  ISSN={2168-2275},
  month={Dec},}@INPROCEEDINGS{10732856,
  author={Mertzani, Asimina and Pitt, Jeremy},
  booktitle={2024 IEEE International Symposium on Technology and Society (ISTAS)}, 
  title={Social Implications of Socially-Guided Machine Learning for Innovation Support}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Automated decision support might be insufficient for managing wicked problems; instead, new social arrangements might be required to alter the trajectory of a social system. Since socio-technical systems will comprise human and computational intelligences interacting, we can leverage that interaction by using Artificial Intelligence (AI) to build innovation-support systems (ISS), which interleaves human expertise, generative AI and multi-agent simulation to imagine alternative social arrangements and envision societal trajectories. We define a socially-guided machine-learning framework that provides a ‘safe space’ for exploration and an evidence-base for selecting and modifying social arrangements. The significance of socially guided machine learning for innovation support is to assist societies with mitigating wicked problems through co-production with AI.},
  keywords={Technological innovation;Sociotechnical systems;Generative AI;Machine learning;Trajectory;Innovation;Decision Support Systems;Socially-Guided Machine Learning;Social Implications of Technology},
  doi={10.1109/ISTAS61960.2024.10732856},
  ISSN={2158-3412},
  month={Sep.},}@ARTICLE{10576683,
  author={Liu, Minjie and Wang, Hongjian and Yoon, Kuk-Jin and Wang, Lin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Disentangled Cross-modal Fusion for Event-Guided Image Super-resolution}, 
  year={2024},
  volume={5},
  number={10},
  pages={5314-5324},
  abstract={Event cameras detect the intensity changes and produce asynchronous events with high dynamic range and no motion blur. Recently, several attempts have been made to superresolve the intensity images guided by events. However, these methods directly fuse the event and image features without distinguishing the modality difference and achieve image superresolution (SR) in multiple steps, leading to error-prone image SR results. Also, they lack quantitative evaluation of real-world data. In this article, we present an end-to-end framework, called event-guided image (EGI)-SR to narrow the modality gap and subtly integrate the event and RGB modality features for effective image SR. Specifically, EGI-SR employs three crossmodality encoders (CME) to learn modality-specific and modality-shared features from the stacked events and the intensity image, respectively. As such, EGI-SR can better mitigate the negative impact of modality varieties and reduce the difference in the feature space between the events and the intensity image. Subsequently, a transformer-based decoder is deployed to reconstruct the SR image. Moreover, we collect a real-world dataset, with temporally and spatially aligned events and color image pairs. We conduct extensive experiments on the synthetic and real-world datasets, showing EGI-SR favorably surpassing the existing methods by a large margin.},
  keywords={Image reconstruction;Superresolution;Transformers;Image edge detection;Event detection;Data integration;Bio-inspired engineering;Event-based vision;feature fusion;image superresolution},
  doi={10.1109/TAI.2024.3418376},
  ISSN={2691-4581},
  month={Oct},}@ARTICLE{9756908,
  author={Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan and Shao, Ling},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning Enriched Features for Fast Image Restoration and Enhancement}, 
  year={2023},
  volume={45},
  number={2},
  pages={1934-1948},
  abstract={Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2, achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2.},
  keywords={Feature extraction;Image restoration;Streaming media;Spatial resolution;Image denoising;Cameras;Superresolution;Multi-scale feature representation;dual-pixel defocus deblurring;image denoising;super-resolution;low-light image enhancement;and contrast enhancement},
  doi={10.1109/TPAMI.2022.3167175},
  ISSN={1939-3539},
  month={Feb},}@INPROCEEDINGS{10203955,
  author={Wu, Sijing and Yan, Yichao and Li, Yunhao and Cheng, Yuhao and Zhu, Wenhan and Gao, Ke and Li, Xiaobo and Zhai, Guangtao},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GANHead: Towards Generative Animatable Neural Head Avatars}, 
  year={2023},
  volume={},
  number={},
  pages={437-447},
  abstract={To bring digital avatars into people's lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel generative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of implicit representations. Specifically, GANHead represents coarse geometry, fine-gained details and texture via three networks in canonical space to obtain the ability to generate complete and realistic head avatars. To achieve flexible animation, we define the deformation filed by standard linear blend skinning (LBS), with the learned continuous pose and expression bases and LBS weights. This allows the avatars to be directly animated by FLAME [22] parameters and generalize well to unseen poses and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves superior performance on head avatar generation and raw scan fitting.},
  keywords={Geometry;Deformation;Avatars;Fitting;Fires;Aerospace electronics;Rendering (computer graphics);3D from multi-view and sensors},
  doi={10.1109/CVPR52729.2023.00050},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9459015,
  author={Zhang, Hong and Liu, Mingzhe and Wang, Tao and Jiang, Xin and Liu, Bingqi and Dai, Pengyu},
  booktitle={2021 4th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={An Overview of Landslide Detection: Deep Learning and Machine Learning Approaches}, 
  year={2021},
  volume={},
  number={},
  pages={265-271},
  abstract={Landslide is a devastating natural disaster with the frequent occurrence and tremendous destructive power. Once it happened, human society, the safety of life and property, and the natural environment would suffer enormous losses. The purpose of landslide research is to reduce landslide occurrence probability through manual intervention to some extent, in which landslide detection is one of the fundamental researches in this field. For state-of-art studies, the hotspot of landslide detection primarily focuses on Deep Learning (DL) and Machine Learning (ML) approaches. In this paper, we summarize the primary works in the field of landslide research firstly. Then the acquisition and usage of landslide data for DL and ML approaches are introduced. Next, the most frequently used evaluation indexes of object detection and image segmentation. Finally, the relevant progress of DL and ML approaches in landslide detection research are reviewed. Meanwhile, the challenges and future research directions in this field are further discussed.},
  keywords={Deep learning;Image segmentation;Object detection;Manuals;Learning (artificial intelligence);Big Data;Terrain factors;landslide detection;dataset;deep learning;machine learning},
  doi={10.1109/ICAIBD51990.2021.9459015},
  ISSN={},
  month={May},}@ARTICLE{9785372,
  author={Uzolas, Lukas and Rico, Javier and Coupé, Pierrick and SanMiguel, Juan C. and Cserey, György},
  journal={IEEE Access}, 
  title={Deep Anomaly Generation: An Image Translation Approach of Synthesizing Abnormal Banded Chromosome Images}, 
  year={2022},
  volume={10},
  number={},
  pages={59090-59098},
  abstract={Advances in deep-learning-based pipelines have led to breakthroughs in a variety of microscopy image diagnostics. However, a sufficiently big training data set is usually difficult to obtain due to high annotation costs. In the case of banded chromosome images, the creation of big enough libraries is difficult for multiple pathologies due to the rarity of certain genetic disorders. Generative Adversarial Networks (GANs) have proven to be effective in generating synthetic images and extending training data sets. In our work, we implement a conditional GAN (cGAN) that allows generation of realistic single chromosome images following user-defined banding patterns. To this end, an image-to-image translation approach based on automatically created 2D chromosome segmentation label maps is used. Our validation shows promising results when synthesizing chromosomes with seen as well as unseen banding patterns. We believe that this approach can be exploited for data augmentation of chromosome data sets with structural abnormalities. Therefore, the proposed method could help to tackle medical image analysis problems such as data simulation, segmentation, detection, or classification in the field of cytogenetics.},
  keywords={Biological cells;Image segmentation;Biomedical imaging;Interpolation;Training data;Computer vision;Generative adversarial networks;Deep learning;Biomedical imaging;chromosomes;computer vision;deep learning;generative adversarial networks;image processing},
  doi={10.1109/ACCESS.2022.3178786},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9862723,
  author={Zhou, Ruizhi and Pan, Yanling},
  booktitle={2022 IEEE 5th International Conference on Big Data and Artificial Intelligence (BDAI)}, 
  title={FloodDAN: Unsupervised Flood Forecasting based on Adversarial Domain Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={6-12},
  abstract={As a natural disaster, floods can cause enormous social and economic losses. However, both physical models and learning-based flood forecasting models require massive historical flood data to fit the model parameters. When come to new site that does not have sufficient historical data, the model performance will drop dramatically due to the overfitting problem. In this paper, we propose a Flood Domain Adaptation Network (FloodDAN) that is able to perform unsupervised flood forecasting. Specifically, we design a two-stage learning procedure to train our FloodDAN: in the first stage, we pretrain a rainfall encoder and a prediction head to learn general transferable hydrological knowledge on large-scale source domain data; in the second stage, we transfer the knowledge in the pretrained encoder into the rainfall encoder of target domain through adversarial domain alignment. During inference, we utilize the target domain rainfall encoder trained in the second stage and the prediction head trained in the first stage to get flood forecasting predictions. Experimental results on Tunxi and Changhua flood dataset show that our proposed FloodDAN can perform flood forecasting effectively with zero target domain supervision. The performance of the FloodDAN is on par with supervised models that uses 450–500 hours of supervision.},
  keywords={Adaptation models;Costs;Biological system modeling;Supervised learning;Learning (artificial intelligence);Predictive models;Data models;Flood Forecasting;Adversarial learning;Domain adaptation;Deep learning},
  doi={10.1109/BDAI56143.2022.9862723},
  ISSN={},
  month={July},}@ARTICLE{10551792,
  author={Herr, Anna and Herr, Quentin},
  journal={IEEE Spectrum}, 
  title={A Data Center in a Shoebox: IMEC's Plan to use Superconductors to Shrink Computers}, 
  year={2024},
  volume={61},
  number={6},
  pages={37-41},
  abstract={What's more, this projection was made before the sudden explosion of generative AI. The amount of computing resources used to train the largest AI models has been doubling roughly every 6 months for more than the past decade. At this rate, by 2030 training a single artificial-intelligence model would take one hundred times as much computing resources as the combined annual resources of the current top 10 super-computers. Simply put, computing will require colossal amounts of power, soon exceeding what our planet can provide.},
  keywords={Training;Data centers;Generative AI;Computational modeling;Explosions;Artificial intelligence},
  doi={10.1109/MSPEC.2024.10551792},
  ISSN={1939-9340},
  month={June},}@INPROCEEDINGS{10674465,
  author={Hsu, Chung-Chian and Wu, I-Zhen and Liu, Shih-Mao},
  booktitle={2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={Decoding AI Complexity: SHAP Textual Explanations via LLM for Improved Model Transparency}, 
  year={2024},
  volume={},
  number={},
  pages={197-198},
  abstract={With the continuous advancement of artificial intelligence (AI), particularly in widespread domains such as healthcare and environmental applications, there is an increasing demand for model interpretability. Understanding the decision-making process of models contributes to building trust in them. Hence, the development of Explainable AI (XAI) has become crucial. This study proposes an approach to generate text via a large language model (LLM) for interpretation to enhance the interpretability of SHAP (Shapley Additive exPlanations) plots. The goal is to make the interpretability of model decisions accessible even to non-IT experts through textual explanations.},
  keywords={Additives;Explainable AI;Large language models;Decision making;Buildings;Medical services;Decoding;Explainable AI;Generative AI;SHAP;LLM},
  doi={10.1109/ICCE-Taiwan62264.2024.10674465},
  ISSN={2575-8284},
  month={July},}@INPROCEEDINGS{9643321,
  author={Dai, Ruiqi and Lefort, Mathieu and Armetta, Frédéric and Guillermin, Mathieu and Duffner, Stefan},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Novelty detection for unsupervised continual learning in image sequences}, 
  year={2021},
  volume={},
  number={},
  pages={493-500},
  abstract={Recent works in the domain of deep learning for object recognition on common image classification benchmarks often address the representation learning problem under the assumption of i.i.d. input data. Although achieving satisfying results, this assumption seems not realistic when agents have to learn autonomously. An autonomous agent receives a continual visual flow of objects which is far from an i.i.d. distribution of objects. Moreover, agents have to construct their representations of the world and adapt to unknown environments, without relying on external sources of information such as labels that would be provided post-classification and are unavoidable when an over-segmentation is done. Then, in order to exploit the learned representation effectively for object recognition, a clear and meaningful relationship w.r.t. real object categories is required, which has been largely neglected in existing unsupervised algorithms.In this paper, we propose a novelty detection method for continual and unsupervised object recognition, as an extension for the recent CURL model, which allows to moderate over-segmentation while preserving accuracy, in order to meet the requirements for autonomy. We experimentally validated our approach on two standard image classification benchmarks, MNIST and Fashion-MNIST, in this unsupervised and continual learning setting and improve the state of the art in terms of cluster purity, which is crucial for subsequent object recognition, since it facilitates clustering when information on ground truth labels is not available for free.},
  keywords={Representation learning;Visualization;Image recognition;Learning (artificial intelligence);Benchmark testing;Image sequences;Object recognition;Continual learning;class-incremental learning;novelty Detection;object recognition;unsupervised learning},
  doi={10.1109/ICTAI52525.2021.00080},
  ISSN={2375-0197},
  month={Nov},}@INBOOK{10953197,
  author={Shrivastava, Prashant Kumar and Chaturvedi, Ashish and Kamble, Megha and Jain, Megha},
  booktitle={The New Advanced Society: Artificial Intelligence and Industrial Internet of Things Paradigm}, 
  title={Categorization Model for Parkinson's Disease Occurrence and Severity Prediction}, 
  year={2022},
  volume={},
  number={},
  pages={163-189},
  abstract={Summary <p>Machine learning and AI center on empowering PC programs naturally to build their exhibition at certain undertakings through old experience. Healthcare is one of a zone where the utilization of AI can be exceptionally useful. This work depicts the investigation of a few AI procedures for medical healthcare. AI gives the answer for a large number of the old and new healthcare difficulties; AI is comprehensively examined by different scientists and medical professionals. Here, this part introduces a review of healthcare and how AI can be utilized to help diagnosis.</p> <p>In the case of medical diagnosis, many information&#x2010;based frameworks are utilized to computerize various activities. For physical impairment based diseases such as Parkinson's disease, models are generated using gazzetts, questionnaires, surveys, puzzles, activities etc. These Knowledge&#x2010;based frameworks can be collaborated with AI and machine learning techniques. AI calculations help in mechanizing the tedious procedure of information assembling that is fundamentally for improvement of information&#x2010;based framework.</p> <p>Machine learning and AI empowers PC programs consequently to expand their presentation for certain assignments through old information. This field is generally identified with measurable derivation and example acknowledgment. A decent effect of research in AI is center around characterization, the activity of model advancement, from a lot of recently ordered models, which can accurately sort new models from the equivalent dataset.</p> <p>Parkinson disease (PD) is most typical to handle and irreparable loss cannot be recovered just by medication. Even severity of the same can damage the person and quality of life degrades drastically. Early diagnosis of Parkinson's will be helpful for elderly person. If examined later this ailment can become hopeless. Henceforth, evaluating the illness at an early stage is critical. Diagnosis of the disease can be done using three major activities. Motor movement, behaviour and Voice impairment. This chapter introduces the investigation for the finding of Parkinson's diagnosis using different ML (Machine Learning) for categorization and severity prediction through the measure of 16 voice and 8 kinematic feature information accomplished from various archives [1]. The data set includes 40 people with Parkinson's disease and healthy patients generated with the help of spiral drawings and voice readings. Various machine learning algorithms estimate, including the highest accuracy (94.87 percent) demonstrated by ANN, while Na&#xef;ve Byes has indicated the least precision (71.79%). The work also predicted severity score by suggesting some scientific measure with prototype dataset.</p>},
  keywords={Diseases;Artificial intelligence;Spirals;Medical diagnostic imaging;Accuracy;Feature extraction;Support vector machines;Kinematics;Hands;Speech analysis},
  doi={10.1002/9781119884392.ch8},
  ISSN={},
  publisher={Wiley},
  isbn={9781119884385},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953197},}@INPROCEEDINGS{10948003,
  author={Padhy, Sasmita and Suman, Preetam and Suman, Amrit and Das, Sachikant and Siddiqui, Masood H},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={Automated Classification of Mouth Cancers for Early Cancer Detection Using a VGG-16 Inspired CNN}, 
  year={2024},
  volume={},
  number={},
  pages={1200-1205},
  abstract={Mouth cancers, a risky sort of cancer, are typically caused by tobacco use and cigarette smoking. Early detection of oral cancer is essential for green remedy and affected person survival. This study pursues to broaden an automatic approach for diagnosing sicknesses, which can come across probably cancerous oral lesions at an early degree, saving lives. A novel convolutional neural network (CNN) structure, stimulated via the VGG-sixteen community, has been particularly designed for this purpose. The look evaluates various gadgets getting to know and deep studying fashions, highlighting their effectiveness in identifying and categorizing oral cancers. The Convolutional Neural Network (CNN) version gives excessive accuracy in distinguishing between ordinary and cancerous pictures, indicating the capacity for stepped-forward early detection of oral cancerous tumors. This look underscores the efficacy of computerized diagnostic tools in detecting oral cancer early, permitting spark-off and potentially life-saving intervention.},
  keywords={Accuracy;Computational modeling;Mouth;Feature extraction;Cancer detection;Convolutional neural networks;Lesions;Artificial intelligence;Cancer;Mouth cancer;CNN;Texture features;ML models;ANN},
  doi={10.1109/GlobalAISummit62156.2024.10948003},
  ISSN={},
  month={Sep.},}@INBOOK{10952929,
  author={Blount, Jeb and Iannarino, Anthony},
  booktitle={The AI Edge: Sales Strategies for Unleashing the Power of AI to Save Time, Sell More, and Crush the Competition}, 
  title={If It Quacks Like a Duck}, 
  year={2024},
  volume={},
  number={},
  pages={103-108},
  abstract={Summary <p>There are always people who want to cheat nature. These days, sales reps are sending prospects a massive amount of email that is clearly written by robots. Artificial Intelligence's (AI's) patterns stand out and are so easy to spot that it takes mere seconds to mark these messages as spam and block the senders. This is exactly why the major email service providers have put into place rules to end this practice. This disingenuous approach does not work and has quickly led to the demise and devaluation of prospecting email. Prospects and customers everywhere are taking notice. The human advantage is authenticity, empathy, creativity, randomness, fallibility, and unpredictability. While generative AI can process information and generate text at superhuman speeds and volumes, it lacks genuine understanding or emotional context. To get the most out of AI as a writing tool, start by getting better at the fundamentals of writing, including sentence structure, grammar, and editing.</p>},
  keywords={Artificial intelligence;Robots;Electronic mail;Chatbots;Oral communication;Generative AI;Feeds;Companies;Social networking (online);Reviews},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394244492},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952929},}@ARTICLE{10737043,
  author={Nissar, Mehvish and Subudhi, Badri Narayan and Mishra, Amit Kumar and Jakhetiya, Vinit},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={NICASU: Neurotransmitter Inspired Cognitive AI Architecture for Surveillance Underwater}, 
  year={2025},
  volume={6},
  number={3},
  pages={626-638},
  abstract={The human brain is exceedingly good at learning rich narratives from highly limited experiences. One of the ways this is achieved in our brain is through neuromodulators or neurotransmitters, such as dopamine and nor-epinephrine, in cortical circuits. In terms of symbolic processing, these neuromodulators add “salience” to various emotions and experiences. A salience-based neural network (SANN) architecture was proposed in [1]. We have taken this architecture and have developed a discriminator to enable efficient change detection for underwater applications. In the context of underwater, surveillance can be elucidated as one of the processes of detecting and tracking the moving objects present in underwater videos. Several researchers working on the same tried to develop different techniques for identifying moving objects from outdoor scenes. However, while applying the same for underwater environments, it is found to be unable to preserve the minute details that are important for defining an object's boundary. This is mainly due to the complex scene dynamics of the aquatic environment. Moreover, the intricate natural properties of water and some of its characteristics, such as excessive turbidity, scattering, and low visibility, also make the task of detecting the object present in underwater videos extremely challenging. In this regard, we put forth an adversarial learning-based end-to-end deep learning architecture inspired by the way neurotransmitters work in the human brain to detect underwater moving objects. The proposed architecture uses two modules for underwater object detection. The initial module is a generator composed of a probabilistic learner which is based on multiple down- and up-sampling modules. Further, the discriminator network is composed of a multilevel feature-concatenation component, which can perpetuate specifics at distinct levels. The effectiveness of the proposed method (PM) is confirmed using the underwater change detection and Fish4Knowledge benchmark datasets by contrasting its outcomes with those of different state-of-the-art methods.},
  keywords={Feature extraction;Videos;Object detection;Artificial intelligence;Surveillance;Deep learning;Neurotransmitters;Fish;Cameras;Adaptation models;Adversarial learning;deep learning;multilevel feature concatenation;neuromodulators;neurotransmitters;underwater object detection},
  doi={10.1109/TAI.2024.3486675},
  ISSN={2691-4581},
  month={March},}@INPROCEEDINGS{11167331,
  author={Chandel, Garima and Kumar, Ankit and Malik, Krishan and Gurani, Kunal and Gahlawat, Krrish and Saini, Sandeep Kumar},
  booktitle={2025 IEEE International Conference on Computer, Electronics, Electrical Engineering & their Applications (IC2E3)}, 
  title={Deepfake Detection Using AI And Machine Learning Algorithms}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Deepfake technology has rapidly evolved, which makes it difficult to differentiate between real and AI-generated content. The malicious use of deep fake technology has raised some serious concerns. Deepfake face-swapping is commonly practiced across the internet and has raised a huge number of concerns in society. How to detect deepfake content is nowadays a very popular research topic. This paper presents a comprehensive study of deep fake detection techniques using Artificial intelligence (AI) and machine learning (ML) algorithms. In this work, various detection techniques have been analysed using different approaches, including audio, image, and video-based deepfake detection models. The study highlights recent advancements, challenges, methods and potential future research directions. Furthermore, the legal concerns about using deepfake technology in unethical ways have been discussed. Additionally, this paper also examines the effectiveness of different datasets and benchmarks in detecting deepfake content.},
  keywords={Deepfakes;Machine learning algorithms;Text analysis;Forensics;Self-supervised learning;Media;Benchmark testing;Transformers;Forgery;Standards;Deepfake;Deep Learning;Machine Learning;Video Forensics;Generative Adversarial Networks (GANs);Deep-fake Detection;Cross-Modal Detection},
  doi={10.1109/IC2E365635.2025.11167331},
  ISSN={},
  month={May},}@INBOOK{10952510,
  author={Blount, Jeb and Iannarino, Anthony},
  booktitle={The AI Edge: Sales Strategies for Unleashing the Power of AI to Save Time, Sell More, and Crush the Competition}, 
  title={The Six Million Dollar Man}, 
  year={2024},
  volume={},
  number={},
  pages={14-19},
  abstract={Summary <p>As a kid, the author was obsessed with The Six Million Dollar Man TV show. The author had the action figures and fantasized constantly about being a dynamic, part&#x2010;man, part&#x2010;robot. Today, we are closer than ever before to making this core theme of sci&#x2010;fi a reality. Artificial intelligence is, and though we are still at the dawn of this cutting&#x2010;edge revolution, visionaries are already working on ways to plug AI directly into the human brain. This chapter shows how generative AI will help us do the things we do today faster and better: personalized content creation, prospecting messages, building prospecting lists, automated follow&#x2010;ups, advanced sales forecasting, and virtual role&#x2010;playing for training. In a rapidly evolving world dominated by AI and technology, the human touch is an indispensable competitive advantage for delivering a powerful, personalized, and humanized buying experience that extends across the entire customer journey.</p>},
  keywords={Artificial intelligence;Generative AI;Behavioral sciences;Stakeholders;Oral communication;Neural implants;Hands;Forecasting;Buildings;Writing},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394244492},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952510},}@INPROCEEDINGS{11031483,
  author={P, Kumar and G L, Shreenidhi and M, Rakesh Kumar and S, Sowmiya},
  booktitle={2025 International Conference on Frontier Technologies and Solutions (ICFTS)}, 
  title={Medify-AI based LLM Based Healthcare System}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Artificial Intelligence stands at the helm of the dramatic change in the management of illness in the health care sector. The ability of Artificial Intelligence to handle large-scale data analysis, thus aiding clinical decision-making, and automating diagnostic procedures portends new hope in facing global predictive care. According to current research on healthcare technology, LLM and artificial intelligence are instrumental components of medical services. With the aim of designing a more advanced healthcare system, using Large Language Models (LLM) to enhance medical diagnostics by individualised health management is one approach. It may help the user with supplements using the symptoms they give. An integrated system also includes offering you a medical chatbot to be your virtual health assistant. It analyses your health data using advanced AI and provides real-time answers to your questions. It will give you basic advice and can help prepare you for a further knowledgeable conversation with medical experts. It is accurate and offers personalized information based on your health profile; it is available anytime, anywhere to answer questions about symptoms or to request assistance from the medical world. It can be applied to anyone, regardless of age, from young adults to old people, due to its design. The general public will live healthier, better lives as a result of using the healthcare system.},
  keywords={Technological innovation;Accuracy;Sensitivity;Large language models;Scalability;Medical services;Real-time systems;Public healthcare;Predictive analytics;Medical diagnostic imaging;LLM in healthcare;Gen AI;Health App;Med Palm2;GAN;NLP;BERT;Electronic Health Records (EHR);Virtual Health Assistant},
  doi={10.1109/ICFTS62006.2025.11031483},
  ISSN={},
  month={March},}@INPROCEEDINGS{10039545,
  author={Bas, Anil and Topal, M. Onat and Duman, Çağdaş and van Heerden, Imke},
  booktitle={2022 International Conference on Computer and Applications (ICCA)}, 
  title={A Brief History of Deep Learning-Based Text Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={A dynamic domain in Artificial Intelligence research, Natural Language Generation centres on the automatic generation of realistic text. To help navigate this vast and swiftly developing body of work, the study provides a concise overview of noteworthy stages in the history of text generation. To this end, the paper describes deep learning models for a broad audience, focusing on traditional, convolutional, recurrent and generative adversarial networks, as well as transformer architecture.},
  keywords={Deep learning;Navigation;Natural languages;Neural networks;Focusing;Computer architecture;Transformers;natural language generation;text generation;deep learning;neural networks;transformer},
  doi={10.1109/ICCA56443.2022.10039545},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10541758,
  author={Singh, J.N. and Gautam, Ashutosh and Tomar, Harsh},
  booktitle={2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)}, 
  title={Deep Fake in picture using Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1107},
  abstract={Deepfake is a system that combines fake pictures and videos with deep learning. Deep learning is the source of Deepfake. The unethical practice of creating falsified photographs and movies is now possible because of neoteric advances in the fields of Artificial Intelligence and Machine Learning. Today, it is very easy to create photo simulative images using generative adversarial networks. These fake images and videos are widely available on the internet and social media. It is difficult to tell which of them is real or not. These images are typically taken with the goal of stirring up social disturbance, political turmoil, or distributing false information among the general public. Viewers will easily comprehend these images because they will look to be real. Deepfake is rapidly harming individuals, communities, companies, security, religion, and democracy, according to recent studies. These videos and photographs are of astounding quality, and they have a huge social media reach. It has far-reaching effects that are destructive beyond comprehension. An extensive overview of the different deep-fake methods is provided in this publication. So, the objective of this paper is to identify these fake images using a conventional neural network. In order to address this issue, we train a model for particular datasets, produce deep fakes, and then use that model to try to identify the deep fake. An authentic and false image is required for the training process in order to train the model.},
  keywords={Deep learning;Training;Deepfakes;Social networking (online);Pipelines;Neural networks;Motion pictures;CNN;Image Detection;RNN},
  doi={10.1109/ICAC3N60023.2023.10541758},
  ISSN={},
  month={Dec},}@ARTICLE{9312188,
  author={Lu, Hsueh-Ping and Su, Chao-Ton},
  journal={IEEE Transactions on Semiconductor Manufacturing}, 
  title={CNNs Combined With a Conditional GAN for Mura Defect Classification in TFT-LCDs}, 
  year={2021},
  volume={34},
  number={1},
  pages={25-33},
  abstract={Mura defect classification is a critical concern for thin-film transistor liquid crystal display (TFT-LCD) manufacturers. In recent years, artificial intelligence technologies have been successfully applied in numerous areas. However, such approaches require large amounts of training image data. Simultaneously, product differentiation and customization strategies have forced the TFT-LCD manufacturing industry to shift from mass production to high-mix, low-volume, and short-life-cycle production. In this environment, collecting a large amount of training data is difficult. Moreover, images with Mura defects captured at inspection stations remain challenging because they are often contaminated with moiré patterns. Moiré patterns severely affect the visual quality of images and cause difficulty in determining Mura defects. This study proposes an approach to eliminate moiré patterns from defect images using a conditional generative adversarial network. In addition, we develop a transfer learning ensemble model that aggregates multiple convolutional neural networks based on a denoising network for defect classification in a limited training data set. The results from an industrial case study demonstrate that the proposed method provides improved accuracy for Mura defect classification. This method can therefore become a viable alternative to manual classification in the TFT-LCD manufacturing industry.},
  keywords={Thin film transistors;Training;Generators;Training data;Generative adversarial networks;Gallium nitride;Noise reduction;Thin-film transistor liquid crystal display (TFT-LCD);convolutional neural network (CNN);Mura defect;conditional generative adversarial network (CGAN);moiré pattern;transfer learning;ensemble learning},
  doi={10.1109/TSM.2020.3048631},
  ISSN={1558-2345},
  month={Feb},}@INPROCEEDINGS{10635911,
  author={Gudepu, Venkateswarlu and Chirumamilla, Bhargav and Chintapalli, Venkatarami Reddy and Castoldi, Piero and Valcarenghi, Luca and Tamma, Bheemarjuna Reddy and Kataria, Deepak and Kondepu, Koteswararao},
  booktitle={2024 IEEE 25th International Conference on High Performance Switching and Routing (HPSR)}, 
  title={GAN-Based Drift and Anomaly Detection for Open Radio Access Networks}, 
  year={2024},
  volume={},
  number={},
  pages={124-129},
  abstract={Next-Generation Radio Access Networks (NG-RANs) aim to facilitate high data rates, low-latency applications, and dense mobile connectivity — benefit from the integration of Artificial Intelligence and Machine Learning (AI/ML) to enhance performance and efficiency. Nevertheless, the dynamic service demands within NG-RAN (namely Open RAN) lead to AI/ML performance degradation known as drift, resulting in violations of Service Level Agreements (SLA) and issues like over-or under-provisioning of resources. Detecting and adapting to drift becomes crucial to meet the diverse requirements of intelligent networks. Due to frequent retraining, the existing threshold and classifier-based approaches have potential disadvantages such as SLA violations and resource inefficiency. This paper introduces a novel approach that exploits the Generative Adversarial Network (GAN) architecture to determine the drift and anomaly. The proposed approach is evaluated for a throughput prediction use case over a real-time dataset and compared to the threshold and classifier-based approaches. The results show that the proposed approach outperforms the threshold and classifier-based approaches.},
  keywords={Open RAN;Switches;Machine learning;Generative adversarial networks;Throughput;Routing;Real-time systems;AI/ML Model;Open RAN;Generative Adversarial Networks (GANs)},
  doi={10.1109/HPSR62440.2024.10635911},
  ISSN={2325-5609},
  month={July},}@INPROCEEDINGS{10377581,
  author={Cao, Shiyue and Yin, Yueqin and Huang, Lianghua and Liu, Yu and Zhao, Xin and Zhao, Deli and Huang, Kaiqi},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers}, 
  year={2023},
  volume={},
  number={},
  pages={7334-7343},
  abstract={Vector-quantized image modeling has shown great potential in synthesizing high-quality images. However, generating high-resolution images remains a challenging task due to the quadratic computational overhead of the self-attention process. In this study, we seek to explore a more efficient two-stage framework for high-resolution image generation with improvements in the following three aspects. (1) Based on the observation that the first quantization stage has solid local property, we employ a local attention-based quantization model instead of the global attention mechanism used in previous methods, leading to better efficiency and reconstruction quality. (2) We emphasize the importance of multi-grained feature interaction during image generation and introduce an efficient attention mechanism that combines global attention (long-range semantic consistency within the whole image) and local attention (fined-grained details). This approach results in faster generation speed, higher generation fidelity, and improved resolution. (3) We propose a new generation pipeline incorporating autoencoding training and autoregressive generation strategy, demonstrating a better paradigm for image synthesis. Extensive experiments demonstrate the superiority of our approach in high-quality and high-resolution image reconstruction and generation.},
  keywords={Training;Solid modeling;Quantization (signal);Image synthesis;Computational modeling;Semantics;Transformers},
  doi={10.1109/ICCV51070.2023.00677},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{8354317,
  author={Sarkar, Kripasindhu and Varanasi, Kiran and Stricker, Didier},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={3D Shape Processing by Convolutional Denoising Autoencoders on Local Patches}, 
  year={2018},
  volume={},
  number={},
  pages={1925-1934},
  abstract={We propose a system for surface completion and inpainting of 3D shapes using denoising autoencoders with convolutional layers, learnt on local patches. Our method uses height map based local patches parameterized using 3D mesh quadrangulation of the low resolution input shape. This provides us sufficient amount of local 3D patch dataset to learn deep generative Convolutional Neural Networks (CNNs) for the task of repairing moderate sized holes. We design generative networks specifically suited for the 3D encoding following ideas from the recent progress in 2D inpainting, and show our results to be better than the previous methods of surface inpainting that use linear dictionary. We validate our method on both synthetic shapes and real world scans.},
  keywords={Three-dimensional displays;Shape;Noise reduction;Task analysis;Two dimensional displays;Image reconstruction;Dictionaries},
  doi={10.1109/WACV.2018.00213},
  ISSN={},
  month={March},}@INPROCEEDINGS{8491572,
  author={Song, Yiliao and Zhang, Guangquan and Lu, Haiyan and Lu, Jie},
  booktitle={2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={A Self-adaptive Fuzzy Network for Prediction in Non-stationary Environments}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Prediction in non-stationary environments, where data streams are ever-changing at very high speeds, has become more and more important in real-world applications. The uncertainty in data streams caused by changes in data distribution is described as concept drift. The appearance of concept drift in a data stream results in inconsistencies between the existing data and incoming data. Such inconsistencies pose a great challenge to conventional machine learning methods, given they are built on the assumption of independent and identically distributed data and cannot adapt to unpredictable changes in knowledge patterns. To solve such data stream uncertainty problem, this paper presents a window-based self-adaptive fuzzy network called adaptive fuzzy network (AFN), which can continuously modify the network through identifying new knowledge from the previous data samples. Three components are embedded in ANF: a drift detection module to identify whether the current window of data samples presents different pattern from the previous; a drift adaption module to retain useful knowledge in previous samples; and a fuzzy inference system, which integrates the detection and adaption modules for prediction. ANF has been evaluated through a set of experiments on non-stationary data streams. The experimental results show a good effectiveness of our method.},
  keywords={Adaptation models;Gallium nitride;Fuzzy logic;Data models;Weather forecasting;Uncertainty;Data stream;concept drift;fuzzy inference system},
  doi={10.1109/FUZZ-IEEE.2018.8491572},
  ISSN={},
  month={July},}@INPROCEEDINGS{8261672,
  author={Maldonado-Mendez, Carolina and Solis, Ana Luisa and Rios-Figueroa, Homero Vladimir and Marin-Hernandez, Antonio},
  booktitle={2017 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
  title={Human fallen pose detection by using feature selection and a generative model}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper we are interesting in knowing which features provide useful information for detecting a fall and how the set of selected characteristics impact the accuracy of detection. For this purpose two sets of features are used. The first one describes the shape of the detected person, and the second one, the change of the shape over the time. All of features are extracted from a cloud of points of a detected person by the Kinect device. To determinate a fallen pose, a generative model is used. Two experiments are carried out to analyze the effect of using two different subset of features, one of them selected by a Genetic Algorithm and the second by Principal Component Analysis (PCA). The obtained results suggest that the success of detection of fall depends on the selected features, and the genetic algorithm is a good technique to select them, when compared with PCA.},
  keywords={Feature extraction;Shape;Principal component analysis;Three-dimensional displays;Genetic algorithms;Skeleton;Data mining},
  doi={10.1109/ROPEC.2017.8261672},
  ISSN={2573-0770},
  month={Nov},}@INPROCEEDINGS{10421863,
  author={Ye, Zhenhao and Li, Yanlong and Cui, Zhichao and Liu, Yuehu and Li, Li and Wang, Le and Zhang, Chi},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Unsupervised Video Anomaly Detection with Self-Attention Based Feature Aggregating}, 
  year={2023},
  volume={},
  number={},
  pages={3551-3556},
  abstract={Anomaly detection in surveillance videos is a crucial and challenging task in the intelligent transportation systems. Previous methods utilize a memory module to store prototypical feature embeddings as normal patterns learned from normal training data. However, due to the complexity of real-world scenarios, it is difficult to choose an appropriate size of memory module which can not only memorize normal patterns comprehensively, but also capable of dealing with unseen normal scenarios. To tackle this problem, we learn normal video patterns by constructing and exploring correlations between visual semantics. In the training stage, we act the self-attention map between embeddings as a description of information association between different visual semantics. A self-attention based feature aggregating module is designed to regenerate a feature map through aggregating embeddings with similar information based on the attention map. By decoding the generated feature map instead of the original one to predict the future frame of the input video clip, the model learns to build strong information association between normal visual semantics. Moreover, we observe that abnormal embeddings hardly build strong association with others. Thus, we design a feature-level anomaly criterion referred as prior deviation to increase the difference between attention maps generated by normal and abnormal frames. In the inferring stage, the proposed prior deviation jointly detects anomalies with pixel-level frame prediction error. Experiment results and ablation studies on mainstream benchmarks demonstrate the effectiveness of our design.},
  keywords={Visualization;Correlation;Semantics;Memory modules;Benchmark testing;Intelligent transportation systems;Videos},
  doi={10.1109/ITSC57777.2023.10421863},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{9308284,
  author={Leangarun, Teema and Tangamchit, Poj and Thajchayapong, Suttipong},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Using Generative Adversarial Networks for Detecting Stock Price Manipulation: The Stock Exchange of Thailand Case Study}, 
  year={2020},
  volume={},
  number={},
  pages={2162-2169},
  abstract={We implemented an automated system that uses unsupervised learning to detect stock price manipulation events. Generative adversarial networks (GANs) were trained with regular market transactions in a limit order book format. GANs can recognize normal trading behaviors of good governance stocks with the various price ranges, trading volume, and market capitalization. Stocks that were traded differently were assumed to be suspicious, thus required further manual investigation. We tested the system with 6 real manipulation cases that had been prosecuted from the stock exchange of Thailand. The proposed system can identify 5 out of 6 cases correctly with a very low false-positive rate.},
  keywords={Data models;Support vector machines;Stock markets;Generators;Unsupervised learning;Generative adversarial networks;Manipulators;anomaly detection;stock market;stock price manipulation detection;unsupervised learning},
  doi={10.1109/SSCI47803.2020.9308284},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10422058,
  author={Yu, Feiyang and Long, Tingting and Cui, Zhichao and Xu, Linhai and Li, Li and Zhang, Chi},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Pedestrian Simulation by Learning from Online Human Demonstration with VR Headset}, 
  year={2023},
  volume={},
  number={},
  pages={3680-3685},
  abstract={Pedestrian simulation is essential for verifying the safety of autonomous vehicles in simulators. The goal of pedes-trian simulation is to create realistic virtual representations of the pedestrian. However, current simulations lack empirical knowledge about real pedestrian behavior. In this paper, we propose a virtual reality (VR)-based method that enables real-time interaction between an individual and a simulated traffic environment. Our human-in-the-loop approach incorporates the empirical knowledge of a VR user into the simulator. The user can view the traffic simulation by wearing a VR device, which includes a head-mounted display and two controllers and provides sparse sensor data about their position and rotation. We combine representation learning techniques to develop a full-body motion-tracking agent that takes in sparse signals from the VR device and simulates full-body motions. This agent can estimate the full-body pose and visualize a pedestrian avatar in a traffic simulation scene. To validate our method, we constructed a VR traffic simulation environment and demonstrated that our approach can produce a pedestrian avatar whose behavior closely resembles that of the real VR user.},
  keywords={Pedestrians;Tracking;Avatars;Traffic control;Real-time systems;Behavioral sciences;Autonomous vehicles},
  doi={10.1109/ITSC57777.2023.10422058},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{10364338,
  author={Weiming, Qu and Tianlin, Liu and Xihong, Wu and Dingsheng, Luo},
  booktitle={2023 IEEE International Conference on Development and Learning (ICDL)}, 
  title={Embodied Self-Supervised Learning (EMSSL) with Sampling and Training Coordination for Robot Arm Inverse Kinematic Model Learning}, 
  year={2023},
  volume={},
  number={},
  pages={100-106},
  abstract={Forward and inverse kinematic models are fundamental to robot arms, serving as the basis for the robot arm's operational tasks. However, in model learning of robot arms, especially in the presence of redundant degrees of freedom, inverse kinematic model learning is more challenging than forward kinematic model learning due to the non-convex problem caused by multiple solutions. Besides, Current learning-based methods often segregate data sampling from model training, potentially leading to suboptimal data utilization and restricted model adaptability. In this paper, we introduce the concept of “Embodiment” and propose a framework for autonomous learning of the robot arm inverse kinematic model based on embodied self-supervised learning (EMSSL) with sampling and training coordination, effectively solving the non-convex problem of the inverse kinematic model and significantly enhancing data sampling efficiency. Concurrently, we investigate batch inference and parallel computation strategies for data sampling to expedite model learning. Additionally, we develop two approaches for the fast adaptation of the robot arm models. A series of experimental evaluations attest to the efficacy of our proposed method.},
  keywords={Training;Adaptation models;Robot kinematics;Computational modeling;Kinematics;Self-supervised learning;Manipulators;robot arm inverse kinematics model;embodied self-supervised learning (EMSSL);data sampling;model training;coordination},
  doi={10.1109/ICDL55364.2023.10364338},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10890347,
  author={Zheng, Rongcan and Song, Xiaodan and Zuo, Xuguang and Yang, Minxi and Gao, Dahua and Xie, Xuemei},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Transmitter-Model Unaware Generative Image Compression Framework for Semantic Communication}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Unlike traditional bit-level data transmission methods, semantic communication focuses on conveying the meaning behind the data. Though promising results have been achieved, existing end-to-end learning-based semantic communication frameworks often require a synchronization of deep models between the transmitter and the receiver. Such design leads to tens of thousands models to be stored at receiver since different manufactures may optimize their own models. To address this problem, we propose a novel model-unaware generative image compression framework for semantic communication. It features at employing human-understandable multi-modality representations as an intermediate layer to enhance information transmission efficiency and semantic consistency. Our framework introduces a mask-based rate-distortion optimization module, which effectively removes low-relevance information for image generation and reduces the bit rate while maintaining semantic consistency. Experimental results demonstrate that the framework can still reconstruct high-quality images at very low bit rates, showcasing its potential for applications in modern communication systems.},
  keywords={Image coding;Transmitters;Bit rate;Rate-distortion;Receivers;Information processing;Semantic communication;Synchronization;Optimization;Image reconstruction;Semantic Communication;Model-Unaware Communication;Image Compression;Semantic Representation;Rate-Distortion Optimization},
  doi={10.1109/ICASSP49660.2025.10890347},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10725987,
  author={Patil, Viraj and Arepally, Disha Reddy and Shah, Harshad},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Performance Analysis of DCGAN and WGAN in Synthesizing Brain MRI Scans for Alzheimer’s Disease Detection Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Alzheimer’s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and memory impairment, affecting millions of individuals worldwide. Accurate diagnosis is crucial for timely intervention and improved patient care. In this realm, the role of Generative Adversarial Networks (GANs) in addressing class imbalance and enhancing model performance has come to the forefront. In this project, we embarked on a comprehensive exploration of GAN-augmented MRI scans and their impact on Alzheimer’s disease detection models. Leveraging the power of DenseNet169, DenseNet-121, and a deep CNN model, we crafted a robust ensemble framework, and our results suggest a potential influence in the field of Alzheimer’s diagnosis. We harnessed the prospect of GANs to generate synthetic images, and subsequently retrained our classification model using the augmented data. Our endeavor serves as a validation to the transformative influence of GANs in enhancing the reliability of diagnostic models while ensuring equitable representation across classes, by mitigating class imbalance issues. This study not only highlights the effectiveness of GANs in addressing data scarcity issues in AD diagnosis but also further contributes to the broader understanding of the synergistic relationship between generative models and medical image analysis.},
  keywords={Analytical models;Accuracy;Magnetic resonance imaging;Medical services;Brain modeling;Data models;Performance analysis;Reliability;Alzheimer's disease;Medical diagnostic imaging;Alzheimer’s Disease;GANs;DCGANs;WGANs;DenseNet;CNNs},
  doi={10.1109/ICCCNT61001.2024.10725987},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{10651122,
  author={Shen, Chengji and Feng, Zunlei and Xie, Zhongle and Lei, Jie and Wang, Huiqiong and Song, Mingli},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={GAN Doctor: Diagnosing and Treating Inherent Semantic Errors}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Generative Adversarial Network (GAN), as a popular generative model in the field of Artificial Intelligence Generated Content (AIGC), has been intensively developed in previous research, with significant improvements in the quality and diversity of image generation. However, there are still many cases where the results are not satisfactory. A primary concern pertains to the chaotic and blurred local details within the generated images. In this work, through the diagnosis and analysis of high-quality and low-quality images produced by the GAN model, we identified that this issue stems from inherent semantic errors of the GAN, that is, convolutional kernels responsible for certain semantics are not properly involved in the generation process of corresponding image regions. To this end, we propose a straightforward yet effective treatment method, which constrains each image region to be generated by its corresponding semantic convolutional kernels. Experimental results demonstrate that our proposed optimization method can improve the issue of chaotic and blurred local regions in generated images and enhance the overall generation quality. Our work pioneers a novel paradigm for diagnosing and treating GANs, driving the research development and practical application of AIGC image generation technology.},
  keywords={Analytical models;Image synthesis;Semantics;Neural networks;Optimization methods;Medical services;Generative adversarial networks;Generative Adversarial Network;Model Diagnosing;Model Optimization},
  doi={10.1109/IJCNN60899.2024.10651122},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10488214,
  author={Chung, Hyunseok and Hyun, Sunyoung and Ha, Young-Guk},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Battlefield Situation Awareness Using Pretrained Generative LLM}, 
  year={2024},
  volume={},
  number={},
  pages={397-398},
  abstract={Since ChatGPT was opened, many researches about the Large Language Model (LLM) are being conducted. This is because LLM has better performance and many advantages compared to previous models. In this paper, we propose Military GPT (MGPT), an adapted LLM for the task of battlefield situation awareness in the defense field. MGPT was designed based on Pretrained Generative LLM, a model specific to the Korean language, and was trained by defense data in Korean.},
  keywords={Adaptation models;Ethics;Computational modeling;Context awareness;Big Data;Benchmark testing;Chatbots;LLM;Battlefield Situation Awareness;LoRA;P-Tuning;Fine-tuning},
  doi={10.1109/BigComp60711.2024.00087},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10605375,
  author={Fan, Jiajie and Vuaille, Laure and Bäck, Thomas and Wang, Hao},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis}, 
  year={2024},
  volume={},
  number={},
  pages={119-124},
  abstract={Deep Generative Models (DGMs) have been successfully employed to synthesize general images, e.g., animals, human faces, and landscapes. This promising advancement leads to the idea of utilizing DGMs to generate novel structural designs, thereby facilitating industrial engineering processes. However, industrial design data, e.g., blueprints or engineering drawings, is fundamentally different from the images of natural scenes. They contain rich structural patterns and long-range dependencies, which are challenging for convolution-based DGMs to generate. We tackle this challenge by proposing the Self-Attention Adversarial Latent Autoencoder (SA-ALAE), which allows for generating realistic structure designs of complex engineering parts. With SA-ALAE, users can explore novel variants of an existing design and control the generation process by operating in the learned latent space. We showcase the potential of SA-ALAE by generating engineering blueprints in a real automotive design task.},
  keywords={Space vehicles;Training;Analytical models;Shape;Image synthesis;Process control;Industrial engineering;generative modeling;attention mechanism;la-tent space manipulation;structure generation},
  doi={10.1109/CAI59869.2024.00030},
  ISSN={},
  month={June},}@INPROCEEDINGS{10962609,
  author={V, Anand and G, Senthil Kumar and K, Suresh Kumar and C, Selvaganesan},
  booktitle={2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)}, 
  title={Enhancing Ransomware Detection - A Comparative Review of XGBoost, Random Forest, and Neural Network Approaches}, 
  year={2025},
  volume={},
  number={},
  pages={710-715},
  abstract={Ransomware detection has now become an essential challenge in cybersecurity as attackers use using smart and tricky methods to get interrupts our simple defences. This review discusses how the work of leading machine learning (ML) models like, XGBoost, Random Forest (RF), and Artificial Neural Networks (ANNs), improves ransomware detection systems. The most prominent reason why XGBoost has been selected as the model is because of its high accuracy rate, and random forests are considered due to their appreciation for model stability, which is why they have been used within studies for efficient base classifiers for most of them. ANNs also offer a strong nonlinear approach when handling ransomware's complex data patterns. In this paper, we evaluate some of these approaches- hybrids that include XGBoost and RF algorithms as well as advanced ANNs configurations for their very distinct strengths and weaknesses in the sense of ransomware detection. In synthesizing the results, The review explains how applicable and comparatively advantageous these approaches are at detecting ransomware. The study ultimately seeks to guide future implementations by summarizing key model features, evaluation metrics, and adaptability to evolving ransomware behaviours, there by contributing to enhanced cybersecurity frameworks.},
  keywords={Radio frequency;Adaptation models;Accuracy;Reviews;Computational modeling;Artificial neural networks;Stability analysis;Ransomware;Computer security;Random forests;Ransomware Detection;Machine Learning (ML) Models;XGBoost;Random Forest (RF);Artificial Neural Networks (ANNs);Cybersecurity Frameworks},
  doi={10.1109/ESIC64052.2025.10962609},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10488164,
  author={Cho, YoungHyeon and Shin, MiYeun and Lee, Kwangil and Lee, Yunsok},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Performance Analysis for Generative Image Restoration Algorithm for Battlefield Objects}, 
  year={2024},
  volume={},
  number={},
  pages={393-394},
  abstract={As Artificial Intelligence has been spotlighted recently, its integration into human decision-support systems, particularly in battlefields, becomes crucial. Given the increasing usage of U A V s and drones in warfare, AI -based systems play a significant role in situational awareness, which involves the detection, identification, and classification of various battlefield objects. However, the challenge lies in identifying obscured objects due to natural battlefield situations. To address this, we study removing obstacles using an image generation algorithm. This paper studied generative image generation algorithms for battlefield objects, specifically the AOT -GAN model, Stable Diffusion, and Palette. The performance of these models in obstacle removal and object restoration was analyzed. In our analysis, stable diffusion shows better performance than other algorithm where SSIM is used as a performance metric.},
  keywords={Measurement;Analytical models;Image recognition;Image synthesis;Image restoration;Classification algorithms;Performance analysis;image restoration;object recognition;GAN;Diffusion model},
  doi={10.1109/BigComp60711.2024.00085},
  ISSN={2375-9356},
  month={Feb},}@ARTICLE{8688367,
  author={Han, Jing and Zhang, Zixing and Cummins, Nicholas and Schuller, Björn},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives [Review Article]}, 
  year={2019},
  volume={14},
  number={2},
  pages={68-81},
  abstract={Over the past few years, adversarial training has become an extremely active research topic and has been successfully applied to various Artificial Intelligence (AI) domains. As a potentially crucial technique for the development of the next generation of emotional AI systems, we herein provide a comprehensive overview of the application of adversarial training to affective computing and sentiment analysis. Various representative adversarial training algorithms are explained and discussed accordingly, aimed at tackling diverse challenges associated with emotional AI systems. Further, we highlight a range of potential future research directions. We expect that this overview will help facilitate the development of adversarial training for affective computing and sentiment analysis in both the academic and industrial communities.},
  keywords={Training;Sentiment aalysis;Emotion recognition;Generative adversarial networks;Artificial intelligence;Smart devices;Machine learning},
  doi={10.1109/MCI.2019.2901088},
  ISSN={1556-6048},
  month={May},}@INPROCEEDINGS{10903889,
  author={Pyreddy, Shireesh Reddy and Zaman, Tarannum Shaila},
  booktitle={2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC)}, 
  title={EmoXpt: Analyzing Emotional Variances in Human Comments and LLM-Generated Responses}, 
  year={2025},
  volume={},
  number={},
  pages={00088-00094},
  abstract={The widespread adoption of generative AI has generated diverse opinions, with individuals expressing both support and criticism of its applications. This study investigates the emotional dynamics surrounding generative AI by analyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and LLMs. To further understand the emotional intelligence of ChatGPT, we examine its responses to selected tweets, highlighting differences in sentiment between human comments and LLM-generated responses. We introduce EmoXpt, a sentiment analysis framework designed to assess both human perspectives on generative AI and the sentiment embedded in ChatGPT's responses. Unlike prior studies that focus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional expression of ChatGPT. Experimental results demonstrate that LLM-generated responses are notably more efficient, cohesive, and consistently positive than human responses.},
  keywords={Sentiment analysis;Generative AI;Conferences;Chatbots;ChatGPT;LLMs;OpenAI;Emotional Intelligence;Generative AI},
  doi={10.1109/CCWC62904.2025.10903889},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{4769497,
  author={Zhao, Yu and Yang, Miaomiao and Qi, Chunjie},
  booktitle={2009 International Conference on Computer Engineering and Technology}, 
  title={Recognition of Trade Barrier Based on General RBF Neural Network}, 
  year={2009},
  volume={1},
  number={},
  pages={405-408},
  abstract={Trade barriers correct externalities, but distort trade as well, which brings negative effect on international distribution of resources. Study on how to recognize there are trade barriers or would-be threat of trade barriers in international trade is of great necessity. Based on previous studies, taking horticultural products as an example, this paper designs a general RBF neural network model to detect whether there are trade barriers among the main destinations of Chinese horticultural products. The study shows that general RBF neural network can identify trade barriers correctly with an error probability about 10%, so it is a helpful tool for identification of trade barriers.},
  keywords={Neural networks;Radial basis function networks;Pattern recognition;Educational institutions;Vectors;International trade;Friction;Artificial neural networks;Kernel;Higher order statistics;pattern recognition;trade barrier;RBF neural network;Chinese horticultural products},
  doi={10.1109/ICCET.2009.25},
  ISSN={},
  month={Jan},}@INBOOK{10950603,
  author={Orange, Erica},
  booktitle={AI + The New Human Frontier: Reimagining the Future of Time, Trust + Truth}, 
  title={Redefining What It Means to Be Human}, 
  year={2024},
  volume={},
  number={},
  pages={177-184},
  abstract={Summary <p>We may believe the idea of human enhancement to be a modern one, but stories of physical adaptation and modification are found in Greek mythology and throughout many ancient cultures. We are also moving into a future in which our biology is becoming increasingly self&#x2010;defined. Much of this sits at the intersection between AI, synthetic biology, and genetic engineering. Transhumanism is the school of thought that humanity is destined to evolve beyond its current physical and mental limitations, primarily through the means of advanced technology. The intersection of AI and religious practices has sparked a fascinating debate among Hindu and Buddhist scholars. The emergence of robots performing Hinduism's holiest rituals raises profound questions about the role of technology in spirituality. The origin of life is a foundational question of theology. New issues surrounding it are shaking religious beliefs, and AI is compounding the confusion.</p>},
  keywords={Artificial intelligence;Human augmentation;Ethics;Artificial general intelligence;Generative AI;Virtual assistants;Urban areas;Neurons;Neural implants;Internet},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394276998},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950603},}@INPROCEEDINGS{10695524,
  author={Zeng, Wenzhe and Liu, Xingyuan},
  booktitle={2024 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)}, 
  title={RoomGen: 3D Interior Furniture Layout Generation}, 
  year={2024},
  volume={},
  number={},
  pages={529-534},
  abstract={Lowering the costs of generating realistic indoor scenes upon customer’s requests, automatic synthesis of diverse indoor furniture layouts opens the door to various innovative business models for the real estate and home improvement industry. With the continuous development of deep generative networks, the quality of their generation results is constantly improving and occupies an important position in industrial design. Therefore, how to utilize deep generative networks to design indoor 3D scene layout with high quality and high level is of great significance to improve the design efficiency. This paper introduce RoomGen, a 3D Generative Adversarial Network (GAN) tailored for this purpose. RoomGen integrates an enhanced Variational Autoencoder (VAE) and a UNet++ discriminator for discriminate feature learning. Specifically, VAE introduces residual module in the encoder and deep supervision in the decoder, which improves the fidelity of the generated layouts. In addition, RoomGen incorporates a novel attention mechanism in the UNet++ recognizer, which enables it to capture fine-grained features and better distinguish between real and generated layouts. Experimental results demonstrate RoomGen can proficiently generate high-quality indoor furniture layouts with impressive reconstruction accuracy. Additionally, RoomGen’s capability to produce diverse and realistic scenes offers a more cost-effective solution for the real estate and home improvement sectors. Moreover, it provides a fully automated solution for designing virtual reality scenes and 3D game environments.},
  keywords={Representation learning;Industries;Solid modeling;Three-dimensional displays;Layout;Virtual reality;Games;Generative adversarial networks;Decoding;Image reconstruction;3D indoor scene;3D furniture layout generation;scene synthesis;layout arrangement;interior design;generating adversarial network},
  doi={10.1109/IPEC61310.2024.00096},
  ISSN={},
  month={April},}@INPROCEEDINGS{10605771,
  author={Flores-Monroy, Jonathan and Cedillo-Hernandez, Manuel and Nakano-Miyatake, Mariko and Perez-Meana, Hector},
  booktitle={2024 47th International Conference on Telecommunications and Signal Processing (TSP)}, 
  title={Ownership Authentication and Integrity Verification of Digital Images Using Generative Models and Custom Signature}, 
  year={2024},
  volume={},
  number={},
  pages={101-106},
  abstract={In this paper we present an alternative distortion-free algorithm to ownership authentication and integrity veri-fication of digital images that use Contrastive Language-Image Pre-Training (CLIP), Vector Quantized Generative Adversarial Network (VQGAN) and a custom signature based on crypto-graphic message digest. Authentication and integrity of digital media are evaluated by the naked eye, as shown in the output of the generative architecture, as well as numerical metrics such as PSNR and SSIM, respectively. The code is publicly available at https://github.com/JonathanFlores250/NoiseHashGAN.},
  keywords={Measurement;Digital images;Authentication;Streaming media;Signal processing;Media;Vectors;ownership authentication;integrity verification;generative models;artificial intelligence},
  doi={10.1109/TSP63128.2024.10605771},
  ISSN={2768-3311},
  month={July},}@INPROCEEDINGS{10488930,
  author={Singh, Yashovardhan and Jaihind, Yadav Abhijit and Chauhan, Vishal and Prakash, Vinayak and Chauhan, Sansar Singh and Fatima, Eram},
  booktitle={2024 2nd International Conference on Disruptive Technologies (ICDT)}, 
  title={Generative Aquatica: Crafting Unique Fish Species through GAN-Driven Evolution}, 
  year={2024},
  volume={},
  number={},
  pages={274-278},
  abstract={Generative Adversarial Organizations (GANs) have arisen as a progressive structure in AI, working with the age of sensible manufactured information through a serious cycle between a generator and a discriminator. This paper gives a far-reaching review of the development, applications, and headways in GAN innovation. The basic standards behind GANs, including the antagonistic preparation process, are clarified, featuring their capacity to learn complex information appropriations. Different compositional varieties, from vanilla GANs to Wasserstein GANs and contingent GANs, are investigated, revealing insight into their one-of-a-kind capacities and constraints. Besides, this survey frames the different utilizations of GANs across numerous areas, including picture age, style move, information expansion, and abnormality discovery. The difficulties and moral contemplations encompassing GANs, like predispositions in produced information and security suggestions, are additionally talked about. Moreover, late exploration leap forwards and future headings in GANs, like moderate developing procedures, self-consideration systems, and novel misfortune capabilities, are analysed. These progressions prepare for improved dependability, versatility, and the age of high-constancy information. In rundown, this overview gives an extensive comprehension of the development, applications, difficulties, and future possibilities of Generative Ill-disposed Organizations, exhibiting their massive potential to reform different spaces of computerized reasoning and information age.},
  keywords={Technological innovation;Reviews;Computational modeling;Neural networks;Standards organizations;Organizations;Games;Generative Adversarial Networks (GANs);Artificial Intelligence;Wasserstein GAN;Machine Learning;Adversarial Training},
  doi={10.1109/ICDT61202.2024.10488930},
  ISSN={},
  month={March},}@INPROCEEDINGS{10976960,
  author={Aly, Karim and Sharpanskykh, Alexei},
  booktitle={2025 Integrated Communications, Navigation and Surveillance Conference (ICNS)}, 
  title={Synthetic Flight Data Generation Using Generative Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={The increasing adoption of synthetic data in aviation research offers a promising solution to data scarcity and confidentiality challenges. This study investigates the potential of generative models to produce realistic synthetic flight data and evaluates their quality through a comprehensive four-stage assessment framework. The need for synthetic flight data arises from their potential to serve as an alternative to confidential real-world records and to augment rare events in historical datasets. These enhanced datasets can then be used to train machine learning models that predict critical events, such as flight delays, cancellations, diversions, and turnaround times. Two generative models, Tabular Variational Autoencoder (TVAE) and Gaussian Copula (GC), are adapted to generate synthetic flight information and compared based on their ability to preserve statistical similarity, fidelity, diversity, and predictive utility. Results indicate that while GC achieves higher statistical similarity and fidelity, its computational cost hinders its applicability to large datasets. In contrast, TVAE efficiently handles large datasets and enables scalable synthetic data generation. The findings demonstrate that synthetic data can support flight delay prediction models with accuracy comparable to those trained on real data. These results pave the way for leveraging synthetic flight data to enhance predictive modeling in air transportation.},
  keywords={Adaptation models;Atmospheric modeling;Computational modeling;Autoencoders;Predictive models;Traffic control;Air transportation;Data models;Delays;Synthetic data;Generative Artificial Intelligence;Variational Autoencoders;Gaussian Copula;Synthetic Flight Information;Synthetic Data Quality Assessment;Flight Delay Prediction;Air Traffic Management;Air Transportation Deep Learning;Statistical Modeling},
  doi={10.1109/ICNS65417.2025.10976960},
  ISSN={2155-4951},
  month={April},}@INPROCEEDINGS{10837679,
  author={Ranasinghe, Harshani and Gide, Ergun and ElKhodr, Mahmoud},
  booktitle={2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)}, 
  title={The Significance of GenAI Empowered ERP Systems Course Teaching in Quality Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper aims to analyse the significance of Generative Artificial Intelligence (GenAI) Empowered Enterprise Resource Planning (ERP) systems as a course in higher education. With the advancement of GenAI technology, ERP systems are capable of automating and streamlining business operations across various industries. The usage of ERP systems has become prevalent in small to medium-sized organizations worldwide, akin to the utilization of office packages for day-to-day operations. Therefore, Fundamental knowledge and understanding of GenAI Empowered ERP system concepts is an advantageous skill for undergraduate and graduate students as potential job seekers in various industries.},
  keywords={Training;Industries;Generative AI;Decision making;Customer satisfaction;Curriculum development;Organizations;Enterprise resource planning;Information technology;ERP systems;GenAI;Higher Education;Systematic review;Job market;Quality},
  doi={10.1109/ITHET61869.2024.10837679},
  ISSN={2473-2060},
  month={Nov},}
