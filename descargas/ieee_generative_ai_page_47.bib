@ARTICLE{10336816,
  author={Magalhães, Bruno and Neto, Alexandre and Cunha, António},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks for Augmenting Endoscopy Image Datasets of Stomach Precancerous Lesions: A Review}, 
  year={2023},
  volume={11},
  number={},
  pages={136292-136307},
  abstract={Gastric cancer (GC) is still a significant public health issue, among the most common and deadly cancers globally. The identification and characterization of precancerous lesions of the stomach using endoscopy are crucial for determining the risk of cancer and guiding appropriate surveillance. In this scenario, deep learning (DL)-based computer vision methods have the potential to help us classify and identify particular patterns in endoscopic images, leading to a more accurate classification of these types of lesions. The quantity and quality of the data used heavily influence the classification performance of DL networks. However, one of the major setbacks for developing high-performance DL classification models is the typical need for more available data in the medical field. This review explores the use of Generative Adversarial Networks (GANs) and classical data augmentation techniques for improving the classification of precancerous stomach lesions. GANs are DL models that have shown promising results in generating synthetic data, which can be used to augment limited medical datasets. This review discusses recent studies that have implemented GANs and classical data augmentation methods to improve the accuracy of cancerous lesion classification. The results indicate that GANs can effectively increase the dataset’s size, enhance the classification models’ performance. In specific applications, such as the augmentation of endoscopic images depicting gastrointestinal polyps and Barrett’s esophagus Adenocarcinoma, our review reveals instances where GANs, including models like Deep Convolutional GANs and conditional GANs, outperform classical data augmentation methods. Furthermore, this review highlights the challenges and limitations of the recent works using GANs and classical data augmentation techniques in medical imaging analysis and proposes directions for future research.},
  keywords={Lesions;Medical diagnostic imaging;Data models;Stomach;Cancer;Solid modeling;Endoscopes;Data augmentation;Deep learning;Generative adversarial networks;Gastroenterology;Dataset augmentation;deep learning;generative adversarial networks;precancerous lesions;upper gastro endoscopy},
  doi={10.1109/ACCESS.2023.3338545},
  ISSN={2169-3536},
  month={},}@ARTICLE{9214812,
  author={Guo, Jifeng and Pang, Zhiqi and Yu, Ming and Xie, Peijiao and Liu, Dan},
  journal={IEEE Access}, 
  title={A Novel Pedestrian Reidentification Method Based on a Multiview Generative Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={181943-181954},
  abstract={Emerging deep learning (DL) techniques have greatly improved pedestrian reidentification (PRI) performance. However, the existing DL-based PRI methods cannot learn robust feature representations owing to the single view of query images and the limited number of extractable features. Inspired by generative adversarial networks (GANs), this paper proposes a novel PRI method based on a pedestrian multiview GAN (PmGAN) and a classification recognition network (CRN). The PmGAN consists of three generators and one multiclass discriminator. The three generators produce pedestrian images from the front, side and back, while the multiclass discriminator determines whether the input image is a real image or a generated image. In addition to expanding the existing pedestrian datasets, the PmGAN can generate pedestrian images from front, side and back views based on a given query image and thereby increase the feature semantic space of the query image. To verify the performance of our method, the PmGAN was compared with mainstream pedestrian image generation models, and then the proposed method was contrasted with mainstream PRI methods. The results show that the proposed PmGAN greatly improved the performance of mainstream PRI methods. For example, the combination of the PmGAN and Pyramidal increased the mean average precision (mAP) on three common datasets by 1.2% on average. The research findings provide new insights into the application of multiview generation in PRI tasks.},
  keywords={Feature extraction;Gallium nitride;Generators;Generative adversarial networks;Training;Task analysis;Machine learning;Deep learning (DL);generative adversarial networks (GANs);image generation;pedestrian reidentification (PRI);classification recognition network (CRN)},
  doi={10.1109/ACCESS.2020.3029180},
  ISSN={2169-3536},
  month={},}@ARTICLE{9870777,
  author={Mashudi, Nurul Amirah and Ahmad, Norulhusna and Mohd Noor, Norliza},
  journal={IEEE Access}, 
  title={LiWGAN: A Light Method to Improve the Performance of Generative Adversarial Network}, 
  year={2022},
  volume={10},
  number={},
  pages={93155-93167},
  abstract={Generative adversarial networks (GANs) gained tremendous growth due to the potency and efficiency in producing realistic samples. This study proposes a light-weight GAN (LiWGAN) to learn non-image synthesis with minimum computational time for less power computing. Hence, the LiWGAN method enhanced a new skip-layer channel-wise excitation module (SLE) and a self-supervised discriminator design for non-synthesis performance using the facemask dataset. Facemask is one of the preventative strategies pioneered by the current COVID-19 pandemic. LiWGAN manipulates a non-image synthesis of facemasks that could be beneficial for some researchers to identify an individual using lower power devices, occlusion challenges for face recognition, and alleviate the accuracy challenges due to limited datasets. The study evaluates the performance of the processing time in terms of batch sizes and image resolutions using the facemask dataset. The Fréchet inception distance (FID) was also measured on the facemask images to evaluate the quality of the augmented image using LiWGAN. The findings for 3000 generated images showed a nearly similar FID score at 220.43 with significantly less processing time per iteration at 1.03s than StyleGAN at 219.97 FID score. One experiment was conducted using the CelebA dataset to compare with GL-GAN and DRAGAN, proving LiWGAN is appropriate for other datasets. The outcomes found LiWGAN performed better than GL-GAN and DRAGAN at 91.31 FID score with 3.50s processing time per iteration. Therefore, LiWGAN could aim to enhance the FID score to be near zero in the future with less processing time by using different datasets.},
  keywords={Generative adversarial networks;Training data;Generators;Face recognition;Data models;Optimization;Neural networks;Self-supervised learning;Data augmentation;Deep learning;Non-image synthesis;self-supervised discriminator;data augmentation;deep learning;generative adversarial network},
  doi={10.1109/ACCESS.2022.3203065},
  ISSN={2169-3536},
  month={},}@ARTICLE{10755038,
  author={Oveis, Amir Hosein and Meucci, Giulio and Mancuso, Francesco and Berizzi, Fabrizio and Cantelli-Forti, Alessandro},
  journal={IEEE Access}, 
  title={Generative AI Threats to Maritime Navigation Using Deceptive ISAR Images}, 
  year={2024},
  volume={12},
  number={},
  pages={173800-173809},
  abstract={The state of the art in deepfake technology demonstrates rapid technological evolution. Machine learning algorithms are becoming increasingly talented at generating synthetic yet realistic media, raising ethical, social security, and political concerns. Concurrently, the increasing frequency of supply chain attacks and Advanced Persistent Threats (APTs) poses an evolving threat to the security of critical infrastructure, drawing the attention of state-level actors. Our research on AI-generated threats explores the fabrication of images that depict non-existent events, demonstrating the ability to create false radar images. Specifically, counterfeit Inverse Synthetic Aperture Radar (ISAR) images, created using Generative Adversarial Networks (GANs), closely resemble real targets and present a significant threat to maritime operations when exploited in supply chain attacks or by APTs. Radar systems are one of the main elements of ship navigational chains that provide vital information on the surrounding area in terms of distance and speed. Real data analysis in this paper has been conducted on an ISAR database extracted from the NATO SET-196 trials, demonstrating the capability of GANs to create convincing fake ISAR images. Such experiments raise awareness of the vulnerabilities of imaging radar systems to novel, AI-generated cyberattacks.},
  keywords={Radar imaging;Radar;Generative adversarial networks;Spaceborne radar;Navigation;Marine vehicles;Imaging;Radar polarimetry;Supply chains;Radar antennas;Deepfake;inverse synthetic aperture radar (ISAR);generative adversarial networks (GANs);advanced persistent threats (APT);supply chain attacks;cyber-attacks;maritime navigation},
  doi={10.1109/ACCESS.2024.3500774},
  ISSN={2169-3536},
  month={},}@ARTICLE{9146108,
  author={Hazra, Debapriya and Byun, Yung-Cheol},
  journal={IEEE Access}, 
  title={OEBR-GAN: Object Extraction and Background Recovery Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={135730-135741},
  abstract={Generative adversarial networks (GAN) have been widely used in the field of image-to-image translation. In this paper, we have proposed a novel object extraction and background recovery (OEBR-GAN) model, which can extract objects from an image and then complete the image by inpainting the background of the image. This model has been developed for a solar panel installation project, where the user would like to input an original colored image of the roof, and as output, the user requires an edge detected roof image. However, the condition in user requirement is that any object that is hiding the roof edges should be removed first and the background of that part of the roof image should be recovered so that the user can obtain a complete connected edge detected image of the roof. Therefore, the model also completes the image by connecting the hidden edges of the roof. We could achieve the user objective by building a GAN model with a dual generator and dual discriminator network. The generators have been built using an encoder-decoder network with and without skip connections and the discriminators have been built using deep convolutional neural networks and encoder architecture. Quantitative comparisons in the result section shows that OEBR-GAN performs much better than other adversarial models on our collected dataset.},
  keywords={Image edge detection;Gallium nitride;Generative adversarial networks;Generators;Training;Feature extraction;Machine learning;Generative adversarial networks;object extraction;background recovery;dual generator;dual discriminator},
  doi={10.1109/ACCESS.2020.3011187},
  ISSN={2169-3536},
  month={},}@ARTICLE{10813343,
  author={Dare, Oluwatobi Emmanuel and Okokpujie, Kennedy and Adetiba, Emmanuel and Idowu-Bismark, Olabode and Abayomi, Abdultaofeek and Jules Kala, Raymond and Owolabi, Emmanuel and Christopher Ukpong, Udeme},
  journal={IEEE Access}, 
  title={Development of a Conditional Generative Adversarial Network Model for Television Spectrum Radio Environment Mapping}, 
  year={2024},
  volume={12},
  number={},
  pages={197632-197644},
  abstract={To efficiently use the finite wireless communication resource (radio spectrum), a Radio Environment Map (REM) is needed to monitor, analyse and provide rich awareness of spectrum activities in a radio propagating environment. REM shows radio coverage metrics in a geographical region. A REM construction model with few constraints and optimal performance is needed to better support cognitive radio for dynamic spectrum sharing (DSS) and other benefits of REM. This study aims to estimate fine-resolution REM from sparse radio signal strength measurement. In this study, we utilised conditional generative adversarial network (CGAN) to create a television spectrum radio environment map in order to improve cognitive television white space (TVWS) radio performance in real-time propagation environments. Measurement campaign was carried out to acquire a TV-band (470-862MHz) radio frequency and geographical dataset at Covenant University, Ota, Nigeria. A preprocessing procedure which was implemented with Python script was employed to group the dataset using Nigerian Communications Commission TV spectrum channel spacing and to create incomplete spectrograms for 49 channels. Xgboost, SVM, and kriging variogram models were explored to generate ground truth datasets for the CGAN model training, and the best algorithm was employed. A CGAN REM model was developed using U-Net as a generator and PatchGan as a discriminator. The U-Net generator is a 3-channel input, 16-layer architecture while the PatchGan discriminator is a 6-channel input, 7-layer architecture. The model performance was evaluated using mean square error (MSE) and mean absolute error (MAE). 12 different experiments were carried out varying the training parameters of the CGAN architecture to obtain an optimal model. The achieved root mean square error (RMSE) is 0.1145dBm and MAE is 0.0820dBm, which shows the deviation between the ground truth and the generated REM. This low deviation means that the proposed CGAN REM model possesses an improved accuracy in predicting the spectrum activities within the television spectrum which is considered appropriate for DSS technology. This study also revealed that 41 channels within TV-band in Covenant University are totally unoccupied.},
  keywords={TV;Generative adversarial networks;Training;Radio transmitters;Generators;Atmospheric modeling;Accuracy;White spaces;Estimation;Current measurement;Conditional generative adversarial network (CGAN);dynamic spectrum sharing (DSS);radio environment map (REM);received signal strength (RSS);television white spaces (TVWS)},
  doi={10.1109/ACCESS.2024.3521998},
  ISSN={2169-3536},
  month={},}@ARTICLE{10967485,
  author={Ahmad, Zeeshan and Bao, Shudi and Chen, Meng},
  journal={IEEE Access}, 
  title={DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis}, 
  year={2025},
  volume={13},
  number={},
  pages={69324-69340},
  abstract={In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model’s ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67 M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.},
  keywords={Generative adversarial networks;Convolution;Vocoders;Training;Real-time systems;Spectrogram;Signal resolution;Recurrent neural networks;Kernel;Generators;Audio synthesis;deformable convolution;generative adversarial networks;periodic activation function},
  doi={10.1109/ACCESS.2025.3561857},
  ISSN={2169-3536},
  month={},}@ARTICLE{10804166,
  author={Serafim Rodrigues, Thiago and Rogério Pinheiro, Plácido},
  journal={IEEE Access}, 
  title={Hyperparameter Optimization in Generative Adversarial Networks (GANs) Using Gaussian AHP}, 
  year={2025},
  volume={13},
  number={},
  pages={770-788},
  abstract={This study explores optimizing hyperparameters in Generative Adversarial Networks (GANs) using the Gaussian Analytical Hierarchy Process (Gaussian AHP). By integrating machine learning techniques and multi-criteria decision methods, the aim is to enhance the performance and efficiency of GAN models. It trains GAN models using the Fashion MNIST dataset. It applies Gaussian AHP to optimize hyperparameters based on multiple performance criteria, such as the quality of generated images, training stability, and training time. Iterative experiments validate the methodology by automatically adjusting hyperparameters based on the obtained scores, thereby maximizing the model’s efficiency and quality. Results indicate significant improvements in image generation quality and computational efficiency. The study highlights the effectiveness of combining Gaussian AHP with GANs for systematic hyperparameter optimization, providing insights into achieving higher performance in image generation tasks. Future research could extend this approach to other neural network architectures and diverse datasets, further demonstrating the versatility of this optimization technique. This method’s potential applications extend across various domains, including data augmentation and anomaly detection, indicating its broad applicability and impact.},
  keywords={Training;Hyperparameter optimization;Generative adversarial networks;Optimization;Computational modeling;Bayes methods;Scalability;Analytic hierarchy process;Load modeling;Iterative methods;Generative adversarial networks;hyperparameter optimization;gaussian analytical hierarchy process;multicriteria decision-making;machine learning},
  doi={10.1109/ACCESS.2024.3518979},
  ISSN={2169-3536},
  month={},}@ARTICLE{9454510,
  author={Khatun, Amena and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={End-to-End Domain Adaptive Attention Network for Cross-Domain Person Re-Identification}, 
  year={2021},
  volume={16},
  number={},
  pages={3803-3813},
  abstract={Person re-identification (re-ID) remains challenging in a real-world scenario, as it requires a trained network to generalise to totally unseen target data in the presence of variations across domains. Recently, generative adversarial models have been widely adopted to enhance the diversity of training data. These approaches, however, often fail to generalise to other domains, as existing generative person re-identification models have a disconnect between the generative component and the discriminative feature learning stage. To address the on-going challenges regarding model generalisation, we propose an end-to-end domain adaptive attention network to jointly translate images between domains and learn discriminative re-id features in a single framework. To address the domain gap challenge, we introduce an attention module for image translation from source to target domains without affecting the identity of a person. More specifically, attention is directed to the background instead of the entire image of the person, ensuring identifying characteristics of the subject are preserved. The proposed joint learning network results in a significant performance improvement over state-of-the-art methods on several challenging benchmark datasets.},
  keywords={Adaptive systems;Adaptation models;Cameras;Generative adversarial networks;Transforms;Task analysis;Lighting;Person re-identification;domain adaptation;attention;image translation;end-to-end network},
  doi={10.1109/TIFS.2021.3088012},
  ISSN={1556-6021},
  month={},}@ARTICLE{9690234,
  author={Cousins, Stephen},
  journal={Engineering & Technology}, 
  title={Artificial intelligence architecture: AI takes on city design}, 
  year={2021},
  volume={16},
  number={11},
  pages={34-37},
  abstract={ARCHITECTS AND ENGINEERS have long harnessed the power of software to automate aspects of the design of buildings and structures, but a new breed of generative design tools based on artificial intelligence is pushing the boundaries of scale, complexity and computational power like never before.},
  keywords={Buildings;Software;Proposals;Planning;Urban planning;Design tools;Computer architecture},
  doi={10.1049/et.2021.1104},
  ISSN={1750-9637},
  month={Dec},}@INPROCEEDINGS{10935580,
  author={Chen, Haoyu and Luo, Jiahua and Shi, Wenzhe and Wu, Junjie and Wang, Jiaxiang and Leach, Mark and Zhang, Xiaojun and Zhang, Quan},
  booktitle={2024 14th International Conference on Information Technology in Medicine and Education (ITME)}, 
  title={Development of An AI Desktop Assistant based on ESP32 : Practice on Experiential Learning in Engineering Education}, 
  year={2024},
  volume={},
  number={},
  pages={1156-1160},
  abstract={In the present study, we have attempted to implement experiential learning (EL) which is, in particular, considering to including practice of using generative AI in the learning contents, in education of the discipline of mechatronics and robotics. In particular, we introduce to students knowledge of generative AI, and their role in modern engineering systems, as well as encouraging them to practice relevant technology in their group projects. We have presented a typical case study of a student group project, which efficiently practices EL integrated with generative AI technology, i.e. the development of an AI desktop assistant, in which students have practiced and integrated in the device the following AI-based technology. Results and outcomes can readily show that the developed AI desktop assistant boasts impeccable functions and is highly in line with the demands of the present education market. Meanwhile, such projects examples, under the EL framework, whilst integrated with recent AI technology, can greatly motivate students’ learning and their performance.},
  keywords={Knowledge engineering;Mechatronics;Generative AI;Service robots;Education;Market research;Information technology;Microphones;Clocks;Experiential learning (EL);Generative AI;Education;Mechatronics and Robotics},
  doi={10.1109/ITME63426.2024.00230},
  ISSN={2474-3828},
  month={Sep.},}@INPROCEEDINGS{9853749,
  author={Gopal, Anuj},
  booktitle={2022 International Conference on Advanced Learning Technologies (ICALT)}, 
  title={Automatic Question Generation for Hindi and Marathi}, 
  year={2022},
  volume={},
  number={},
  pages={19-21},
  abstract={Artificial Intelligence has contributed remarkably towards educational purposes in the last few decades. With the rise of online learning, most of the tedious tasks have been transferred from the human hands to machines and programs. Question generation from educational paragraphs has been a challenging activity for educators as it requires significant resources. Although there have been major breakthroughs in the English language towards this goal, there is a need for technology-enabled educational services in Indian regional languages. In this paper, we present automatic question generation for two Indian languages - Hindi and Marathi utilizing Sentence Constituency Parsing with openAI Generative Pre-Trained(GPT) and Text-to-Text Transfer Transformers(T5) models, evaluated by content experts of corresponding languages, generating optimum questions after removing repetitions and inaccuracies in structural constraints on composition.},
  keywords={Manuals;Transformers;Task analysis;Artificial intelligence;Portals;Edtech;Reading Skills;Assessment;Question Generation},
  doi={10.1109/ICALT55010.2022.00012},
  ISSN={2161-377X},
  month={July},}@ARTICLE{10471256,
  author={Hurlburt, George F. and Dabirian, Amir},
  journal={IT Professional}, 
  title={Why IT Lessons Matter}, 
  year={2024},
  volume={26},
  number={1},
  pages={19-25},
  abstract={Is IT bringing humanity headlong to the brink of extinction, or does technology offer profound solutions to stave off existential doom? The elusive answer has increasingly become the topic of ongoing intellectual speculation, both pro and con. Artificial intelligence (AI) has experienced disappointing initiatives since it was first proposed in 1950. In 2023, AI exploded into popular culture. Then, Open AI unveiled the Q-Star mathematical algorithm in late 2023. This introduced the ability for generative AI to reason numerically, dramatically augmenting large language models. This rudimentary breakthrough likely created upheaval at Open AI, possibly even redimensioning the ongoing debate. In the meantime, as humankind approaches the second quarter of the 21st century, further insights may arise through evaluating the dramatic paradigm shifts brought to bear through IT in the preceding 25 years.},
  keywords={Information technology;Social implications of technology;Technology forecasting;Ethics;Social factors;Artificial intelligence;Open systems;Machine learning;Philosophical considerations;Human computer interaction;Human-robot interaction},
  doi={10.1109/MITP.2024.3356112},
  ISSN={1941-045X},
  month={Jan},}@INBOOK{10952571,
  author={McGeorge, Donna},
  booktitle={The ChatGPT Revolution: Get Curious, Get Productive and Get Creative with AI}, 
  title={What is ChatGPT?}, 
  year={2024},
  volume={},
  number={},
  pages={5-30},
  abstract={Summary <p>As a language model, ChatGPT has been trained on a massive amount of data to be able to understand and generate human&#x2010;like (natural) language. ChatGPT has been trained on a wide range of topics, from simple trivia questions to more complex topics like science, history and politics. Artificial intelligence (AI) and machine learning have come a long way since their inception in the mid&#x2010;twentieth century. ChatGPT, like its peers, is a &#x2018;large language model&#x2019; that has been trained on a massive dataset of text. This gives it the ability to understand and respond to natural language questions. The chapter provides some incredible stats around what's happening in the world of generative AI. One of the biggest challenges of modern life is achieving some semblance of work&#x2010;life balance. Technology is constantly evolving, and new tools and applications are being developed every day.</p>},
  keywords={Chatbots;Artificial intelligence;Social networking (online);Virtual assistants;History;Accuracy;Roads;Oral communication;Feeds;Engines},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394283149},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952571},}@ARTICLE{10005626,
  author={Dhar, Tribikram and Dey, Nilanjan and Borra, Surekha and Sherratt, R. Simon},
  journal={IEEE Transactions on Technology and Society}, 
  title={Challenges of Deep Learning in Medical Image Analysis—Improving Explainability and Trust}, 
  year={2023},
  volume={4},
  number={1},
  pages={68-75},
  abstract={Deep learning has revolutionized the detection of diseases and is helping the healthcare sector break barriers in terms of accuracy and robustness to achieve efficient and robust computer-aided diagnostic systems. The application of deep learning techniques empowers automated AI-based utilities requiring minimal human supervision to perform any task related to medical diagnosis of fractures, tumors, and internal hemorrhage; preoperative planning; intra-operative guidance, etc. However, deep learning faces some major threats to the flourishing healthcare domain. This paper traverses the major challenges that the deep learning community of researchers and engineers faces, particularly in medical image diagnosis, like the unavailability of balanced annotated medical image data, adversarial attacks faced by deep neural networks and architectures due to noisy medical image data, a lack of trustability among users and patients, and ethical and privacy issues related to medical data. This study explores the possibilities of AI autonomy in healthcare by overcoming the concerns about trust that society has in autonomous intelligent systems.},
  keywords={Medical diagnostic imaging;Deep learning;Medical services;Artificial intelligence;Training;Attenuation;Task analysis;Adversarial machine learning;Computer aided diagnosis;Convolutional neural networks;Data augmentation;Biomedical imaging;Adversarial attacks;computer-aided diagnostic systems;convolutional neural network;data augmentation;deep learning;medical image analysis},
  doi={10.1109/TTS.2023.3234203},
  ISSN={2637-6415},
  month={March},}@INPROCEEDINGS{10928193,
  author={Iqal, Zeinab M. and Selamat, Ali},
  booktitle={2024 IEEE International Conference on Computing (ICOCO)}, 
  title={A Comprehensive Analysis of Risk-Based Access Control Models for IoT: Balancing Security, Adaptability, and Resource Efficiency}, 
  year={2024},
  volume={},
  number={},
  pages={344-349},
  abstract={The Internet of Things (IoT) has revolutionized how devices connect and interact, but it also brings serious security challenges, especially in managing access control. Traditional models like Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC) often fall short in the face of IoT’s dynamic and complex environments. This paper offers a comprehensive analysis of 14 risk-based access control models, evaluated across five key criteria: security effectiveness, scalability, resource efficiency, adaptability, and integration of AI and machine learning. Advanced models leveraging AI, fuzzy logic, and neuro-fuzzy systems demonstrate exceptional accuracy and adaptability but face many other resource demands and scalability challenges. Conversely, resource-efficient models often trade off security and adaptability. Our findings underscore the need for innovative hybrid solutions that balance these competing priorities. This study provides practical insights for IoT practitioners and policymakers while highlighting research gaps to inspire future advancements in risk-based access control for IoT systems.},
  keywords={Access control;Fuzzy logic;Adaptation models;Analytical models;Accuracy;Scalability;Computational modeling;Internet of Things;Artificial intelligence;Faces;Access Control;Risk-Based;Risk Estimation;Internet of Things;IoT},
  doi={10.1109/ICOCO62848.2024.10928193},
  ISSN={},
  month={Dec},}@ARTICLE{9872056,
  author={Michelinakis, Foivos and Pujol-Roig, Joan S. and Malacarne, Sara and Xie, Min and Dreibholz, Thomas and Majumdar, Sayantini and Poe, Wint Yi and Patounas, Georgios and Guerrero, Carmen and Elmokashfi, Ahmed and Theodorou, Vasileios},
  journal={IEEE Transactions on Network and Service Management}, 
  title={AI Anomaly Detection for Cloudified Mobile Core Architectures}, 
  year={2023},
  volume={20},
  number={2},
  pages={1976-1992},
  abstract={IT systems monitoring is a crucial process for managing and orchestrating network resources, allowing network providers to rapidly detect and react to most impediment causing network degradation. However, the high growth in size and complexity of current operational networks (2022) demands new solutions to process huge amounts of data (including alarms) reliably and swiftly. Further, as the network becomes progressively more virtualized, the hosting of NFV on cloud environments adds a magnitude of possible bottlenecks outside the control of the service owners. In this paper, we propose two deep learning anomaly detection solutions that leverage service exposure and apply it to automate the detection of service degradation and root cause discovery in a cloudified mobile network that is orchestrated by ETSI OSM. A testbed is built to validate these AI models. The testbed collects monitoring data from the OSM monitoring module, which is then exposed to the external AI anomaly detection modules, tuned to identify the anomalies and the network services causing them. The deep learning solutions are tested using various artificially induced bottlenecks. The AI solutions are shown to correctly detect anomalies and identify the network components involved in the bottlenecks, with certain limitations in a particular type of bottlenecks. A discussion of the right monitoring tools to identify concrete bottlenecks is provided.},
  keywords={Monitoring;Artificial intelligence;5G mobile communication;Anomaly detection;Cloud computing;Quality of service;Deep learning;Anomaly detection;autoencoders;deep learning;5G;AI;smart networks;mobile networks},
  doi={10.1109/TNSM.2022.3203246},
  ISSN={1932-4537},
  month={June},}@INPROCEEDINGS{11061343,
  author={Cools, Kasper and Maathuis, Clara and Cubber, Geert de and VandewaL, Marijke and Deligiannis, Nikos},
  booktitle={2025 International Conference on Military Technologies (ICMT)}, 
  title={Evaluation Techniques for Modern Military Camouflage}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The integration of AI in military operations has enhanced the ability to detect camouflaged objects, as evidenced by the increased use of drones in conflicts such as those in the Middle East and the Ukraine-Russia war. This shift towards AI-powered detection systems necessitates a reevaluation of camouflage evaluation methods and metrics. This systematic literature review examines the evolution of camouflage techniques from naturalistic patterns to advanced designs taking into account AI and drone technology. It explores both traditional and modern evaluation methods, focusing on their applicability in military contexts. The study employs a mixed-methods approach, combining qualitative and quantitative analysis to provide a comprehensive evaluation of camouflage effectiveness. It also incorporates context-specific and generalisable metrics to ensure thorough evaluation across different environments and scenarios. The findings highlight the importance of developing robust evaluation techniques that can address the challenges posed by both human and AI detection systems. This review underscores the need for continuous improvement in camouflage design and evaluation to maintain the effectiveness of military strategies in modern warfare.},
  keywords={Measurement;Visualization;Machine learning algorithms;Statistical analysis;Observers;Real-time systems;Pattern recognition;Artificial intelligence;Drones;Systematic literature review;Defence Technology;Camouflage pattern evaluation;military camouflage;AI},
  doi={10.1109/ICMT65201.2025.11061343},
  ISSN={2996-4474},
  month={May},}@INPROCEEDINGS{11146346,
  author={Khamaisi, Karim and Crazzolara, Anton and Ristic, Aleksandar and Brügger, Samuel and Rodrigues, Bruno and von der Assen, Jan and Stiller, Burkhard},
  booktitle={2025 IEEE 50th Conference on Local Computer Networks (LCN)}, 
  title={Moving Target Offense: RL-based Adaptive Evasion Strategies for C2 Frameworks}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Modern Intrusion Detection Systems (IDS) that rely on signature-based detection struggle to detect novel threats like AI-based Command-and-Control (C2) frameworks, highlighting a gap in the ability of current systems to detect and mitigate emerging botnet threats. This work addresses this gap by investigating NimPlant’s detection and evasion capabilities.This paper (1) shows strategies to detect NimPlant bots using network traffic analysis, (2) enhances their evasion capabilities using Reinforcement Learning (RL), and (3) evaluates AI-driven evasion against IDS. Using a controlled testing environment, an infected device is simulated by implementing evasion strategies and integrating them into an RL-based AI system. Herewith, AI significantly improves evasion, with the RL-enhanced NimPlant achieving higher detection bypass rates. However, the IDS configuration heavily impacts AI effectiveness, underscoring the need for robust security setups. Thus, recommendations are provided for detecting bot infections and countering AI-enhanced botnets.},
  keywords={Command and control systems;Botnet;Intrusion detection;Reinforcement learning;Telecommunication traffic;Malware;Computer networks;Security;Artificial intelligence;Testing;Botnet;Command-and-Control;Malware;Reinforcement Learning;Intrusion Detection},
  doi={10.1109/LCN65610.2025.11146346},
  ISSN={2832-1421},
  month={Oct},}@INPROCEEDINGS{10871665,
  author={Arnaz, Azadeh and Lipman, Justin and Abolhasan, Mehran},
  booktitle={2024 17th International Conference on Security of Information and Networks (SIN)}, 
  title={Adversarial Attack Vectors Against Near-Real-Time AI xApps in the Open RAN}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={OpenRAN is revolutionizing wireless telecommunications, enabling more flexible and innovative network architectures. Within this framework, near-real-time applications in RAN Intelligent Controllers (near-RT RIC) are pushing the boundaries of ultra-reliable low latency communications. However, security concerns challenge their adoption. This paper investigates vulnerabilities in near-RT RIC AI xApps through systematic experiments, focusing on a Handover AI xApp. Using four distinct attack strategies, we demonstrate that current security measures are inadequate, exposing these Ultra-Reliable Low Latency Communications (URLLC) AI xApps to various attacks. Our findings highlight the potential for malicious exploitation, emphasizing the need for robust security frameworks in OpenRAN deployments utilizing near-RT applications.},
  keywords={Wireless communication;Technological innovation;Systematics;Open RAN;Ultra reliable low latency communication;Vectors;Telecommunications;Security;Artificial intelligence;Monitoring;Open RAN;AI security;xApps;URLLC;ML attacks},
  doi={10.1109/SIN63213.2024.10871665},
  ISSN={},
  month={Dec},}@ARTICLE{10975840,
  author={Iqbal, Saeed and Zhong, Xiaopin and Khan, Muhammad Attique and Shabaz, Mohammad and Wu, Zongze and AlHammadi, Dina Abdulaziz and Liu, Weixiang and Algamdi, Shabbab Ali and Li, Yang},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Transforming Healthcare Diagnostics With Tensorized Attention and Continual Learning on Multi-Modal Data}, 
  year={2025},
  volume={71},
  number={2},
  pages={3391-3412},
  abstract={Analyzing multi-modal medical data in the setting of uncertain healthcare situations continues to be a major topic in medical image analysis and healthcare big data. Traditional machine learning algorithms are severely hampered by inaccurate data fusion, a lack of adaptability to changing patient data, and challenges managing uncertainty. These difficulties are made worse by complicated medical images and diverse data sources, which results in less accurate diagnosis and worse-than-ideal healthcare choices. To tackle these urgent problems, this paper suggests two new approaches: Continual Learning using Progressive Neural Networks (PNNs) and Tensorized Attention Mechanism for Data Fusion. The Tensorized Attention Mechanism improves multi-modal data fusion by using dynamic, task-specific attention to improve feature alignment across modalities, and the PNNs framework uses continual learning, memory augmentation, and domain adaptation to ensure robust learning under data uncertainty. We test these methods on a variety of multi-modal datasets, such as MIMIC-IV, CheXpert, MOST, OAI, and Heart Murmur, which offer a comprehensive representation of medical data from clinical reports, chest X-rays, heart murmurs, and other heterogeneous data sources. Our experimental results show notable improvements in diagnostic performance, with notable results like a CFI of 0.10, a KR score of 90.4%, and an MMC score of 0.097, indicating superior generalization and robustness across domains. Healthcare AI applications could be revolutionized by the use of specialized losses, such as Conditional Variational Autoencoder (CVAE), Adversarial Contrastive Learning (ACL), Reciprocal Regularization, and domain adaptation losses, which are essential for preventing forgetting and guaranteeing learning stability across shifting data streams.},
  keywords={Medical services;Uncertainty;Medical diagnostic imaging;Artificial intelligence;Data integration;Continuing education;Big Data;Attention mechanisms;Soft sensors;Robot sensing systems;Multi-modal data;continual learning;catastrophic forgetting;tensorized attention mechanism;progressive neural networks (PNNs);data fusion},
  doi={10.1109/TCE.2025.3563986},
  ISSN={1558-4127},
  month={May},}@INPROCEEDINGS{11105325,
  author={Kumar, Harsh and Tshakwanda, Petro M. and Devetsikiotis, Michael},
  booktitle={2025 IEEE World AI IoT Congress (AIIoT)}, 
  title={AI-Driven Smart Grid Optimization: Enhancing Urban Communication Networks}, 
  year={2025},
  volume={},
  number={},
  pages={0273-0279},
  abstract={As the global demand for electrical energy escalates, Smart Grids (SGs) have emerged as a vital component in managing this transition, particularly within the framework of smart cities. Integrating distributed renewable energy sources (DRES) and energy storage systems (ESS) demands innovative approaches to power management, ensuring efficient and environmentally sustainable distribution networks. The communication infrastructure within SGs must be robust and reliable, capable of handling significant data volumes from various smart technologies utilized in urban spaces. This paper investigates the incorporation of machine learning algorithms with smart grid communications to predict communication patterns and optimize data routing in dynamic environments, particularly focusing on the interplay between SGs and the advanced communication needs of smart cities. A unified mathematical model is introduced to encapsulate critical parameters such as latency, reliability, throughput, and jitter, showcasing the symbiotic relationship between AI technologies and urban smart infrastructures. The paper illustrates practical applications of these AI-driven methods through case studies, ultimately enhancing communication efficiencies and contributing to the resilient management of power and data flows within smart cities.},
  keywords={Symbiosis;Renewable energy sources;Smart cities;Power system management;Throughput;Routing;Smart grids;Reliability;Artificial intelligence;Optimization},
  doi={10.1109/AIIoT65859.2025.11105325},
  ISSN={},
  month={May},}@INPROCEEDINGS{11083734,
  author={Ullah, Salim and Sahoo, Siva Satyendra and Kumar, Akash},
  booktitle={2025 14th International Conference on Modern Circuits and Systems Technologies (MOCAST)}, 
  title={Approximate Arithmetic Circuits Enabling Energy-Efficient Edge Computing}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Approximate Computing (AxC) has emerged as a powerful technique for enabling efficient AI/ML workloads on resourceconstrained edge devices. By judiciously introducing controlled inaccuracies, AxC leverages the inherent resilience of AI/ML models to unlock substantial gains in power, performance, and area (PPA). Nonetheless, the widespread adoption of AxC is hampered by the need for applicationspecific designs, which increase design and implementation complexity. To address this, we highlight advanced techniques for approximate arithmetic operator design-focusing on LUT-level optimizations, automated operator modeling, and design-space exploration on FPGAs. Through these methods, AI-enabled Electronic Design Automation (EDA) can effectively reduce development complexity and drive large-scale AxC adoption, paving the way for energy-efficient, scalable AI computing at the edge.},
  keywords={Computational modeling;Search methods;Approximate computing;Circuits;Energy efficiency;Complexity theory;Reconfigurable architectures;Artificial intelligence;Arithmetic;Resilience;AI Computing;Arithmetic Circuits;Approximate Computing;Reconfigurable Architectures},
  doi={10.1109/MOCAST65744.2025.11083734},
  ISSN={2993-4443},
  month={June},}@INPROCEEDINGS{10451584,
  author={Tian, LiHua and Wang, AoZe and Sun, ZhiGang and Xiao, Li},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Efficient Fabric Defect Detection Algorithm Based on Improved YOLOv5}, 
  year={2023},
  volume={},
  number={},
  pages={1538-1543},
  abstract={In order to address the issues of real-time performance and the low dependency between feature channels in fabric defect detection networks, this paper proposes the ESE_YOLOv5 network based on YOLOv5. Firstly, to address the relative redundancy of the neck detection network feature channels, a relatively lightweight and efficient convolution module is adopted to ensure accuracy while reducing computation and parameter volume. Furthermore, the Efficient Squeeze-Excitation (ESE) module is introduced into the backbone to optimize the dependency of feature channels, which enhances the model's feature extraction capacity and improves detection accuracy. Experimental results show that compared to YOLOv5, the proposed ESE_YOLOv5 model reduces computation and parameter volume while improving accuracy, meeting the needs of fabric defect detection for recognizing fabric defects that have similar characteristics to the background while maintaining real-time performance.},
  keywords={YOLO;Performance evaluation;Solid modeling;Computational modeling;Redundancy;Feature extraction;Fabrics;Surface defect detection in fabric;ESE_YOLOv5;Lightweight;Attention mechanism},
  doi={10.1109/CAC59555.2023.10451584},
  ISSN={2688-0938},
  month={Nov},}@ARTICLE{9917504,
  author={Yan, Chengdong and Ding, Jurong and Zhang, Hui and Tong, Ke and Hua, Bo and Shi, Shaolong},
  journal={IEEE Access}, 
  title={SEResU-Net for Multimodal Brain Tumor Segmentation}, 
  year={2022},
  volume={10},
  number={},
  pages={117033-117044},
  abstract={Glioma is the most common type of brain tumor, and it has a high mortality rate. Accurate tumor segmentation based on magnetic resonance imaging (MRI) is of great significance for the diagnosis and treatment of brain tumors. Recently, the automatic segmentation of brain tumors based on U-Net has gained considerable attention. However, brain tumor segmentation is a challenging task due to the structural variations and inhomogeneous intensity of tumors. Existing brain tumor segmentation studies have shown that the problems of insufficient down-sampling feature extraction and loss of up-sampling information arise when using U-Net to segment brain tumors. In this study, we proposed an improved U-Net model, SEResU-Net, which combines the deep residual network and the Squeeze-and-Excitation Network. The deep residual network solves the problem of network degradation so that SEResU-Net can extract more feature information. The Squeeze-and-Excitation Network avoids information loss and enables the network to focus on the useful feature map, which solves the problem of insufficient segmentation accuracy of small-scale brain tumors. Furthermore, a fusion loss function combining Dice loss and cross-entropy loss was proposed to solve the problems of network convergence and data imbalance. The performance of SEResU-Net was evaluated on the dataset of BraTS2018 and BraTS2019. Experimental results revealed that the mean Dice similarity coefficients of SEResU-Net were 0.9373, 0.9108, and 0.8758 for the whole tumor, the tumor core, and the enhanced tumor, which were 7.10%, 11.88%, and 15.33% greater than those of the U-Net benchmark network, respectively. Our findings demonstrate that the proposed SEResU-Net has a competitive effect in segmenting multimodal brain tumors.},
  keywords={Tumors;Image segmentation;Feature extraction;Magnetic resonance imaging;Residual neural networks;Deep learning;Neural networks;Brain;MRI;brain tumor segmentation;deep learning;U-Net;residual network;squeeze-and-excitation network},
  doi={10.1109/ACCESS.2022.3214309},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10346978,
  author={Chakravarty, Diksha and Thomas, Amy Mariam and Vivek, V.},
  booktitle={2023 International Conference on Computer Science and Emerging Technologies (CSET)}, 
  title={A Survey on Decentralization and Virtualization of Medical Trials: An approach through Ensemble learning models and Convolutional Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={- In sectors such as medicine where accuracy and time are the most prioritized factors the use of technology and its tools have led to huge advancements and contributed to saving millions of lives. Technology has been bifurcated into various categories to suit multiple requirements of the medical sector. The use of Machine Learning along with well-designed models and sister technologies such as Computer Vision has opened up a whole new dimension in medical sciences. Machine learning deals with the building of algorithms that can predict using fed patterns which further specializes in Deep Learning. Deep learning's success in a variety of pattern recognition applications has sparked excitement and raised expectations that deep learning, or Artificial Intelligence (AI), could revolutionize the health care that we know today. Right from diagnosis to treatment it can revolutionize the way we construct and operate the medical sector. Over the last decade, Convolutional Neural Networks have achieved breakthroughs in a range of pattern recognition domains. CNN's are generally employed to solve complex image-driven pattern recognition tasks. They have proven to be a great tool in image recognition and classification and are tuned according to the needs of the user. Ensemble learning, as a research area, strives to bring data fusion, data modeling, and data mining together in a single framework. Ensemble learning combines the informative knowledge gained from the previous outcomes to produce knowledge, discovery, and improved predictive performance using adaptive voting methods. In this study, the methodologies of numerous models of image recognition and classification, as well as ensemble learning from multiple authors, are evaluated. In addition, this study shall discuss any feasible enhancements that could be implemented to the existing models as well as how the ideologies of the models could be used for efficient diagnosis in the field of medicine.},
  keywords={Deep learning;Surveys;Adaptation models;Image recognition;Computational modeling;Data models;Ensemble learning;Ensemble learning;Image recognition;Convolutional Neural Network;Image classification;Deep- learning},
  doi={10.1109/CSET58993.2023.10346978},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9182406,
  author={Zhang, Xinyu and Zhao, Yang and Zhang, Hao},
  booktitle={2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Dual-discriminator GAN: A GAN way of profile face recognition}, 
  year={2020},
  volume={},
  number={},
  pages={162-166},
  abstract={A wealth of angle problems occur when facial recognition is performed. At present, the feature extraction network presents eigenvectors with large differences between the frontal face and profile face recognition of the same person in many cases. For this reason, the state-of-the-art facial recognition network have a relatively poor performance on profile faces. In this paper, we proposed a method of image-to-image generating frontal faces from profile faces based on Generative Adversarial Network (GAN). By adopting a new dual discriminator Gan structure and an artificial programed training process, the authority and identity of the generated frontal faces have been greatly improved.},
  keywords={Face;Generators;Gallium nitride;Training;Face recognition;Decoding;Feature extraction;Face Recognition;GAN},
  doi={10.1109/ICAICA50127.2020.9182406},
  ISSN={},
  month={June},}@ARTICLE{11020744,
  author={Kuzdeuov, Askat and Zakaryanov, Miras and Tleuliyev, Alim and Varol, Huseyin Atakan},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science}, 
  title={OpenThermalPose2: Extending the Open-Source Annotated Thermal Human Pose Dataset With More Data, Subjects, and Poses}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Human pose estimation has many applications in action recognition, human-robot interaction, motion capture, augmented reality, sports analytics, and healthcare. Numerous datasets and deep learning models have been developed for human pose estimation within the visible domain. However, poor lighting conditions and privacy issues persist. These challenges can be addressed using thermal cameras; however, there is a limited number of annotated thermal human pose datasets for training deep learning models. Previously, we presented the OpenThermalPose dataset with 6,090 thermal images of 31 subjects and 14,315 annotated human instances. In this work, we extend OpenThermalPose with more thermal images, human instances, and poses. The extended dataset, OpenThermalPose2, contains 21,125 elaborately annotated human instances within 11,391 thermal images of 170 subjects. To show the efficacy of OpenThermalPose2, we trained the YOLOv8-pose and YOLO11-pose models on the dataset. The experimental results showed that models trained with OpenThermalPose2 outperformed the previous YOLOv8-pose models trained with OpenThermalPose. Additionally, we optimized the YOLO11-pose models trained on OpenThermalPose2 by converting their checkpoints from PyTorch to TensorRT formats. We deployed the PyTorch and TensorRT models on an NVIDIA Jetson AGX Orin 64GB and measured their inference time and accuracy. The TensorRT models using half-precision floating-point (FP16) achieved the best balance between speed and accuracy, making them suitable for real-time applications. We have made the dataset, source code, and pre-trained models publicly available at https://github.com/IS2AI/OpenThermalPose to bolster research in this field.},
  keywords={Cameras;Training;Annotations;Lighting;Image resolution;Pose estimation;Accuracy;Testing;Indoor environment;Biological system modeling;Thermal human pose estimation;thermal human pose dataset;YOLOv8-pose;YOLO11-pose},
  doi={10.1109/TBIOM.2025.3575499},
  ISSN={2637-6407},
  month={},}@ARTICLE{11016129,
  author={Li, Ying and Zhu, Shichang and Liu, Jianjian and Yu, Zhengtao and Huang, Yuxin},
  journal={IEEE Transactions on Audio, Speech and Language Processing}, 
  title={Multi-source Domain Adaptation for Dependency Parsing via In-depth Feature Transfer}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Dependency parsing endeavours to extract both syntactic and semantic knowledge of the input sentence via a dependency tree. Recently, supervised dependency parsers have achieved significant improvements thanks to the strong representation of pre-trained language models. However, the parsing accuracy drops dramatically when the model is trained on multiple out-of-domain training datasets. The key to addressing this problem is to learn the commonalities and differences between different domains. Although the widely used sharedprivate model attempts to capture domain-invariant and domainspecific feature representations by separated encoders, the two representations may interfere with each other, and the in-depth relationship between different domain features may be ignored. In this work, we explore to emphasize the useful domain-specific features and filter out the harmful ones in explicit and implicit aspects via different feature transfer strategies. Simultaneously, we leverage adversarial learning to capture more effective domain-invariant features that belong to both source and target domains. Experimental results show that our proposed model can improve the parsing accuracy of several strong baseline models significantly. The thorough analysis helps us to gain more insights into the knowledge transfer process between different modules.},
  keywords={Feature extraction;Adaptation models;Bidirectional long short term memory;Adversarial machine learning;Training;Accuracy;Syntactics;Data mining;Speech processing;Semantics;natural language processing;dependency parsing;multi-source domain adaptation;feature transfer;adversarial learning},
  doi={10.1109/TASLPRO.2025.3572796},
  ISSN={2998-4173},
  month={},}@INPROCEEDINGS{8823346,
  author={Lang, Jiaqi and Li, Linjing and Chen, Weiyun and Zeng, Daniel},
  booktitle={2019 IEEE International Conference on Intelligence and Security Informatics (ISI)}, 
  title={Privacy Protection in Transformer-based Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={182-184},
  abstract={With the great success of neural networks, it is important to improve the information security of application systems based on them. This paper investigates a scenario where an attacker eavesdrops the intermediate representation computed by the encoder layers and tries to recover the private information of the input text. We propose a new metric to evaluate the encoder's ability to protect privacy and evaluate the Transformer-based encoder, which is the first privacy research conducted on Transformer-based neural networks. We also propose an adversarial training method to enhance the privacy of Transformer-based neural networks.},
  keywords={Privacy;Training;Task analysis;Neural networks;Measurement;Natural language processing;Computational linguistics;Privacy protection;Neural network;Transformer;Representation learning},
  doi={10.1109/ISI.2019.8823346},
  ISSN={},
  month={July},}@ARTICLE{10677551,
  author={Wang, Lu and Liu, Lulu and Lu, Xiaoxia},
  journal={IEEE Access}, 
  title={Robot Path Planning Based on Generative Learning Particle Swarm Optimization}, 
  year={2024},
  volume={12},
  number={},
  pages={130063-130072},
  abstract={Path planning refers to finding the optimal path from the starting point to the endpoint in a given environment, avoiding obstacles. To solve the problem of slow convergence speed in particle swarm optimization in path planning, this paper proposes a robot path planning based on generative learning Particle Swarm Optimization (LPSO). This algorithm constructs a generative double-adversarial network. In the first stage, the generator was used to analyze and process the initial map to obtain a foreground area with feasible paths. This area is used for heuristic search of particle swarms, reducing unnecessary exploration areas for particles throughout the state space, and quickly achieving path planning goals. In the second stage, the foreground region obtained in the first stage is used as the global optimal particle path for particle swarm optimization, and the particles are guided to move in the direction of high-density pheromones. Finally, the obstacle avoidance strategy enables the robot to avoid moving obstacles safely. In addition to being adapted to simple raster maps, this method also performs well in actual environment maps, showing superior generalization ability. To verify the effectiveness of the proposed algorithm, a series of simulation experiments are compared with the traditional PSO, ant colony algorithm (ACO), and improved algorithms, and the results show that under the same map environment, the LPSO algorithm has a faster convergence speed and shorter planning time and path length.},
  keywords={Path planning;Robots;Particle swarm optimization;Generators;Heuristic algorithms;Convergence;Planning;Path planning;particle swarm optimization;generative double-adversarial networks;foreground area},
  doi={10.1109/ACCESS.2024.3457957},
  ISSN={2169-3536},
  month={},}@ARTICLE{10620285,
  author={Duy, Phan The and Minh, Vo Quang and Dang, Bui Tan Hai and Son, Ngo Duc Hoang and Quyen, Nguyen Huu and Pham, Van-Hau},
  journal={IEEE Access}, 
  title={A Study on Adversarial Sample Resistance and Defense Mechanism for Multimodal Learning-Based Phishing Website Detection}, 
  year={2024},
  volume={12},
  number={},
  pages={137805-137824},
  abstract={Recent advancements in Artificial Intelligence (AI) have greatly impacted cybersecurity, particularly in detecting phishing websites. Traditional methods struggle to address evolving vulnerabilities, but research shows that Machine Learning (ML), Ensemble Learning (EL), and Deep Learning (DL) are effective in developing defenses. However, these methods face challenges with adversarial examples (AEs). The multimodal model (MM) is a promising solution, yet there is a significant lack of research using multimodal techniques specifically for phishing website detection (PWD) against adversarial websites. To tackle this challenge, this paper assesses 15 learning-based models, particularly multimodal ones, for phishing and adversarial detection, aiming to enhance their defense capabilities. Due to the scarcity of adversarial websites, training and testing models are limited. Therefore, this study proposes an innovative attack framework, AWG - Adversarial Website Generation that employs Generative Adversarial Networks (GAN) and transfer-based black box attacks to create AEs. This framework closely mirrors real-world attack scenarios, ensuring high effectiveness and realism. Finally, we present defense strategies with straightforward implementation and high effectiveness to enhance the resistance of models. The models underwent training and testing on a dataset collected from reputable sources such as OpenPhish, PhishTank, Phishing Database, and Alexa. This approach was chosen to ensure the dataset’s diversity and relevance to reflect real-world conditions. Experimental results highlight that the Generator’s effectiveness is demonstrated by a domain structure generation rate exceeding 90%. Moreover, AEs generated by this Generator effectively bypass most state-of-the-art ML, DL, and EL models with an evasion rate of up to 88%. Notably, the Support Vector Machine (SVM) model is the most vulnerable, with a detection rate of only 10.02%. On the other hand, the MM Shark-Eyes demonstrates outstanding resistance against AEs, with a detection rate of up to 99%. Upon applying our defense strategy, the resistance of models is significantly boosted, with all detection rates surpassing 90%. These findings underscore the robustness of our methods and pave the way for further exploration into advanced attack and defense strategies in the context of phishing and adversarial website detection.},
  keywords={Phishing;Feature extraction;Resistance;Accuracy;Artificial intelligence;Solid modeling;Robustness;Multimodal;deep learning;machine learning;ensemble learning;phishing website detection;phishing domain;adversarial attacks;transferability of adversarial samples},
  doi={10.1109/ACCESS.2024.3436812},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10534029,
  author={Pathariya, Mohammed Johar and Basavraj Jalkote, Pratyush and Patil, Aniket Maharudra and Ashok Sutar, Abhishek and Ghule, Rajashree L.},
  booktitle={2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS)}, 
  title={Tunes by Technology: A Comprehensive Survey of Music Generation Models}, 
  year={2024},
  volume={},
  number={},
  pages={506-512},
  abstract={Notable developments in Artificial Intelligence-based music production have brought about a transformation in the field of musical composition. The aim of this article is to give a summary of the different models and techniques used in AI-driven music production. It explores the need for these developments, highlighting the shortcomings of traditional music production methods, which often call for a significant investment of time and expertise and presenting Artificial Intelligence (AI) as a revolutionary solution to these challenges. Several models are examined in the review, including Transformer Architecture, Long Short-Term Memory (LSTM), Generative Adversarial Network (GAN) and many more. This study aims to give a comprehensive overview of Music generating techniques and their potential to improve the efficiency, accessibility, and collaborative nature of music creation within the musical environment.},
  keywords={Surveys;Visualization;Reviews;Music;Production;Transformers;Generative adversarial networks;Music Generation;Artificial Intelligence;GAN;LSTM;Transformer Architecture;CVAE;Traditional Music Composition;Democratization;Innovative Models;Music Composition Techniques;AI-Driven Creativity;Musical Landscape},
  doi={10.1109/ICC-ROBINS60238.2024.10534029},
  ISSN={},
  month={April},}@INPROCEEDINGS{10275437,
  author={Vietz, Hannes and Hirth, Manuel and Baum, Sebastian and Weyrich, Michael},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Synthetic Data Generation for improving Deep Learning-based 5G Indoor Positioning}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Gathering sufficient labeled training data to effectively train a high-performing deep learning model can be particularly challenging in the realm of industrial automation. Depending on the data type, this may require expensive interruptions to production processes or similar disruptions on the factory floor for data collection. It is often uncertain which data types are crucial for enhancing the performance of the trained model. For vision models, factors such as specific viewing angles or lighting conditions may be important, while for models utilizing radio signals, unique reflections generated by moving metal surfaces could be significant. Moreover, data labeling is expensive as it is primarily conducted manually by human workers. This paper demonstrates how to automatically generate relevant, labeled synthetic training data to boost a neural network's accuracy for deep learning-based 5G indoor positioning tasks. We reveal that employing this generated synthetic data to train a convolutional neural network can improve its median positioning accuracy by a notable 25%.},
  keywords={Training;Deep learning;5G mobile communication;Training data;Data models;Reflection;Production facilities;synthetic training data;generative models;generative adversarial networks;adversarial attacks;5G;indoor positioning;deep learning;artificial intelligence;localization},
  doi={10.1109/ETFA54631.2023.10275437},
  ISSN={1946-0759},
  month={Sep.},}@ARTICLE{11162510,
  author={Mohammed, Thura J. and Xinying, Chew and Albahri, A. S. and Alnoor, Alhamzah and Khai Wah, Khaw},
  journal={IEEE Access}, 
  title={A Novel Trusted Framework for Orthopedic Disease Detection With Reliability Against Adversarial Attacks}, 
  year={2025},
  volume={13},
  number={},
  pages={160193-160220},
  abstract={This study introduces a novel explainable orthopaedic disease detection framework that is robust to both white-box and black-box adversarial attack examples based on hybrid fuzzy Multi-Criteria Decision-Making (MCDM). It addresses the lack of a resilient framework against adversarial attacks with complex evaluation matrices and the absence of a reliable method for selecting optimal orthopedic disease detection models using four-phase methodology. The dataset was augmented with edge-based techniques, improving generalizability and adversarial robustness. A comprehensive evaluation of nine Machine Learning (ML) models was conducted under Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), Label Poisoning Method (LPM), and Model Extraction Method (MEM) adversarial attacks to increase the robustness of the developed models. The selection of the most robust developed model is established via a novel decision matrix framework and the optimized Weight Fuzzy Judgment Method (WFJM) and Z-Score Multi-Attribute-Based Aggregation (Z-MABAC) method. According to the WFJM, the fooling rate and prevalence obtained the highest average importance weights of 0.090 and 0.094, respectively, across the four attack scenarios. In contrast, specificity, precision, and Cohen’s kappa received the lowest weights, with values of 0.057, 0.058, and 0.069, respectively. The benchmarking results revealed the following top-performing models across adversarial scenarios: the LPM-Decision Tree, BIM-Support Vector Machin, FGSM-Gradient Boosting, and MEM-Random Forest with scores of 0.122, 0.180, 0.122 and 0.170, respectively. The proposed framework validated through sensitivity-correlation analysis and explainability analysis via Local Interpretable Model-agnostic Explanations (LIME). The results underscore the importance of adversarial robustness in AI-based orthopaedic detection systems, contributing to the broader field of secure and ethical AI adoption in healthcare.},
  keywords={Artificial intelligence;Accuracy;Robustness;Diseases;Decision making;Benchmark testing;Perturbation methods;Measurement;Glass box;Closed box;Adversarial machine learning;multi-criteria decision-making (MCDM);orthopaedic disease detection;robustness evaluation;trustworthy artificial intelligence},
  doi={10.1109/ACCESS.2025.3609330},
  ISSN={2169-3536},
  month={},}@ARTICLE{11003961,
  author={Lara, Melvin A. and Hameed, Ibrahim A. and Koziorek, Jiri},
  journal={IEEE Access}, 
  title={Anomaly-Focused Augmentation Method for Industrial Visual Inspection}, 
  year={2025},
  volume={13},
  number={},
  pages={89118-89139},
  abstract={In industrial inspection, the detection of surface defects—such as scratches, dents, or other defects—is crucial for ensuring product quality. However, the limited availability of annotated images of such defects poses challenges for developing reliable detection models. This paper proposes a novel image augmentation algorithm to address this limitation by generating synthetic imperfections from a small sample set. Unlike traditional approaches that augment entire images, the proposed approach isolates defects and applies spatial transformations (e.g., rotation, shear) before blending them into background images. This allows for the creation of diverse training datasets. The algorithm employs a 2D binary map to encode the spatial relationship between defect and background regions of images and then applies an AI-driven model to generate new realistic blended images. By leveraging a generator-discriminator framework, the augmentation process is iteratively refined during training to maintain the style and texture of the original imperfections. Experimental results show that the proposed method significantly enhances the performance of defect detection models compared to existing augmentation techniques while preserving the realism of the generated images.},
  keywords={Artificial intelligence;Mathematical models;Adaptation models;Visualization;Inspection;Image augmentation;Data models;Training;Accuracy;Training data;Image augmentation;artificial intelligence;neural networks;defect detection;image processing},
  doi={10.1109/ACCESS.2025.3570075},
  ISSN={2169-3536},
  month={},}@ARTICLE{10247034,
  author={Shokrollahi, Peyman and Chaves, Juan M. Zambrano and Lam, Jonathan P. H. and Sharma, Avishkar and Pal, Debashish and Bahrami, Naeim and Chaudhari, Akshay S. and Loening, Andreas M.},
  journal={IEEE Access}, 
  title={Radiology Decision Support System for Selecting Appropriate CT Imaging Titles Using Machine Learning Techniques Based on Electronic Medical Records}, 
  year={2023},
  volume={11},
  number={},
  pages={99222-99236},
  abstract={Radiologists use an imaging order from the ordering physician, which includes a radiology title, to select the most suitable imaging protocol. Inappropriate radiology titles can disrupt protocol selection and result in mistaken or delayed diagnosis. The objective of this work is to develop an algorithm to predict correct radiology titles from incoming exam order data. The proposed instrument is an ensemble of five decision tree-based machine learning (ML) techniques (Light Gradient Boosting Machine, eXtreme Gradient Boosting Machine, Random Forest, Adaptive Boosting, and Random UnderSampling Boosting Model) trained to recommend radiology titles of computed tomography imaging examinations based on electronic medical records. Issues of imbalanced data and generalization were addressed. The tuned models were used to predict the top three radiology titles for the radiologist revision. The models were evaluated using a 10-fold cross-validation method, yielding an approximate average accuracy of  $80.5\% \pm 2.02\%$  and F1-score of  $80.3\% \pm 1.67\%$  for all models, while the ensemble classifier (~83% F1-score) outperformed individual models. An accumulated average accuracy of ~92% was obtained for the top three predictions. ML techniques can predict radiology titles and identify highly important features. The proposed system can guide physicians toward selecting appropriate radiology titles and alert radiologists to inconsistencies between the radiology title in the exam order and the patient’s underlying conditions, thereby improving imaging utility and increasing diagnostic accuracy, which favors better patient outcomes.},
  keywords={Radiology;Protocols;Boosting;Biomedical imaging;Medical services;Computed tomography;Prediction algorithms;Artificial intelligence;Boosting;Electronic medical records;Artificial intelligence;machine learning;boosting;electronic medical records;protocols;computed tomography},
  doi={10.1109/ACCESS.2023.3314380},
  ISSN={2169-3536},
  month={},}@ARTICLE{8853246,
  author={Jiang, Wenqian and Hong, Yang and Zhou, Beitong and He, Xin and Cheng, Cheng},
  journal={IEEE Access}, 
  title={A GAN-Based Anomaly Detection Approach for Imbalanced Industrial Time Series}, 
  year={2019},
  volume={7},
  number={},
  pages={143608-143619},
  abstract={Imbalanced time series are universally found in industrial applications, where the number of normal samples is far larger than that of abnormal cases. Traditional machine learning algorithms, such as support vector machine and convolutional neural networks, are struggling to attain high classification accuracies for class-imbalanced problems, because they tend to ensure the accuracy of the majority class. Hereby, this paper proposes a novel anomaly detection approach based on generative adversarial networks (GAN) to overcome this problem. In particular, an encoder-decoder-encoder three-sub-network generator is trained involving the elaborately extracted features from normal samples alone. Anomaly scores for anomaly detection are made up of apparent loss and latent loss. Without having any knowledge of the abnormal samples, our approach can diagnose faults by generating much higher anomaly scores when a fault sample is fed into the trained model. Experimental studies are conducted to verify the validity and feasibility of our approach, including a benchmark rolling bearing dataset acquired by Case Western Reserve University and another rolling bearing dataset which is acquired by our laboratory. Our approach can distinguish abnormal samples from normal samples with 100% accuracies on both datasets.},
  keywords={Feature extraction;Anomaly detection;Generative adversarial networks;Time series analysis;Generators;Training;Rolling bearings;Anomaly detection;generative adversarial networks;imbalanced industrial time series;rolling bearings},
  doi={10.1109/ACCESS.2019.2944689},
  ISSN={2169-3536},
  month={},}@ARTICLE{9301233,
  author={Wu, Hanlin and Zhang, Libao and Ma, Jie},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Remote Sensing Image Super-Resolution via Saliency-Guided Feedback GANs}, 
  year={2022},
  volume={60},
  number={},
  pages={1-16},
  abstract={In remote sensing images (RSIs), the visual characteristics of different regions are versatile, which poses a considerable challenge to single image super-resolution (SISR). Most existing SISR methods for RSIs ignore the diverse reconstruction needs of different regions and thus face a serious contradiction between high perception quality and less spatial distortion. The mean square error (MSE) optimization-based methods produce results of unsatisfactory visual quality, while generative adversarial networks (GANs) can produce photo-realistic but severely distorted results caused by pseudotextures. In addition, increasingly deeper networks, although providing powerful feature representations, also face problems of overfitting and occupying too much storage space. In this article, we propose a new saliency-guided feedback GAN (SG-FBGAN) to address these problems. The proposed SG-FBGAN applies different reconstruction principles for areas with varying levels of saliency and uses feedback (FB) connections to improve the expressivity of the network while reducing parameters. First, we propose a saliency-guided FB generator with our carefully designed paired-feedback block (PFBB). The PFBB uses two branches, a salient and a nonsalient branch, to handle the FB information and generate powerful high-level representations for salient and nonsalient areas, respectively. Then, we measure the visual perception quality of salient areas, nonsalient areas, and the global image with a saliency-guided multidiscriminator, which can dramatically eliminate pseudotextures. Finally, we introduce a curriculum learning strategy to enable the proposed SG-FBGAN to handle complex degradation models. Comprehensive evaluations and ablation studies validate the effectiveness of our proposal.},
  keywords={Visualization;Image reconstruction;Generative adversarial networks;Distortion;Gallium nitride;Sensors;Optimization;Deep learning (DL);generative adversarial network (GAN);remote sensing;saliency detection;super-resolution (SR)},
  doi={10.1109/TGRS.2020.3042515},
  ISSN={1558-0644},
  month={},}@ARTICLE{9110728,
  author={Hu, Bingwen and Zheng, Zhedong and Liu, Ping and Yang, Wankou and Ren, Mingwu},
  journal={IEEE Transactions on Cybernetics}, 
  title={Unsupervised Eyeglasses Removal in the Wild}, 
  year={2021},
  volume={51},
  number={9},
  pages={4373-4385},
  abstract={Eyeglasses removal is challenging in removing different kinds of eyeglasses, e.g., rimless glasses, full-rim glasses, and sunglasses, and recovering appropriate eyes. Due to the significant visual variants, the conventional methods lack scalability. Most existing works focus on the frontal face images in the controlled environment, such as the laboratory, and need to design specific systems for different eyeglass types. To address the limitation, we propose a unified eyeglass removal model called the eyeglasses removal generative adversarial network (ERGAN), which could handle different types of glasses in the wild. The proposed method does not depend on the dense annotation of eyeglasses location but benefits from the large-scale face images with weak annotations. Specifically, we study the two relevant tasks simultaneously, that is, removing eyeglasses and wearing eyeglasses. Given two face images with and without eyeglasses, the proposed model learns to swap the eye area in two faces. The generation mechanism focuses on the eye area and invades the difficulty of generating a new face. In the experiment, we show the proposed method achieves a competitive removal quality in terms of realism and diversity. Furthermore, we evaluate ERGAN on several subsequent tasks, such as face verification and facial expression recognition. The experiment shows that our method could serve as a preprocessing method for these tasks.},
  keywords={Face;Glass;Image reconstruction;Task analysis;Training;Generative adversarial networks;Visualization;Eyeglasses removal;generative adversarial network (GAN);image manipulation},
  doi={10.1109/TCYB.2020.2995496},
  ISSN={2168-2275},
  month={Sep.},}@ARTICLE{10258453,
  author={Yin, Haitao and Xiao, Jinghu and Chen, Hao},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={CSPA-GAN: A Cross-Scale Pyramid Attention GAN for Infrared and Visible Image Fusion}, 
  year={2023},
  volume={72},
  number={},
  pages={1-11},
  abstract={Infrared and visible image fusion (IVIF) aims to combine high contrast of infrared image and rich texture details of visible image, which can break through the imaging limitations of single modality. Generative adversarial network (GAN) has recently received lots of attentions on IVIF due to the adversarial learning without requiring paired multimodality image and label image. Nevertheless, the existing GAN-based approaches are still severely affected by information bias between infrared image and visible image, and it may result in unnatural visual effects. To mitigate the issue, this article proposes a novel cross-scale pyramid attention GAN-based IVIF method (CSPA-GAN), which adopts one generator and dual discriminators to approximate the distribution of fused image. The generator is composed of head module, pyramid decomposition path, feature fusion path, decoding path, and reconstruction module. First, the low-level features, extracted by head module, are further decomposed into multiscale features though pyramid decomposition path. In the feature fusion path, we develop a residual attention weight fusion rule (Res-AWFR) to fuse the multiscale features at each scale. The decoding path with bidirectional interactions decodes the fused features pyramid, which is constructed by long short-term memory module and cross-scale pyramid attention (CSPA). Finally, the reconstruction module produces the fused image. Comparing with current popular deep learning (DL)-based methods, our CSPA-GAN delivers high-performance gains on the TNO, INO, and MSRS datasets qualitatively and quantitatively.},
  keywords={Feature extraction;Generators;Generative adversarial networks;Convolution;Image fusion;Head;Decoding;Attention weight;cross-scale pyramid attention (CSPA);generative adversarial network (GAN);image fusion;long short-term memory},
  doi={10.1109/TIM.2023.3317932},
  ISSN={1557-9662},
  month={},}@ARTICLE{10237199,
  author={Jiang, Yuechi and Drescher, Benny and Yuan, Guoguang},
  journal={IEEE Access}, 
  title={A GAN-Based Multi-Sensor Data Augmentation Technique for CNC Machine Tool Wear Prediction}, 
  year={2023},
  volume={11},
  number={},
  pages={95782-95795},
  abstract={The prediction of CNC machine tool wear is limited due to the scarcity of data in industry. A sufficient amount of experimental and real-live factory data is missing to train accurate supervised machine learning prediction models. Considering that collecting a large amount of data in real industrial environments can be a great challenge, this paper investigates the applicability of data augmentation techniques for generating synthetic data. Our paper focuses on a multi-sensor approach for the classification of tool wear states in the milling process. Multi-sensor data fusion is performed in the frequency domain. Extracted features are then used for tool wear classification. In order to increase the amount of training data, a generative adversarial network (GAN) is designed for data augmentation purposes. An early stopping strategy is designed to improve the effectiveness of the proposed GAN. The experimental results show that the proposed GAN helps to significantly improve the performance of tool wear classification models. With the inclusion of GAN, the number of required real-environment industrial data can be reduced. The research shows promising results to supplement experimental data by GAN-based synthetic data for predicting tool wear.},
  keywords={Generative adversarial networks;Feature extraction;Predictive models;Data models;Milling;Time-domain analysis;Force;Tool wear classification;multi-sensor fusion;generative adversarial network;data augmentation;CNC machines;industrial small data},
  doi={10.1109/ACCESS.2023.3311269},
  ISSN={2169-3536},
  month={},}@ARTICLE{9872037,
  author={Zheng, Zhentan and Liu, Jianyi and Zheng, Nanning},
  journal={IEEE Transactions on Multimedia}, 
  title={P$^{2}$-GAN: Efficient Stroke Style Transfer Using Single Style Image}, 
  year={2023},
  volume={25},
  number={},
  pages={6000-6012},
  abstract={Style transfer is a useful image synthesis technique that can re-render given image into another artistic style while preserving its content information. Generative Adversarial Network (GAN) is a widely adopted framework toward this task for its better representation ability on local style patterns than the traditional Gram-matrix based methods. However, most previous methods rely on sufficient amount of pre-collected style images to train the model. In this paper, a novel Patch Permutation GAN (P$^{2}$-GAN) network that can efficiently learn the stroke style from a single style image is proposed. We use patch permutation to generate multiple training samples from the given style image. A patch discriminator that can simultaneously process patch-wise images and natural images seamlessly is designed. We also propose a local texture descriptor based criterion to quantitatively evaluate the style transfer quality. Experimental results showed that our method can produce finer quality re-renderings from single style image with improved computational efficiency compared with many state-of-the-arts methods.},
  keywords={Training;Generative adversarial networks;Real-time systems;Generators;Feature extraction;Task analysis;Learning systems;Style transfer;generative adversarial network;stroke style;patch permutation},
  doi={10.1109/TMM.2022.3203220},
  ISSN={1941-0077},
  month={},}@ARTICLE{10243087,
  author={Lu, Zhenyu and Cai, Zhenliang and Qian, Weiwei and Zhou, Dong},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Intelligent Fault Diagnosis of Bearings With Both Working Condition Variation and Target Data Scarcity}, 
  year={2023},
  volume={72},
  number={},
  pages={1-12},
  abstract={Intelligent fault diagnosis (IFD) methods allow effective feature extraction of mechanical big data and perform well in fault diagnosis tasks. Numerous domain adaptation (DA)-based IFD methods have also been applied to achieve domain-invariant fault diagnosis under working condition variation. Nevertheless, a sufficient amount of unlabeled testing data need to be available in training, which is not well suited to real-world applications. In real-world applications, only data in machine normal condition (NC) can be easily accessed. Targeting at this more hash real-world data scenario with both domain shift and target domain fault data scarcity, a two-stage method is proposed. First, a cycle-consistency-based 1-D generative adversarial network (GAN) is designed for fault data generation to provide data closer to the target domain distribution, which relives the target data scarcity problem. Second, based on the self-challenge mechanism, a parallel inverse attention (PIA) module is proposed and inserted into a diagnosis network to dig the learning potential of all features, which alleviates the problem of imbalance optimization among features. Massive experiments on two bearing datasets reveal that the proposed method has better adaptability under the more hash real-world fault data scenario.},
  keywords={Training;Generative adversarial networks;Fault diagnosis;Testing;Employee welfare;Transfer learning;Task analysis;Domain generalization (DG);generative model;intelligent fault diagnosis (IFD);roller bearing;style transfer},
  doi={10.1109/TIM.2023.3312470},
  ISSN={1557-9662},
  month={},}@ARTICLE{10258251,
  author={Zhou, Yuan and Wang, Peng and Xiang, Lei and Zhang, Haofeng},
  journal={Tsinghua Science and Technology}, 
  title={Feature-Grounded Single-Stage Text-to-Image Generation}, 
  year={2024},
  volume={29},
  number={2},
  pages={469-480},
  abstract={Recently, Generative Adversarial Networks (GANs) have become the mainstream text-to-image (T2I) framework. However, a standard normal distribution noise of inputs cannot provide sufficient information to synthesize an image that approaches the ground-truth image distribution. Moreover, the multistage generation strategy results in complex T2I applications. Therefore, this study proposes a novel feature-grounded single-stage T2I model, which considers the “real” distribution learned from training images as one input and introduces a worst-case-optimized similarity measure into the loss function to enhance the model's generation capacity. Experimental results on two benchmark datasets demonstrate the competitive performance of the proposed model in terms of the Frechet inception distance and inception score compared to those of some classical and state-of-the-art models, showing the improved similarities among the generated image, text, and ground truth.},
  keywords={Training;Visualization;Semantics;Gaussian distribution;Generative adversarial networks;Linear programming;Feature extraction;text-to-image (T2I);feature-grounded;single-stage generation;Generative Adversarial Network (GAN)},
  doi={10.26599/TST.2023.9010023},
  ISSN={1007-0214},
  month={April},}@ARTICLE{9520770,
  author={Luo, Xiaoqing and Wang, Anqi and Zhang, Zhancheng and Xiang, Xinguang and Wu, Xiao-Jun},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={LatRAIVF: An Infrared and Visible Image Fusion Method Based on Latent Regression and Adversarial Training}, 
  year={2021},
  volume={70},
  number={},
  pages={1-16},
  abstract={In this article, we propose a novel method for infrared and visible image fusion based on latent regression and adversarial training, which is named as LatRAIVF. Compared to existing deep learning (DL)-based image fusion method that only focuses on the spatial information, we consider to utilize the information provided by high-level feature maps from latent space, which can guide the network to learn about semantically important feature information. The proposed method is based on the framework of conditional generative adversarial network (GAN), and two encoders are adopted to learn the respective semantic latent representations for the infrared and visible images, which are then combined by max-selection strategy and input into the decoder, with skip connections between the corresponding layers of the encoder and the decoder, to achieve the fused image. Apart from the adversarial process that enables the fused image to obtain more realistic details, we design two branches to constrain the generation of the image: a content loss to make the fused image close to the label image, and a latent regression loss to ensure the fused image with salient features from the infrared and visible images. Due to the lack of physical ground-truth fused images in public infrared and visible image datasets and the difficulties in defining desired fused image, we make use of existing RGB-D dataset to synthesize an infrared and visible image dataset with ground truths based on the widely used optical model for better network training. Comparison experiments show that the fused results of the proposed method can transfer meaningful features from the source image and provide good fusion quality.},
  keywords={Image fusion;Training;Generative adversarial networks;Task analysis;Semantics;Generators;Optical imaging;Deep learning (DL);generative adversarial networks (GANs);image fusion;infrared and visible image;latent space regression},
  doi={10.1109/TIM.2021.3105250},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10393552,
  author={Wang, Wenchu},
  booktitle={2023 IEEE 3rd International Conference on Data Science and Computer Application (ICDSCA)}, 
  title={Research on AI composition based on deep learning techniques}, 
  year={2023},
  volume={},
  number={},
  pages={1486-1489},
  abstract={With the recent wave of deep learning networks, its application in generating creative content, including music, grows wider. The research of automatic music generation, as a way of understanding human’s artful expression, and a promising field of high commercial value, has been progressed rapidly. Multiple networks have been used for music generation. Recurrent Neural Network (RNN) and Generative Adversarial Network (GAN) are most frequently used because of their advantages in processing time sequences and generating new creations. The results of diverse generative algorithms for intelligent composition vary greatly. This article analyzes some cases of RNN-based and GAN-based music generation methods, and discusses how the application of multiple networks, such as C-RNN-GAN, integrates the advantages of different networks, showing a trend in the future progress of music generation. It also discusses the necessity of building precise methods for evaluating generated music, and describes some cases where deep learning method has helped in this realm.},
  keywords={Deep learning;Training;Recurrent neural networks;Computational modeling;Weapons;Time series analysis;Generative adversarial networks;Music generation;deep learning;RNN;GAN},
  doi={10.1109/ICDSCA59871.2023.10393552},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10822385,
  author={Magalhães, Bruno and Pedrosa, João and Renna, Francesco and Paredes, Hugo and Filipe, Vitor},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Image Captioning for Coronary Artery Disease Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={5302-5308},
  abstract={Coronary artery disease (CAD) remains a leading cause of morbidity and mortality worldwide, underscoring the need for accurate and reliable diagnostic tools. While AI-driven models have shown significant promise in identifying CAD through imaging techniques, their 'black box' nature often hinders clinical adoption due to a lack of interpretability. In response, this paper proposes a novel approach to image captioning specifically tailored for CAD diagnosis, aimed at enhancing the transparency and usability of AI systems. Utilizing the COCA dataset, which comprises gated coronary CT images along with Ground Truth (GT) segmentation annotations, we introduce a hybrid model architecture that combines a Vision Transformer (ViT) for feature extraction with a Generative Pretrained Transformer (GPT) for generating clinically relevant textual descriptions. This work builds on a previously developed 3D Convolutional Neural Network (CNN) for coronary artery segmentation, leveraging its accurate delineations of calcified regions as critical inputs to the captioning process. By incorporating these segmentation outputs, our approach not only focuses on accurately identifying and describing calcified regions within the coronary arteries but also ensures that the generated captions are clinically meaningful and reflective of key diagnostic features such as location, severity, and artery involvement. This methodology provides medical practitioners with clear, context-rich explanations of AI-generated findings, thereby bridging the gap between advanced AI technologies and practical clinical applications. Furthermore, our work underscores the critical role of Explainable AI (XAI) in fostering trust, improving decision-making, and enhancing the efficacy of AI-driven diagnostics, paving the way for future advancements in the field.},
  keywords={Solid modeling;Image segmentation;Decision making;Focusing;Transformers;Feature extraction;Hybrid power systems;Arteries;Medical diagnostic imaging;Diseases;Coronary artery disease;Explainable AI;Medical Image Captioning},
  doi={10.1109/BIBM62325.2024.10822385},
  ISSN={2156-1133},
  month={Dec},}@INBOOK{10850587,
  author={Mariprasath, T. and Cheepati, Kumar Reddy and Rivera, Marco},
  booktitle={Practical Guide to Machine Learning, NLP, and Generative AI: Libraries, Algorithms, and Applications}, 
  title={1 Machine Learning Libraries}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={This is an essential resource for beginners and experienced practitioners in machine learning. This comprehensive guide covers a broad spectrum of machine learning topics, starting with an in-depth exploration of popular machine learning libraries. Readers will gain a thorough understanding of Scikit-learn, TensorFlow, PyTorch, Keras, and other pivotal libraries like XGBoost, LightGBM, and CatBoost, which are integral for efficient model development and deployment. The book delves into various neural network architectures, providing readers with a solid foundation in understanding and applying these models. Beginning with the basics of the Perceptron and its application in digit classification, it progresses to more complex structures such as multilayer perceptrons for financial forecasting, radial basis function networks for air quality prediction, and convolutional neural networks (CNNs) for image classification. Additionally, the book covers recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) and gated recurrent units (GRUs), which are crucial for time-series analysis and sequential data applications. Supervised machine learning algorithms are meticulously explained, with practical examples to illustrate their application. The book covers logistic regression and its use in predicting sports outcomes, decision trees for plant classification, random forests for traffic prediction, and support vector machines for house price prediction. Gradient boosting machines and their applications in genomics, AdaBoost for bioinformatics data classification, and extreme gradient boosting (XGBoost) for churn prediction are also discussed, providing readers with a robust toolkit for various predictive tasks. Unsupervised learning algorithms are another significant focus of the book, introducing readers to techniques for uncovering hidden patterns in data. Hierarchical clustering for gene expression data analysis, principal component analysis (PCA) for climate predictions, and singular value decomposition (SVD) for signal denoising are thoroughly explained. The book also explores applications like robot navigation and network security, demonstrating the versatility of these techniques. Natural language processing (NLP) is comprehensively covered, highlighting its fundamental concepts and various applications. The book discusses the overview of NLP, its fundamental concepts, and its diverse applications such as chatbots, virtual assistants, clinical NLP applications, and social media analytics. Detailed sections on text pre-processing, syntactic analysis, machine translation, text classification, named entity recognition, and sentiment analysis equip readers with the knowledge to build sophisticated NLP models. The final chapters of the book explore generative AI, including generative adversarial networks (GANs) for image generation, variational autoencoders for vibrational encoder training, and autoregressive models for time series forecasting. It also delves into Markov chain models for text generation, Boltzmann machines for pattern recognition, and deep belief networks for financial forecasting. Special attention is given to the application of recurrent neural networks (RNNs) for generation tasks, such as wind power plant predictions and battery range prediction, showcasing the practical implementations of generative AI in various fields.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046527},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10850587},}@INPROCEEDINGS{11138050,
  author={Huiyu, Lin and Xuxi, Yin and Jinyi, Xiong},
  booktitle={2025 IEEE 3rd International Conference on Image Processing and Computer Applications (ICIPCA)}, 
  title={Research on Deep Learning Algorithms for Digital Media Content Creation}, 
  year={2025},
  volume={},
  number={},
  pages={1154-1159},
  abstract={This paper presents a deep learning algorithm for digital media content creation. It is designed to efficiently produce high-quality image, audio, and video content via multimodal data fusion and an adaptive generation strategy. The algorithm integrates the Transformer architecture and the Generative Adversarial Network (GAN). It utilizes the robust feature extraction capabilities and self-attention mechanism of the Transformer to encode and blend the input multimodal data, including text descriptions, images, audio, and video. The generation module dynamically modifies parameters through the adaptive strategy to generate high-quality digital media content. Experimental results demonstrate that the Inception Score of the image and audio content generated by the algorithm in this paper reaches 8.5 and 4.2 respectively, outperforming existing methods significantly. Meanwhile, the generation efficiency is enhanced by 30%. The user satisfaction survey indicates that over 80% of users consider the generated content to be of high quality and creative. Additionally, in terms of video generation, the algorithm in this paper achieves a Video Structural Similarity Index (V -SSIM) of 0.85 and a Peak Signal-to-Noise Ratio (PSNR) of 32dB for the generated videos, surpassing the comparison algorithms in quality. The efficiency of video generation has also improved, with the average generation time shortened by 25% compared to existing methods. The research of this paper offers a novel technical approach for digital media content creation, holding significant theoretical and application value.},
  keywords={Deep learning;Surveys;PSNR;Heuristic algorithms;Data integration;Media;Transformers;Generative adversarial networks;Optimization;Videos;Deep learning;digital media content creation;multimodal data fusion;generative adversarial network;Transformer},
  doi={10.1109/ICIPCA65645.2025.11138050},
  ISSN={},
  month={June},}@INPROCEEDINGS{10852505,
  author={Petrovic, Nenad and Lebioda, Krzysztof and Zolfaghari, Vahid and Schamschurko, André and Kirchner, Sven and Purschke, Nils and Pan, Fengjunjie and Knoll, Alois},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={LLM-Driven Testing for Autonomous Driving Scenarios}, 
  year={2024},
  volume={},
  number={},
  pages={173-178},
  abstract={In this paper, we explore the potential of leveraging Large Language Models (LLMs) for automated test generation based on free-form textual descriptions in area of automotive. As outcome, we implement a prototype and evaluate the proposed approach on autonomous driving feature scenarios in CARLA open-source simulation environment. Two pre-trained LLMs are taken into account for comparative evaluation: GPT-4 and Llama3. According to the achieved results, GPT-4 outperforms Llama3, while the presented approach speeds-up the process of testing (more than 10 times) and reduces cognitive load thanks to automated code generation and adoption of flexible simulation environment for quick evaluation.},
  keywords={Codes;Large language models;Prototypes;Transformers;Cognitive load;Test pattern generators;Autonomous vehicles;Testing;Load modeling;Automotive engineering;autonomous driving;CARLA;Generative Pre-Trained Transformer (GPT);Large Language Model (LLM);Llama3;Model-Driven Engineering (MDE)},
  doi={10.1109/FLLM63129.2024.10852505},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10446235,
  author={Yi, Jayeon and Koo, Junghyun and Lee, Kyogu},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={DDD: A Perceptually Superior Low-Response-Time DNN-Based Declipper}, 
  year={2024},
  volume={},
  number={},
  pages={801-805},
  abstract={Clipping is a common nonlinear distortion that occurs whenever the input or output of an audio system exceeds the supported range. This phenomenon undermines not only the perception of speech quality but also downstream processes utilizing the disrupted signal. Therefore, a real-time-capable, robust, and low-response-time method for speech declipping (SD) is desired. In this work, we introduce DDD (Demucs-Discriminator-Declipper), a real-time-capable speech-declipping deep neural network (DNN) that requires less response time by design. We first observe that a previously untested real-time-capable DNN model, Demucs, exhibits a reasonable declipping performance. Then we utilize adversarial learning objectives to increase the perceptual quality of output speech without additional inference overhead. Subjective evaluations on harshly clipped speech shows that DDD outperforms the baselines by a wide margin in terms of speech quality. We perform detailed waveform and spectral analyses to gain an insight into the output behavior of DDD in comparison to the baselines. Finally, our streaming simulations also show that DDD is capable of sub-decisecond mean response times, outperforming the state-of-the-art DNN approach by a factor of six.},
  keywords={Training;Artificial neural networks;Speech enhancement;Signal processing;Behavioral sciences;Time factors;Spectral analysis;Speech Declipping;Speech Enhancement;Adversarial Training},
  doi={10.1109/ICASSP48485.2024.10446235},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9166359,
  author={Suraperwata, Raihan Hamid and Suyanto, Suyanto},
  booktitle={2020 8th International Conference on Information and Communication Technology (ICoICT)}, 
  title={Language Modeling for Journalistic Robot based on Generative Pretrained Transformer 2}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={The language model is typically represented as an unsupervised distribution estimate from a set of examples, each consisting of symbol sequences, and it could predict over sequences of words. We demonstrate the language model based on Generative Pretrained 2 will have a readable generated article for the journalistic robot. Nowadays, there is some trending of journalistic in Indonesia, freedom of the press, and it enables every journalist to make unprofessional news on the media. The problem affects the raise of journalist numbers who have lack journalistic knowledge and increases the amount of inappropriate news content in Indonesia. Therefore, to improve the quality of news produced by the mass media in Indonesia, a journalistic robot is needed to produce news content by the guidelines and the journalistic code of ethics. This research uses language modeling based on GPT-2 to generate articles. The program has four primary steps: building dataset, fine tuning GPT-2, modeling the trained data, and create articles. Furthermore, this research will add an Indonesian model for GPT-2 since the main purpose of this research is Indonesian articles. This paper proposes GPT-2 to be applied to news contents and calculate the result with BLEU scores to check if the results are readable content. These findings show that the proposed model is capable of generating a readable article after trained by 110 Indonesian articles with an excellent score of BLEU.},
  keywords={Robots;Media;Natural languages;Entertainment industry;Data models;Computational modeling;Ethics;journalistic robot;language model;Indonesia;Generative Pretrained Transformer 2},
  doi={10.1109/ICoICT49345.2020.9166359},
  ISSN={},
  month={June},}@INPROCEEDINGS{10365937,
  author={Burov, Vasiliy and Soshnikov, Dmitry},
  booktitle={2023 8th IEEE History of Electrotechnology Conference (HISTELCON)}, 
  title={FidoNet and Generative AI: A New Approach to Museumification of Historical Content Resources}, 
  year={2023},
  volume={},
  number={},
  pages={129-132},
  abstract={This report considers the problem of visual representation of historical content resources based on user-generated content for museumification of the most important information resources in the history of digital networks development. The paper proposes an approach based on generative AI and shows its implementation in relation to FidoNet.},
  keywords={Visualization;Portable computers;Information resources;Computational modeling;Atmospheric modeling;User-generated content;Explosions;FidoNet;echos;echomail;newsgroups;content representation;user generated content;UCG;machine learning;large language model;LLM;generative AI;artificial intelligence;ChatGPT;GPT model;virtual museum;cybernetic immortality},
  doi={10.1109/HISTELCON56357.2023.10365937},
  ISSN={2770-8357},
  month={Sep.},}@ARTICLE{8727538,
  author={Zhang, Zhuo and Liu, Jia and Ke, Yan and Lei, Yu and Li, Jun and Zhang, Minqing and Yang, Xiaoyuan},
  journal={IEEE Access}, 
  title={Generative Steganography by Sampling}, 
  year={2019},
  volume={7},
  number={},
  pages={118586-118597},
  abstract={In this paper, a novel data-driven information hiding scheme called “generative steganography by sampling” (GSS) is proposed. Unlike in traditional modification-based steganography, in our method the stego image is directly sampled by a powerful generator: no explicit cover is used. Both parties share a secret key used for message embedding and extraction. The Jensen-Shannon divergence is introduced as a new criterion for evaluating the security of generative steganography. Based on these principles, we propose a simple practical generative steganography method that uses semantic image inpainting. The message is written in advance to an uncorrupted region that needs to be retained in the corrupted image. Then, the corrupted image with the secret message is fed into a Generator trained by a generative adversarial network (GAN) for semantic completion. Message loss and prior loss terms are proposed for penalizing message extraction error and unrealistic stego image. In our design, we first train a generator whose training target is the generation of new data samples from the same distribution as that of existing training data. Next, for the trained generator, backpropagation to the message and prior loss are introduced to optimize the coding of the input noise data for the generator. The presented experiments demonstrate the potential of the proposed framework based on both qualitative and quantitative evaluations of the generated stego images.},
  keywords={Gallium nitride;Generators;Security;Generative adversarial networks;Semantics;Training;Image generation;Generative adversarial network;image inpainting;steganography},
  doi={10.1109/ACCESS.2019.2920313},
  ISSN={2169-3536},
  month={},}@ARTICLE{9510088,
  author={Kazemi, Amir and Meidani, Hadi},
  journal={IEEE Access}, 
  title={IGANI: Iterative Generative Adversarial Networks for Imputation With Application to Traffic Data}, 
  year={2021},
  volume={9},
  number={},
  pages={112966-112977},
  abstract={Increasing use of sensor data in intelligent transportation systems calls for accurate imputation algorithms that can enable reliable traffic management in the occasional absence of data. As one of the effective imputation approaches, generative adversarial networks (GANs) are implicit generative models that can be used for data imputation, which is formulated as an unsupervised learning problem. This work introduces a novel iterative GAN architecture, called Iterative Generative Adversarial Networks for Imputation (IGANI), for data imputation. IGANI imputes data in two steps and maintains the invertibility of the generative imputer, which will be shown to be a sufficient condition for the convergence of the proposed GAN-based imputation. The performance of our proposed method is evaluated on (1) the imputation of traffic speed data collected in the city of Guangzhou in China, and the training of short-term traffic prediction models using imputed data, and (2) the imputation of multi-variable traffic data of highways in Portland-Vancouver metropolitan region which includes volume, occupancy, and speed with different missing rates for each of them. It is shown that our proposed algorithm mostly produces more accurate results compared to those of previous GAN-based imputation architectures.},
  keywords={Generative adversarial networks;Data models;Cost function;Training;Generators;Predictive models;Games;Generative adversarial networks;GAN;missing data;imputation;invertible neural networks (INN)},
  doi={10.1109/ACCESS.2021.3103456},
  ISSN={2169-3536},
  month={},}@ARTICLE{9335934,
  author={Chao, Xiaopeng and Cao, Jiangzhong and Lu, Yuqin and Dai, Qingyun and Liang, Shangsong},
  journal={IEEE Access}, 
  title={Constrained Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={19208-19218},
  abstract={Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator's output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator's output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.},
  keywords={Training;Nash equilibrium;Generators;Standards;Generative adversarial networks;Games;Gallium nitride;Generative adversarial networks;Nash equilibrium;Lipschitz constraint},
  doi={10.1109/ACCESS.2021.3054822},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10877616,
  author={Tejojith, G and Sharma, Kritika and Singh, Ayush and Hoque, Shanidul},
  booktitle={2024 Second International Conference Computational and Characterization Techniques in Engineering & Sciences (IC3TES)}, 
  title={Survey of AI and ML Techniques for Enhancing Performance of 5G Cognitive Radio Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The importance of Cognitive Radio (CR) and Artificial Intelligence (AI) in 5G communication systems is paramount, as these technologies provide effective solutions to critical issues such as spectrum scarcity, escalating data demands, and network congestion. CR facilitates dynamic spectrum access, enabling Secondary Users (SUs) to exploit underutilized licensed spectrum without causing interference to Primary Users (PUs). This paper investigates the integration of AI and machine learning (ML) techniques into Cognitive Radio Networks, focusing on main functions: spectrum sensing, spectrum management, spectrum sharing, and spectrum mobility. Various spectrum handoff strategies including proactive, reactive, and hybrid approaches are discussed to illustrate the balance between latency and operational efficiency. AI-driven techniques such as Q-learning, reinforcement learning, and swarm intelligence are employed to optimize spectrum handoff, enhance decision-making processes, and mitigate service disruptions. Additionally, optimization algorithms like Artificial Bee Colony (ABC), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are reviewed how to significantly increase spectrum availability while minimizing handoff delays in the domain of CRNs. The potential for incorporating advanced ML models, such as transformers and generative adversarial networks, is also examined to further enhance spectrum efficiency within 5G environments. These innovations are critical for maintaining continuous, high-quality communication in next-generation networks.},
  keywords={Technological innovation;Q-learning;5G mobile communication;Decision making;Interference;Sensors;Cognitive radio;Particle swarm optimization;Optimization;Next generation networking;Artificial Intelligence;Cognitive Radio;Machine learning;Next generation networking;Spectrum Management},
  doi={10.1109/IC3TES62412.2024.10877616},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9599356,
  author={Toh, Raymond Kwan How and Sourin, Alexei},
  booktitle={2021 International Conference on Cyberworlds (CW)}, 
  title={Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network}, 
  year={2021},
  volume={},
  number={},
  pages={137-140},
  abstract={Following the rapid advancement of Artificial Intelligence and transition into the era of Big Data, researchers have started to explore the possibility of using machine learning in creative domains such as music generation. However, most research were focused on musical composition and removed expressive attributes during data pre-processing, which resulted in mechanical-sounding generated music. To address this issue, music elements, such as pitch, time and velocity, were extracted from MIDI tracks and encoded with piano-roll data representation. With the piano-roll data representation, Deep Convolutional Generative Adversarial Network (DCGAN) learned the data distribution from the given dataset and generated new data derived from the same distribution. The generated music was evaluated based on its incorporation of music dynamics and a user study. The evaluation results verified that DCGAN could generate expressive music comprising of music dynamics and syncopated rhythm.},
  keywords={Training;Machine learning;Big Data;Generative adversarial networks;Rhythm;Data mining;music generation;DCGAN;dynamics;pianoroll},
  doi={10.1109/CW52790.2021.00030},
  ISSN={2642-3596},
  month={Sep.},}@INPROCEEDINGS{10001410,
  author={Zhao, Yilin and Xun, Yijie and Liu, Jiajia and Ma, Siyu},
  booktitle={GLOBECOM 2022 - 2022 IEEE Global Communications Conference}, 
  title={GVIDS: A Reliable Vehicle Intrusion Detection System Based on Generative Adversarial Network}, 
  year={2022},
  volume={},
  number={},
  pages={4310-4315},
  abstract={5G and artificial intelligence greatly promote the development of intelligent and connected vehicle (ICV). However, ICV opens more ports to the outside world, making it easy for hackers to intrude controller area network (CAN) and control ICV. Therefore, many researchers design intrusion detection systems (IDSs) to detect vehicle intrusion in real-time. In this paper, we propose a highly camouflaged attack method called the same origin method execution (SOME) attack. The intrusion messages of this attack have the same characteristics as normal messages and can bypass most existing IDSs. To detect this attack, we design a reliable IDS for ICV based on a generative adversarial network (GAN) called GVIDS. It takes CAN messages as the input sample and trains the IDS model to distinguish the legality of messages. Experiments on two real vehicles show that GVIDS can detect most existing attacks, including spoofing, bus-off, masquerade, and SOME attacks. The average detection accuracy of GVIDS is 96.64%, and the average running time of each detection is only 0.18 ms. In addition, the experiment also shows that the detection performance of GVIDS is not affected by the value of identifiers in CAN messages.},
  keywords={Connected vehicles;Computer hacking;Intrusion detection;Generative adversarial networks;Reliability engineering;Real-time systems;Robustness},
  doi={10.1109/GLOBECOM48099.2022.10001410},
  ISSN={2576-6813},
  month={Dec},}@INBOOK{10982315,
  author={Bergeret, Olivier and Abbasi, Asif and Farvault, Joel},
  booktitle={GenAI on AWS: A Practical Approach to Building Generative AI Applications on AWS}, 
  title={Retrieval&#x2010;Augmented Generation}, 
  year={2025},
  volume={},
  number={},
  pages={263-294},
  abstract={<p>This chapter discusses the challenges and considerations in deploying retrieval&#x2010;augmented generation (RAG) systems, computational efficiency, and ethical concerns. It aims to provide a comprehensive understanding of RAG's potential to revolutionize artificial intelligence (AI) applications and interactions, offering valuable insights for AI practitioners, business leaders, and technology enthusiasts alike. RAG represents a sophisticated approach to enhancing language model capabilities by leveraging external knowledge sources. At its core, RAG's architecture is designed to bridge the gap between vast external knowledge repositories and the generative capabilities of language models, creating a system that combines the best of both worlds. The RAG architecture consists of three primary components that work in harmony to process user queries and generate accurate, contextually relevant responses: the Retrieval Module; the Augmentation Module; and the Generation Module.</p>},
  keywords={Retrieval augmented generation;Adaptation models;Artificial intelligence;Accuracy;Prompt engineering;Soft sensors;Large language models;Knowledge based systems;Information retrieval;Data models},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394281305},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10982315},}@INPROCEEDINGS{10452075,
  author={Song, Wenjie and Xue, Jianru},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Atten-ganCV: An End-to-End Close-Coupled Image-Generating Cross-View Network}, 
  year={2023},
  volume={},
  number={},
  pages={8920-8925},
  abstract={Cross-view geo-localization aims to match the query input ground-view image and the aerial-view images in the reference dataset one by one to determine the ground image's geographic location. This research is extremely challenging because the variation of the observation angle between cross-view images brings about great geometric appearance differences between image pairs. Nowadays, the introduction of generative networks into matching models has been shown to work well on the CVUSA (Cross-View USA) dataset, and the latest models clarify the paradigm of end-to-end generative cross-view image matching methods. However, this result relies on an assumption on the dataset: for all query input ground images, there must exist a reference aerial image that is exactly centered on the location of that image, which is clearly not consistent with real-world application scenarios; and the performance of state-of-the-art generative models degrades significantly when departing from this assumption of center alignment. To address this problem, this paper provides a generative model (atten-ganCV) for non-center-aligned datasets. This model feeds the query ground image directly into a generative adversarial network to obtain a generated aerial view image, where the generator atten-UNet innovatively introduces an attention mechanism. Then, model matches the synthesized image with the real aerial image in the reference dataset one by one, and finally obtains the matching result with the highest similarity, thus determining the geographic location of the query input. The model is tested on both the center-aligned CVUSA dataset and the non-center-aligned VIGOR (Cross-view Image Geo-localization beyond One-to-one Retrieval) dataset. In the VIGOR dataset, this model achieves approximately the same accuracy as the state-of-the-art model with 3 times the inference speed.},
  keywords={Automation;Impedance matching;Image matching;Generative adversarial networks;Generators;Feeds;Cross-view geo-localization;generative adver-sarial network;attention aggregation mechanism},
  doi={10.1109/CAC59555.2023.10452075},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10796462,
  author={Albano, Norberto and Brignone, Sandro},
  booktitle={2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)}, 
  title={Exploring Bias in Text-to-Image Models: From Body Representation of Teenage Students to Perspectives for the Aging Society}, 
  year={2024},
  volume={},
  number={},
  pages={1212-1217},
  abstract={Starting from the results of a study on body shaming conducted between Italy and Romania in ay 2020/21, this investigation explores the presence of bias in images produced by generative Text-To-Image (TTI) models. Specifically, it proposes an analysis and evaluation of the representation of adolescent students' bodies in the outputs of two different TTI systems, highlighting social, cultural, and technological factors. The findings reveal significant biases in the representations of students, with notable differences between the platforms examined, and underline the need for targeted educational interventions to counter distorted perceptions of the body. This research provides a reflective contribution on the impact of TTIs on society and on the representation of body image that extends to different social and demographic categories, among these, the elderly, who are often victims or passive users of these technologies.},
  keywords={Training;Biological system modeling;Social sciences;Text to image;Neural engineering;Aging;Metrology;Cultural differences;Older adults;Investment;Artificial Intelligence;TTI Models;Fairness;Bias;Body Representation and Perception;Body Shaming;Education and Awareness},
  doi={10.1109/MetroXRAINE62247.2024.10796462},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9338897,
  author={Zhao, Kai and Zhou, Yan and Chen, Xin and Zhang, Huainian},
  booktitle={2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={A generative adversarial network based image augmentation method for ship segmentation in SAR images}, 
  year={2020},
  volume={9},
  number={},
  pages={1285-1289},
  abstract={Synthetic aperture radar (SAR) has good performance and is widely used. Ship monitoring plays an important role in the military field, and ship segmentation plays an important role in related research. SAR image segmentation with neural networks requires large amounts of data. In the case of a small data set, traditional data augmentation methods such as scaling and rotation methods have limited promotion on image segmentation. We propose a new method for SAR image augmentation: based on CycleGAN, combined with Slide Window Filtering, convert aerial images to SAR style images, and use it for SAR image segmentation. Experiments show that compared with the traditional data augmentation methods, this method can improve the segmentation result better.},
  keywords={Image segmentation;Neural networks;Radar polarimetry;Marine vehicles;Synthetic aperture radar;Standards;Monitoring;SAR;Slide Window Filtering;CycleGAN;Image Segmentation},
  doi={10.1109/ITAIC49862.2020.9338897},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10487520,
  author={Bietsch, Dominik and Stahlbock, Robert and Voß, Stefan},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)}, 
  title={Electronic Health Data in the Context of Patient Length-of-Stay Prediction: Using Generative Adversarial Nets for Synthetic Data Creation}, 
  year={2023},
  volume={},
  number={},
  pages={1597-1604},
  abstract={While generative artificial intelligence has gained popularity (e.g., for the creation of images) it can also be used for the creation of synthetic tabular data. This bears great potential, especially for the healthcare industry where data is oftentimes scarce and underlies privacy restrictions. For instance, the creation of synthetic electronic health records (EHR) promises to improve the usage of machine learning (ML) algorithms, which normally work with large amounts of data. This also applies for the prediction of the patient length of stay (LOS), a key measure for hospitals. Thereby, the LOS represents one of the core tools for decision-makers to plan the allocation of resources. This paper aims to add to the young research concerning the application of generative adversarial nets (GAN) on tabular EHR. The intention is to leverage the advantages of synthetic data for the prediction of the LOS in order to contribute to the efficiency -enhancing and cost-saving aspirations of hospitals and insurance companies. Therefore, the applicability of synthetic data generated by GANs as a proxy for scarce real-world EHR for the patient LOS multi-class classification task is examined. In this context the Conditional Tabular GAN (CTGAN) and the Copula GAN are selected. The CTGAN is found to be the superior model for the underlying use case. Nevertheless, the paper shows that there is still room for improvement when applying state-of-the-art GAN architectures to EHR.},
  keywords={Training;Machine learning algorithms;Hospitals;Computer architecture;Generative adversarial networks;Prediction algorithms;Data models;Generative AI;Electronic Health Data;Generative Adversarial Nets;Synthetic Data Generation;Machine Learning},
  doi={10.1109/CSCE60160.2023.00262},
  ISSN={},
  month={July},}@INPROCEEDINGS{11012178,
  author={Parameswari, P. and Manikantan, M. and Surya, P. Arun},
  booktitle={2025 3rd International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)}, 
  title={Enhanced CNN Model Integrated with Generative AI for Real-Time Pest and Disease Detection in Rice Farming}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Rice is a major food to the global food supply, rice production aspects are an important problems from diseases and pests that makes a significant lose in profit. The crop health maintenance can improve farmers' livelihoods, if the primary and specific identification of these issues were addressed earlier. In this research an Enhanced Convolutional Neural Network (ECNN) algorithm wad proposed for rice plant disease diagnosis using advances in Artificial Intelligence and Artificial Neural Network methodologies. It is an integrated mobile technology; the outcome of the research enables farmers to upload photos in the mobile application for real-time diagnosis and to get insights about the pest attacks. The proposed algorithm was enhanced by adding a layer to address overfitting, environmental variability, and gradient issues, the proposed Enhanced CNN(ECNN) grows on traditional CNN designs. The model guarantees strength in a range of perspectives by simulating a range of real-world scenarios applying generative AI tools and strong data augmentation techniques. Existing datasets were used for training data, then enhanced with generated and field-collected images. Performance metrics helped significantly to improve the results for proposed methodology. This ECNN model, effectively diagnoses the crop disease and pests in challenging environments including dim lighting and complex backgrounds, with a validation accuracy of 92.5%. Integration of ANN components improve the model's capability to simplify through diverse pest and disease types.},
  keywords={Accuracy;Generative AI;Crops;Training data;Real-time systems;Telecommunication computing;Convolutional neural networks;Diseases;Farming;Overfitting;CNN;Rice disease;Generative AI;Chatbot},
  doi={10.1109/ICAECA63854.2025.11012178},
  ISSN={},
  month={April},}@ARTICLE{9490307,
  author={Gao, Xingyu and Shi, Feng and Shen, Dinggang and Liu, Manhua},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Task-Induced Pyramid and Attention GAN for Multimodal Brain Image Imputation and Classification in Alzheimer's Disease}, 
  year={2022},
  volume={26},
  number={1},
  pages={36-43},
  abstract={With the advance of medical imaging technologies, multimodal images such as magnetic resonance images (MRI) and positron emission tomography (PET) can capture subtle structural and functional changes of brain, facilitating the diagnosis of brain diseases such as Alzheimer's disease (AD). In practice, multimodal images may be incomplete since PET is often missing due to high financial costs or availability. Most of the existing methods simply excluded subjects with missing data, which unfortunately reduced the sample size. In addition, how to extract and combine multimodal features is still challenging. To address these problems, we propose a deep learning framework to integrate a task-induced pyramid and attention generative adversarial network (TPA-GAN) with a pathwise transfer dense convolution network (PT-DCN) for imputation and classification of multimodal brain images. First, we propose a TPA-GAN to integrate pyramid convolution and attention module as well as disease classification task into GAN for generating the missing PET data with their MRI. Then, with the imputed multimodal images, we build a dense convolution network with pathwise transfer blocks to gradually learn and combine multimodal features for final disease classification. Experiments are performed on ADNI-1/2 datasets to evaluate our method, achieving superior performance in image imputation and brain disease diagnosis compared to state-of-the-art methods.},
  keywords={Magnetic resonance imaging;Diseases;Convolution;Generative adversarial networks;Task analysis;Alzheimer's disease;Deep learning;Multimodal brain images;generative adversarial network;dense convolution network;image classification;Alzheimer's disease},
  doi={10.1109/JBHI.2021.3097721},
  ISSN={2168-2208},
  month={Jan},}@INPROCEEDINGS{10882285,
  author={Shelar, Swati M. and Parkale, Yuvraj V.},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Generative Point Clouds: A Novel Approach for 3D Data Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The rising interest in three-dimensional data use in tasks like computer vision, robotics, and self-driving cars has created an ever-greater requirement for quick and effective methods of creating and scaling up high-quality three-dimensional point clouds. Point cloud generation techniques, such as LiDAR tools and structured light devices, continue to have high costs, labor intensity, and weak capability to image complex scenes. In this study, we present an original concept of a point cloud generator based on generative models. These advances empower users to produce varied and realistic point clouds through deep learning which can be extended for practical use cases. The process in question has a number of advantages over conventional point cloud production technologies, according to its novel character. The first noteworthy advantage is speed and scalability, which enable the creation of point cloud datasets in enormous quantities in a matter of minutes. The second is the capability of producing point clouds with various shapes and other characteristics. Furthermore, because expensive sensors and instruments are not needed, the procedure is inexpensive. We anticipate being able to provide a substantial advancement in the 3D data generation methodology, It would have numerous uses in robotics, computer vision, and driverless vehicles, and other fields, considering the sensitivity of our method with the related problems. In the future, we intend to investigate our method's capabilities further by employing it for a variety of purposes.},
  keywords={Point cloud compression;Deep learning;Computer vision;Cloud computing;Three-dimensional displays;Sensitivity;Shape;Data collection;Sensor phenomena and characterization;Testing;Generative Adversarial Networks Generative Point Clouds;3D Data Generation;Deep Learning;Computer Vision;Robotics;Autonomous Vehicles},
  doi={10.1109/ICAIQSA64000.2024.10882285},
  ISSN={},
  month={Dec},}@ARTICLE{9609669,
  author={Du, Ruoyi and Xie, Jiyang and Ma, Zhanyu and Chang, Dongliang and Song, Yi-Zhe and Guo, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Progressive Learning of Category-Consistent Multi-Granularity Features for Fine-Grained Visual Classification}, 
  year={2022},
  volume={44},
  number={12},
  pages={9521-9535},
  abstract={Fine-grained visual classiﬁcation (FGVC) is much more challenging than traditional classiﬁcation tasks due to the inherently subtle intra-class object variations. Recent works are mainly part-driven (either explicitly or implicitly), with the assumption that fine-grained information naturally rests within the parts. In this paper, we take a different stance, and show that part operations are not strictly necessary – the key lies with encouraging the network to learn at different granularities and progressively fusing multi-granularity features together. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a consistent block convolution that encourages the network to learn the category-consistent features at specific granularities. We evaluate on several standard FGVC benchmark datasets, and demonstrate the proposed method consistently outperforms existing alternatives or delivers competitive results. Codes are available at https://github.com/PRIS-CV/PMG-V2.},
  keywords={Training;Convolution;Visualization;Birds;Beak;Task analysis;Semantics;Fine-grained visual classification;convolutional neural network;progressive training;consistency constraint},
  doi={10.1109/TPAMI.2021.3126668},
  ISSN={1939-3539},
  month={Dec},}@INPROCEEDINGS{9898573,
  author={Zhao, Zeyu and Gao, Nan and Zeng, Zhi and Zhang, Shuwu},
  booktitle={2022 International Conference on Culture-Oriented Science and Technology (CoST)}, 
  title={Generating Diverse Gestures from Speech Using Memory Networks as Dynamic Dictionaries}, 
  year={2022},
  volume={},
  number={},
  pages={163-168},
  abstract={People naturally enhance their speeches with body motion or gestures. Generating human gestures for digital humans or virtual avatars from speech audio or text remains challenging for its indeterministic nature. We observe that existing neural methods often give gestures with an inadequate amount of movement shift, which can be characterized as slow or dull. Thus, we propose a novel generative model coupled with memory networks to work as dynamic dictionaries for generating gestures with improved diversity. Under the hood of the proposed model, a dictionary network dynamically stores previously appeared pose features corresponding to text features for the generator to lookup, while a pose generation network takes in audio and pose features and outputs the resulting gesture sequences. Seed poses are utilized in the generation process to guarantee the continuity between two speech segments. We also propose a new objective evaluation metric for diversity of generated gestures and succeed in demonstrating that the proposed model has the ability to generate gestures with improved diversity.},
  keywords={Measurement;Dictionaries;Neural networks;Training data;Speech enhancement;Reliability engineering;Generators;Digital humans;Synchronization;Cultural differences;gesture generation;memory networks;objective metrics for generative models;gesture diversity},
  doi={10.1109/CoST57098.2022.00042},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10017727,
  author={Khalida, Rakhmi and Madenda, Sarifuddin and Harmanto, Suryadi and Wiryana, I Made},
  booktitle={2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)}, 
  title={Concatenate Word Embedding for Text to Image through Generative Adversarial Network}, 
  year={2022},
  volume={},
  number={},
  pages={259-264},
  abstract={Since the explosion of deep learning, automatic image generation from natural language is highly desirable through artificial intelligence (AI) as it makes it easier for users to create visually rich images through the ease of language. One of the methods used to generate images from text is GAN, in this study using word embedding is to process natural language and develop GAN by concatenate word embedding. In this work, we carry out our early-stage research which is to explore the simple technique of concatenate word embedding as the input of the GAN neural network. We show that this model is a novelty for the GAN model with the concept of multimodal input that is able to generate text to image and is expected to improve performance on a stronger GAN. Based on the explanation of our research method, this model can be implemented and can be developed for various GAN tasks such as style transfer, image to image, face inpainting or image repair semantically, and super resolution.},
  keywords={Training;Vocabulary;Image resolution;Computational modeling;Natural languages;Maintenance engineering;Generative adversarial networks;text;image;word embedding;GAN},
  doi={10.1109/ICIMCIS56303.2022.10017727},
  ISSN={},
  month={Nov},}@ARTICLE{10628026,
  author={Tao, Zhenyu and Xu, Wei and Huang, Yongming and Wang, Xiaoyun and You, Xiaohu},
  journal={IEEE Wireless Communications}, 
  title={Wireless Network Digital Twin for 6G: Generative AI as a Key Enabler}, 
  year={2024},
  volume={31},
  number={4},
  pages={24-31},
  abstract={Digital twin, which enables emulation, evaluation, and optimization of physical entities through synchronized digital replicas, has gained increasing attention as a promising technology for intricate wireless networks. For 6G, numerous innovative wireless technologies and network architectures have posed new challenges in establishing wireless network digital twins. To tackle these challenges, artificial intelligence (AI), particularly the flourishing generative AI, emerges as a potential solution. In this article, we discuss emerging prerequisites for wireless network digital twins, considering the complicated network architecture, tremendous network scale, extensive coverage, and diversified application scenarios in the 6G era. We further explore the applications of generative AI, such as transformer and diffusion models, to empower the 6G digital twin from multiple perspectives, including physical-digital modeling, synchronization, and slicing capability. Subsequently, we propose a hierarchical generative AI-enabled wireless network digital twin at both the message-level and policy-level, and provide a typical use case with numerical results to validate effectiveness and efficiency. Finally, open research issues for wireless network digital twins in the 6G era are discussed.},
  keywords={6G mobile communication;Generative AI;Wireless networks;Emulation;Network architecture;Transformers;Digital twins},
  doi={10.1109/MWC.002.2300564},
  ISSN={1558-0687},
  month={August},}@INPROCEEDINGS{11011774,
  author={G, Mahalaksmi and G, Subhashini and K, Sundarraj and T, Sanjai Raam},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Advanced Tamil Character Recognition using Generative Adversarial Networks and Augmented Reality}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This system on an advanced Tamil character recognition theme uses the benefit of FPGA-based acceleration for gaining the enhancement of high-performance processing, keeping it scalable. What makes such a system interesting is achieving recognition of Tamil text being 3D carvings or embossed inside monuments with an adequate preservation of cultural heritage material. As an added theme, its support to multiscript recognitions-Tamil, Hindi, English and others-would enable it smoothly multilingually be well-integrated into a vast variety of applications. The integration of AR enhances user interaction as it superimposes live recognition results onto the physical object and also provides an immersive learning experience along with hands-on guidance. The architecture of CNNs and GANs used in data augmentation can make this system reliable for recognition across diverse ranges of writing styles, fonts, and languages. It happens to be an ideal tool that fulfils the demands of state-of-the-art Optical Character Recognition (OCR), interpretation of 3D texts, educational applications, as well as real-time multi-lingual functionalities while facing contemporary demands in text digitization.},
  keywords={Accuracy;Text recognition;Optical character recognition;Writing;Three-dimensional printing;Generative adversarial networks;Data augmentation;Real-time systems;Multilingual;Character recognition;Tamil Character Recognition;FPGA Acceleration;Augmented Reality (AR);Multilingual Text Recognition;Generative Adversarial Networks (GANs);3D Text Recognition;Multiscript Recognition},
  doi={10.1109/ICDSAAI65575.2025.11011774},
  ISSN={},
  month={March},}@ARTICLE{8867888,
  author={Shen, Jiaquan and Liu, Ningzhong and Sun, Han and Zhou, Huiyu},
  journal={IEEE Access}, 
  title={Vehicle Detection in Aerial Images Based on Lightweight Deep Convolutional Network and Generative Adversarial Network}, 
  year={2019},
  volume={7},
  number={},
  pages={148119-148130},
  abstract={Vehicle detection in aerial images is a challenging task and plays an important role in a wide range of applications. Traditional detection algorithms are based on sliding-window searching and shallow-learning-based features, which limits the ability to represent features and generates a lot of computational costs. Recently, with the successful application of convolutional neural network in computer vision, many state-of-the-art detectors have been developed based on deep CNNs. However, these CNN-based models still face some difficulties and challenges in vehicle detection in aerial images. Firstly, the CNN-based detection model requires extensive calculations during training and detection, and the accuracy of detection for small objects is not high. In addition, deep learning models often require a large amount of sample data to train a robust detection model, while the annotated data of aerial vehicles is limited. In this study, we propose a lightweight deep convolutional neural network detection model named LD-CNNs. The detection algorithm not only greatly reduces the computational costs of the model, but also significantly improves the accuracy of the detection. What's more, in order to cope with the problem of insufficient training samples, we develop a multi-condition constrained generative adversarial network named MC-GAN, which can effectively generate samples. The detection performance of the proposed model has been evaluated on the Munich public dataset and the collected dataset respectively. The results show that on the Munich dataset, the proposed method achieves 86.9% on mAP (mean average precision), F1-score is 0.875, and the detection time is 1.64s on Nvidia Titan XP. At present, these detection indicators have reached state-of-the-art level in vehicle detection of aerial images.},
  keywords={Feature extraction;Vehicle detection;Object detection;Computational modeling;Generative adversarial networks;Convolutional neural networks;Computational efficiency;Vehicle detection;lightweight convolutional network;generative adversarial network;aerial images},
  doi={10.1109/ACCESS.2019.2947143},
  ISSN={2169-3536},
  month={},}@ARTICLE{9693152,
  author={Gao, Rui and Hou, Xingsong and Qin, Jie and Shen, Yuming and Long, Yang and Liu, Li and Zhang, Zhao and Shao, Ling},
  journal={IEEE Transactions on Multimedia}, 
  title={Visual-Semantic Aligned Bidirectional Network for Zero-Shot Learning}, 
  year={2023},
  volume={25},
  number={},
  pages={1649-1664},
  abstract={Zero-shot learning (ZSL) aims to recognize unknown categories that are unavailable during training. Recently, generative models have shown the potential to address this challenging problem by synthesizing unseen features conditioned on semantic embeddings such as attributes. However, unidirectional generative models cannot guarantee the effective coupling between visual and semantic spaces. To this end, we propose a visual-semantic aligned bidirectional network with cycle consistency to alleviate the gap between these two spaces, generating unseen features of high quality. More importantly, we incorporate two carefully designed strategies into our bidirectional framework to improve the overall ZSL performance. Specifically, we enhance the intra-domain class divergence in both visual and semantic spaces, and in the meantime, mitigate the inter-domain shift to preserve seen-unseen domain discrimination. Experimental results on four standard benchmarks show the superiority of our framework over existing state-of-the-art methods under both conventional and generalized ZSL settings.},
  keywords={Semantics;Visualization;Task analysis;Training;Standards;Data models;Computational modeling;Bidirectional network;generative model;zero-shot learning},
  doi={10.1109/TMM.2022.3145666},
  ISSN={1941-0077},
  month={},}@ARTICLE{10494393,
  author={Bi, Xia-An and Yang, Zicheng and Huang, Yangjun and Xing, Zhaoxu and Xu, Luyun and Wu, Zihao and Liu, Zhengliang and Li, Xiang and Liu, Tianming},
  journal={IEEE Transactions on Medical Imaging}, 
  title={CE-GAN: Community Evolutionary Generative Adversarial Network for Alzheimer’s Disease Risk Prediction}, 
  year={2024},
  volume={43},
  number={11},
  pages={3663-3675},
  abstract={In the studies of neurodegenerative diseases such as Alzheimer’s Disease (AD), researchers often focus on the associations among multi-omics pathogeny based on imaging genetics data. However, current studies overlook the communities in brain networks, leading to inaccurate models of disease development. This paper explores the developmental patterns of AD from the perspective of community evolution. We first establish a mathematical model to describe functional degeneration in the brain as the community evolution driven by entropy information propagation. Next, we propose an interpretable Community Evolutionary Generative Adversarial Network (CE-GAN) to predict disease risk. In the generator of CE-GAN, community evolutionary convolutions are designed to capture the evolutionary patterns of AD. The experiments are conducted using functional magnetic resonance imaging (fMRI) data and single nucleotide polymorphism (SNP) data. CE-GAN achieves 91.67% accuracy and 91.83% area under curve (AUC) in AD risk prediction tasks, surpassing advanced methods on the same dataset. In addition, we validated the effectiveness of CE-GAN for pathogeny extraction. The source code of this work is available at https://github.com/fmri123456/CE-GAN.},
  keywords={Diseases;Generative adversarial networks;Imaging;Genetics;Community networks;Mathematical models;Generators;Generative adversarial networks;imaging genetics;community evolutionary convolution;disease risk prediction;pathogeny extraction;Alzheimer’s disease},
  doi={10.1109/TMI.2024.3385756},
  ISSN={1558-254X},
  month={Nov},}@INPROCEEDINGS{11129378,
  author={Chamnankij, Poonnakan and Charoenchaiprakit, Kanok and Naowanich, Ekachai},
  booktitle={2025 10th International STEM Education Conference (iSTEM-Ed)}, 
  title={Development of an Intelligent Learning Assistance System for Military Curriculum Using Python and Generative AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This article details the development and workflow of an AI-powered learning assistance system designed for military educational institutions, utilizing NLP and intelligent information retrieval techniques, to analyze PDF documents. The system, built with Python and libraries like Google Generative AI and scikit-learn, facilitates personalized learning and teaching by summarizing content, generating question-and-answer responses, and creating multiple-choice questions with answer keys. Experimental results, particularly with historical content like the Nine Armies’ War, demonstrate the system’s ability to accurately summarize key events and extract strategic military tactics in a structured format, enhancing user comprehension. The system adapts to individual learning needs, promotes interactive learning, and improves learning efficiency compared to traditional methods. While limitations such as technological access disparities and AI training data dependencies exist, the research concludes that integrating AI into learning assistance significantly enhances educational effectiveness. Future work will focus on transforming the system into a practical application for educational assessments, conducting real-world testing in educational and military settings, and expanding its application across various academic disciplines.},
  keywords={Generative AI;Education;Retrieval augmented generation;Training data;Learning (artificial intelligence);Portable document format;Information retrieval;Libraries;Internet;Testing;artificial intelligence;learning assistance system;retrieval-augmented generation;term frequency-inverse document frequency},
  doi={10.1109/iSTEM-Ed65612.2025.11129378},
  ISSN={},
  month={July},}@ARTICLE{9224732,
  author={Park, Minje},
  journal={IEEE Access}, 
  title={JGAN: A Joint Formulation of GAN for Synthesizing Images and Labels}, 
  year={2020},
  volume={8},
  number={},
  pages={188883-188888},
  abstract={Image generation with explicit condition or label generally works better than unconditional methods. In modern GAN frameworks, both generator and discriminator are formulated to model the conditional distribution of images given with labels. In this article, we provide an alternative formulation of GAN which models the joint distribution of images and labels. There are two advantages in this joint formulation over conditional approaches. The first advantage is that the joint formulation is more robust to label noises if it's properly modeled. This alleviates the burden of making noise-free labels and allows the use of weakly-supervised labels in image generation. The second is that we can use any kinds of weak labels or image features that have correlations with the original image data to enhance unconditional image generation. We will show the effectiveness of our joint formulation on CIFAR10, CIFAR100, and STL dataset with the state-of-the-art GAN architecture.},
  keywords={Generators;Image synthesis;Gallium nitride;Generative adversarial networks;Probability distribution;Training;Network architecture;Deep learning;image synthesis;generative adversarial network},
  doi={10.1109/ACCESS.2020.3031292},
  ISSN={2169-3536},
  month={},}@ARTICLE{8859190,
  author={Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Umar, Abubakar Malah and Linus, Okafor Uchenwa and Arshad, Humaira and Kazaure, Abdullahi Aminu and Gana, Usman and Kiru, Muhammad Ubale},
  journal={IEEE Access}, 
  title={Comprehensive Review of Artificial Neural Network Applications to Pattern Recognition}, 
  year={2019},
  volume={7},
  number={},
  pages={158820-158846},
  abstract={The era of artificial neural network (ANN) began with a simplified application in many fields and remarkable success in pattern recognition (PR) even in manufacturing industries. Although significant progress achieved and surveyed in addressing ANN application to PR challenges, nevertheless, some problems are yet to be resolved like whimsical orientation (the unknown path that cannot be accurately calculated due to its directional position). Other problem includes; object classification, location, scaling, neurons behavior analysis in hidden layers, rule, and template matching. Also, the lack of extant literature on the issues associated with ANN application to PR seems to slow down research focus and progress in the field. Hence, there is a need for state-of-the-art in neural networks application to PR to urgently address the above-highlights problems for more successes. The study furnishes readers with a clearer understanding of the current, and new trend in ANN models that effectively addresses PR challenges to enable research focus and topics. Similarly, the comprehensive review reveals the diverse areas of the success of ANN models and their application to PR. In evaluating the performance of ANN models, some statistical indicators for measuring the performance of the ANN model in many studies were adopted. Such as the use of mean absolute percentage error (MAPE), mean absolute error (MAE), root mean squared error (RMSE), and variance of absolute percentage error (VAPE). The result shows that the current ANN models such as GAN, SAE, DBN, RBM, RNN, RBFN, PNN, CNN, SLP, MLP, MLNN, Reservoir computing, and Transformer models are performing excellently in their application to PR tasks. Therefore, the study recommends the research focus on current models and the development of new models concurrently for more successes in the field.},
  keywords={Artificial neural networks;Task analysis;Fingerprint recognition;Computational modeling;Image recognition;Agriculture;Artificial neural networks;application to pattern recognition;feedforward neural networks;feedback neural networks;hybrid models},
  doi={10.1109/ACCESS.2019.2945545},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9176302,
  author={Cai, Zhuotong and Xin, Jingmin and Wu, Jiayi and Liu, Sijie and Zuo, Weiliang and Zheng, Nanning},
  booktitle={2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Triple Multi-scale Adversarial Learning with Self-attention and Quality Loss for Unpaired Fundus Fluorescein Angiography Synthesis}, 
  year={2020},
  volume={},
  number={},
  pages={1592-1595},
  abstract={Clinically, the Fundus Fluorescein Angiography (FA) is a more common mean for Diabetic Retinopathy (DR) detection since the DR appears in FA much more contrasty than in Color Fundus Image (CF). However, acquiring FA has a risk of death due to the fluorescent allergy. Thus, in this paper, we explore a novel unpaired CycleGAN-based model for the FA synthesis from CF, where some strict structure similarity constraints are employed to guarantee the perfectly mapping from one domain to another one. First, a triple multi-scale network architecture with multi-scale inputs, multi-scale discriminators and multi-scale cycle consistency losses is proposed to enhance the similarity between two retinal modalities from different scales. Second, the self-attention mechanism is introduced to improve the adaptive domain mapping ability of the model. Third, to further improve strict constraints in the feather level, quality loss is employed between each process of generation and reconstruction. Qualitative examples, as well as quantitative evaluation, are provided to support the robustness and the accuracy of our proposed method.},
  keywords={Retina;Image reconstruction;Image generation;Angiography;Loss measurement;Generators;Diabetes},
  doi={10.1109/EMBC44109.2020.9176302},
  ISSN={2694-0604},
  month={July},}@ARTICLE{10168237,
  author={Wu, Yanfeng and Li, Taihao and Zhao, Junan and Wang, Qirui and Xu, Jing},
  journal={IEEE Signal Processing Letters}, 
  title={A Fused Speech Enhancement Framework for Robust Speaker Verification}, 
  year={2023},
  volume={30},
  number={},
  pages={883-887},
  abstract={Robust speaker verification (RSV) under noisy conditions is still a challenging task. Recently, some task-specific speech enhancement (SE) approaches are proposed and achieve excellent performance on RSV. However, all these works adopt only one kind of SE network and thus can not remove noise from different aspects, limiting the performance of the RSV task. In this letter, we propose a fused SE framework (FSEF) for RSV, which integrates both T-F masking-based and feature mapping-based SE networks to collect complementary information and improve the robustness against noise. Two FESF-RSV systems are constructed based on two kinds of fusion methods: score fusion and feature fusion. In addition, we present a Multi-Scale Attentive Context Aggregation Network (MSACAN) as the backbone structure in the FSEF. The MSACAN can not only extract and fuse multi-scale features adaptively but also enhance speaker characteristics against noise and interfering speakers. Experiments conducted on the noise-simulated VoxCeleb1 dataset demonstrate both the FSEF and the MSACAN can improve the performance of RSV compared to previous approaches.},
  keywords={Training;Feature extraction;Task analysis;Speech enhancement;Convolution;Noise measurement;Signal to noise ratio;Robust speaker verification;speech enhancement;multi-scale feature extraction;attention mechanism},
  doi={10.1109/LSP.2023.3290832},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{8925235,
  author={Lowe, Robert and Almér, Alexander and Gander, Pierre and Balkenius, Christian},
  booktitle={2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)}, 
  title={Vicarious Value Learning and Inference in Human-Human and Human-Robot Interaction}, 
  year={2019},
  volume={},
  number={},
  pages={395-400},
  abstract={Among the biggest challenges for researchers of human-robot interaction is imbuing robots with lifelong learning capacities that allow efficient interactions between humans and robots. In order to address this challenge we are developing computational mechanisms for a humanoid robotic agent utilizing both system 1 and system 2-like cognitive processing capabilities. At the core of this processing is a Social Affective Appraisal model that allows for vicarious value learning and inference. Using a multi-dimensional reinforcement learning approach the robotic agent learns affective value-based functions (system 1). This learning can ground representations of affective relations (predicates) relevant to interacting agents. In this article we discuss the existing theoretical basis for developing our neural network model as a system 1-like process. We also discuss initial ideas for developing system 2-like top-down/generative affective (semantic relation-based) processing. The aim of the symbolic-connectionist architectural development is to promote autonomous capabilities in humanoid robots for interacting efficiently/intelligently (recombinant application of learned associations) with humans in changing and challenging environments.},
  keywords={Computational modeling;Instruments;Appraisal;Learning (artificial intelligence);Animals;Human-robot interaction;Robots;Social Affective Appraisal;Reinforcement Learning;Humanoid Robots;Artificial General Intelligence},
  doi={10.1109/ACIIW.2019.8925235},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9747847,
  author={Du, Chenghu and Yu, Feng and Jiang, Minghua and Wei, Xiong and Peng, Tao and Hu, Xinrong},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Multi-Pose Virtual Try-On Via Self-Adaptive Feature Filtering}, 
  year={2022},
  volume={},
  number={},
  pages={2544-2548},
  abstract={With the growing trend of virtual try-on, multi-pose tasks attract researchers due to their higher commercial value. Prior methods lack an effective geometric deformation to maintain the original image details resulting in many details loss in the head and garment. To address this problem, we propose a new multi-pose virtual try-on network, which can fit a garment to the corresponding area of a person in arbitrary poses. First, the target pose’s body-semantic distribution is predicted by the target pose point. Second, the in-shop garment and human body are warped based on a human pose to solve the unnatural alignment and the lack of body details by the Deformation Module (DM). Finally, the human body in the given pose and garment is fine generated by the Filtering Synthesis Network (FSN). Compared to state-of-the-art methods with objective experiments on the MPV dataset, the proposed method achieves the best performance in metrics and the rich details in visual results.},
  keywords={Measurement;Visualization;Filtering;Image synthesis;Clothing;Signal processing;Market research;multi-pose virtual try-on;semantic segmentation;pose transfer;feature filtering;appearance flow},
  doi={10.1109/ICASSP43922.2022.9747847},
  ISSN={2379-190X},
  month={May},}@INPROCEEDINGS{10050592,
  author={Yang, Gang and Wang, XiaoLei and Wang, LuLu and Zhang, Yi and Han, Yu and Tan, Xin and Zhang, Shang Yong},
  booktitle={2022 5th International Conference on Information Communication and Signal Processing (ICICSP)}, 
  title={Adversarial Attack on Communication Signal Modulation Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Convolutional network models (CNN) are very vulnerable to adversarial samples, which poses a serious challenge to the security of CNN models. Based on the task of CNN's modulation and identification of communication signals, we propose a white-box attack algorithm, the shortest distance attack method (SD-Alg), which can generate extremely small disturbances and greatly reduce the classification performance of the model. Experiments show that our algorithm excels in attack success rate, running time and adversarial perturbation size among the same type of algorithms.},
  keywords={Deep learning;Perturbation methods;Signal processing algorithms;Modulation;Pattern classification;Classification algorithms;Convolutional neural networks;adversarial attack;adversarial examples;modulation recognition;CNN},
  doi={10.1109/ICICSP55539.2022.10050592},
  ISSN={2770-792X},
  month={Nov},}@INPROCEEDINGS{9553278,
  author={Duan, Baorui and Quan, Dou and Li, Yi and Lei, Ruiqi and Wang, Shuang and Hou, Biao and Jiao, Licheng},
  booktitle={2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS}, 
  title={A Feature Decomposition Framework for Multi-Modal Image Patch Matching}, 
  year={2021},
  volume={},
  number={},
  pages={2250-2253},
  abstract={Multi-modal remote sensing images have complementary information which is conducive to enhancing the performance of various applications. Image patch matching plays a crucial role in the combination of multi-modal images. However, there are great differences in appearance and texture of multi-modal images, which brings great difficulties to image patching matching. To solve this problem, we propose a novel feature decomposition framework for multi-modal image patch matching. It aims to eliminate the hinder caused by the significant difference in multi-modal images. Specifically, this paper proposes to decompose the feature of images into common feature and modal private feature. Then, only the common feature is used for image patch matching, so as to improve the matching accuracy. Experimental results on optical and SAR images demonstrate that our proposed feature decomposition framework can significantly improve the performance of multi-modal image patch matching.},
  keywords={Training;Apertures;Feature extraction;Optical imaging;Radar polarimetry;Optical sensors;Image reconstruction;patch matching;multi-modal image;synthetic aperture radar;feature decomposition},
  doi={10.1109/IGARSS47720.2021.9553278},
  ISSN={2153-7003},
  month={July},}@INPROCEEDINGS{10960883,
  author={Sabariram, M.Sachin and Vikram, C.B. Sanjay and Kavin, M. and Monieesh, S. and Saravanan, G. and Kumar, M.Senthil},
  booktitle={2024 First International Conference on Data, Computation and Communication (ICDCC)}, 
  title={An End-to-End Pipeline Integrating Segmentation, Anomaly Detection, and Report Generation for Medical Imaging Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={65-70},
  abstract={Medical imaging plays a pivotal role in diagnosing and treating a variety of conditions, from brain abnormalities to retinal diseases. However, interpreting large volumes of imaging data remains an arduous and time-consuming task for radiologists, necessitating the development of automated diagnostic tools. In this paper, we present an integrated end-to-end pipeline that leverages state-of-the-art techniques in segmentation, anomaly detection, and medical report generation to improve both accuracy and efficiency. At the core of our approach lies a modified Visual Large Language Model (VLM), adapted with medical-specific adapters to transition from natural image analysis to medical image diagnostics. Our system, tested in few-shot (k=4) learning settings, demonstrates superior performance with over 90% Area Under the Curve (AUC) for anomaly detection and segmentation across diverse domains such as Brain MRI, Chest X-ray, and Retina scans. Post-anomaly detection, a custom labeling model performs multiclass classification of anomaly regions, assigning severity scores to detected abnormalities. A fine-tuned Open source Large Language Model (LLM) then generates comprehensive diagnostic reports that not only summarize findings but also suggest further diagnostic pathways. Compared to state-of-the-art models such as AprilGAN, our system demonstrates notable improvements in accuracy, adaptability, and clinical utility. These contributions establish our pipeline as a promising tool for enhancing medical imaging workflows.},
  keywords={Image segmentation;Analytical models;Accuracy;Image analysis;Large language models;Pipelines;Retina;Labeling;Medical diagnostic imaging;Anomaly detection;Medical Imaging;Anomaly Detection;Segmentation;Visual-Language Models (VLMs);Diagnostic Report Generation},
  doi={10.1109/ICDCC62744.2024.10960883},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10834253,
  author={Yang, Siqi and Liu, Shu and Shang, Penghui and Wang, Hui},
  booktitle={2024 7th International Conference on Robotics, Control and Automation Engineering (RCAE)}, 
  title={An Overview of Methods of Industrial Anomaly Detection}, 
  year={2024},
  volume={},
  number={},
  pages={603-607},
  abstract={Industrial anomaly detection plays a crucial role in modern manufacturing and production processes. In order to help researchers and professionals in the manufacturing industry understand the developing research results in this field, this paper begins with an introduction of common methods of industrial anomaly detection such as neural networks, GAN, transformer with discussing the limitations in this domain. Subsequently, owing to the emergence of the fourth wave of artificial intelligence, Advanced anomaly detection techniques based on Large Language Models (LLMs) and variants are explored, including AnomalyGPT, VisionGPT and LLMAD which have influenced a lot as the cutting-edge technologies. Additionally, based on the current developing trends of the LLMs-based method, this paper analyzes challenges and deficiencies existing in practice which have the potential to lead the way in future research. Conclusively, this overview exposes the bottleneck of traditional methods and emphasizes the revolutionary impact LLMs could have on multimodal scenes of anomaly detection while putting insight into the extend of methodical practicability.},
  keywords={Manufacturing industries;Service robots;Large language models;Neural networks;Production;Transformers;Market research;Generative adversarial networks;Manufacturing;Anomaly detection;anomaly detection;artificial intelligence;LLMs-based;multimodal scenes},
  doi={10.1109/RCAE62637.2024.10834253},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10898212,
  author={Li, Lan and Dou, Hongzhe and Yu, Zijia and Zhang, Xujuan and Sun, Lijuan and Wang, Jundi},
  booktitle={2024 International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)}, 
  title={Silent Face Anti-Spoofing Detection Algorithm Based on Meta-learning and Attention Mechanism}, 
  year={2024},
  volume={},
  number={},
  pages={394-398},
  abstract={In the field of facial identity authentication, silent face anti-spoofing detection, as a key technology, to prevent fake photos and videos, is receiving widespread attention. To address the issue of low detection accuracy in existing algorithms that similar tasks have not appear in training, silent face anti-spoofing detection algorithm based on meta-learning and attention mechanism is proposed. First, by leveraging meta-learning to quickly adapt to shot samples, the proposed algorithm uses MAML as the main network to capture the commonalities between training tasks, allowing it to quickly adjust parameters in new scenarios or new attack types to achieve efficient detection. Then, in order to focus the model on extracting and analyzing the key features of facial regions, an attention mechanism is introduced in MAML to reduce the interference from the facial background region. Finally, a Fourier spectrum aided supervision branch is designed to improve the accuracy of classification by using the difference of the Fourier spectrum of real/fake face in the frequency domain. Experimental results show that the proposed algorithm achieves good accuracy on public NUAA dataset, with AUC and F1 scores of 0.99 and 0.90, respectively.},
  keywords={Metalearning;Training;Analytical models;Adaptation models;Accuracy;Attention mechanisms;Interference;Feature extraction;Detection algorithms;Faces;meta-learning;attention mechanism;Fourier spectrum diagram;face attack;silent anti-spoofing detection},
  doi={10.1109/AEECA62331.2024.00075},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10961096,
  author={Sabariram, M. Sachin and Vikram, C.B. Sanjay and Kavin, M. and Monieesh, S. and Saravanan, G. and Kumar, M. Senthil},
  booktitle={2024 First International Conference on Data, Computation and Communication (ICDCC)}, 
  title={An End-to-End Pipeline Integrating Segmentation, Anomaly Detection, and Report Generation for Medical Imaging Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={71-76},
  abstract={Medical imaging plays a pivotal role in diagnosing and treating a variety of conditions, from brain abnormalities to retinal diseases. However, interpreting large volumes of imaging data remains an arduous and time-consuming task for radiologists, necessitating the development of automated diagnostic tools. In this paper, we present an integrated end-to-end pipeline that leverages state-of-the-art techniques in segmentation, anomaly detection, and medical report generation to improve both accuracy and efficiency. At the core of our approach lies a modified Visual Large Language Model (VLM), adapted with medical-specific adapters to transition from natural image analysis to medical image diagnostics. Our system, tested in few-shot (k=4) learning settings, demonstrates superior performance with over 90% Area Under the Curve (AUC) for anomaly detection and segmentation across diverse domains such as Brain MRI, Chest X-ray, and Retina scans. Post-anomaly detection, a custom labeling model performs multiclass classification of anomaly regions, assigning severity scores to detected abnormalities. A fine-tuned Open source Large Language Model (LLM) then generates comprehensive diagnostic reports that not only summarize findings but also suggest further diagnostic pathways. Compared to state-of-the-art models such as AprilGAN, our system demonstrates notable improvements in accuracy, adaptability, and clinical utility. These contributions establish our pipeline as a promising tool for enhancing medical imaging workflows.},
  keywords={Image segmentation;Analytical models;Accuracy;Image analysis;Large language models;Pipelines;Retina;Labeling;Medical diagnostic imaging;Anomaly detection;Medical Imaging;Anomaly Detection;Segmentation;Visual-Language Models (VLMs);Diagnostic Report Generation},
  doi={10.1109/ICDCC62744.2024.10961096},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10911583,
  author={S, Priyanka and Krishnasamy, Lalitha and P, Jayadharshini and S, Shri Sashmitha and P, Srisuthi and S, Vikas P},
  booktitle={2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={Detection of Pneumonia Using CNN and its Variants}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Severe respiratory infections like pneumonia pose a major global health threat, requiring prompt and precise diagnosis for appropriate treatment. Conventional diagnostic techniques are often subjective and time-consuming due to reliance on radiological expertise. This study leverages deep learning to improve pneumonia detection from chest X- rays, evaluating multiple CNN designs such as ResNet50v2, VGG16, MobileNetV2, CheXNet, InceptionV3, and a customized CNN. Models are trained and tested on a large dataset of labelled chest X-rays, employing preprocessing techniques like data augmentation, scaling, and normalization. Transfer learning optimizes the models where applicable. Performance is assessed using recall, accuracy, precision, and F1-score, alongside ROC curve and confusion matrix analysis. Findings indicate that deep learning models significantly enhance pneumonia detection accuracy, with CheXNet and ResNet50v2 outperforming others. This comparative analysis highlights each model's strengths and weaknesses, advancing the development of reliable, automated diagnostic tools for pneumonia and potentially enabling quicker and more precise clinical decision- making.},
  keywords={Deep learning;Training;Pneumonia;Accuracy;Transfer learning;Software;Real-time systems;Reliability;X-ray imaging;Biomedical imaging;CNN;ResNet50v2;VGG16;MobileNetV2;CheXNet;InceptionV3},
  doi={10.1109/ICTBIG64922.2024.10911583},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11167676,
  author={Wibowo, Mars Caroline and Manongga, Danny and Hendry, Hendry and Bayu, Teguh Indra},
  booktitle={2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT)}, 
  title={Interactive Co-Creation with StyleGAN for Enhancing Visual Design Using Generative AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The integration of artificial intelligence (AI) into creative workflows has significantly transformed digital media design, enabling more efficient and innovative processes. This study proposes an interactive co-creation framework driven by StyleGAN2-ADA, designed to enhance human-AI collaboration in visual content generation. The purpose of the research is to evaluate the potential of AI-assisted ideation in creative industries, focusing on the development of a real-time system that allows users to manipulate semantic visual attributes such as expression, style intensity, and lighting. A total of 18 participants, including design professionals and students, engaged with the system to perform creative tasks, providing both quantitative and qualitative data on system usability and effectiveness. The study utilized Fréchet Inception Distance (FID), System Usability Scale (SUS), and interaction logs to measure image quality, user satisfaction, and engagement. Key findings include a low FID score of 4.82, an average task completion time of 6.2 minutes, and a SUS score of 84.1, indicating high usability and efficiency. User feedback highlighted the system’s ability to facilitate rapid ideation and foster a productive balance between AI assistance and creative autonomy. The findings suggest that the proposed framework can serve as an effective tool for enhancing the creative process, with potential applications in design, education, and multimedia systems. Future research will explore expanding the framework’s applicability to diverse creative tasks and further improving user interaction features.},
  keywords={Industries;Image quality;Visualization;Multimedia systems;Semantics;Lighting;Media;Particle measurements;Real-time systems;Usability;StyleGAN;Co-Creation;Creative AI;Visual Design;Human-AI Interaction},
  doi={10.1109/ICCIT65724.2025.11167676},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11036048,
  author={Ranjith, K.S. and Anusha, N and Ummeruman, S and Kumar, Pambala Naveen and Sandeep, K and Kumar, Moturu Dinesh},
  booktitle={2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)}, 
  title={AgriVision-B7 A Smart Agriculture Model for Disease Severity Assessment and Treatment Planning}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Plant leaf disease detection plays a vital role in modern precision agriculture, aimed at enhancing crop yield and minimizing losses. Identifying plant diseases early and accurately is crucial for timely intervention, which helps stop the spread of diseases and reduces reliance on chemical treatments. Standard disease testing protocols base their detection on manual visual checks that are time-consuming and prone to both labor costs and personal mistakes. Computer vision and machine learning technologies enabled the creation of automatic plant leaf disease identifying systems which have addressed previous detection challenges. Deep learning methods specifically use Convolutional Neural Networks to evaluate plant leaf disease types based on their appearance. We extract essential features from high definition leaf images including shape information combined with texture components and color data through diagnosis comparison of disease and healthy leaf images. Our model is trained on extensive annotated datasets to accurately recognize diseases like blight, rust, mildew, and leaf spots. To further improve model performance, methods like transfer learning, image augmentation, and ensemble learning are incorporated.},
  keywords={Deep learning;Plant diseases;Visualization;Transfer learning;Feature extraction;Image augmentation;Ensemble learning;Convolutional neural networks;Diseases;Testing;Convolutional Neural networks;Deep learning techniques;High-resolution;transfer learning;image augmentation;ensemble learning},
  doi={10.1109/ICKECS65700.2025.11036048},
  ISSN={},
  month={April},}@INPROCEEDINGS{10527826,
  author={Ma, Qi and Yang, Zeyu and Zhang, Qingyao and Yang, Yejia and Ran, Haibin and Bian, Yuhao},
  booktitle={2023 3rd International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)}, 
  title={Research on Feature-based Image Recognition based on Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={242-245},
  abstract={Recently, image identification has achieved promising performance and become the most essential component in real-life image-based applications through training a machine learning model. However, existing methods extract the features and the identification accuracy is almost depended on these features with extractions process, which is the fundamental process for learning models. Existing methods are almost concentrated on the utilization of these feature and ignore the process of extracting procedure. As far as we can concern, the image features contain numerous information and can assist the learning model to enhance the identification accuracy. In this work, we initially utilize a convolutional neural network to obtain the features in the feature space. Subsequently, an attention mechanism and the feature clustering are applying to optimize the extracted features. Finally, we evaluate our proposed model with most used methods in the identical environment setups. From our extensive experimental results, we can observe that our model outperforms compared methods with reasonable computation costs.},
  keywords={Training;Adaptation models;Computational modeling;Scalability;Transfer learning;Process control;Feature extraction;Image features;Identification;Convolutional neural network;Attention mechanism;Clustering},
  doi={10.1109/CEI60616.2023.10527826},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10985601,
  author={Dai, Jiahui and Yan, Tao and Jiang, Xin and Xiao, Hanguang and Xin, Hongguo and Zhang, Chengbao},
  booktitle={2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE)}, 
  title={ELL-ORB-SLAM: Improved Robust Monocular Visual SLAM Algorithm Based on Image Enhancement and Dynamic FAST Threshold for Low-light Environment}, 
  year={2025},
  volume={},
  number={},
  pages={699-704},
  abstract={The accuracy of visual localization estimation heavily relies on the quality of input images and feature point extraction, with variations in illumination significantly impacting matching results under challenging conditions. To address this, we propose an enhanced ORB-SLAM algorithm (ELL-ORB-SLAM) based on dynamic adaptive FAST thresholds and image enhancement techniques, including illumination map estimation, improved Truncated Adaptive Gamma Correction, and CLAHE, to improve performance in low-light conditions and refine feature point extraction. Experiments conducted on the EuRoC, KITTI, and ETH3D datasets demonstrate that ELL-ORB-SLAM significantly outperforms the state-of-the-art ORB-SLAM3, particularly in challenging environments. Additionally, real-world tests using a monocular camera showcase the system's robustness and versatility across diverse settings.},
  keywords={Location awareness;Image quality;Visualization;Simultaneous localization and mapping;Heuristic algorithms;Lighting;Estimation;Feature extraction;Robustness;Image enhancement;Visual SLAM;image enhancement;feature point extraction;illumination},
  doi={10.1109/ICCECE65250.2025.10985601},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11086721,
  author={Bo, Hao and Xia, Tian and Hu, Jingtian and Deng, Xiao and Cheng, Lifeng and An, Xiang},
  booktitle={2025 10th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={The ECG Signal Denoising Method Based on Temporal Convolutional Network and Attention Mechanism}, 
  year={2025},
  volume={},
  number={},
  pages={32-37},
  abstract={Denoising electrocardiogram (ECG) signals is a critical task in arrhythmia analysis and diagnosis. However, traditional signal processing methods face limitations when addressing the challenge of noise spectrum overlapping with the signal spectrum. To tackle this issue, this paper proposes a deep learning model, TACA-Net, which is based on Temporal Convolutional Networks (TCN) and channel attention mechanisms, for efficiently denoising noisy ECG signals. TACA-Net leverages temporal dilated convolutions to capture multi-scale temporal features, integrates ECA and SE channel attention modules to enhance the extraction of critical information, and incorporates residual connections to improve training stability and signal reconstruction quality. Experiments conducted on the MIT-BIH Arrhythmia Database and Noise Stress Test Database demonstrate that TACA-Net outperforms traditional deep learning methods in terms of Signal-to-Noise Ratio Improvement (SNRimp), Root Mean Square Error (RMSE), and Percentage Root Mean Square Difference (PRD), particularly under low signal-to-noise ratio (SNR) conditions. This study highlights TACA-Net's robustness and efficiency in complex noisy environments, offering a novel approach to ECG signal denoising and providing significant reference value for signal denoising research.},
  keywords={Deep learning;Attention mechanisms;Databases;Noise reduction;Electrocardiography;Feature extraction;Signal denoising;Convolutional neural networks;Root mean square;Signal to noise ratio;Deep Learning;ECG Signals;Signal Denoising;Temporal Convolutional Network;Channel Attention Mechanism},
  doi={10.1109/ICSP65755.2025.11086721},
  ISSN={},
  month={May},}@INPROCEEDINGS{10903815,
  author={Rampelli, Sriram and Bukatia, Wasim},
  booktitle={2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC)}, 
  title={Empowering Healthcare Data Systems with an Innovative Chatbot Application Utilizing Python and Advanced Generative AI Models}, 
  year={2025},
  volume={},
  number={},
  pages={00672-00680},
  abstract={The exponential growth of healthcare data presents significant challenges in data accessibility, patient engagement, and operational efficiency. This paper introduces a novel chatbot application that leverages advanced generative AI models, specifically a fine-tuned LLaMA and an enhanced BioBERT, to empower healthcare data systems. The chatbot integrates innovative techniques in natural language processing (NLP), data preprocessing, and model fine-tuning to handle diverse tasks such as answering complex medical queries, analyzing various document types, recognizing medical conditions from prescriptions, and providing personalized health recommendations. The system's architecture is designed with scalability and real-world deployment in mind, utilizing asynchronous processing and optimized model serving strategies. Comprehensive experiments demonstrate significant improvements over baseline models in terms of accuracy, response time, and user satisfaction. Ethical considerations, including data privacy and compliance with regulations like HIPAA and GDPR, are thoroughly addressed to ensure responsible deployment in healthcare settings.},
  keywords={Ethics;Generative AI;Biological system modeling;Scalability;Medical services;Chatbots;Data models;Regulation;Data systems;Time factors;Healthcare Chatbot;LLaMA;BioBERT;Web Scraping;Mod-ular System Architecture;Patient Interaction;Model Fine-Tuning},
  doi={10.1109/CCWC62904.2025.10903815},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10895706,
  author={Sathe, Pranav and Sabane, Varad and Undale, Chaitanya and Uttarkar, Atharva and Chavhan, Vaibhav and Sable, Nilesh P. and Yenkikar, Anuradha},
  booktitle={2024 IEEE Pune Section International Conference (PuneCon)}, 
  title={Deepfake Image Detection Using Yolov8}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Due to growing number of fake media and the possible problems with misinformation and identity theft, deepfake image recognition has become a hot topic. In order to evaluate the efficacy of widely recognized deep learning models in detecting and classifying deepfake images, we compare and contrast YOLO (You Only Look Once) V8, CNN (Convolutional Neural Network), combination of LSTM (Long Short-Term Memory) with CNN and ResNet in this paper. The capacity to recognize photos that have been manipulated effectively is essential for maintaining trust regarding digital media and preventing the spread of deceptive data. The objective of our research is to explain each model's abilities, limitations and performance in the context of deepfake image recognition. We evaluated each model's accuracy, precision, and F1-scores using a dataset of 190402 images, half real and half fake that cover a wide spectrum of deep fake images. Based on the evaluation metrics, each model was ranked in terms of its ability to separately describe and determine original versus generated photos with focus on detecting subtle changes eluding human senses. The comparison research shows each model's nuanced capabilities, offering light on the implications for applications in the real world like detecting and controlling the spread of deep fake information across numerous internet platforms. Our findings help towards building deep fake detection systems, further understanding the comparative performance of state of the art deep architectures. This study shall inform more effective deep fake detection systems that can guarantee trust and security in digital media environments. Accuracy achieved by model is 96%, precision score was 94%, recall value is 0.98, F1 score has a value of 0.95.},
  keywords={YOLO;Deepfakes;Accuracy;Image recognition;Computational modeling;Scalability;Computer architecture;Convolutional neural networks;Security;Long short term memory;Deepfake detection;Image classification;YOLOV8;Long Short-Term Memory;Convolutional Neural Network;ResNet;Deep Learning},
  doi={10.1109/PuneCon63413.2024.10895706},
  ISSN={2831-5022},
  month={Dec},}@INPROCEEDINGS{10821754,
  author={Yang, Yingjia and Wang, Yuan and Li, Cheng and Cao, Dehua and Pang, Baochuan and Xiao, Hailian and Wang, Junfeng and Liu, Juan},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Three-Step Framework for Automatically Registering Immunohistochemical and H&E Stained Whole Slide Images Targeting Clinical Applications}, 
  year={2024},
  volume={},
  number={},
  pages={3887-3890},
  abstract={Joint analysis of multiple biomarker images and tissue morphology plays an important role in disease diagnosis, treatment planning, and drug development. This necessitates precise cross-staining comparison among Whole Slide Images (WSIs) of immunohistochemical (IHC) and hematoxylin and eosin (H&E) stained microscopic slides. In this paper, we propose a novel three-step, feature-based, automatic cross-staining WSI alignment method to support clinical practice. The comparative and ablation experiments have shown the good performance of our method, suggesting that it can offer a simple and effective cross-staining WSIs alignment contributing to pathological diagnosis.},
  keywords={Deep learning;Training;Pathology;Computational modeling;Microscopy;Morphology;Planning;Medical diagnosis;Immune system;Biomedical imaging;image registration;H&E stained;immunohistochemistry;whole slide image;segmentation},
  doi={10.1109/BIBM62325.2024.10821754},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10473423,
  author={Ye, Meng and Zhou, Jianwen and Zhou, Xudong and Wang, Yitao and Li, Zhigang and Wang, Changling},
  booktitle={2023 9th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={Research on Semi Supervised GAN Network Algorithm for Automatic Generation of Cable Construction Plan}, 
  year={2023},
  volume={},
  number={},
  pages={151-154},
  abstract={Cable construction has always relied on the experience of construction personnel to deploy and implement, lacking advanced construction plan automatic generation algorithms for digital information planning of the entire construction process, resulting in low efficiency. Changing the current situation of the entire industry requires comprehensive innovation and intelligent transformation of the power construction process from the software and hardware aspects of cable construction plan generation methods, advanced and innovative IoT construction devices, etc. This paper adopts the artificial intelligence countermeasure network GAN model, proposes the cluster consistency Loss function, improves the feature matching method and feature matching method, optimizes the training generator parameters, generates self-learning data, and cooperates with the construction scheme generation platform to generate a more intelligent and optimized automatic output cable construction scheme.},
  keywords={Training;Technological innovation;Power cables;Software algorithms;Generative adversarial networks;Data models;Software;Cable construction;Artificial intelligence;Adversarial network;Loss function of cluster consistency},
  doi={10.1109/ICNISC60562.2023.00056},
  ISSN={},
  month={Oct},}@ARTICLE{10609455,
  author={Hu, Xinrong and Fang, Chao and Yang, Kai and Liang, Jinxing and Luo, Ruiqi and Peng, Tao},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Toward High-Fidelity 3D Virtual Try-On via Global Collaborative Modeling}, 
  year={2024},
  volume={70},
  number={3},
  pages={5312-5325},
  abstract={Existing 3D virtual try-on pipelines rely on purely convolutional operations that directly warp the input garment to align with the boundary of the preserved region. However, these methods fail to consider the mutual correlation between the human body and in-shop clothing, leading to texture distortion in the try-on images. Furthermore, these methods face challenges in learning spatial features at the pixel-level representation since they depend on a simple encoder to extract prior information for 3D reconstruction. To address these issues, we propose a Highly Realistic 3D Virtual Try-on (H3DVT+) network. Specifically, we first design a collaborative transformer matching block (CTMB) to establish more reasonable global mutual dependencies that can warp the garment into the spatial distribution under the more natural try-on state. Second, we propose a novel collaborative transformer-based U-Net generating block (CTUGB) using activated context representations to guide the synthesis of more realistic 2D try-on results. Based on these context representations, the significant region within the input data can be enhanced. Finally, we improve a dual-stage 3D reconstruction method: 1) In the first stage, we introduce a multi-resolution parallel network to learn more comprehensive prior features from 2D person images; 2) In the second stage, we introduce a cascaded attention U-Net and a multi-scale discriminator to more accurately infer local fine details of the 3D shape prior for creating fine-grained dressed 3D human body. We conducted performance evaluations on the MPV-3D and VITON datasets, and the experimental results demonstrate that our method outperforms existing state-of-the-art 3D try-on methods in terms of accuracy and realism.},
  keywords={Three-dimensional displays;Clothing;Image reconstruction;Transformers;Feature extraction;Collaboration;Generative adversarial networks;Virtual try-on;modeling of long-range dependencies;3D human reconstruction;generative adversarial network},
  doi={10.1109/TCE.2024.3433526},
  ISSN={1558-4127},
  month={Aug},}
