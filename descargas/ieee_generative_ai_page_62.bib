@ARTICLE{10985773,
  author={Adornetto, Carlo and Mora, Adrian and Hu, Kai and Garcia, Leticia Izquierdo and Atchade-Adelomou, Parfait and Greco, Gianluigi and Pastor, Luis Alberto Alonso and Larson, Kent},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Generative Agents in Agent-Based Modeling: Overview, Validation, and Emerging Challenges}, 
  year={2025},
  volume={},
  number={},
  pages={1-20},
  abstract={The advent of Generative Agents (GAs) based on Large Language Models (LLMs) has significantly influenced the evolution of Agent-Based Modeling (ABM), offering new perspectives across various domains, including engineering and social sciences. This paper provides an extensive overview of the integration of GAs into ABMs, emphasizing the advancements and emerging challenges in their validation. Traditional ABMs, characterized by their simplistic yet powerful approach to modeling complex systems, have been redefined with the introduction of GAs. This new generation of agents is often equipped with conversational capabilities. These agents, capable of simulating believable human behaviors and interactions, present unique opportunities and hurdles, especially in urban simulations and social dynamics. We explore the nuanced differences between traditional ABMs and ABMs populated by GAs—called GABMs. We delve into the state-of-the-art implementations of GAs, and review various validation methods. Through this comprehensive examination, we aim to shed light on the potential and limitations of GAs, advocating for the design of hybrid ABM-GABM approaches and systematic validation.},
  keywords={Biological system modeling;Behavioral sciences;Reviews;Urban areas;Large language models;Terminology;Predictive models;Media;Mathematical models;Decision making;Agent-Based Models;Generative Agents;Generative AI;Large Language Models},
  doi={10.1109/TAI.2025.3566362},
  ISSN={2691-4581},
  month={},}@INPROCEEDINGS{11126605,
  author={Kabir, Md Shahriar and Alamgeer, Sana and Debnath, Minakshi and Ngu, Anne H. H.},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare}, 
  year={2025},
  volume={},
  number={},
  pages={866-875},
  abstract={The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.},
  keywords={Training;Generative AI;Computational modeling;Predictive models;Data collection;Brain modeling;Diffusion models;Transformers;Data models;Synthetic data;Time-Series Data Generation;Generative AI;Synthetic Data Generation;Diffusion Models;Temporal Dependencies},
  doi={10.1109/COMPSAC65507.2025.00114},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{10614457,
  author={Wang, Jize and Nakano, Kazuhisa and Chen, Daiyannan and Huang, Zhengyu and Fukusato, Tsukasa and Miyata, Kazunori and Xie, Haoran},
  booktitle={2024 Nicograph International (NicoInt)}, 
  title={A Study on Cognitive Effects of Canvas Size for Augmenting Drawing Skill}, 
  year={2024},
  volume={},
  number={},
  pages={49-53},
  abstract={In recent years, the field of generative artificial intelligence (AI), particularly in the domain of image generation, has exerted a profound influence on society. Despite the capability of AI to produce images of high quality, the augmentation of users' drawing abilities through the provision of drawing support systems emerges as a challenging issue. In this study, we propose that a cognitive factor-specifically, the size of the canvas-may exert a considerable influence on the outcomes of imitative drawing sketches when utilizing reference images. To investigate this hypothesis, a web-based drawing interface was utilized, designed specifically to evaluate the effect of the canvas size's proportionality to the reference image on the fidelity of the drawings produced. The findings from our research lend credence to the hypothesis that a drawing interface, featuring a canvas whose dimensions closely match those of the reference image, markedly improves the precision of user-generated sketches.},
  keywords={Visualization;Image synthesis;Generative AI;Standards;Augmenting Drawing Skill;Reproduction Drawing;Canvas Size;User Interface},
  doi={10.1109/NICOInt62634.2024.00018},
  ISSN={},
  month={June},}@INPROCEEDINGS{10771565,
  author={Sakai, Koga and Uehara, Yuji and Kashihara, Shigeru},
  booktitle={2024 IEEE Global Humanitarian Technology Conference (GHTC)}, 
  title={Implementation and Evaluation of LLM-Based Conversational Systems on a Low-Cost Device}, 
  year={2024},
  volume={},
  number={},
  pages={392-399},
  abstract={The rapid evolution of artificial intelligence (AI) technologies has highlighted the potential of generative AI, particularly large language models (LLMs), to revolutionize various sectors by automating content creation, enhancing personalized education, supporting medical diagnostics, and creating immersive entertainment experiences. However, deploying these advanced models often requires substantial computational resources, posing a challenge in resource-limited environments. This paper explores implementing LLM-based conversational systems on a low-cost device, specifically the Raspberry Pi. We designed and implemented a conversational system utilizing four LLMs, ChatGPT, Bard, Llama, and Rinna, and evaluated their performance in terms of execution time and computational resource usage. Our findings reveal that API-based models (ChatGPT and Bard) are more efficient in processing time and resource consumption, making them suitable for real-time applications. In contrast, locally-run models (Llama and Rinna) provide the advantage of offline operation despite higher computational demands.},
  keywords={Performance evaluation;Generative AI;Computational modeling;Large language models;Entertainment industry;Chatbots;Real-time systems;Data models;Reliability;Medical diagnosis;LLM;Conversational system;Raspberry Pi;Chat GPT;Bard;Llama;Rinna},
  doi={10.1109/GHTC62424.2024.10771565},
  ISSN={2473-5728},
  month={Oct},}@ARTICLE{10962243,
  author={Xue, Hao and Jin, Ming and Pan, Shirui and Salim, Flora},
  journal={IEEE Intelligent Systems}, 
  title={Transforming Urban Dynamics: Harnessing Large Language Models for Smarter Mobility}, 
  year={2025},
  volume={40},
  number={2},
  pages={5-7},
  abstract={Artificial intelligence (AI) has the potential to analyze mobility data and make mobility systems smarter by leveraging diverse data sources such as geospatial data, transportation logs, and real-time sensor data to optimize traffic flow, enhance public transportation systems, and support the development of autonomous vehicles. With the newly emerged generative AI paradigm, exemplified by large language models (LLMs), there is great potential to transform the current AI applications in mobility, transportation, and urban domains. This article provides an overview of recent efforts and aims to shed light on the challenges and future opportunities to facilitate the adaptation of LLMs for smarter mobility systems.},
  keywords={Generative AI;Large language models;Soft sensors;Transforms;Real-time systems;Geospatial analysis;Vehicle dynamics;Intelligent systems;Public transportation;Autonomous vehicles},
  doi={10.1109/MIS.2025.3544937},
  ISSN={1941-1294},
  month={March},}@INPROCEEDINGS{10317780,
  author={Corici, Marius and Buhr, Hauke and Magedanz, Thomas},
  booktitle={2023 2nd International Conference on 6G Networking (6GNet)}, 
  title={Generative Twin for 6G and Beyond 5G Networks: Vision, Challenges and Architecture}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Within the current 6G and beyond 5G research, digital twins have become an indispensable asset for network management. These digital twins offer real-time and dynamic system representations, enabling precise decision-making and comprehensive testing in simulation environments before deployment. Going further, we introduce the novel concept of “Generative Twin”, an approach enabling the automatic generation system configurations and test system situations using Generative Artificial Intelligence (AI) and surpassing the limitations of human administrators. Our work reaches further, presenting a high-level architecture that outlines the additional functional elements and interfaces required to enhance the digital twins for 5G networks. We also demonstrate a mock-up simulation of information generation using the Large Language Model (LLM) ChatGPT and a best-practices digital twin based on the Open5GCore toolkit. Furthermore, we present a practical implementation roadmap, offering guidance for effective integration of generative twins and the technology challenges that necessitate further research, highlighting the very large potential of leveraging generative AI as an integral part of network management.},
  keywords={6G mobile communication;5G mobile communication;Decision making;Chatbots;Real-time systems;Digital twins;Artificial intelligence;Digital Twin;6G;5G;Generative AI;Dynamic Twin},
  doi={10.1109/6GNet58894.2023.10317780},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10859659,
  author={Charanya, P. and B S, Santhos Raj and T, Naveen Kumar and A, Pradheeban},
  booktitle={2024 9th International Conference on Communication and Electronics Systems (ICCES)}, 
  title={Marching Forward: Redefining Human-Machine Interactions in Conversational AI Through Hybrid Intelligence, Blockchain Security, and Autonomous Agents}, 
  year={2024},
  volume={},
  number={},
  pages={792-800},
  abstract={Conversational AI has emerged as an essential instrument in enhancing human-computer interaction, with applications spanning customer service to personal assistants. This paper offers a comprehensive examination of recent developments in conversational AI, including novel techniques in natural language generation (NLG), dialogue systems, and dynamic response optimization. We analyze advancements including transformer-based architectures, reinforcement learning for dialogue generation, and retrieval-augmented models, emphasizing their roles in enhancing the quality and contextual accuracy of conversations. Additionally, we examine the feasibility of blockchain technology as the foundation for decentralized AI, promoting secure, transparent, and distributed frameworks for AI applications, hence improving privacy and control in data exchanges. Hybrid methodologies that combine symbolic reasoning with deep learning are analyzed, highlighting their effectiveness in addressing complex dialogues. These advancements, supported by empirical research and benchmark performance, signify the evolution towards more personalized, intelligent, and decentralized conversational systems.},
  keywords={Human computer interaction;Deep learning;Conversational artificial intelligence;Scalability;Natural language generation;Reinforcement learning;Transformers;Real-time systems;Blockchains;Security;Conversational AI;Chatbot;Generative Model;Human-machine interaction},
  doi={10.1109/ICCES63552.2024.10859659},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10893393,
  author={Amani, Sara and White, Lance L.A. and Balart, Trini and Shryock, Kristi J. and Watson, Karan L.},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={WIP: Faculty Perceptions of ChatGPT - A Survey in Engineering Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This work in progress research paper presents findings from a survey conducted about ChatGPT that was circulated in both spring 2023 and 2024 at an R1 engineering university, with responses from faculty doubling from the first to the second year. As Generative Artificial Intelligence (GAI) and online tools continue to reshape engineering education, it can be difficult for educators to keep up. Emerging technologies like ChatGPT by OpenAI have spurred conversations across academia, and questions about its capabilities and potential uses in the classroom have led to countless ethical discussions about academic integrity and how educators need to adapt to rapid growth in technological advancement. Understanding faculty perceptions of emerging technologies like ChatGPT by OpenAI becomes an essential part of embracing these advancements. This study aims to answer the following research question: What are the current perceptions of faculty towards emerging technologies like ChatGPT by OpenAI in engineering education, as assessed by the Technology Acceptance Model (TAM), and what factors influence their attitudes towards its potential integration into engineering curricula? The TAM assesses survey data by examining responses related to perceived usefulness and ease of use of a technology. It analyzes the extent to which these factors influence attitudes towards, and intention to use the technology, thus predicting its acceptance. Applied to the context of this survey instrument developed by the authors we should glean a good understanding of how faculty may have begun to accept the technology of GAI from the initial survey to the most current survey. Preliminary results shed light on faculty familiarity with ChatGPT and their perspectives on its potential utility for student learning as well as their concerns. By examining faculty attitudes towards integrating ChatGPT into engineering curricula, this research contributes to ongoing discussions on the role of GAI in higher education and provides insight into the extent of which educators currently embrace advancements in a technology-enhanced world.},
  keywords={Surveys;Ethics;Technology acceptance model;Generative AI;Instruments;Oral communication;Chatbots;Engineering education;Springs;Faculty;Faculty Attitudes;Faculty Development;Technology Applications;Survey},
  doi={10.1109/FIE61694.2024.10893393},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10803829,
  author={Suleykin, Alexander and Panfilov, Peter},
  booktitle={2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)}, 
  title={Smart Technical Support System Development Using Knowledge Map-Aided Approach}, 
  year={2024},
  volume={},
  number={},
  pages={455-460},
  abstract={Strong technical support is crucial to success of any brand. It provides answers to customer's questions and solutions to different situations and is an important factor in keeping customers loyal. A new level of technical support can be achieved through combination of traditional service automation offers with the current advancement of Generative Artificial Intelligence (GenAI) embedded into technical support. GenAI in general and the large language models (LLM s) in particular create new opportunities for natural conversational interfaces and the development of ‘smart’ technical support systems. Based on experiences and observations from other intelligent assistance projects, this paper presents new methodological perspectives from academia and best practice from industry on architecting intelligent technical support systems. It discusses the impact of GenAI and LLMs through real cases supporting an ongoing validation. The maj or focus of this paper is on architectural models for intelligent technical support systems, showing the fundamental Knowledge Map mechanism of AIbased customer service, which allows for not only controlling the information used to generate responses and minimizing errors but also automatically processes requests in accounting systems. An example of successful implementation of Knowledge Mapbased approach is given, where the intelligent assistant chat-bot demonstrates high accuracy of answers and in case of uncertainty redirects requests to human operators. The paper presents the key architectural model perspectives for the development of intelligent technical support systems.},
  keywords={Industries;Automation;Uncertainty;Generative AI;Large language models;Customer services;Process control;Control systems;Mathematical models;Energy efficiency;technical support;smart assistant;chat-bot;large language model;knowledge map;knowledge graph},
  doi={10.1109/SUMMA64428.2024.10803829},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10548975,
  author={Smith, Carol J.},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={Trustworthy by Design}, 
  year={2024},
  volume={},
  number={},
  pages={8-11},
  abstract={The relatively recent public release of generative artificial intelligence (AI) systems has ignited a significant leap in awareness of the capabilities of AI. In parallel, there has been a recognition of AI system limitations and the bias inherent in systems created by humans. Expectations are rising for more trustworthy, human-centered, and responsible software connecting humans to powerful systems that augment their abilities. There are decades of practice designing systems that work with, and for humans, that we can build upon to face the new challenges and opportunities brought by dynamic AI systems.},
  keywords={Ethics;Generative AI;Face recognition;Buildings;Software;Planning;Software measurement;Keynote;ethics;trust;emerging technology;AI},
  doi={10.1145/3597503.3649400},
  ISSN={1558-1225},
  month={April},}@ARTICLE{9947047,
  author={Kim, Eunjin and Cho, Hwan-Ho and Kwon, Junmo and Oh, Young-Tack and Ko, Eun Sook and Park, Hyunjin},
  journal={IEEE Journal of Translational Engineering in Health and Medicine}, 
  title={Tumor-Attentive Segmentation-Guided GAN for Synthesizing Breast Contrast-Enhanced MRI Without Contrast Agents}, 
  year={2023},
  volume={11},
  number={},
  pages={32-43},
  abstract={Objective: Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a sensitive imaging technique critical for breast cancer diagnosis. However, the administration of contrast agents poses a potential risk. This can be avoided if contrast-enhanced MRI can be obtained without using contrast agents. Thus, we aimed to generate T1-weighted contrast-enhanced MRI (ceT1) images from pre-contrast T1 weighted MRI (preT1) images in the breast. Methods: We proposed a generative adversarial network to synthesize ceT1 from preT1 breast images that adopted a local discriminator and segmentation task network to focus specifically on the tumor region in addition to the whole breast. The segmentation network performed a related task of segmentation of the tumor region, which allowed important tumor-related information to be enhanced. In addition, edge maps were included to provide explicit shape and structural information. Our approach was evaluated and compared with other methods in the local (n = 306) and external validation (n = 140) cohorts. Four evaluation metrics of normalized mean squared error (NRMSE), Pearson cross-correlation coefficients (CC), peak signal-to-noise ratio (PSNR), and structural similarity index map (SSIM) for the whole breast and tumor region were measured. An ablation study was performed to evaluate the incremental benefits of various components in our approach. Results: Our approach performed the best with an NRMSE 25.65, PSNR 54.80 dB, SSIM 0.91, and CC 0.88 on average, in the local test set. Conclusion: Performance gains were replicated in the validation cohort. Significance: We hope that our method will help patients avoid potentially harmful contrast agents. Clinical and Translational Impact Statement—Contrast agents are necessary to obtain DCE-MRI which is essential in breast cancer diagnosis. However, administration of contrast agents may cause side effects such as nephrogenic systemic fibrosis and risk of toxic residue deposits. Our approach can generate DCE-MRI without contrast agents using a generative deep neural network. Thus, our approach could help patients avoid potentially harmful contrast agents resulting in an improved diagnosis and treatment workflow for breast cancer.},
  keywords={Tumors;Magnetic resonance imaging;Image segmentation;Generators;Breast;Generative adversarial networks;Image edge detection;Breast magnetic resonance imaging;image synthesis;tumor-attentive;segmentationguided;adversarial learning},
  doi={10.1109/JTEHM.2022.3221918},
  ISSN={2168-2372},
  month={},}@ARTICLE{8979339,
  author={Chen, Liangfeng and Zhang, Shutao and Tan, Haibo and Lv, Bo},
  journal={IEEE Access}, 
  title={Progressive RSS Data Augmenter With Conditional Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={26975-26983},
  abstract={Accuracies of most fingerprinting approaches for WiFi-based indoor localization applications are affected by the qualities of fingerprint databases, which are time-consuming and labor-intensive. Recently, many methods have been proposed to reduce the localization accuracy reliance on the qualities of the established fingerprint databases. However, studies on establishing fingerprint databases are relatively rare under the condition of sparse reference points. In this paper, we propose a novel data augmenter based on the adversarial networks to build fingerprint databases with sparse reference points. Additionally, two conditions of these networks are designed to generate data effectively and stably, which are 0-1 sketch and Gaussian sketch. Based on the networks, we design two augmenters with different cyclic training strategies to evaluate the augmenting effects comparatively. Meanwhile, five quantitative evaluation metrics of the augmenters are proposed from two perspectives of the artificial experiences and the data features, and some of them are also used as the gradient penalties for generators. Finally, experiments corresponding to these metrics and localization accuracies demonstrate that the data augmenter with the 0-1 sketch adversarial network is more efficient, effective and stable totally.},
  keywords={Databases;Training;Generators;Measurement;Neural networks;Generative adversarial networks;Gallium nitride;CGAN;sketch;quantitative evaluation metrics;RSS},
  doi={10.1109/ACCESS.2020.2971269},
  ISSN={2169-3536},
  month={},}@ARTICLE{10536641,
  author={Su, Qinliang and Tian, Bowen and Wan, Hai and Yin, Jian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Anomaly Detection Under Contaminated Data With Contamination-Immune Bidirectional GANs}, 
  year={2024},
  volume={36},
  number={11},
  pages={5605-5620},
  abstract={Anomaly detection aims to detect instances that deviate significantly from the majority. Due to the difficulties of collecting a large amount of anomalies in practice, existing methods generally assume the availability of a clean normal dataset and leverage it to detect anomalies by characterizing the normality of normal samples. However, for many application scenarios, collecting a normal dataset that is sufficiently clean is not easy. What is often observed is that a small amount of anomalies are often falsely mixed into the normal dataset, resulting in a contaminated dataset. Obviously, the contamination in the normal dataset could significantly compromise the model's ability to detect anomalies. To alleviate this issue, two contamination-immune bidirectional generative adversarial networks (BiGAN) are developed, which can learn the probability distribution of normal samples from a contaminated dataset under some mild conditions. Rigorous proofs are provided to guarantee the theoretical correctness of the proposed models. Thanks to the removing of negative influences from the contamination samples, the proposed contamination-immune models can thus be applied to detect anomalies accurately for the scenarios with contaminated datasets. Extensive experimental results show that the proposed method outperforms the current state-of-the-art (SOTA) ones significantly under the scenarios with contaminated training datasets.},
  keywords={Training;Contamination;Anomaly detection;Probability distribution;Support vector machines;Prototypes;Medical diagnostic imaging;Anomaly detection;data contamination;generative adversarial networks (GAN);generative models;outlier detection},
  doi={10.1109/TKDE.2024.3404027},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{9464111,
  author={Sheth, Amit and Thirunarayan, Krishnaprasad},
  journal={IT Professional}, 
  title={The Duality of Data and Knowledge Across the Three Waves of AI}, 
  year={2021},
  volume={23},
  number={3},
  pages={35-45},
  abstract={We discuss how, over the last 30–50 years, artificial intelligence (AI) systems that focused only on data have been handicapped and how knowledge has been critical in developing smarter, intelligent, and more effective systems. In fact, the vast progress in AI can be viewed in terms of the three waves of AI as identified by DARPA. During the first wave, handcrafted knowledge has been at the center, while during the second wave, the datadriven approaches supplanted knowledge. Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making and seek to become trusted assistants and companions for humans. We find a wider availability of knowledge created from diverse sources, using manual to automated means both by repurposing as well as by extraction. Using knowledge with statistical learning is becoming increasingly indispensable to help make AI systems more transparent and auditable. We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence.},
  keywords={Navigation;Multimodal sensors;Mental health;Linguistics;Cognition;Planning;Artificial intelligence},
  doi={10.1109/MITP.2021.3070985},
  ISSN={1941-045X},
  month={May},}@INPROCEEDINGS{11091839,
  author={Zhou, Junjie and Tang, Jiao and Zuo, Yingli and Wan, Peng and Zhang, Daoqiang and Shao, Wei},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Robust Multimodal Survival Prediction with Conditional Latent Differentiation Variational AutoEncoder}, 
  year={2025},
  volume={},
  number={},
  pages={10384-10393},
  abstract={The integrative analysis of histopathological images and genomic data has received increasing attention for survival prediction of human cancers. However, the existing studies always hold the assumption that full modalities are available. As a matter of fact, the cost for collecting genomic data is high, which sometimes makes genomic data unavailable in testing samples. A common way of tackling such incompleteness is to generate the genomic representations from the pathology images. Nevertheless, such strategy still faces the following two challenges: (1) The gigapixel whole slide images (WSIs) are huge and thus hard for representation. (2) It is difficult to generate the genomic embeddings with diverse function categories in a unified generative framework. To address the above challenges, we propose a Conditional Latent Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal survival prediction, even with missing genomic data. Specifically, a Variational Information Bottleneck Transformer (VIBTrans) module is proposed to learn compressed pathological representations from the gigapixel WSIs. To generate different functional genomic features, we develop a novel Latent Differentiation Variational AutoEncoder (LD-VAE) to learn the genomic and function-specific posteriors for the genomic embeddings with diverse functions. Finally, we use the product-of-experts technique to integrate the genomic posterior and image posterior for the joint latent distribution estimation in LD-CVAE. We test the effectiveness of our method on five different cancer datasets, and the experimental results demonstrate its superiority in both complete and missing modality scenarios. The code is released †.},
  keywords={Pathology;Image coding;Autoencoders;Genomics;Transformers;Pattern recognition;Bioinformatics;Image reconstruction;Cancer;Testing},
  doi={10.1109/CVPR52734.2025.00971},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9022130,
  author={Jeon, Hyeonseong and Bang, Youngoh and Woo, Simon S.},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)}, 
  title={FakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset}, 
  year={2019},
  volume={},
  number={},
  pages={1285-1287},
  abstract={Detecting realistic fake images and videos is an increasingly important and urgent problem because they can be maliciously used. In this work, we propose FakeTalkerDetect, which is based on siamese networks to detect the recently proposed realistic talking head with few-shot learning. Unlike conventional methods, we propose to use pre-trained models with only a few real images for fine-tuning in siamese networks to effectively detect the fake images in a highly unbalanced data setting. Our FakeTalkerDetect achieves the overall accuracy 98.81% accuracy in detecting fake images generated from the latest neural talking head models. In particular, our preliminary work also demonstrates the effectiveness for the highly unbalanced dataset.},
  keywords={Head;Gallium nitride;Videos;Training;Conferences;Data models;Feature extraction;DeepFake;Talking Heads;Generative Adversarial Networks;siamese networks;few shot learning},
  doi={10.1109/ICCVW.2019.00163},
  ISSN={2473-9944},
  month={Oct},}@INPROCEEDINGS{9326848,
  author={Shen, Tianyu and Gou, Chao and Wang, Jiangong and Wang, Fei-Yue},
  booktitle={2020 Chinese Automation Congress (CAC)}, 
  title={Collaborative Adversarial Networks for Joint Synthesis and Segmentation of X-ray Breast Mass Images}, 
  year={2020},
  volume={},
  number={},
  pages={1743-1747},
  abstract={In this paper, we propose Collaborative Adversarial Networks (CAN) to enable simultaneous forward synthesis and backward segmentation of X-ray breast mass image. The proposed CAN consists of a generator (G), an inverter (I) and a discriminator (D). G aims to reconstruct mass images from corresponding annotated masks, while I is trained for mapping images back to accurate segmentation masks. All the obtained mask-image pairs are fed to D trained in an adversarial learning scheme. Through the collaborative adversarial training using a joint loss function, G synthesizes realistic mass images consistent with provided masks and I effectively segments the tumor regions from the images. Qualitative and quantitative evaluations on publicly available INbreast database demonstrate the effectiveness of our model. Furthermore, different from conventional GANs-based methods that can only perform either image synthesis or segmentation, the proposed model can be generalized to other bidirectional image-to-image translation of multimodal medical data.},
  keywords={Image segmentation;X-ray imaging;Collaboration;Breast;Training;Task analysis;Mammography;generative adversarial network;medical image synthesis;mass segmentation;X-ray breast mass},
  doi={10.1109/CAC51589.2020.9326848},
  ISSN={2688-0938},
  month={Nov},}@ARTICLE{10754638,
  author={Zhao, Zunjin and Shi, Daming},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={LoRaDIP: Low-Rank Adaptation With Deep Image Prior for Generative Low-Light Image Enhancement}, 
  year={2025},
  volume={6},
  number={4},
  pages={909-920},
  abstract={This article presents LoRaDIP, a novel low-light image enhancement (LLIE) model based on deep image priors (DIPs). While DIP-based enhancement models are known for their zero-shot learning, their expensive computational cost remains a challenge. In addressing this issue, our proposed LoRaDIP introduces a low-rank adaptation technique, significantly reducing computational expenses without compromising performance. The contributions of this work are threefold. First, we eliminate the need for estimating initial illumination and reflectance, opting instead to directly estimate the illumination map from the observed image in a generative fashion. The illumination is parameterized by a DIP network. Second, considering the overparameterization of DIP networks, we introduce a low-rank adaptation technique to decrease the number of trainable parameters, thereby reducing computational demands. Third, differing from the existing DIP-based models that rely on a preset fixed number of iterations to halt the optimization process of Retinex decomposition, we propose an automatic stopping criterion based on stable rank, preventing unnecessary iterations. LoRaDIP not only inherits the advantage of requiring only the single input image but also exhibits reduced computational costs while maintaining or even surpassing the performance of state-of-the-art models.},
  keywords={Lighting;Electronics packaging;Computational modeling;Adaptation models;Matrix decomposition;Optimization;Computational efficiency;Artificial intelligence;Inverse problems;Estimation;Deep image prior (DIP);low-light enhancement;low-rank adaptation;overparameterization;Retinex decomposition},
  doi={10.1109/TAI.2024.3499950},
  ISSN={2691-4581},
  month={April},}@ARTICLE{11010159,
  author={Tang, Lu and Zhang, Yaxuan and Hui, Yu and Hou, Nailong and Li, Leida and Zhou, Qi and Tian, Chuangeng and Zhu, Guanyu},
  journal={IEEE Transactions on Medical Imaging}, 
  title={GAN-guided Few-shot Attention Network for Medical Images Fusion Quality Assessment}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Medical image fusion (MIF) plays an important role in precision diagnostics and treatment planning management, and medical image fusion quality assessment (MIFQA) has an aggressive effect in improving MIF performance. However, obtaining medical reference images is difficult, and the significant demand for medical prior knowledge and reference images is an important challenge in the field of MIFQA. To address this issue, this paper proposes a two-stage model for MIFQA. In the first stage, we design a GAN-based Quality-aware Network called QANet. By fusing the radiologist’s mean opinion score (MOS) with the source image, the model is guided to generate one reference images of each quality. Then, in the second stage, the reference images are fed into our proposed class attention siamese network (CASNet) based on class activation mapping (CAM) under few-shot learning to fully explore the information in limited reference images. It can enforce the model to focus on the key lesion area and effectively reduce the dependence of MIFQA on medical fused images. Finally, the quality score of the unlabeled fused image is predicted by calculating the distance with reference image. Experiments on home-made MIFQA dataset shows that our method can achieve results that are ahead of the state-of-the-art methods.},
  keywords={Medical diagnostic imaging;Quality assessment;Image quality;Image fusion;Lesions;Training;Few shot learning;Image synthesis;Generative adversarial networks;Visualization;Image Fusion;Quality Assessment;Generative Adversarial Networks;Few-shot Learning},
  doi={10.1109/TMI.2025.3572511},
  ISSN={1558-254X},
  month={},}@ARTICLE{10962257,
  author={Zhu, Peiyuan and Zhao, Shengjie and Deng, Hao and Han, Fengxia},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Attentive Radiate Graph for Pedestrian Trajectory Prediction in Disconnected Manifolds}, 
  year={2025},
  volume={26},
  number={6},
  pages={7755-7769},
  abstract={Pedestrian trajectory prediction grapples with the demanding feat of modeling complex interactions and learning multimodal distribution to navigate different human-centric environments. Despite superior performance in reducing distance-based metrics, recent works tend to predict out-of-distribution trajectories, as the distribution of forthcoming paths comprises a blend of various manifolds that may be disconnected. These unrealistic trajectories can potentially jeopardize the safety of traffic participants and result in significant damage. To meet these challenges, we propose DMPred, a graph-based generator adversarial network that generates realistic multimodal trajectory predictions by better modeling the social interactions of pedestrians across different scenes in disconnected manifolds. The core of DMPred is an attentive radiate graph sequence constructed by considering the localized influence radiating from pedestrian movements, which is followed by a spatiotemporal extractor that stores and reuses potentially forgotten neighboring pedestrian information to allow for better extraction of complex interactions. Additionally, a collection of generators is utilized for forecasting, which incorporates spectral clustering on trajectories during the prior learning process of multiple generators to help reduce model redundancy and enhance flexibility for various prediction scenarios. Through extensive experiments on multiple real-world and simulation datasets, we demonstrate that DMPred obtains highly competitive results with efficacy in predicting realistic multimodal trajectories.},
  keywords={Trajectory;Pedestrians;Manifolds;Generators;Predictive models;Spatiotemporal phenomena;Measurement;Redundancy;Adaptation models;Generative adversarial networks;Trajectory prediction;generative adversarial networks;multimodality;interaction modeling},
  doi={10.1109/TITS.2025.3555390},
  ISSN={1558-0016},
  month={June},}@INPROCEEDINGS{11088838,
  author={Zhou, Feiyu and Mao, Xudong},
  booktitle={2025 IEEE 2nd International Conference on Deep Learning and Computer Vision (DLCV)}, 
  title={Synthetic Code Mixing for StyleGAN Image Manipulation}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Lately, there has been a proliferation of StyleGAN inversion methods aimed at facilitating facial editing. While many of these methods emphasize reconstruction quality and editability, the disentanglement of the inverted latent code has rarely been studied. In this work, we focus on both disentanglement and editability for facial editing. Our primary insight for achieving disentangled and editable StyleGAN inversion is to enforce a linear subspace for the inverted latent code. To this end, we introduce Synthetic Code Mixing (SCM), a method improves the disentanglement and editability of StyleGAN inversion from two aspects. Firstly, we leverage the synthetic latent code from the original $\mathcal{W}$ space to guide the encoder's output space towards $\mathcal{W}$. This is built upon the fact that the original $\mathcal{W}$ space of StyleGAN is more linear. Secondly, we impose a linear change between two inverted latent codes, ensuring that the interpolated code aligns with the line connecting them. We provide comprehensive empirical evidence to demonstrate that our method achieves superior disentanglement and editability for StyleGAN inversion.},
  keywords={Deep learning;Computer vision;Codes;Computational modeling;Image reconstruction;GAN;StyleGAN Inversion;Image Manipulation;Generative Models;Synthetic Code Mixing},
  doi={10.1109/DLCV65218.2025.11088838},
  ISSN={},
  month={June},}@ARTICLE{8798635,
  author={Lu, Xiaotong and Wang, Heng and Dong, Weisheng and Wu, Fangfang and Zheng, Zhonglong and Shi, Guangming},
  journal={IEEE Access}, 
  title={Learning a Deep Vector Quantization Network for Image Compression}, 
  year={2019},
  volume={7},
  number={},
  pages={118815-118825},
  abstract={Deep convolutional neural network (DCNN) based image codecs, consisting of encoder, quantizer and decoder, have achieved promising image compression results. The major challenge in learning these DCNN models lies in the joint optimization of the encoder, quantizer and decoder, as well as the adaptivity to the input images. In this paper, we proposed a DCNN architecture for image compression, where the encoder, quantizer and decoder are jointly learned. Specifically, a fully convolutional vector quantization network (VQNet) has been proposed to quantize the feature vectors of the image representation, where the representative vectors of the VQNet are jointly optimized with other network parameters through end-to-end training. While most of current DCNN-based methods were only trained once on large-scale datasets, we further perform fine-tuning of the encoder and the codes generated by the VQNet on the input images to further improve the compression performance. Extensive experimental results show that the proposed method achieves state-of-the-art compression results with simple encoder-decoder.},
  keywords={Image coding;Decoding;Transform coding;Vector quantization;Entropy;Convolutional codes;Image compression;deep learning;vector quantization;fine tune},
  doi={10.1109/ACCESS.2019.2934731},
  ISSN={2169-3536},
  month={},}@ARTICLE{9247219,
  author={Yoo, In-Chul and Lee, Keonnyeong and Leem, Seonggyun and Oh, Hyunwoo and Ko, Bonggu and Yook, Dongsuk},
  journal={IEEE Access}, 
  title={Speaker Anonymization for Personal Information Protection Using Voice Conversion Techniques}, 
  year={2020},
  volume={8},
  number={},
  pages={198637-198645},
  abstract={As speech-based user interfaces integrated in the devices such as AI speakers become ubiquitous, a large amount of user voice data is being collected to enhance the accuracy of speech recognition systems. Since such voice data contain personal information that can endanger the privacy of users, the issue of privacy protection in the speech data has garnered increasing attention after the introduction of the General Data Protection Regulation in the EU, which implies that restrictions and safety measures for the use of speech data become essential. This study aims to filter the speaker-related voice biometrics present in speech data such as voice fingerprint without altering the linguistic content to preserve the usefulness of the data while protecting the privacy of users. To achieve this, we propose an algorithm that produces anonymized speeches by adopting many-to-many voice conversion techniques based on variational autoencoders (VAEs) and modifying the speaker identity vectors of the VAE input to anonymize the speech data. We validated the effectiveness of the proposed method by measuring the speaker-related information and the original linguistic information retained in the resultant speech, using an open source speaker recognizer and a deep neural network-based automatic speech recognizer, respectively. Using the proposed method, the speaker identification accuracy of the speech data was reduced to 0.1-9.2%, indicating successful anonymization, while the speech recognition accuracy was maintained as 78.2-81.3%.},
  keywords={Training;Data privacy;Vocoders;Training data;Speech recognition;Linguistics;User interfaces;Data privacy;deep neural networks;speaker anonymization;variational autoencoder;voice conversion},
  doi={10.1109/ACCESS.2020.3035416},
  ISSN={2169-3536},
  month={},}@ARTICLE{10332146,
  author={Xu, Qi and Wei, Yantao and Gao, Jie and Yao, Huang and Liu, Qingtang},
  journal={IEEE Access}, 
  title={ICAPD Framework and simAM-YOLOv8n for Student Cognitive Engagement Detection in Classroom}, 
  year={2023},
  volume={11},
  number={},
  pages={136063-136076},
  abstract={Research has shown that cognitive engagement plays a key role in effective learning, resulting in extensive efforts have been devoted to measuring it. Whereas most of the literature explores manual methods to measure cognitive engagement, the research on automatic detection of cognitive engagement levels in real classrooms is very limited. Automatic detection of cognitive engagement has been a problem for a long time due to the lack of behavior annotation guidance and effective detection algorithms. For the first challenge, a theory of cognitive engagement called Interactive-Constructive-Active-Passive-Disengage (ICAPD) is proposed in this paper. ICAPD links visual behaviors with cognitive engagement in the classroom. According to the ICAPD framework, a cognitive engagement dataset is constructed to train the detection model. To tackle the second challenge, the simAM-based You Only Look Once version 8 Nano (simAM-YOLOv8n) model is designed, simAM-YOLOv8n utilizes the simAM attention module to strengthen feature extraction and detect different levels of cognitive engagement precisely and efficiently. The experimental results on the self-build dataset have demonstrated the effectiveness of the proposed theory framework and detection algorithm, indicating that the proposed methodology could be used to detect real-time cognitive engagement in the classroom scenario. This work has the potential to help teachers to carry out learning analysis and instructional adjustments.},
  keywords={Behavioral sciences;Feature extraction;Computational modeling;Annotations;Visualization;Real-time systems;Cognitive engagement detection;attention mechanism;ICAP;YOLO},
  doi={10.1109/ACCESS.2023.3337435},
  ISSN={2169-3536},
  month={},}@ARTICLE{9815048,
  author={Li, Jiangtao and Su, Yuwei and Cui, Zhaojun and Tian, Jida and Zhou, Huiling},
  journal={IEEE Access}, 
  title={A Method to Establish a Synthetic Image Dataset of Stored-Product Insects for Insect Detection}, 
  year={2022},
  volume={10},
  number={},
  pages={70269-70278},
  abstract={In recent years, deep-learning models have resulted in significant progress in insect recognition. However, training deep neural networks requires a large amount of data, and data collection and labeling are time consuming and labor intensive. This study proposes a method for establishing a synthetic image dataset of stored-product insects to provide well-labelled image data for insect detection tasks. Proxy virtual worlds are leveraged to obtain synthetic data with annotations. A dynamic generation approach was presented to generate synthetic images with diverse insect targets, various backgrounds, and changing lighting conditions by using a camera module in the constructed virtual scene. The coordinates of the bounding boxes and the category labels of insect targets in each synthetic image were obtained by calculating the geometrical relationships between the insect targets and the camera module. A texture translation network was developed to conduct image-to-image translation and launch to enhance the verisimilitude of the synthetic images. A synthetic image dataset was established for three insect species, Cyptolestes ferrugineus (Stephens), Sitophilus oryzae (Linnaeus), and Tribolium castaneum (Herbst).A set of assessments was introduced to evaluate the synthetic image dataset, including the statistical characteristics and experimental verification. The experimental results demonstrated that the use of synthetic data reduces the demand for real data. The proposed method may provide a novel solution for providing training data with correct annotations for insect detection, without tedious image collection and manual labeling.},
  keywords={Insects;Three-dimensional displays;Solid modeling;Cameras;Annotations;Task analysis;Monitoring;Dynamic generation;insect detection;stored-product insect;synthetic image dataset;texture translation;virtual world},
  doi={10.1109/ACCESS.2022.3188282},
  ISSN={2169-3536},
  month={},}@ARTICLE{10566889,
  author={Dharmawan, Dhimas Arief and Nugroho, Anto Satriyo},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science}, 
  title={Toward Deep Face Spoofing: Taxonomy, Recent Advances, and Open Challenges}, 
  year={2025},
  volume={7},
  number={1},
  pages={16-32},
  abstract={Deep neural networks are increasingly employed to create adversarial face images, aiming to deceive face recognition systems. While the majority of studies concentrate on digital attacks, their relevance extends to face spoofing. Notably, they have the capability to generate potential face images of victims when attackers lack knowledge about individuals registered in face recognition systems. Regrettably, recent advances in attacking face recognition systems using deep neural networks, their performance, and their transferability to physical attacks (deep face spoofing) lack systematic exploration. This paper addresses this gap by presenting the first comprehensive survey of current research in this domain. The review initiates with the definition of the deep face spoofing concept and introduces a pioneering taxonomy to systematically consolidate recent advances towards deep face spoofing. The main section of the paper provides in-depth evaluations of the mechanism, performance, and applicability of diverse deep neural network-based attacking algorithms against face recognition systems. Subsequently, the paper outlines current challenges in deep face spoofing, including the absence of evaluations of recent attacks against state-of-the-art face anti-spoofing algorithms and the limited transferability of recent digital attacks to physical attacks. This part also covers open challenges in deep face spoofing detection since it is crucial to note that studying various deep face spoofing algorithms should always be seen as an effort to investigate the vulnerability of face recognition systems against such evolved attacks, and not as an endeavor to gain access for illegal purposes. To enhance accessibility to a broad range of research papers in this area, an accompanying web page (https://github.com/dhimasarief/DFS_DFAS) has been established. This serves as a dynamic repository of studies focusing on deep face spoofing, continuously curated with new findings and contributions.},
  keywords={Face recognition;Artificial neural networks;Reviews;Taxonomy;Feature extraction;Adversarial machine learning;Detection algorithms;Deep neural networks;deep face spoofing;digital attacks;face recognition;physical attacks},
  doi={10.1109/TBIOM.2024.3417372},
  ISSN={2637-6407},
  month={Jan},}@INPROCEEDINGS{10092216,
  author={Kone, Rachida and Toutsop, Otily and Thierry, Ketchiozo Wandji and Kornegay, Kevin and Falaye, Joy},
  booktitle={2022 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)}, 
  title={Adversarial Machine Learning Attacks in Internet of Things Systems}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Researchers are looking into solutions to support the enormous demand for wireless communication, which has been exponentially increasing along with the growth of technology. The sixth generation (6G) Network emerged as the leading solution for satisfying the requirements placed on the telecommunications system. 6G technology mainly depends on various machine learning and artificial intelligence techniques. The performance of these machine learning algorithms is high. Still, their security has been neglected for some reason, which leaves the door open to various vulnerabilities that attackers can exploit to compromise systems. Therefore, it is essential to evaluate the security of machine learning algorithms to prevent them from being spoofed by malicious hackers. Prior research has shown that the decision tree is one of the most popular algorithms used by 80% of researchers for classification problems. In this work, we collect the dataset from a laboratory testbed of over 100 Internet of things (IoT) devices. The devices include smart cameras, smart light bulbs, Alexa, and others. We evaluate classifiers using the original dataset during the experiment and record a 98% accuracy. We then use the label-flipping attack approach to poison our dataset and record the output. As a result, flipping 10%, 20%, 30%, 40%, and 50% of the poison data generated accuracies of 86%, 74%, 64%, 54%, and 50%, respectively.},
  keywords={6G mobile communication;Wireless communication;Machine learning algorithms;Toxicology;Virtual assistants;Smart cameras;Telecommunications;Adversarial Machine Learning;Internet of Every-thing (IoE);Internet of Things (IoT);wireless communication;label-flipping;decision tree},
  doi={10.1109/AIPR57179.2022.10092216},
  ISSN={2332-5615},
  month={Oct},}@ARTICLE{10806716,
  author={Kudryavtsev, Aleksei A. and Simkin, Ivan V. and Dragun, Maksim A. and Alexandrova, Olga P. and Malashin, Ivan P. and Sukhanov, Denis A. and Nelyub, Vladimir A. and Borodulin, Aleksei S. and Yurchenko, Stanislav O. and Tynchenko, Vadim S.},
  journal={IEEE Access}, 
  title={BCSnet: A U-Net-Based Model for Segmentation of Brain Cells in Trypan Blue Images}, 
  year={2024},
  volume={12},
  number={},
  pages={192915-192930},
  abstract={One of the methods for quantifying the survival of brain cells is the direct counting of stained neurons after they are fixed on the plate. This method requires a lot of time and effort associated with the need for manual cell counting and qualification in histology to detect visual differences between alive neuron cells and non-alive ones. The article deals with the problem of semantic segmentation of images of rat brain cells stained with trypan blue for automatic counting of alive brain cells. To solve the problem, a mathematical model has been developed in the form of a convolutional neural network based on the U-Net architecture. As a result of research, the best model has a metric of 0.9189 (Sorensen-Dies coefficient). The trained neural network makes it possible to evaluate the alive neurons three orders of magnitude faster than with manual calculation and with an accuracy that is not inferior to a histologist.},
  keywords={Image segmentation;Deep learning;Microscopy;Imaging;Brain cells;Accuracy;Training;Brain modeling;Convolutional neural networks;Three-dimensional displays;Brain cells;biomedical image segmentation;convolutional neural networks;U-Net},
  doi={10.1109/ACCESS.2024.3519893},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9760667,
  author={Madhurya, Ch and E, Ajith Jubilson and N, Goutham},
  booktitle={2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP)}, 
  title={Facemask Detection using Convolutional Neural Networks (CNN)}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={In last quarter of 2019, Corona Virus Disease (COVID-19), has flared up globally due to which many organizations and institutions are suffering and practically they are going to be closed if the current scenario does not change. COVID-19 is an transmissible disease causes due to Serious Acute Respiratory Syndrome Corona Virus-2 (SARS-CoV-2), which spreads from small liquid particles released from mouth or nose of an infected person. With this virus, anyone can get sick and become seriously ill or even die at any age. The best way to protect our self and others is by wearing a properly fitted facemask, washing hands regularly or frequently rubbing your hands by using an alcohol-based sanitizer and the way is to get vaccinated when ones turn comes. The proposed study uses Convolutional Neural Networks (CNNs) which is a technique of deep learning is used for classification by processing images. This study uses deep learning techniques for identifying if the person is with proper facemask or with no facemask from live video streams. For training the model the dataset is collected kaggle repository which contains 2000 images and attained an accuracy of 98.2% while training the model. The created system is put into action with the help of openCV, python and mobileV2 architecture v2 for recognizing the persons who are wearing and not wearing the facemasks.},
  keywords={COVID-19;Training;Deep learning;Analytical models;Signal processing algorithms;Organizations;Streaming media;COVID-19;Serious Acute Respiratory Syndrome Corona Virus-2;Convolutional Neural Networks;Artificial Neural Network;Face mask Detection;Generative Adversarial Network(GAN)},
  doi={10.1109/AISP53593.2022.9760667},
  ISSN={2640-5768},
  month={Feb},}@INPROCEEDINGS{11004742,
  author={Li, Jinfeng and Li, Haorong},
  booktitle={2025 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Evolution of Application Security based on OWASP Top 10 and CWE/SANS Top 25 with Predictions for the 2025 OWASP Top 10}, 
  year={2025},
  volume={},
  number={},
  pages={1178-1183},
  abstract={The Open Web Application Security Project (OWASP) is widely recognized for its role in identifying and publishing the most critical vulnerabilities in the web application security domain through its OWASP Top 10 list. This study provides the first comprehensive evolutionary analysis of the OWASP Top 10, tracing its development from its inception to the most recent edition. By systematically analyzing historical trends, this research highlights key shifts in vulnerability patterns and emerging security challenges, offering a thorough perspective that expands upon existing literature on web application security. In particular, the study also presents a forward-looking projection for the upcoming 2025 OWASP Top 10, which is anticipated to be released later this year. This prediction is grounded in four primary factors influencing the evolving cybersecurity landscape: the increasing risks associated with artificial intelligence (AI) and machine learning (ML), the growing complexities of API and cloud security, the rising frequency of software supply chain attacks (SSCA), and the expanding impact of regulatory and compliance frameworks. The findings contribute significantly to the understanding of the dynamic threat environment, providing actionable insights for researchers, practitioners, and policymakers to enhance security strategies and mitigate future risks effectively.},
  keywords={Access control;Publishing;Cloud computing security;Supply chains;Machine learning;Market research;Application security;Software;Complexity theory;Application Security;AI-based Security Risks;Broken Access Control;Cloud-native Security;CWE/SANS;Cybersecurity Threats;Injection Attacks;OWASP;OWASP Top 10;Software Supply Chain Security},
  doi={10.1109/ICICT64420.2025.11004742},
  ISSN={2767-7788},
  month={April},}@INPROCEEDINGS{10271689,
  author={Sousa, Alexandre Ladeira de and OKey, Ogobuchi Daniel and Rosa, Renata Lopes and Saadi, Muhammad and Rodriguez, Demostenes Zegarra},
  booktitle={2023 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)}, 
  title={Unified Approach to Video-Based AI Inference Tasks in Augmented Reality Systems Assisted by Mobile Edge Computing}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In the rapidly evolving field of Augmented Reality (AR), delivering real-time, immersive experiences places a significant demand on computational resources, particularly in the context of video-based Artificial Intelligence (AI) inference tasks. Some solutions explorate optimize the tasks using the potential of Mobile Edge Computing (MEC), in which MEC brings the advantages of reduced latency and network congestion, which are critical for AR systems. In this work, we designed a novel algorithm named AR-MEC Optimization for the distribution of AI inference tasks among edge servers while considering several factors including task complexity, server capacity, and network conditions. Our approach not only ensures low-latency responses but also improves overall system efficiency and the user’s Quality of Experience (QoE). Experimental results demonstrate that our unified approach significantly outperforms traditional cloud-based methods in terms of response time and computational efficiency, thereby highlighting the potential of MEC-assisted AR systems for real-world applications.},
  keywords={Multi-access edge computing;Software;Real-time systems;Telecommunications;Servers;Quality of experience;Time factors;Augmented Reality;immersive experiences;edge computing;QoE.},
  doi={10.23919/SoftCOM58365.2023.10271689},
  ISSN={1847-358X},
  month={Sep.},}@INPROCEEDINGS{11155781,
  author={Parsonage, Graham and Dempsey, John},
  booktitle={2025 International Conference on Software, Knowledge, Information Management & Applications (SKIMA)}, 
  title={Automated Generative AI-Driven Forensic Analysis (AGAFA). An Explainable Neuro-Symbolic Approach to Digital Forensics}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Digital Forensics (DF) encompasses the processes of collecting, analysing, and preserving digital evidence crucial for investigations involving cybercrime, security breaches and other criminal cases. The rising number of digital devices requiring investigation, coupled with diminishing confidence in the legal process due to the extensive time needed to process these devices, has prompted the development of a novel framework known as AGAFA. This framework, powered by Explainable AI, aims to meet the growing demand for digital forensic services. The primary objectives of this hybrid neuro-symbolic approach are to enhance the transparency of Artificial Intelligence (AI) in forensic analyses and to protect digital systems. Additionally, it leverages the capabilities of Large Language Models (LLMs) to extract insights from vast datasets in a timely and cost-effective manner. A potential use case for this framework is also illustrated, showcasing its practical application.},
  keywords={Explainable AI;Law enforcement;Digital systems;Large language models;Digital forensics;Personal digital devices;Ontologies;Security;Logic;Computer crime;Digital Forensics;Machine Learning;Large Language Models;Neuro-Symbolic AI;Explainable AI},
  doi={10.1109/SKIMA66621.2025.11155781},
  ISSN={},
  month={June},}@INBOOK{10614259,
  author={Ojanperä, Tero},
  booktitle={AI Revolution: Mastering AI for Personal and Organizational Growth}, 
  title={Epilogue}, 
  year={2024},
  volume={},
  number={},
  pages={143-144},
  abstract={"The AI Revolution" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770042314},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10614259},}@ARTICLE{10614204,
  author={Liang, Chengsi and Du, Hongyang and Sun, Yao and Niyato, Dusit and Kang, Jiawen and Zhao, Dezong and Imran, Muhammad Ali},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Generative AI-Driven Semantic Communication Networks: Architecture, Technologies, and Applications}, 
  year={2025},
  volume={11},
  number={1},
  pages={27-47},
  abstract={Generative artificial intelligence (GAI) has emerged as a rapidly burgeoning field demonstrating significant potential in creating diverse content intelligently and automatically. To support such artificial intelligence-generated content (AIGC) services, future communication systems must fulfill stringent requirements, including high data rates, throughput, and low latency, while efficiently utilizing limited spectrum resources. Semantic communication (SemCom) has been deemed as a revolutionary communication scheme to tackle this challenge by conveying the meaning of messages instead of bit reproduction. GAI algorithms serve as the foundation for enabling intelligent and efficient SemCom systems in terms of model pre-training and fine-tuning, knowledge base construction, and resource allocation. Conversely, SemCom can provide AIGC services with low latency and high reliability due to its ability to perform semantic-aware encoding and compression of data, as well as knowledge- and context-based reasoning. In this survey, we break new ground by investigating the architecture, wireless communication schemes, and network management of GAI-driven SemCom networks. We first introduce a novel architecture for GAI-driven SemCom networks, comprising the data plane, physical infrastructure, and network control plane. In turn, we provide an in-depth analysis of the transceiver design and semantic effectiveness calculation of end-to-end GAI-driven SemCom systems. Subsequently, we present innovative generation level and knowledge management strategies in the proposed networks, including knowledge construction, update, and sharing, ensuring accurate and timely knowledge-based reasoning. Finally, we explore several promising use cases, i.e., autonomous driving, smart cities, and the Metaverse, to provide a comprehensive understanding and future direction of GAI-driven SemCom networks.},
  keywords={Semantics;Surveys;Knowledge engineering;Knowledge management;Computer architecture;Wireless networks;Resource management;Semantic communication;AIGC;generative AI;intelligent wireless networks;knowledge management},
  doi={10.1109/TCCN.2024.3435524},
  ISSN={2332-7731},
  month={Feb},}@ARTICLE{11131292,
  author={Wang, Yixian and Sun, Geng and Sun, Zemin and Wang, Jiacheng and Li, Jiahui and Zhao, Changyuan and Wu, Jing and Liang, Shuang and Yin, Minghao and Wang, Pengfei and Niyato, Dusit and Sun, Sumei and Kim, Dong In},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Toward Realization of Low-Altitude Economy Networks: Core Architecture, Integrated Technologies, and Future Directions}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The rise of the low-altitude economy (LAE) is propelling urban development and emerging industries by integrating advanced technologies to enhance efficiency, safety, and sustainability in low-altitude operations. The widespread adoption of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft plays a crucial role in enabling key applications within LAE, such as urban logistics, emergency rescue, and aerial mobility. However, unlike traditional UAV networks, LAE networks encounter increased airspace management demands due to dense flying nodes and potential interference with ground communication systems. In addition, there are heightened and extended security risks in real-time operations, particularly the vulnerability of low-altitude aircraft to cyberattacks from ground-based threats. To address these, this paper first explores related standards and core architecture that support the development of LAE networks. Subsequently, we highlight the integration of technologies such as communication, sensing, computing, positioning, navigation, surveillance, flight control, and airspace management. We also analyze how generative artificial intelligence (GAI) contributes to this integration by enabling intelligent adaptation, uncertainty modeling, and real-time optimization. This synergy of multi-technology drives the advancement of real-world LAE applications, particularly in improving operational efficiency, optimizing airspace usage, and ensuring safety. Finally, we outline future research directions for LAE networks, such as intelligent and adaptive optimization, security and privacy protection, sustainable energy and power management, quantum-driven coordination, generative governance, and three-dimensional (3D) airspace coverage, which collectively underscore the potential of collaborative technologies to advance LAE networks.},
  keywords={Aircraft;Collaboration;Sensors;Atmospheric modeling;Logistics;Autonomous aerial vehicles;Aircraft propulsion;Artificial intelligence;Aircraft navigation;Solid modeling;Low-altitude economy (LAE) networks;unmanned aerial vehicles (UAVs);electric vertical takeoff and landing (eVTOL);generative artificial intelligence (GAI);multi-technology integration},
  doi={10.1109/TCCN.2025.3601015},
  ISSN={2332-7731},
  month={},}@ARTICLE{9112217,
  author={Zhang, Zhiqiang and Yu, Wenxin and Zhou, Jinjia and Zhang, Xuewen and Jiang, Ning and He, Gang and Yang, Zhuo},
  journal={IEEE Access}, 
  title={Customizable GAN: A Method for Image Synthesis of Human Controllable}, 
  year={2020},
  volume={8},
  number={},
  pages={108004-108017},
  abstract={In the research of computer vision, artificial controllability of image synthesis is a significant and challenging task. At present, there are two available methods. One is to utilize a simple contour to determine the shape of the synthetic object. This method has a promising effect, but it can only control the shape information of the synthetic object, but not the specific content. The other is to employ the text description to synthesize the corresponding image, which effectively controls the specific content of the synthesis, but it cannot do anything for the synthesized shape. In this paper, we propose a highly flexible and human customizable image synthesis model based on simple contour and natural language description, in which the specific content of contour and text description can be determined artificially. The contour determines basic synthetic object shape, and the natural language describes specific object content. Based on these, highly authentic and customizable images can be synthesized. The experiments are executed in the Caltech-UCSD Birds (CUB) and Oxford-102 flower datasets, and the experimental results demonstrate the effectiveness and superiority of our method. The results not only maintain the contour but also conform to the natural language description. Simultaneously, the high-quality image synthesis results, based on artificial hand-drawing contour and text description, are displayed to illustrate the high flexibility and customizability of our model.},
  keywords={Image synthesis;Shape;Generative adversarial networks;Birds;Training;Image color analysis;Computer vision;Artificial neural networks;computer vision;image generation;image processing;text analysis;text processing},
  doi={10.1109/ACCESS.2020.3001070},
  ISSN={2169-3536},
  month={},}@ARTICLE{10844299,
  author={Kim, Euyoung and Lee, Soochahn and Mu Lee, Kyoung},
  journal={IEEE Access}, 
  title={Disentangled Contrastive Learning From Synthetic Matching Pairs for Targeted Chest X-Ray Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={15453-15468},
  abstract={Disentangled generation enables the synthesis of images with explicit control over disentangled attributes. However, traditional generative models often struggle to independently disentangle these attributes while maintaining the ability to generate entirely new, fully randomized, and diverse synthetic data. In this study, we propose a novel framework for disentangled Chest X-ray (CXR) generation that enables explicit control over person-specific and disease-specific attributes. This framework synthesizes CXR images that preserve the same patient identity—either real or randomly generated—while selectively varying the presence or absence of specific diseases. These synthesized matching-paired CXRs not only augment training datasets but also aid in identifying lesions more effectively by comparing attribute-specific differences between paired images. The proposed method leverages contrastive learning to disentangle latent spaces for patient and disease attributes, modeling these spaces with multivariate Gaussians for precise and exclusive attribute sampling. This disentangled representation enables the training of a controllable generative model capable of manipulating disease attributes in CXR images. Experimental results demonstrate the fidelity and diversity of the generated images through qualitative assessments and quantitative comparisons, outperforming state-of-the-art class-conditional generative adversarial networks on two public CXR datasets. Further experiments on clinical efficacy demonstrate that our method improves disease classification and detection tasks by leveraging data augmentation and employing the difference maps generated from paired images as effective attention maps for lesion localization. These findings underscore the potential of our framework to improve medical imaging analysis and facilitate novel clinical applications.},
  keywords={Diseases;Contrastive learning;Biomedical imaging;Aerospace electronics;Generative adversarial networks;Codes;Vectors;Pathology;Image synthesis;Generators;Contrastive learning;controllable generation;generative adversarial network;latent space disentanglement},
  doi={10.1109/ACCESS.2025.3531366},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{5380797,
  author={Wan, Jing and Ren, Han},
  booktitle={2009 International Conference on Asian Language Processing}, 
  title={Information Focus Synthesis Based on Question Answer Chain}, 
  year={2009},
  volume={},
  number={},
  pages={34-38},
  abstract={While speech synthesis technologies have come a long way in recent ten years, there is still room for improvement. This paper describes a technique called based on joint information structure, syntax and prosody method, which demonstrates noticeable improvements to existing speech synthesis system. As an important parameter for prosody proceedings in mandarin, information focus prosodic distribution features are typical for hearing natural, speech understanding and information acquisition. Because of the complex mapping relation between information structure, syntax and prosody, we present an efficient method for retrieval information focus to augment a naturalness speech synthesis. We use question answering chain to extract information focus and discover them how to move. Then, we adopt feature classification and prosody predictive modeling to deal with focus's F0 and time period and obtain them features module. Based on the features module, should significantly increase the accuracy and naturalness of speech synthesis. The rest of this paper is organized as follows. Section 2 summarizes the previously proposed theory for information focus extraction, and derives a new method. Experiments are expressed in Section 3. And experimental results are shown in Section 4. Concluding remarks are presented in the final section.},
  keywords={Speech synthesis;Natural languages;Data mining;Predictive models;Stress;Auditory system;Information retrieval;Expert systems;Artificial intelligence;Production systems;Information focus;Speech synthesis;Question answer chain},
  doi={10.1109/IALP.2009.16},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11104869,
  author={Boubakri, Fatima-Ezzahra and Kadri, Mohammed and Kaghat, Fatima Zahra and Azough, Ahmed and Tairi, Hamid},
  booktitle={2025 International Conference on Smart Learning Courses (SCME)}, 
  title={Exploring 3D Cardiac Anatomy with Text-Based AI Guidance in Virtual Reality}, 
  year={2025},
  volume={},
  number={},
  pages={43-48},
  abstract={This paper presents the design and development of a social virtual reality (VR) classroom focused on cardiac anatomy education for students in grades K-12. The application allows multiple learners to explore a detailed 3D heart model within an immersive and collaborative environment. A crucial part of the system is the integration of a text-based conversational AI interface powered by ChatGPT, which provides immediate, interactive explanations and addresses student inquiries about heart anatomy. The system supports both guided and exploratory learning modes, encourages peer collaboration, and offers personalized support through natural language dialogue. We evaluated the system’s effectiveness through a comprehensive study measuring learning perception (LPQ), VR perception (VRPQ), AI perception (AIPQ), and VR-related symptoms (VRSQ). Potential applications include making high-quality cardiac anatomy education more affordable for K-12 schools with limited resources, offering an adaptable AI-based tutoring system for students to learn at their own pace, and equipping educators with an easy-to-use tool to integrate into their science curriculum with minimal additional training.},
  keywords={Heart;Training;Visualization;Solid modeling;Three-dimensional displays;Natural languages;Collaboration;Virtual reality;Systems support;Anatomy;social virtual reality;collaborative learning;generative AI;anatomy education},
  doi={10.1109/SCME62582.2025.11104869},
  ISSN={},
  month={July},}@INPROCEEDINGS{10426487,
  author={Komatireddy, Sanjay Reddy and Meghana, Karnam and Gude, Venkataramaiah and Ramesh, G.},
  booktitle={2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)}, 
  title={Facial Shape Analysis and Accessory Recommendation: A Human-Centric AI Approach}, 
  year={2023},
  volume={},
  number={},
  pages={182-191},
  abstract={In this digital world, the fusion of Artificial Intelligence (AI) and Human-Computer Interaction (HCI) opens new avenues for enhancing user experiences through personalization. This research centers on the realm of facial shape analysis and its applications in providing tailored accessory recommendations, harnessing AI techniques such as Machine Learning (ML) algorithms to offer customized suggestions. The primary objective is to create an intelligent system adept at recognizing facial shapes and delivering accessory recommendations that harmonize seamlessly with an individual’s unique facial characteristics. To accomplish this, advanced Artificial Intelligence (AI) algorithms specialized in facial shape recognition and analysis have been utilized. These algorithms include K-Means, DBSCAN, and Agglomerative Clustering, each playing a crucial role in the process. The system’s recommendations are founded on a comprehensive analysis that combines both quantitative data and qualitative insights. The findings underscore the effectiveness of AI-driven facial shape analysis in personalizing accessory recommendations. This study highlights AI’s capacity to elevate user experiences and showcases the intersection of technology and fashion within HCI. As a contribution to the evolving field of AI and HCI, this research emphasizes the importance of user centered solutions. Moreover, the results act as a foundational pillar for future research in personalized recommendations and user interfaces, effectively bridging the gap between AI advancements and human-centric design.},
  keywords={Human computer interaction;Machine learning algorithms;Shape;Face recognition;Clustering algorithms;Classification algorithms;Artificial intelligence;Artificial Intelligence;Human-Computer Interaction;Facial Shape Analysis;Virtual Try-on;Accessory Recommendation;User-Centric Solutions},
  doi={10.1109/ICIMIA60377.2023.10426487},
  ISSN={},
  month={Dec},}@ARTICLE{10478897,
  author={Rodriguez-Echeverría, Roberto and Gutiérrez, Juan D. and Conejero, José M. and Prieto, Álvaro E.},
  journal={IEEE Revista Iberoamericana de Tecnologias del Aprendizaje}, 
  title={Analysis of ChatGPT Performance in Computer Engineering Exams}, 
  year={2024},
  volume={19},
  number={},
  pages={71-80},
  abstract={The appearance of ChatGPT at the end of 2022 was a milestone in the field of Generative Artificial Intelligence. However, it also caused a shock in the academic world. For the first time, a simple interface allowed anyone to access a large language model and use it to generate text. These capabilities have a relevant impact on teaching-learning methodologies and assessment methods. This work aims to obtain an objective measure of ChatGPT’s possible performance in solving exams related to computer engineering. For this purpose, it has been tested with actual exams of 15 subjects of the Software Engineering branch of a Spanish university. All the questions of these exams have been extracted and adapted to a text format to obtain an answer. Furthermore, the exams have been rewritten to be corrected by the teaching staff. In light of the results, ChatGPT can achieve relevant performance in these exams; it can pass many questions and problems of different natures in multiple subjects. A detailed study of the results by typology of questions and problems is provided as a fundamental contribution, allowing recommendations to be considered in the design of assessment methods. In addition, an analysis of the impact of the non-deterministic aspect of ChatGPT on the answers to test questions is presented, and the need to use a strategy to reduce this effect for performance analysis is concluded.},
  keywords={Chatbots;Education;Artificial intelligence;Guidelines;Oral communication;Computational modeling;Generative AI;Computer science education;Testing;Performance evaluation;Learning systems;Artificial intelligence;ChatGPT;education;experiment},
  doi={10.1109/RITA.2024.3381842},
  ISSN={1932-8540},
  month={},}@INPROCEEDINGS{10581031,
  author={Desai, Maitry Bimalbhai and Pandey, Shilpa and Kumar, Yogesh},
  booktitle={2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)}, 
  title={Advancements in Artificial Intelligence Techniques for Diagnosis and Detection in 3D Medical Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Medical imaging is an important part of modern healthcare. It helps us understand the complexities of the human body using different tools like X-rays, MRIs, and CT scans. This technology plays a pivotal role in early disease detection, empowering healthcare professionals with crucial insights. In this paper, we explore the exciting progress in 3D medical imaging, a new and improved way to see inside the human body using tools like X-rays, MRIs, and CT scans. We focus on the benefits of segmentation, which is like highlighting specific parts in the images. This helps doctors to be more accurate in identifying and understanding different body structures, making it easier to plan treatments. We also look at different ways of doing segmentation and compare how well they work. Importantly, we discuss how smart computer programs, known as machine learning (ML) and deep learning (DL), are crucial in this field. These clever programs help to automate and refine the segmentation process, making it faster and more efficient for healthcare professionals.},
  keywords={Deep learning;Image segmentation;Three-dimensional displays;Accuracy;Magnetic resonance imaging;Computed tomography;Surgery;Image Fusion;Cognitive Health;Respiratory Disorders;3D Imaging;Image modalities},
  doi={10.1109/ISCS61804.2024.10581031},
  ISSN={},
  month={May},}@INPROCEEDINGS{11101751,
  author={Abdelalim, Mahmoud and Bashar, Mohammad and Nemer, Hazem and Elmasry, Wisam},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Music Generation Using RNN-LSTM with Self-Attention Mechanism}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Music generation using artificial intelligence is a rapidly evolving domain that bridges the gap between creativity and computational intelligence, offering promising applications in entertainment, education, and therapy. In this paper, a Recurrent Neural Network (RNN) model with Long Short-Term Memory (LSTM) networks for music generation was employed, utilizing the Pretty Midi library. Features were extracted from MIDI files in the dataset and fed these notes into a model composed of three LSTM layers. To prevent overfitting, dropout layers were incorporated. The model was trained on a diverse set of MIDI files, allowing it to capture various musical styles and patterns. The trained model demonstrated high accuracy in music generation, producing coherent and stylistically consistent pieces. Experimental results show that the LSTM + Self-Attention model outperformed baseline RNN, LSTM, and BiLSTM models, achieving the lowest validation loss (0.47), confirming its effectiveness for the complex task of music generation.},
  keywords={Deep learning;Accuracy;Computational modeling;Music;Feature extraction;Transformers;Libraries;Artificial intelligence;Long short term memory;Overfitting;Deep Learning;Music Generation;RNN-LSTM;Maestro MIDI;Augmented Grand Piano},
  doi={10.1109/ISAS66241.2025.11101751},
  ISSN={},
  month={June},}@INPROCEEDINGS{10911455,
  author={Li, Lu and Zhu, Fengming and Yang, Zichen},
  booktitle={2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)}, 
  title={A transfer-based adversarial attack method}, 
  year={2024},
  volume={},
  number={},
  pages={1344-1350},
  abstract={Deep convolutional neural network models are vulnerable to adversarial attacks, which can lead to incorrect classification by the model. This adversarial attack has become one of the challenges facing the full-scale deployment of artificial intelligence. Examining adversarial attacks, especially in image recognition, is beneficial. It not only uncovers the weaknesses of deep convolutional neural networks but also enhances our understanding of the security threats associated with deep neural networks, thereby facilitating their enhancement. Through an indepth analysis of the principles behind adversarial attacks, this paper proposes an adversarial attack algorithm aimed at target recognition models.One of the key challenges with current adversarial attack methodologies is their limited ability to produce adversarial examples that can be consistently effective across different models. The generation of such universally effective adversarial samples is a significant hurdle. Nonetheless, achieving this transferability is essential for assessing and bolstering the resilience of AI models against potential security threats. Therefore,this paper introduces a novel adversarial attack algorithm called AdvGAN-G Attack, building on the research surrounding adversarial attack methods that utilize Adversarial Generative Networks (AdvGAN). By utilizing the AdvGAN adversarial training principle and combining the conversion of sample space domain and frequency domain, the gradient editing mechanism is used to optimize the generator gradient parameters in the AdvGAN model, thereby generating adversarial samples with high transferability. Finally, by comparing the performance of AdvGAN-G Attack with other adversarial attack algorithms with transferability, the advantages of AdvGAN-G Attack in terms of sample transferability and algorithm runtime cost were verified, providing an effective solution for improving model robustness and addressing adversarial attack challenges.},
  keywords={Training;Analytical models;Runtime;Target recognition;Robustness;Security;Convolutional neural networks;Artificial intelligence;Robots;Resilience;Deep Neural Networks;Adversarial Attacks;Generating adversarial networks;Transferability},
  doi={10.1109/RICAI64321.2024.10911455},
  ISSN={},
  month={Dec},}@ARTICLE{9288937,
  author={Fang, Zhiwen and Liang, Jiafei and Zhou, Joey Tianyi and Xiao, Yang and Yang, Feng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Anomaly Detection With Bidirectional Consistency in Videos}, 
  year={2022},
  volume={33},
  number={3},
  pages={1079-1092},
  abstract={The core component of most anomaly detectors is a self-supervised model, tasked with modeling patterns included in training samples and detecting unexpected patterns as the anomalies in testing samples. To cope with normal patterns, this model is typically trained with reconstruction constraints. However, the model has the risk of overfitting to training samples and being sensitive to hard normal patterns in the inference phase, which results in irregular responses at normal frames. To address this problem, we formulate anomaly detection as a mutual supervision problem. Due to collaborative training, the complementary information of mutual learning can alleviate the aforementioned problem. Based on this motivation, a SIamese generative network (SIGnet), including two subnetworks with the same architecture, is proposed to simultaneously model the patterns of the forward and backward frames. During training, in addition to traditional constraints on improving the reconstruction performance, a bidirectional consistency loss based on the forward and backward views is designed as the regularization term to improve the generalization ability of the model. Moreover, we introduce a consistency-based evaluation criterion to achieve stable scores at the normal frames, which will benefit detecting anomalies with fluctuant scores in the inference phase. The results on several challenging benchmark data sets demonstrate the effectiveness of our proposed method.},
  keywords={Training;Anomaly detection;Videos;Testing;Biomedical imaging;Trajectory;Semantics;Anomaly detection;consistency loss;mutual learning;SIamese generative network (SIGnet)},
  doi={10.1109/TNNLS.2020.3039899},
  ISSN={2162-2388},
  month={March},}@INPROCEEDINGS{10005441,
  author={Petersen, Patrick and Stage, Hanno and Langner, Jacob and Ries, Lennart and Rigoll, Philipp and Philipp Hohl, Carl and Sax, Eric},
  booktitle={2022 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Towards a Data Engineering Process in Data-Driven Systems Engineering}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Highly Automated Driving (HAD) has become one of the leading trends in the automotive industry. Mandatory tasks like environment perception and scene understanding challenge existing rule-based methods. Thus, data-driven technologies and Artificial Intelligence (AI) have been introduced to automotive software development. Utilizing data in the development process has become essential as these systems are no longer developed with classical systems engineering methods, but rather by deriving requirements from and training the algorithms with recorded real-world data. This entails the introduction of data-driven workflows and data-management as new aspects of Automotive Systems Engineering (ASE). Tasks related to the development of Artificial Intelligence (AI) software differ from their classical engineering and programming counterparts. Thus, engineers require new tools and methods for developing safe and accurate AI-based software and handling data efficiently during ASE. Another important aspect of data-driven development is ensuring data quality throughout the systems engineering process. Hence, this paper aims to take a step towards the introduction of a data engineering process in data-driven automotive systems engineering. Putting a spotlight on developing well-designed data sets as the central element for training and validating AI-based software. Besides determining the quality of data sets, we present steps towards improving data and data set quality.},
  keywords={Training;Industries;Software algorithms;Programming;Data engineering;Market research;Software;automotive systems engineering;data-driven development;ai-engineering},
  doi={10.1109/ISSE54508.2022.10005441},
  ISSN={2687-8828},
  month={Oct},}@ARTICLE{8926406,
  author={Zhu, Dandan and Zhou, Qiangqiang and Han, Tian and Chen, Yongqing},
  journal={IEEE Access}, 
  title={360 Degree Panorama Synthesis From Sequential Views Based on Improved FC-Densenets}, 
  year={2019},
  volume={7},
  number={},
  pages={180503-180511},
  abstract={Inspired by the effectiveness of deep learning model, many panorama saliency prediction models based on deep learning began to emerge and achieved significant performance improvement. However, this kind of model requires a large number of labeled ground-truth data, and the existing panorama datasets are small-scale and difficult to train the deep learning models. To address this problem, we propose a novel panorama generative model for synthesizing realistic and sharp-looking panorama. In particular, our proposed panorama generative model consists of two sub-networks of generator and discriminator. At first, in order to make the synthesized panorama more realistic, we employ the improved Fully-Convolutional Densely Connected Convolutional Networks (FC-DenseNets) as the generator network. Secondly, we design a new correlation layer in the discriminator network, which can calculate the similarity between the generated image and the ground-truth image, and achieve the pixel level accuracy. The experimental results show that our proposed method outperforms other baseline work and has superior generalization ability to synthesize real-world data.},
  keywords={Hidden Markov models;Computational modeling;Data models;Generators;Head;Magnetic heads;Machine learning;Virtual reality;panorama;saliency prediction;generative model;correlation layer},
  doi={10.1109/ACCESS.2019.2958111},
  ISSN={2169-3536},
  month={},}@ARTICLE{10330555,
  author={Zhou, Linjiang and Ma, Chao and Wang, Zepeng and Zhang, Yixuan and Shi, Xiaochuan and Wu, Libing},
  journal={IEEE Internet of Things Journal}, 
  title={Robust Frame-Level Detection for Deepfake Videos With Lightweight Bayesian Inference Weighting}, 
  year={2024},
  volume={11},
  number={7},
  pages={13018-13028},
  abstract={Deepfake threatens the authenticity of the information in artificial intelligence Internet of Things (IoT) systems. Recently, several deepfake detection methods have been proposed in academia and industry for securing the authenticity of visual information in the face of artificial intelligence advances. Frame-level detection methods, a widely employed security method against deepfake, have a small model size and offer real-time responsiveness, despite basing their classification decision only on the information contained within the frame they are evaluating. We propose a new lightweight frame-level detection technique based on Bayesian inference weighting (BIW) to improve the robustness of existing frame-level detection models. Our proposed BIW technique employs the Naive Bayesian algorithm to estimate the reliability of any candidate model’s detection results. Comprehensive experiments were conducted on the attacked data sets by four designed video interference approaches and edge computing platform, showing that BIW enhances the robustness of all the baselines and improves their detection accuracy with a real-time response.},
  keywords={Deepfakes;Computational modeling;Feature extraction;Robustness;Internet of Things;Face recognition;Bayes methods;Bayesian inference;deepfake video;frame-level detection;IoT security},
  doi={10.1109/JIOT.2023.3337128},
  ISSN={2327-4662},
  month={April},}@INPROCEEDINGS{10714580,
  author={Tamime, Reham Al and Salminen, Joni and Jung, Soon-Gyo and Jansen, Bernard},
  booktitle={2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
  title={Evaluating LLM-Generated Topics from Survey Responses: Identifying Challenges in Recruiting Participants through Crowdsourcing}, 
  year={2024},
  volume={},
  number={},
  pages={412-416},
  abstract={The evolution of generative artificial intelligence (AI) technologies, particularly large language models (LLMs), has lead to consequences for the field of Human-Computer Interaction (HCI) in areas such as personalization, predictive analytics, automation, and data analysis. This research aims to evaluate LLM-generated topics derived from survey responses in comparison with topics suggested by humans, particularly participants recruited through a crowdsourcing experiment. We present an evaluation results to compare LLM-generated topics with human-generated topics in terms of Quality, Usefulness, Accuracy, Interestingness, and Completeness. This involves three stages: (1) Design and Generate Topics with an LLM (OpenAI’s GPT-4); (2) Crowdsourcing Human-Generated Topics; and (3) Evaluation of Human-Generated Topics and LLM-Generated Topics. However, a feasibility study with 33 crowdworkers indicated challenges in using participants for LLM evaluation, particularly in inviting humans participants to suggest topics based on open-ended survey answers. We highlight several challenges in recruiting crowdsourcing participants for generating topics from survey responses. We recommend using well-trained human experts rather than crowdsourcing to generate human baselines for LLM evaluation.},
  keywords={Crowdsourcing;Surveys;Human computer interaction;Visualization;Data analysis;Automation;Accuracy;Generative AI;Large language models;Predictive analytics;Crowdsourcing for Human-centric Computing;Challenges in Recruitment;Feasibility Study;LLM Evaluation},
  doi={10.1109/VL/HCC60511.2024.00064},
  ISSN={1943-6106},
  month={Sep.},}@INPROCEEDINGS{9202767,
  author={Soares de Siqueira, Vilson and de Castro Rodrigues, Diego and Nunes Dourado, Colandy and Marcos Borges, Moisés and Gomes Furtado, Rogério and Pereira Delfino, Higor and Stelle, Diego and Melgaço Barbosa, Rommel and Martins da Costa, Ronaldo},
  booktitle={2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Machine Learning Applied to Support Medical Decision in Transthoracic Echocardiogram Exams: A Systematic Review}, 
  year={2020},
  volume={},
  number={},
  pages={400-407},
  abstract={The echocardiogram (ECHO) is an ultrasound of the heart used to diagnose heart diseases (DHC). The analysis and interpretation of ECHO are dependent on the doctor's experience. However, software that uses artificial intelligence to analyze ECHO images or videos is contributing to support the physician's decision. This paper aims to perform a Systematic Literature Review (SLR) on artificial intelligence (AI) techniques applied in the automation of Transthoracic Echocardiogram (TTE) processes, to support medical decisions. The study identified more than 800 articles on the topic in the leading scientific research platforms. To select the most relevant studies, inclusion and exclusion criteria were applied, where 45 articles were selected to compose the detailed study of the SRL. The results obtained with the extraction of information from the papers, identified 3 groups of primary studies, namely: identification of the cardiac vision plan, analysis of cardiac functions and detection of cardiac diseases. SRL identifies that the set of Machine learning (ML) techniques are being widely applied in the tasks of segmentation, detection and classification of images obtained from ECHO. The techniques based on Convolutional Neural Network (CNN), presented the best Accuracy rates. Research shows a strong interest in automating ECHO processes. However, it is still an open research field, with the potential to generate many publications for researchers.},
  keywords={Two dimensional displays;Three-dimensional displays;Machine learning;Task analysis;Medical diagnostic imaging;Echocardiogram, Ecocardiography, Machine Learning, Deep Learning, Systematic Review},
  doi={10.1109/COMPSAC48688.2020.0-215},
  ISSN={0730-3157},
  month={July},}@INPROCEEDINGS{9465908,
  author={Yegemberdiyeva, Gulzhan and Amirgaliyev, Beibut},
  booktitle={2021 IEEE International Conference on Smart Information Systems and Technologies (SIST)}, 
  title={Study Of AI Generated And Real Face Perception}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The face is the most informative sign in a people's recognition. The face contains such features as identity, gender, race, mood, attention and emotions. Face recognition is critical for some services, while recent research shows that people recognition can be very different from person to person.In the past few years, a new type of algorithm Generative Adversarial Network (GAN) has appeared that allows you to generate artificial faces that are identical to real faces. This algorithm is currently widely used in the generation of new faces for marketing campaigns, video processing, increasing the resolution of images, as well as in entertainment applications.This study focuses on the effectiveness of recognizing, distinguishing and memorizing real and fake faces. In the introduction, a literature review is presented. It covers issues of decision-making by people, face recognition, and factors affecting the memorization of faces. The second part contains a description of research methodology - data collection, research design, concerns the work (collection, analysis) with data and procedures. Further hypotheses are put forward and the analysis and conclusion are given.},
  keywords={Brain;Face recognition;Decision making;Generative adversarial networks;Skin;Cultural differences;Artificial intelligence;AI generated faces;face perception;individual differences;statistical package for social sciences},
  doi={10.1109/SIST50301.2021.9465908},
  ISSN={},
  month={April},}@INPROCEEDINGS{10586630,
  author={Gong, Min and Yan, Kun and Peng, Shenglin and Qu, Shuyi},
  booktitle={2024 3rd International Conference on Image Processing and Media Computing (ICIPMC)}, 
  title={Remote Sensing Object Detection Method Based on Three-Dimensional Weight-Guided Selection Kernel}, 
  year={2024},
  volume={},
  number={},
  pages={16-22},
  abstract={In recent years, research on remote sensing object detection has primarily focused on oriented box representation and optimization, while overlooking the importance of studying feature extraction networks, leading to inaccurate identification of some targets. The conventional feature extraction networks employ fixed-size convolutional kernels, which fail to capture different levels of visual features for targets of varying sizes. In this paper, we propose a mechanism called the 3D-Weight Guide Selective Kernel (3D-WGSK) to jointly utilize spatial and channel information of input features to derive three-dimensional weights. These weights are then employed for convolutional kernel selection, enabling adaptive receptive field acquisition for capturing features of targets with different sizes. We devise a network architecture named the 3D-WGSK Net for feature extraction in remote sensing object detection. It possesses the capability of multi-scale information fusion and adaptive kernel adjustment, effectively extracting crucial features of targets with varying sizes. Experimental results on the publicly available DOTA-v1.5 dataset demonstrate the superior detection performance of our method.},
  keywords={Location awareness;Visualization;Target recognition;Object detection;Feature extraction;Reliability engineering;Kernel;Remote sensing object detection;Selection kernel;Three-dimensional weights;Dynamic receptive field adjustment},
  doi={10.1109/ICIPMC62364.2024.10586630},
  ISSN={},
  month={May},}@ARTICLE{10980410,
  author={Zhang, Jiayang and Qu, Hongjia and Jia, Junhao and Li, Yaowei and Jiang, Bo and Chen, Xiaoxuan and Peng, Jinye},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Multiscale Spatial-Spectral CNN-Transformer Network for Hyperspectral Image Super-Resolution}, 
  year={2025},
  volume={18},
  number={},
  pages={12116-12132},
  abstract={Remarkable strides have been made in super-resolution methods based on deep learning for hyperspectral images (HSIs), which are capable of enhancing the spatial resolution. However, these methods predominantly focus on capturing local features using convolutional neural networks (CNNs), neglecting the comprehensive utilization of global spatial-spectral information. To address this limitation, we innovatively propose a multiscale spatial-spectral CNN-transformer network for hyperspectral image super resolution, namely, MSHSR. MSHSR not only applies the local spatial-spectral characteristics but also innovatively facilitates the collaborative exploration and application of spatial details and spectral data globally. Specifically, we first design a multiscale spatial-spectral fusion module, which integrates dilated-convolution parallel branches and a hybrid spectral attention mechanism to address the strong local correlations in HSIs, effectively capturing and fusing multiscale local spatial-spectral information. Furthermore, in order to fully exploit the global contextual consistency in HSIs, we introduce a sparse spectral transformer module. This module processes the previously obtained local spatial-spectral features, thoroughly exploring the elaborate global interrelationship and long-range dependencies among different spectral bands through a coarse-to-fine strategy. Extensive experimental results on three hyperspectral datasets demonstrate the superior performance of our method, outperforming comparison methods both in quantitative metrics and visual performance.},
  keywords={Transformers;Feature extraction;Three-dimensional displays;Hyperspectral imaging;Superresolution;Correlation;Data mining;Convolutional neural networks;Convolution;Spatial resolution;Hyperspectral image (HSI);self-attention;super-resolution (SR);transformer},
  doi={10.1109/JSTARS.2025.3565840},
  ISSN={2151-1535},
  month={},}@INPROCEEDINGS{11103536,
  author={Babić, Snježana},
  booktitle={2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC)}, 
  title={Trust, Digital Competencies, and Self-Efficacy in Using ChatGPT: A Preliminary Study of Croatian Users}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The use of generative artificial intelligence chatbots (Gen AI chatbots) in conversation is becoming increasingly widespread in everyday life. Users are enabled to communicate with a computer in natural language, whereby their skill in writing instructions has become extremely important for obtaining satisfactory outputs with which they can perform a certain activity. Understanding user behavior is extremely important for acceptance and adoption of this AI technology. The research in this paper was conducted among 110 Croatian users with the aim of examining the relationship between trust, self-assessment of digital competences and perceived self-efficacy in using ChatGPT in everyday life. The results of Pearson's correlation analysis indicate a statistically significant positive relationship between all three factors. Furthermore, the respondents stated on average to have the necessary digital competencies to use this tool and to apply it independently in everyday activities; however, they lacked confidence in this technology. In particular, the need to increase users’ awareness of the ethical aspects of using ChatGPT and to develop their digital competence for integrating ChatGPT with other digital tools was identified. The aforementioned indicates the importance of empowering users to use tools in their daily activities. Therefore, the results of this research can be useful for developing strategies and educational programs that encourage effective and responsible use of ChatGPT, as well as for further research in this area.},
  keywords={Ethics;Technological innovation;Generative AI;Natural languages;Europe;Oral communication;Writing;Reliability theory;Chatbots;Zinc;Self-efficacy;trust;self-assessment of digital competencies for use of ChatGPT;ChatGPT;Croatian users},
  doi={10.1109/ZINC65316.2025.11103536},
  ISSN={2995-2689},
  month={May},}@BOOK{10769317,
  author={Suda, Vijaya Kumar},
  booktitle={Data Labeling in Machine Learning with Python: Explore modern ways to prepare labeled data for training and fine-tuning ML and generative AI models},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Take your data preparation, machine learning, and GenAI skills to the next level by learning a range of Python algorithms and tools for data labelingKey FeaturesGenerate labels for regression in scenarios with limited training dataApply generative AI and large language models (LLMs) to explore and label text dataLeverage Python libraries for image, video, and audio data analysis and data labelingPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionData labeling is the invisible hand that guides the power of artificial intelligence and machine learning. In today’s data-driven world, mastering data labeling is not just an advantage, it’s a necessity. Data Labeling in Machine Learning with Python empowers you to unearth value from raw data, create intelligent systems, and influence the course of technological evolution. With this book, you'll discover the art of employing summary statistics, weak supervision, programmatic rules, and heuristics to assign labels to unlabeled training data programmatically. As you progress, you'll be able to enhance your datasets by mastering the intricacies of semi-supervised learning and data augmentation. Venturing further into the data landscape, you'll immerse yourself in the annotation of image, video, and audio data, harnessing the power of Python libraries such as seaborn, matplotlib, cv2, librosa, openai, and langchain. With hands-on guidance and practical examples, you'll gain proficiency in annotating diverse data types effectively. By the end of this book, you’ll have the practical expertise to programmatically label diverse data types and enhance datasets, unlocking the full potential of your data.What you will learnExcel in exploratory data analysis (EDA) for tabular, text, audio, video, and image dataUnderstand how to use Python libraries to apply rules to label raw dataDiscover data augmentation techniques for adding classification labelsLeverage K-means clustering to classify unsupervised dataExplore how hybrid supervised learning is applied to add labels for classificationMaster text data classification with generative AIDetect objects and classify images with OpenCV and YOLOUncover a range of techniques and resources for data annotationWho this book is forThis book is for machine learning engineers, data scientists, and data engineers who want to learn data labeling methods and algorithms for model training. Data enthusiasts and Python developers will be able to use this book to learn data exploration and annotation using Python libraries. Basic Python knowledge is beneficial but not necessary to get started.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804613788},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10769317},}@ARTICLE{10772243,
  author={Li, Yaowei and Yang, Hanmei and Chen, Xiaoxuan and An, Hang and Jiang, Bo},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Implicit Neural Attention for Removing Blur in Remote Sensing Images}, 
  year={2025},
  volume={22},
  number={},
  pages={1-5},
  abstract={Deblurring in remote sensing images is a challenging task due to the long-range imaging capabilities of remote sensing sensors, which often results in image blur. Factors contributing to image blur include atmospheric disturbances during long-range imaging or the orbital motion of remote sensing platforms. The existing methods remove blur in remote sensing images using the traditional attention mechanism, which focuses on a limited number of features. However, they often overlook the features among neighboring positions in blurry areas, and these areas contain more relevant features. Leveraging these features can effectively assist in restoring the complex object textures of remote sensing blurry images. To achieve this, we propose a novel implicit neural attention mechanism for assembling more relevant features implied by surrounding coordinates. Specifically, we use the features and their corresponding coordinates to learn the enhanced feature representation with more relevant features, and this representation can be used to derive the deblurred images. Extensive experiments demonstrate that our proposed method, INA-RSDeblur, outperforms the state-of-the-art deblurring methods in remote sensing blurry images.},
  keywords={Remote sensing;Attention mechanisms;Feature extraction;Decoding;Image reconstruction;Image restoration;Orbits;Noise;Information science;Image sensors;Deblurring;remote sensing},
  doi={10.1109/LGRS.2024.3509894},
  ISSN={1558-0571},
  month={},}@ARTICLE{10440085,
  author={Irshad, Reyazur Rashid and Hussain, Zakir and Hussain, Ihtisham and Hussain, Shahid and Asghar, Ehtisham and Alwayle, Ibrahim M. and Alalayah, Khaled M. and Yousif, Adil and Ali, Awad},
  journal={IEEE Access}, 
  title={Enhancing Cloud-Based Inventory Management: A Hybrid Blockchain Approach With Generative Adversarial Network and Elliptic Curve Diffie Helman Techniques}, 
  year={2024},
  volume={12},
  number={},
  pages={25917-25932},
  abstract={In contemporary business organizations, the pivotal role of automation in control processes is evident through Inventory Management Systems (IMS), which leverage advanced techniques and data analytics algorithms to optimize inventory levels, enhance accuracy, and minimize costs. However, existing security techniques for IMS, including access controls, firewalls, and audits, face challenges in effectively addressing the evolving threat landscape. These limitations, including struggles with dynamic user roles, susceptibility to data manipulation, and challenges in thwarting various cyber threats, necessitate innovative solutions for robust real-time management and security. Consequently, this work proposes a novel hybrid approach that integrates blockchain with RFID data, Generative Adversarial Networks, and Elliptic Curve Diffie-Hellman cryptographic techniques. In the developed hybrid approach, RFID readers are leveraged to collect inventory data, while the Generative Adversarial Network is specifically designed for processing the raw dataset, encompasses data filtering, normalization, and error correction tasks. The utilization of the Elliptic Curve Diffie-Hellman technique is integral for generating both private and shared keys, facilitating secure transmission between the IMS client and cloud-based servers. The blockchain module is engineered to enhance data security and protect shared secret keys, which is achieved through a two-layer mechanism involving encryption via the Advanced Encryption Standard algorithm and SHA-256 hashing function. Additionally, it incorporates the Artificial Algae Algorithm and an Elman Neural Network to ensure robust data access and integrity. To assess the effectiveness of the proposed hybrid approach, it is implemented on a publicly available dataset. The performance assessment involves a comparison with state-of-the-art security methods, considering key metrics such as encryption time, decryption time, key generation time, throughput, latency, and data confidentiality rate. Simulation results conclusively demonstrate that the proposed hybrid approach significantly reduces encryption time, decryption time, key generation time, and latency. Furthermore, it notably improves throughput and data confidentiality rates while aligning with stringent IMS security requirements.},
  keywords={Blockchains;Security;Radiofrequency identification;Inventory management;Encryption;Real-time systems;Elliptic curves;Generative adversarial networks;Hybrid power systems;Blockchain mechanism;elliptic curve Diffie Helman;generative adversarial network;hybrid intelligent RFID;inventory management systems},
  doi={10.1109/ACCESS.2024.3367445},
  ISSN={2169-3536},
  month={},}@ARTICLE{10699337,
  author={Alrowais, Fadwa and Abbas Hassan, Asma and Sulaiman Almukadi, Wafa and Alanazi, Meshari H. and Marzouk, Radwa and Mahmud, Ahmed},
  journal={IEEE Access}, 
  title={Boosting Deep Feature Fusion-Based Detection Model for Fake Faces Generated by Generative Adversarial Networks for Consumer Space Environment}, 
  year={2024},
  volume={12},
  number={},
  pages={147680-147693},
  abstract={In the consumer space, deep fakes refer to highly realistic, AI-generated images, audio, or videos that mimic real people generated by cutting-edge technologies such as Generative Adversarial Networks (GANs). In the digital age, recognizing and detecting deepfakes is a critical problem. The most common solutions for deepfake creation are those based on GANs, which can efficiently manipulate multimedia data or create from scratch. GANs comprise two neural networks, a Generator (G) and a Discriminator (D), that concurrently work during competition. The generator generates artificial data, whereas the discriminator calculates the authenticity of generated and real data. This adversarial procedure causes the generator to generate more realistic content. Identifying deep fakes produced by GANs using deep learning (DL) includes leveraging complex neural networks to detect subtle anomalies and artefacts that GANs accidentally introduce. Convolutional Neural Network (CNN) are very effective for these tasks, as they learn to discern inconsistencies and complex features in image textures, lighting, and facial features frequently missed by human eyes. This CNN model is trained on a massive database of fake and authentic images, allowing them to detect minor defects. This study presents a Deep Feature Fusion-based Fake Face Detection Generated by Generative Adversarial Networks (DF4D-GGAN) technique for Consumer Space Environment. The goal of the DF4D-GGAN technique is to detect the presence of real or deepfake images generated by DL. In the DF4D-GGAN technique, the Gaussian filtering (GF) approach is used for preprocessing the input images. Besides, the feature fusion process uses EfficientNet-b4 and ShuffleNet. Moreover, the hyperparameter selection of the DL models is performed by an improved slime mould algorithm (ISMA). Finally, an extreme learning machine (ELM) classifier has been employed to proficiently recognize real and fake images. To validate the results of the DF4D-GGAN technique, a series of simulations were made on benchmark datasets. The results stated that the DF4D-GGAN technique gains improved results over other models.},
  keywords={Feature extraction;Deepfakes;Generative adversarial networks;Training;Convolutional neural networks;Accuracy;Image recognition;Generators;Forgery;Image processing;Consumer space;deepfake image detection;generative adversarial network;slime mould algorithm;image processing;CNN},
  doi={10.1109/ACCESS.2024.3470128},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10895965,
  author={Al Balawi, Mohammad and Alnabhan, Mohammad and Atoum, Mohammed S},
  booktitle={2024 4th Intelligent Cybersecurity Conference (ICSC)}, 
  title={Generative AI for Advanced Malware Detection}, 
  year={2024},
  volume={},
  number={},
  pages={214-221},
  abstract={Identifying and analyzing malicious software in high accuracy is critical requirement for mitigating security risks. This research paper presents a new AI-enabled malware detection model utilizing characteristics of Portable Executable (PE) files. Generative AI is used to analyze extracted strings, section names, digital signatures, and company information of PE files. In addition, the proposed model will detect patterns associated with known malicious behaviors of anti-debugging, code injection, and keylogging by analyzing functions imported from external libraries and the API calls. Hence, AI techniques will integrate with traditional parsing methods to process data. The results achieved prove that the proposed hybrid model is robust and develops the accuracy detection and depth of malware analysis.},
  keywords={Analytical models;Accuracy;Codes;Generative AI;Companies;Malware;Libraries;Data mining;Computer security;Digital signatures;artificial intelligence;malware analysis;portable Executable files;cybersecurity;machine learning;static analysis;Threat detection;Cyber threat intelligence},
  doi={10.1109/ICSC63108.2024.10895965},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11004747,
  author={CM, Mohana and Arivanantham, Vishnu Priya and Ananthakrishnan, Gayathri},
  booktitle={2025 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Smarter., Faster, Better: Evaluating a Next-Gen Face Recognition System Attendance Tracking System using AI}, 
  year={2025},
  volume={},
  number={},
  pages={1929-1934},
  abstract={Management systems operate in various industries. This paper presents the development and implementation of a facial recognition system specifically designed for real-time attendance monitoring. By integrating widely used libraries such as OpenCV and Dlib with machine learning techniques, this system aims to address the inefficiencies found in traditional attendance systems, providing a faster, more reliable, and secure alternative. The challenges faced in the deployment of such systems, including varying lighting conditions, face angles, and privacy concerns, are explored. The system prioritizes accuracy in real-time settings and scalability for large datasets while complying with stringent data privacy laws such as GDPR. Our findings demonstrate that with optimization, the system can achieve over 95%.},
  keywords={Privacy;Face recognition;Scalability;Lighting;Machine learning;Real-time systems;Libraries;Reliability;Monitoring;Optimization;Facial Recognition;Attendance Monitoring;Real-Time System;OpenCv;Dlib;FaceNet;Biometric Data Security and Machine Learning},
  doi={10.1109/ICICT64420.2025.11004747},
  ISSN={2767-7788},
  month={April},}@INPROCEEDINGS{11090926,
  author={Zhang, Hao-hao and Liu, Jian-wei},
  booktitle={2025 37th Chinese Control and Decision Conference (CCDC)}, 
  title={Memory-Free Incremental Learning on Pretrained Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={5202-5207},
  abstract={Class incremental learning (CIL) aims to tackle the challenge of catastrophic forgetting when processing continuous data streams. Current state-of-the-art methods based on experience replay (ER) face limitations due to data privacy concerns and high memory usage. To mitigate this, traditional generative replay approaches train a classification network alongside a generative model, but convergence can be difficult without sufficient training data. Inspired by recent breakthroughs in large-scale pretrained generative models, we propose a Diffusion-based Generative Replay method. This approach leverages pretrained diffusion models within the generative replay framework, eliminating the need for memory buffers and avoiding the difficulties of training individual generative models from scratch. To address the slow generation speed of diffusion models, we integrate a Latent Consistency Model (LCM) module, which significantly accelerates the training process. Our framework is highly adaptable, enabling seamless integration with various incremental learning techniques. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms traditional generative replay methods and achieves competitive performance with ER methods without relying on external memory buffers.},
  keywords={Training;Data privacy;Adaptation models;Incremental learning;Memory management;Training data;Benchmark testing;Diffusion models;Data models;Faces;Incremental learning;Generative Replay;Diffusion Model},
  doi={10.1109/CCDC65474.2025.11090926},
  ISSN={1948-9447},
  month={May},}@ARTICLE{9915386,
  author={Liu, Juntao and Hou, Weimin and Luo, Xin and Su, Jia and Hou, Yanli and Wang, Zhenzhou},
  journal={IEEE Access}, 
  title={SI-SA GAN: A Generative Adversarial Network Combined With Spatial Information and Self-Attention for Removing Thin Cloud in Optical Remote Sensing Images}, 
  year={2022},
  volume={10},
  number={},
  pages={114318-114330},
  abstract={In agricultural remote sensing monitoring, climate often affects the quality of optical remote sensing image data acquisition. The acquired satellite imagery results usually contain cloud information, leading to a lack of ground data information. Unlike thick clouds, the semi-transparent nature of thin clouds prevents thin clouds from completely obscuring the ground scene. In order to remove thin clouds in the cultivated land and restore the actual ground information as much as possible, we proposed a cloud removal method of spatial information fusion self-attention generative adversarial network (SI-SA GAN) based on multi-directional perceptual attention and self-attention mechanism. The proposed method identifies and focuses on cloud regions using spatial attention, channel attention, and self-attention mechanism, which can enhance image information. The modules of the discriminator utilize residual networks and self-attention non-local neural networks to guide image information output. The generative adversarial network (GAN) is applied to remove clouds and restore the corresponding irregular occlusion area according to the depth characteristics of the input information. A gradient penalty is applied to improve the robustness of the generative network. In this paper, we compared the evaluation indexes of other advanced models. The qualitative and quantitative results of Sentinel-2A and public RICE datasets confirmed that the proposed method could enhance image quality effectively after cloud removal. The model has excellent thin cloud removal performance with small-scale training data.},
  keywords={Generative adversarial networks;Spatial resolution;Agriculture;Remote sensing;Feature extraction;Optical sensors;Remote sensing;thin cloud removal;GAN;self-attention;spatial multi-directional perception},
  doi={10.1109/ACCESS.2022.3213354},
  ISSN={2169-3536},
  month={},}@ARTICLE{11119775,
  author={He, Hanxiang and Huan, Xintao and Luo, Yong and Fan, Rongfei and Xu, Jie and Hu, Han},
  journal={IEEE Transactions on Wireless Communications}, 
  title={SigGen: Signal Generation for Wireless Sensing Based on Disentangled Representation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the thriving artificial intelligence-generated content (AIGC), it is becoming increasingly appealing to exploit generative AI to generate wireless signals for facilitating wireless sensing. However, this is a challenging task, as wireless signals are highly random in general and contain rich physical information. To tackle these challenges, we propose a novel signal disentanglement and generation framework termed SigGen, which is inspired by the Fourier Transform (FT) that converts signals to the frequency domain and accordingly separates objectives by distinct frequency bands. In our proposed framework, we first disentangle the features of objects embedded in the signal and subsequently modify these features to generate the desired signals. Specifically, we devise a neural network based on the vision transformer (ViT) to extract effective features for signal generation. In this neural network, we incorporate both local and global frequency attention modules to adaptively leverage frequency features, and introduce a hybrid patch embedding module to enhance information interaction for the ViT architecture. Furthermore, we propose a novel sequential training method to improve the disentanglement and generation capability of the neural network. Finally, extensive experiments on two benchmark public wireless sensing datasets demonstrate that our framework can effectively decouple wireless signals and generate diverse signals closely resembling real ones, surpassing state-of-the-art methods by 30.83%. A practical case study further demonstrates that our framework can be used as a data augmentation method to improve gesture recognition accuracy by 12.74%.},
  keywords={Wireless communication;Wireless sensor networks;Feature extraction;Sensors;Neural networks;Artificial intelligence;Training;Frequency-domain analysis;Accuracy;Transformers;Wireless sensing;signal generation;feature disentanglement},
  doi={10.1109/TWC.2025.3594547},
  ISSN={1558-2248},
  month={},}@ARTICLE{10547275,
  author={Huang, Xiaowei and Guo, Fabin and Chen, Long},
  journal={IEEE Access}, 
  title={A RES-GANomaly Method for Machine Sound Anomaly Detection}, 
  year={2024},
  volume={12},
  number={},
  pages={80099-80114},
  abstract={Automatic detection of mechanical failure is an essential technology in the fourth industrial revolution, which involves artificial-intelligence-based factory automation. The primary challenge in encoder-based machine sound anomaly detection lies in ensuring high-quality reconstruction of feature maps, as this directly impacts the precise definition of reconstruction error thresholds for normal and abnormal sound feature maps. This study proposes an improved deep convolutional generative adversarial network combined with the GANomaly method to introduce a unique anomaly detection model. This model leverages a residual deep convolutional generative adversarial network with an integrated attention mechanism as the generator and a multi-scale, multi-layer convolutional neural network as the discriminator to address the issue of information loss in reconstruction feature maps with increasing network depth, enhancing model generalization capabilities. The proposed approach introduces custom hyperparameters and tailored loss functions, utilizing Wasserstein distance to measure sample differences and promote model convergence. Researching the DCASE Challenge 2023 Task 2 development dataset, improvements are observed in experimental metrics such as AUC and pAUC, demonstrating the superiority of the model. We also analyze aspects such as feature map quality, parameter settings, and experimental ablation, and compare with other state-of-the-art methods to showcase the contributions of our model.},
  keywords={Anomaly detection;Image reconstruction;Data models;Training;Feature extraction;Generators;Generative adversarial networks;Unsupervised learning;Anomaly detection;attention mechanism;generative adversarial networks;residual connectivity;unsupervised},
  doi={10.1109/ACCESS.2024.3409350},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10887573,
  author={You, Yuhuan and Wu, Xihong and Qu, Tianshu},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={TA-V2A: Textually Assisted Video-to-Audio Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={As artificial intelligence-generated content (AIGC) continues to evolve, video-to-audio (V2A) generation has emerged as a key area with promising applications in multimedia editing, augmented reality, and automated content creation. While Transformer and Diffusion models have advanced audio generation, a significant challenge persists in extracting precise semantic information from videos, as current models often lose sequential context by relying solely on frame-based features. To address this, we present TA-V2A, a method that integrates language, audio, and video features to improve semantic representation in latent space. By incorporating large language models for enhanced video comprehension, our approach leverages text guidance to enrich semantic expression. Our diffusion model-based system utilizes automated text modulation to enhance inference quality and efficiency, providing personalized control through text-guided interfaces. This integration enhances semantic expression while ensuring temporal alignment, leading to more accurate and coherent video-to-audio generation.},
  keywords={Large language models;Semantics;Modulation;Streaming media;Feature extraction;Transformers;Diffusion models;Data mining;Speech processing;Videos;Audio Generation;Multimodality;Diffusion Model;Contrastive Pretraining;AIGC},
  doi={10.1109/ICASSP49660.2025.10887573},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10204889,
  author={Zhang, Renrui and Hu, Xiangfei and Li, Bohao and Huang, Siyuan and Deng, Hanqiu and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners}, 
  year={2023},
  volume={},
  number={},
  pages={15211-15222},
  abstract={Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.},
  keywords={Training;Representation learning;Adaptation models;Visualization;Semantics;Pipelines;Training data;Multi-modal learning},
  doi={10.1109/CVPR52729.2023.01460},
  ISSN={2575-7075},
  month={June},}@ARTICLE{8656492,
  author={Zhu, Wenhan and Zhai, Guangtao and Min, Xiongkuo and Hu, Menghan and Liu, Jing and Guo, Guodong and Yang, Xiaokang},
  journal={IEEE Transactions on Multimedia}, 
  title={Multi-Channel Decomposition in Tandem With Free-Energy Principle for Reduced-Reference Image Quality Assessment}, 
  year={2019},
  volume={21},
  number={9},
  pages={2334-2346},
  abstract={The visual quality of perceptions is highly correlated with the mechanisms of the human brain and visual system. Recently, the free-energy principle, which has been widely researched in brain theory and neuroscience, is introduced to quantize the perception, action, and learning in human brain. In the field of image quality assessment (IQA), on one hand, the free-energy principle can resort to the internal generative model to simulate the visual stimulus of the human beings. On the other hand, abundant psychological and neurobiological studies reveal that different frequency and orientation components of one visual stimulus arouse different neurons in the striate cortex, and the striate cortex processes visual information in the cerebral cortex. Motivated by these two aspects, a novel reduce-reference IQA metric called the multi-channel free-energy based reduced-reference quality metric is proposed in this paper. First, a two-level discrete Haar wavelet transform is used to decompose the input reference and distorted images. Next, to simulate the generative model in the human brain, the sparse representation is leveraged to extract the free-energy-based features in subband images. Finally, the overall quality metric is obtained through the support vector regressor. Extensive experimental comparisons on four benchmark image quality databases (LIVE, CSIQ, TID2008, and TID2013) demonstrate that the proposed method is highly competitive with the representative reduced-reference and classical full-reference models.},
  keywords={Visualization;Wavelet transforms;Image quality;Feature extraction;Measurement;Brain modeling;Image quality assessment (IQA);reduced-reference (RR);multi-channel decomposition;free-energy principle;human visual system},
  doi={10.1109/TMM.2019.2902484},
  ISSN={1941-0077},
  month={Sep.},}@INPROCEEDINGS{10658375,
  author={Wang, Zan and Chen, Yixin and Jia, Baoxiong and Li, Puhao and Zhang, Jinlu and Zhang, Jingze and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Huang, Siyuan},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Move as you Say, Interact as you can: Language-Guided Human Motion Generation with Scene Affordance}, 
  year={2024},
  volume={},
  number={},
  pages={433-444},
  abstract={Despite significant advancements in text-to-motion syn-thesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' in-tensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage frame-work that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting ex-plicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our exten-sive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, in-cluding HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.},
  keywords={Training;Solid modeling;Adaptation models;Three-dimensional displays;Grounding;Affordances;Diffusion models},
  doi={10.1109/CVPR52733.2024.00049},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10302215,
  author={Mazumdar, Hirak and Chakraborty, Chinmay and Sathvik, MSVPJ and Jayakumar, Parvati and Kaushik, Ajeet},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Optimizing Pix2Pix GAN With Attention Mechanisms for AI-Driven Polyp Segmentation in IoMT-Enabled Smart Healthcare}, 
  year={2025},
  volume={29},
  number={6},
  pages={3825-3832},
  abstract={This paper introduces an innovative approach for automated polyp segmentation in colonoscopy images, deploying an enhanced Pix2Pix Generative Adversarial Network (GAN) equipped with an integrated attention mechanism in the discriminator. Addressing prevalent challenges in conventional segmentation methods, such as variable polyp appearances, inconsistent image quality, and limited training data, our model significantly augments the precision and reliability of polyp segmentation. The integration of an attention mechanism enables our model to meticulously focus on the intricate features of polyps, improving segmentation accuracy. A unique training strategy, employing both real and synthetic data, is adopted to ensure the model's robust performance under a variety of conditions. The results, validated through rigorous tests on multiple public colonoscopy datasets, indicate a notable improvement in segmentation performance over existing state-of-the-art methods. Our model's enhanced ability to detect critical details early plays a pivotal role in proactive colorectal cancer detection, a key aspect of smart healthcare systems. This work represents an effective amalgamation of advanced AI techniques and the Internet of Medical Things (IoMT), signifying a noteworthy contribution to the evolution of smart healthcare systems. In conclusion, our attention-enhanced Pix2Pix GAN not only offers efficient and reliable polyp segmentation, but also showcases considerable potential for seamless integration into remote health monitoring systems, underlining the increasing relevance and efficacy of AI in advancing IoMT-enabled healthcare.},
  keywords={Image segmentation;Medical services;Generators;Generative adversarial networks;Colonoscopy;Artificial intelligence;Training;AI in healthcare;attention-enhanced Pix2Pix GAN;early colorectal cancer detection;Internet of Medical Things (IoMT);polyp segmentation},
  doi={10.1109/JBHI.2023.3328962},
  ISSN={2168-2208},
  month={June},}@ARTICLE{10584078,
  author={Piroti, Shirwan and Chawla, Ashima and Zanouda, Tahar},
  journal={IEEE Networking Letters}, 
  title={Mobile Network Configuration Recommendation Using Deep Generative Graph Neural Network}, 
  year={2024},
  volume={6},
  number={3},
  pages={179-182},
  abstract={There are vast number of configurable parameters in a Radio Access Telecom Network. A significant amount of these parameters is configured by Radio Node or cell based on their deployment setting. Traditional methods rely on domain knowledge for individual parameter configuration, often leading to sub-optimal results. To improve this, a framework using a Deep Generative Graph Neural Network (GNN) is proposed. It encodes the network into a graph, extracts subgraphs for each RAN node, and employs a Siamese GNN (S-GNN) to learn embeddings. The framework recommends configuration parameters for a multitude of parameters and detects misconfigurations, handling both network expansion and existing cell reconfiguration. Tested on real-world data, the model surpasses baselines, demonstrating accuracy, generalizability, and robustness against concept drift.},
  keywords={Accuracy;Graph neural networks;Long Term Evolution;Artificial intelligence;Radio access networks;Telecommunication network management;Telecom network configuration management;AI;siamese neural network;graph neural network},
  doi={10.1109/LNET.2024.3422482},
  ISSN={2576-3156},
  month={Sep.},}@INPROCEEDINGS{9201742,
  author={Caidan, Zhao and Zeping, He and Gege, Luo and Caiyun, Chen},
  booktitle={2020 15th International Conference on Computer Science & Education (ICCSE)}, 
  title={An Intelligent Maritime Communication Signal Recognition Algorithm based on ACWGAN}, 
  year={2020},
  volume={},
  number={},
  pages={197-201},
  abstract={In maritime communications systems, there are marine VHF communications systems that meet GMDSS standards in addition to AIS and VDES systems which use very high-frequency signals for information communications such as security rescue. But because its communication does not contain the identity information, the channel is easy to be occupied maliciously, thus interferes in normal maritime communication. This paper studies and analyzes the individual identification technology of the VHF signal based on the rf fingerprint technology of signal. Using the improved adversarial generation network ACWGAN(Auxiliary Classifier Wasserstein Generative Adversarial Networks) to train and identify, we obtain a better classification result. The recognition rate can reach 94% when the SNR is 5 dB for 10 different classes of VHF signal.},
  keywords={Generative adversarial networks;Gallium nitride;Marine vehicles;Artificial intelligence;Computational modeling;Training;Fingerprint recognition;RF fingerprint;ACWGAN algorithm;Marine VHF System},
  doi={10.1109/ICCSE49874.2020.9201742},
  ISSN={2473-9464},
  month={Aug},}@ARTICLE{10910088,
  author={Yin, Juanjuan and Peng, Jinye and Li, Xiaohui and Wang, Jun},
  journal={IEEE MultiMedia}, 
  title={Enhanced Aortic CT Synthesis Based on Multiscale Information Fusion}, 
  year={2025},
  volume={32},
  number={2},
  pages={75-84},
  abstract={For aortic dissection, we aim to synthesize contrast-enhanced computed tomography (CE-CT) from noncontrast-enhanced CT (NCE-CT), avoiding the possible side effects caused by contrast agents in the acquisition of CE-CT. We propose a novel generative adversarial network (GAN) based on multiscale information fusion, named as MIF-GAN. The generator adopts a dual-way Unet architecture, leveraging dense connections to effectively fuse adjacent blocks. Additionally, it incorporates attention-guided feature selection, optimizing the combination of high-resolution and shallow features. Furthermore, a multikernel residual convolution module enhances multiscale contextual features, boosting the overall performance of the model. Experimental results demonstrate that this method can synthesize CE-CT images of high quality and similarity for aortic dissection. With this approach, we hope to reduce the cost for CE-CT scans, thereby reducing patient discomfort and healthcare costs.},
  keywords={Training;Data mining;Artificial intelligence;Frequency-domain analysis;Discrete cosine transforms;Nonhomogeneous media;Image segmentation;Image reconstruction;Generative adversarial networks;Fuses;Heart valves;Computed tomography},
  doi={10.1109/MMUL.2025.3546908},
  ISSN={1941-0166},
  month={April},}@INPROCEEDINGS{10692237,
  author={Galphat, Yugchhaya and Amarnani, Gaurav and Bajaj, Chetaniya and Mulchandani, Kaplesh and Repale, Jayesh},
  booktitle={2024 2nd World Conference on Communication & Computing (WCONF)}, 
  title={ReUnite AI: Harnessing Face Detection and Age Progression for Missing Person Identification}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={ReUnite AI is a system that has been engineered to address the extensive issue of missing individuals, by leveraging advanced face detection and age progression techniques. It offers a comprehensive solution catering to both the general public and law enforcement agencies. Deep learning algorithms are employed in the system to analyze and match facial features from input images against a database of missing individuals. ReUnite AI utilizes Principal Component Analysis (PCA) and Generative Adversarial Networks (GANs) to achieve accuracy and efficiency. System is integrated with age-progression model that generates realistic images depicting how a missing person might appear after a period of time thereby enhancing the identification process. This paper introduces a robust system and provides detailed implementation strategies, showcasing significant potential in locating missing persons.},
  keywords={Deep learning;Accuracy;Law enforcement;Databases;Face recognition;Generative adversarial networks;Artificial intelligence;Facial features;Principal component analysis;Missing persons;Facial recognition;Age progression;Reunite AI project;Deep learning algorithms;Societal challenges},
  doi={10.1109/WCONF61366.2024.10692237},
  ISSN={},
  month={July},}@INPROCEEDINGS{10849004,
  author={Jiang, Justin},
  booktitle={2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC)}, 
  title={Addressing Vulnerabilities in AI-Image Detection: Challenges and Proposed Solutions}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={The rise of advanced AI models like Generative Adversarial Networks (GANs) and diffusion models such as Stable Diffusion has made the creation of highly realistic images accessible, posing risks of misuse in misinformation and manipulation. This study evaluates the effectiveness of convolutional neural networks (CNNs), as well as DenseNet architectures, for detecting AI-generated images. Using variations of the CIFAKE dataset, including images generated by different versions of Stable Diffusion, we analyze the impact of updates and modifications such as Gaussian blurring, prompt text changes, and Low-Rank Adaptation (LoRA) on detection accuracy. The findings highlight vulnerabilities in current detection methods and propose strategies to enhance the robustness and reliability of AI-image detection systems.},
  keywords={Accuracy;Sensitivity;Neural networks;Detectors;Diffusion models;Generative adversarial networks;Robustness;Convolutional neural networks;Artificial intelligence;Fake news;AI-generated image detection;Convolutional neural networks (CNNs);DenseNet architecture;Stable Diffusion;Low-Rank Adaptation(LoRA);Gaussian blurring;Vulnerabilities},
  doi={10.1109/ICAIC63015.2025.10849004},
  ISSN={},
  month={Feb},}@ARTICLE{9384288,
  author={Zhu, Zhen and Huang, Tengteng and Xu, Mengde and Shi, Baoguang and Cheng, Wenqing and Bai, Xiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Progressive and Aligned Pose Attention Transfer for Person Image Generation}, 
  year={2022},
  volume={44},
  number={8},
  pages={4306-4320},
  abstract={This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. We design a progressive generator which comprises a sequence of transfer blocks. Each block performs an intermediate transfer step by modeling the relationship between the condition and the target poses with attention mechanism. Two types of blocks are introduced, namely pose-attentional transfer block (PATB) and aligned pose-attentional transfer block (APATB). Compared with previous works, our model generates more photorealistic person images that retain better appearance consistency and shape consistency compared with input images. We verify the efficacy of the model on the Market-1501 and DeepFashion datasets, using quantitative and qualitative measures. Furthermore, we show that our method can be used for data augmentation for the person re-identification task, alleviating the issue of data insufficiency. Code and pretrained models are available at: https://github.com/tengteng95/Pose-Transfer.git.},
  keywords={Task analysis;Generators;Image synthesis;Manifolds;Gallium nitride;Three-dimensional displays;Shape;Generative adversarial network;person image generation;pose attention;progressive},
  doi={10.1109/TPAMI.2021.3068236},
  ISSN={1939-3539},
  month={Aug},}@ARTICLE{10976665,
  author={Liu, Fan and Yao, Liang and Zhang, Chuanyi and Wu, Ting and Zhang, Xinlei and Jiang, Xiruo and Zhou, Jun},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Boost UAV-Based Object Detection via Scale-Invariant Feature Disentanglement and Adversarial Learning}, 
  year={2025},
  volume={63},
  number={},
  pages={1-13},
  abstract={Detecting objects from uncrewed aerial vehicles (UAVs) are often hindered by a large number of small objects, resulting in low detection accuracy. To address this issue, mainstream approaches typically utilize multistage inferences. Despite their remarkable detecting accuracies, the real-time efficiency is sacrificed, making them less practical to handle real applications. To this end, we propose to improve the single-stage inference accuracy through learning scale-invariant features. Specifically, a scale-invariant feature disentangling (SIFD) module is designed to disentangle scale-related and scale-invariant features. Then, an adversarial feature learning (AFL) scheme is employed to enhance disentanglement. Finally, scale-invariant features are leveraged for robust UAV-based object detection (UAV-OD). Furthermore, we construct a multimodal UAV object detection dataset, State-Air, which incorporates annotated UAV state parameters. We apply our approach to three lightweight detection frameworks on two benchmark datasets. Extensive experiments demonstrate that our approach can effectively improve model accuracy and achieve state-of-the-art (SoTA) performance on three datasets. Our code and dataset are publicly available at: https://github.com/1e12Leon/SIFDAL},
  keywords={Feature extraction;Object detection;Autonomous aerial vehicles;Detectors;Accuracy;Training;Representation learning;Head;Benchmark testing;Artificial intelligence;Adversarial learning;feature disentanglement;scale-invariant feature learning;uncrewed aerial vehicle (UAV)-based object detection},
  doi={10.1109/TGRS.2025.3564261},
  ISSN={1558-0644},
  month={},}@ARTICLE{11007683,
  author={Li, Qingmei and Zhang, Yang and Zheng, Juepeng and Zhang, Yuxiang and Huang, Jianxi and Fu, Haohuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Boosting Universal Domain Adaptation in Remote Sensing With Dual-Classifiers Consistency Discrimination and Cross-Domain Feature Mixup}, 
  year={2025},
  volume={63},
  number={},
  pages={1-15},
  abstract={In the field of remote sensing (RS) image classification, domain adaptation (DA) methods have been extensively utilized to overcome the challenges posed by data discrepancies between source and target domains that arise from varying imaging conditions, sensor differences, or geographical variations. Stemming from the existence of unseen classes in both the source and target domains, universal DA (UniDA) poses the greatest challenge that demands innovative solutions. Existing UniDA methods often overlook intra-domain variations within the target domain and face difficulties in distinguishing between similar known and unknown classes, which significantly hinder cross-domain transfer. To overcome these challenges, we propose a dual-classifier network tailored for cross-domain classification of RS images, named DCmix. DCmix introduces a dual-classifiers network that utilizes both closed-set and open-set classifiers to improve the accuracy of identifying unknown sample classes. To our knowledge, this is the first attempt to introduce dual classifiers into the UniDA RS image classification task. We further enhance the feature generalization capability of the target domain based on sample neighborhood relations, resulting in a more adaptable and robust feature representation. A cross-domain feature mixup (FM) scheme is also designed based on the consistency discrimination of the dual classifiers, achieving smoother decision boundaries and simpler hidden layer representations. Extensive experiments conducted on four hyperspectral image datasets and three RGB datasets prove that the introduced approach attains state-of-the-art (SOTA) performance in RS image classification under the UniDA scenario.},
  keywords={Remote sensing;Image classification;Training;Adaptation models;Accuracy;Data models;Artificial intelligence;Feature extraction;Earth;Vegetation;Consistency discrimination;cross-domain image classification;feature mixup (FM);remote sensing (RS);universal domain adaptation (UniDA)},
  doi={10.1109/TGRS.2025.3571747},
  ISSN={1558-0644},
  month={},}@ARTICLE{10981698,
  author={Su, Cong and Yu, Guoxian and Wang, Jun and Guo, Wei and Zheng, Yongqing and Domeniconi, Carlotta},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Multi-Dimensional Causality Fairness Learning}, 
  year={2025},
  volume={37},
  number={7},
  pages={4166-4178},
  abstract={Causal learning is a recent and widely adopted paradigm to handle algorithmic discrimination. Contemporary causality-based studies on fairness only capture the unfair causal effect of a single-dimensional sensitive attribute (i.e., individual-dimension, like gender) on the decision. They neglect the socially constructed nature of individual attributes, such as macro-dimensional factors. However, social science research shows that discrimination against an individual may be related to disadvantaged treatments, which operate at the macro-dimension (e.g., neighborhood economic level). This multi-dimensional conceptualization is pertinent to matters of fairness, and it is crucial to be fair for individuals across multiple dimensions. The hidden confounder is another bottleneck for addressing fairness concerns based on causal techniques. To tackle these issues, we present an approach, called MultiCFL, which accounts for multi-dimensional sources of discrimination and unifies them via causal tools. To handle hidden confounders, MultiCFL first trains a causal effect variational autoencoder as the causal estimator to learn the causal mechanisms behind observational data. Subsequently, it makes selective use of estimated causal relationships to construct a predictive model with multi-dimensional fairness. Experimental results confirm the effectiveness of MultiCFL, and prove the necessity of considering multiple dimensional properties to mitigate unfairness.},
  keywords={Predictive models;Artificial intelligence;Training;Mathematical models;Estimation;Socioeconomics;Data models;Computational modeling;Cause effect analysis;Software;Causality;fairness;multi-dimension;sensitive attributes;hidden confounders},
  doi={10.1109/TKDE.2025.3566011},
  ISSN={1558-2191},
  month={July},}@INPROCEEDINGS{10647362,
  author={Lee, Keuntek and Park, Jaehyun and Park, Gu Yong and Cho, Nam Ik},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={RFG-HDR: Representative Feature-Guided Transformer For Multi-Exposure High Dynamic Range Imaging}, 
  year={2024},
  volume={},
  number={},
  pages={1521-1527},
  abstract={Multi-exposure fusion is a high dynamic range (HDR) imaging technique that combines multiple low dynamic range (LDR) images of a scene with varying exposure times to produce a single high-quality HDR image. Since each LDR frame is captured with a different exposure time (bias), it is crucial to extract meaningful features from each differently-exposed LDR frame for producing high-quality HDR images. This paper introduces a new contrastive learning method that provides a versatile way of extracting characteristic features from LDR frames by considering the relationship between LDR frames. Additionally, we introduce Representative Feature-Guided Transformer (RFGHDR), a new architecture that utilizes contrastive-learned representations to improve frame alignment and merging. Based on extensive experiments on various datasets, we have found that the RFG-HDR performs better than existing multi-exposure HDR imaging methods in terms of various evaluation metrics. Our work will be released on https://github.com/KeuntekLee/RFG-HDR.},
  keywords={Merging;Imaging;Learning (artificial intelligence);Image representation;Feature extraction;Transformers;Communications technology;HDR imaging;multi-exposure fusion;representation learning},
  doi={10.1109/ICIP51287.2024.10647362},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{11103654,
  author={Slim, Ahmad and El-Howayek, Georges and Abdallah, Chaouki and Allen, Elisha and Orozco, Charla and Slim, Ameer and Zhang, Yiming},
  booktitle={2025 IEEE International Conference on Electro Information Technology (eIT)}, 
  title={AI-Driven Student Data Analytics: Leveraging Generative AI for Institutional Decision-Making}, 
  year={2025},
  volume={},
  number={},
  pages={322-329},
  abstract={Higher education institutions generate vast datasets, yet deriving actionable insights remains difficult due to the technical expertise required. This paper presents an AI-driven student analytics system that leverages large language models (LLMs) to automate SQL generation, statistical analysis, and visualization from natural language inputs. Unlike traditional tools reliant on static dashboards and manual queries, our system enables real-time exploration of student retention, graduation rates, GPA trends, and equity gaps-without requiring SQL knowledge. Em-pirical evaluation across multiple prompting strategies shows high semantic accuracy and execution efficiency. By removing technical barriers, the system supports data-driven decision-making and equity-focused interventions. Future work includes integrating learning management system (LMS) data and enhancing model interpretability.},
  keywords={Structured Query Language;Generative AI;Statistical analysis;Decision making;Education;Semantics;Market research;Real-time systems;Data models;Systems support;Higher education data;Natural language Processing;Generative AI;Learning analytics;Predictive modeling;Student Success},
  doi={10.1109/eIT64391.2025.11103654},
  ISSN={2154-0373},
  month={May},}@ARTICLE{11045330,
  author={Chiheb Ben Nasr, Mohamed and Freitas de Araujo-Filho, Paulo and Kaddoum, Georges and Mourad, Azzam},
  journal={IEEE Internet of Things Journal}, 
  title={A Bayesian Neural Network for Robust Automatic Modulation Classification: Mitigating Adversarial Amplification}, 
  year={2025},
  volume={12},
  number={17},
  pages={35195-35206},
  abstract={In recent years, the rapid advancement of wireless communication technologies, particularly in the development of sixth-generation networks, brought about challenges in spectrum efficiency, security, and reliability. machine learning (ML)-based automatic modulation classification (AMC) plays a critical role in addressing these challenges by enabling efficient signal classification in dynamic environments. However, such systems remain vulnerable to adversarial attacks, which can induce ML-based systems into making mistakes and, by doing so, compromise applications that rely on them. Accordingly, in this study, we propose a robust AMC framework based on Bayesian neural networks (BNNs) to mitigate the impact of adversarial attacks. Our approach uses a regularization term on the weight variance of the BNN to reduce the likelihood of extreme weight values, thereby enhancing model stability in adversarial settings. We also incorporate the Sinh-Arcsinh Gaussian distribution as a flexible prior to control skewness and tail behavior, thus improving the tradeoff between robustness and accuracy. Experimental evaluations against common glass-box adversarial attacks, such as fast gradient sign method, projected gradient descent (PGD), and automatic PGD (Auto-PGD), demonstrate that our proposed model outperforms conventional AMC models, achieving greater resilience in low perturbation-to-noise ratio conditions. Taken together, these findings highlight the potential of Bayesian methods in developing more secure and reliable intelligent wireless communication systems.},
  keywords={Robustness;Perturbation methods;Accuracy;Modulation;Artificial intelligence;Training;Neural networks;Internet of Things;Bayes methods;Interference;Adversarial attack;adversarial robustness;automatic modulation classification (AMC);Bayesian neural network (BNN);Sinh-Arcsinh distribution;glass-box attack},
  doi={10.1109/JIOT.2025.3580286},
  ISSN={2327-4662},
  month={Sep.},}@ARTICLE{11030328,
  author={Yuan, Chengsheng and Ji, Zhaonan and Li, Xinting and Zhou, Zhili and Xia, Zhihua and Wu, Q. M. Jonathan},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={DGADM-GIS: Deterministic Guided Additive Diffusion Model for Generative Image Steganography}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={In recent years, generative steganography has witnessed remarkable progress in the field of covert communication. It leverages techniques such as generative adversarial networks (GANs) or flow-based generative models (GLOW) to generate stego images. However, these approaches often grapple with the dilemma of achieving optimal steganographic capacity while ensuring the accurate extraction of hidden information. Additionally, the models occasionally still generate low-quality images that are highly vulnerable to detection by steganalysis tools. To tackle the aforementioned challenges and enhance the overall performance of generative image steganography, this paper proposes the deterministic guided additive diffusion model for generative image steganography (DGADM-GIS). Initially, we devise a reversible mapping function that is used for deterministic guided by a provided secret message, and then construct a secret latent Gaussian vector. Moreover, the proposed DGADM-GIS framework designs an additive sampling method based on the superposition principle of normal distribution to obtain a Gaussian vector that satisfies independent, random and obeys the standard normal distribution, which is transformed to a stego image in a way of maintaining the distribution by the diffusion model. Furthermore, we conduct error analysis experiments on our proposed scheme and derive methods to enhance the accuracy of secret information extraction. The experimental results show that our proposed steganographic method exhibits robust resistance to steganalysis. When embedding 3 bits of secret information per pixel, it achieves nearly 100% extraction accuracy.},
  keywords={Steganography;Diffusion models;Vectors;Accuracy;Distortion;Additives;Gaussian distribution;Frequency-domain analysis;Data mining;Visualization;generative image steganography;additive sampling;deterministic guided;diffusion model},
  doi={10.1109/TDSC.2025.3578676},
  ISSN={1941-0018},
  month={},}@INBOOK{10948968,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={Part: I Fundamentals of AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-2},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948968},}@INPROCEEDINGS{10370831,
  author={Thombre, Supriya S. and Malik, Latesh and Kumar, Sanjay},
  booktitle={2023 First International Conference on Advances in Electrical, Electronics and Computational Intelligence (ICAEECI)}, 
  title={SynthXNet: Adversarial Learning for Data Augmentation in CoVID Severity Classification from X-Rays}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={For efficient patient care and management, the correct and prompt assessment of CoVID severity using X-Ray images is crucial. Existing approaches for classifying CoVID severity from X-ray images have shortcomings in precision, accuracy, recall, and latency; as a result, more reliable and effective ways must be developed. This study introduces "SynthXNet," a unique approach that uses adversarial learning and data augmentation to improve the performance of CoVID severity classification, in response to such demands. A substantial collection of X-Ray images is acquired, covering cases of CoVID patients with differing degrees of severity, spanning different age groups, genders, and ethnicities, to address the dearth of diverse and representative datasets & samples. The Generative Adversarial Network (GAN) architecture, which consists of a generator and discriminator network, is the central innovation of this work. To create artificial images that closely mimic actual X-Rays of CoVID patients, the GAN is trained on the X-Ray dataset that has been gathered. In an adversarial training procedure, the generator and discriminator teach each other to produce realistic images while the discriminator teaches each other to discern between actual and fake X-Rays. A deep learning model, specifically a Convolutional Neural Network (CNN), used for CoVID severity classification uses the synthetic X-Ray images produced by the GAN as extra training data. In comparison to recently proposed methods, the model’s precision increases by 8.5%, accuracy by 9.4%, and recall by 8.3% when the dataset is supplemented with these synthetic images. The suggested method also speeds up CoVID severity classification by 4.9%, making it more effective for real-time scenarios. In order to overcome the shortcomings of existing approaches, "SynthXNet" presents a potent solution to CoVID severity classification from X-Ray images by utilizing adversarial learning and data augmentation. Its potential to improve patient care and management in the context of CoVID severity evaluation is indicated by the results, which show significant increases in performance indicators.},
  keywords={COVID-19;Training;Training data;Generative adversarial networks;Data augmentation;Adversarial machine learning;Generators;X-Ray Images;CoVID Severity Classification;Data Augmentation;Adversarial Learning;Deep Learning Model;Scenarios},
  doi={10.1109/ICAEECI58247.2023.10370831},
  ISSN={},
  month={Oct},}@INBOOK{10948945,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={Part: II AI Applications}, 
  year={2025},
  volume={},
  number={},
  pages={69-70},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948945},}@INBOOK{10948959,
  author={Banafa, Ahmed},
  booktitle={Artificial Intelligence in Action: Real-World Applications and Innovations}, 
  title={Part: III Challenges and Opportunities}, 
  year={2025},
  volume={},
  number={},
  pages={111-112},
  abstract={This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly "casual AI," and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046190},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948959},}@INPROCEEDINGS{9428229,
  author={Liu, Jingren and Bai, Haoyue and Zhang, Haofeng and Liu, Li},
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Near-Real Feature Generative Network for Generalized Zero-Shot Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Due to the powerful feature synthesis ability, Generative Adversarial Networks (GAN) is well adapted to the Generalized Zero-Shot Learning (GZSL) task and has achieved great success. Most GAN models for GZSL usually employ random noise with normal distribution to synthesize unseen samples. However, the generated samples often have the same normal distribution as the input noise, which is unrealistic in most circumstances. Therefore, in this paper, we consider that the distribution of unseen classes should be follow that of seen classes and propose a near-real feature generative network (NereNet), which utilizes the most semantically similar seen samples to generate the noise for the unseen classes. Specifically, we first calculate the most similar seen classes for the unseen classes, and then train an encoder network to generate the corresponding noise, which is subsequently combined with the unseen classes attributes to generate unseen samples with GAN. Extensive experiments are conducted on four datasets, and the results demonstrate the effectiveness of our proposed method.},
  keywords={Visualization;Adaptation models;Conferences;Gaussian distribution;Generative adversarial networks;Task analysis;Feature Synthesis;Generalized Zero-Shot Learning;Simulate Real Distribution},
  doi={10.1109/ICME51207.2021.9428229},
  ISSN={1945-788X},
  month={July},}@ARTICLE{10702557,
  author={Yang, Yuanfu and Sun, Min},
  journal={IEEE Transactions on Semiconductor Manufacturing}, 
  title={Knowledge Distillation Cross Domain Diffusion Model: A Generative AI Approach for Defect Pattern Segmentation}, 
  year={2024},
  volume={37},
  number={4},
  pages={634-642},
  abstract={In semiconductor manufacturing, defect detection is pivotal for enhancing productivity and yield. This paper introduces a novel weakly supervised method, the Implicit Cross Domain Diffusion Model (ICDDM), designed to tackle defect pattern segmentation challenges in the absence of detailed pixel-wise annotations. ICDDM employs a generative model to estimate the joint distribution of images depicting defect patterns and background circuits, formulating this estimation as a Markov Chain and optimizing it through denoising score matching. Building on this, we propose the Cross Domain Latent Diffusion Model (CDLDM), inspired by the Latent Diffusion Model, which simplifies the diffusion process into a lower-dimensional latent space to boost detection efficiency. Further enhancing our model, we introduce the Knowledge Distillation Cross Domain Diffusion Model (KDCDDM), which utilizes CDLDM as a teacher model and a Generative Adversarial Network (GAN) as a student model. This approach significantly accelerates the diffusion process by reducing the number of necessary denoising iterations while maintaining robust model performance. This suite of techniques offers a comprehensive solution for efficient and effective defect detection in semiconductor production environments.},
  keywords={Noise reduction;Circuits;Diffusion models;Semiconductor device modeling;Defect detection;Training;Generative adversarial networks;Defect detection;diffusion model;weakly supervised learning},
  doi={10.1109/TSM.2024.3472611},
  ISSN={1558-2345},
  month={Nov},}@INPROCEEDINGS{10941273,
  author={Jamalpur, Bhavana and Kiruthiga, S. and Ranjitha, B. and J, Lefty Joyson and K, Jayaram and Soundar, K. Ruba},
  booktitle={2024 International Conference on Distributed Systems, Computer Networks and Cybersecurity (ICDSCNC)}, 
  title={Generative Adversarial Networks (GANs) for Artistic Style Transfer in Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Many in computer vision and computational imaging consider Generative Adversarial Networks (GANs) a big advance. Style transfer converts semantic visual content into creative styles. Recently, GANs for style transfer have gained popularity. Adversarial training of GANs to create realistic counterfeits has yielded encouraging results. High-quality style transfer results have been achieved using this method. Mode collapse is a common problem in classical GANs. This causes unsteady training and poor style transmission. Additionally, the GAN generator only supports one style. Therefore, numerous GANs must be trained to allow users to shift styles. This study examines artistic style transfer in photographs using depth extraction generative adversarial network (DE-GAN). This research aims to improve photo aesthetics by adding famous artworks' distinctive qualities. This study investigates DE-GANs' mechanics and how they modify text while keeping a style's creative qualities. By studying adversarial training, this work hopes to understand how DE-GAN achieves this dual goal. This research analyzes DE-GAN-based style transfer models to evaluate their effectiveness, identify their limits, and recommend areas for improvement. The experiments show that DE-GAN creates realistic and attractive stylized visuals. This study also discusses DE-GAN's practical applications, problems, and future possibilities in artistic style transfer.},
  keywords={Training;Visualization;Computer vision;Transfer learning;Semantics;Neural networks;Generative adversarial networks;Stability analysis;Neural style transfer;Thermal stability;GANs;artistic style transfer;image processing;computer vision;neural networks;deep learning},
  doi={10.1109/ICDSCNC62492.2024.10941273},
  ISSN={},
  month={Sep.},}@INBOOK{10880581,
  author={Singh, Sudhanshu},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Automation of Drug Design and Development}, 
  year={2025},
  volume={},
  number={},
  pages={73-87},
  abstract={Summary <p>Envision an existence where scientists can produce huge measures of reasonable and different clinical information on request. This information could be utilized to speed up drug revelation, work on clinical preliminaries, and customize medication for individual patients. Generative Ill&#x2010;disposed Organizations (GANs), an incredible asset in the field of computerized reasoning, hold colossal potential for making this vision a reality.</p> <p>This part explores the captivating possibilities of using GANs to make produced clinical data. We will discuss the different kinds of data that can be created, for instance, electronic prosperity records, clinical pictures, and genomic data. We will in like manner research the normal purposes of this data in drug exposure, clinical fundamentals, and tweaked prescription.</p> <p>Envision an existence where specialists can produce immense measures of practical clinical information on request, speeding up drug disclosure and improvement. This is the commitment of generative simulated intelligence (GAI), a strong new procedure that is quickly changing the field of clinical exploration. In this part, we investigate the capability of GAI for producing engineered clinical information, including: <ul> <li>New Molecule Generation: GAI can be used to design novel drug candidates with specific properties, such as targeting a particular disease or having fewer side effects.</li> <li>Electronic Health Record (EHR) Synthesis: GAI can be used to create synthetic EHRs that preserve the privacy of real patients while still allowing researchers to study trends and develop new treatments.</li> <li>Medical Imaging Simulation: GAI can be used to generate realistic medical images, such as X&#x2010;rays and MRIs, that can be used to train medical professionals and develop new imaging techniques.</li> </ul> In any case, it is essential to consider the likely difficulties and impediments of utilizing GAI&#x2010;created information. These include: <ul> <li>Bias: GAI models can inherit biases from the data they are trained on, which could lead to inaccurate or unfair results.</li> <li>Explainability: It can be difficult to understand how GAI models generate their outputs, which can make it difficult to trust their results.</li> <li>Regulation: There is currently no clear regulatory framework for the use of GAI&#x2010;generated data in medical research.</li> </ul> </p> <p>By tending to these difficulties, we can guarantee that GAI is utilized capably and actually to propel clinical exploration. Through cooperation between analysts, clinicians, and artificial intelligence engineers, we can open the maximum capacity of GAI for producing manufactured clinical information and upset drug revelation and advancement.</p> <p>Be that as it may, taking into account the difficulties and impediments of utilizing engineered clinical data is significant. One test is guaranteeing that the information is practical and delegate of this present reality. Another test is directing potential inclinations that may be encoded in the data. We will discuss these troubles and propose deals with serious consequences regarding keeping an eye on them.</p> <p>By empowering joint exertion between researchers, clinicians, and reenacted insight trained professionals, we can open the greatest limit of made clinical data to adjust clinical consideration.</p> <p> Innovative Thoughts: <ul> <li>Use GANs to create manufactured clinical information that is custom&#x2010;made to explicit patient populaces or illnesses.</li> <li>Foster new calculations that can produce more perplexing and reasonable clinical information.</li> <li>Utilize engineered clinical information to prepare artificial intelligence models that can be utilized for early infection location and analysis.</li> <li>Make a public storehouse of engineered clinical information that is openly accessible to scientists all over the planet.</li> </ul> </p>},
  keywords={Drugs;Compounds;Robots;Predictive models;Testing;Optimization;Computational modeling;Drug discovery;Biomedical imaging;Accuracy},
  doi={10.1002/9781394280735.ch4},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880581},}@ARTICLE{9229053,
  author={Zhang, Siqi and Wang, Lei and Zhang, Jie and Gu, Ling and Jiang, Xiran and Zhai, Xiaoyue and Sha, Xianzheng and Chang, Shijie},
  journal={IEEE Access}, 
  title={Consecutive Context Perceive Generative Adversarial Networks for Serial Sections Inpainting}, 
  year={2020},
  volume={8},
  number={},
  pages={190417-190430},
  abstract={Image inpainting is a hot topic in computer vision research and has been successfully applied to both traditional and digital mediums, such as oil paintings or old photos mending, image or video denoising and super-resolution. With the introduction of artificial intelligence (AI), a series of algorithms, represented by semantic inpainting, have been developed and better results were achieved. Medical image inpainting, as one of the most demanding applications, needs to meet both the visual effects and strict content correctness. 3D reconstruction of microstructures, based on serial sections, could provide more spatial information and help us understand the physiology or pathophysiology mechanism in histology study, in which extremely high-quality continuous images without any defects are required. In this article, we proposed a novel Consecutive Context Perceive Generative Adversarial Networks (CCPGAN) for serial sections inpainting. Our method can learn semantic information from its neighboring image, and restore the damaged parts of serial sectioning images to maximum extent. Validated with 2 sets of serial sectioning images of mouse kidney, qualitative and quantitative results suggested that our method could robustly restore breakage of any size and location while achieving near realtime performance.},
  keywords={Image restoration;Generative adversarial networks;Feature extraction;Biomedical imaging;Training;Convolution;Semantics;Serial sectioning images;generative adversarial network;consecutive context perceive GAN},
  doi={10.1109/ACCESS.2020.3031973},
  ISSN={2169-3536},
  month={},}@INBOOK{10950966,
  author={Davenport, Thomas H. and Barkin, Ian and Davenport, Chase},
  booktitle={All Hands on Tech: The AI-Powered Citizen Revolution}, 
  title={Citizen Data Science}, 
  year={2025},
  volume={},
  number={},
  pages={81-100},
  abstract={Summary <p>This chapter describes the positive and negative attributions of both citizen data analysis and citizen data science, beginning with citizen data analysis. Citizen data science as practiced in organizations really consists of two categories of activity. One might be more accurately called citizen data analysis because there isn't much science to it. The other category might be called real or true citizen data science, because it involves complex data analysis and the use of sophisticated predictive models. Both types usually involve some degree of data wrangling, although that within true citizen data science is typically more technically challenging. Although cultural change generally requires human intervention, it appears that new technology&#x2014;especially a new technology like generative AI that captures human imaginations&#x2014;can play a role in catalyzing a data&#x2010;oriented culture. Supervised learning models can easily be created by automated machine learning or generative AI data analysis programs and are relatively easy to interpret and deploy.</p>},
  keywords={Data science;Surveys;Companies;Generative AI;Business intelligence;Education;Data models;Hands;Frequency measurement;Analytical models},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394245925},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950966},}@INPROCEEDINGS{9067740,
  author={LAI, ZHILONG and LI, JIANPING and WANG, QINGSONG and GUO, LONGYUAN},
  booktitle={2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing}, 
  title={Application of GAN in Semi-Supervised Learning}, 
  year={2019},
  volume={},
  number={},
  pages={254-257},
  abstract={The third wave of artificial intelligence are inseparable with deep learning. The success of deep learning is mainly attributed to three major factors - big data, big models and big calculations, but they also brought some constraints to the further development and popularization of deep learning. The first challenge is labeling data is expensive, we should design a new learning method that can learn from unlabeled data. Ian Goodfellow who is inventor of GAN, mentioned that the method by which GAN can be used for semi supervised learning is called SSGAN. The experimental results show that the discriminant model trained by this processing method has better effects than other methods in rationally using unlabeled data.},
  keywords={Gallium nitride;Generative adversarial networks;Generators;Training;Semisupervised learning;Supervised learning;Machine learning;Depp Learning;Generative Adversarial;Unsupervised learning},
  doi={10.1109/ICCWAMTIP47768.2019.9067740},
  ISSN={2576-8964},
  month={Dec},}@INBOOK{10850537,
  author={Mariprasath, T. and Cheepati, Kumar Reddy and Rivera, Marco},
  booktitle={Practical Guide to Machine Learning, NLP, and Generative AI: Libraries, Algorithms, and Applications}, 
  title={2 Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={11-42},
  abstract={This is an essential resource for beginners and experienced practitioners in machine learning. This comprehensive guide covers a broad spectrum of machine learning topics, starting with an in-depth exploration of popular machine learning libraries. Readers will gain a thorough understanding of Scikit-learn, TensorFlow, PyTorch, Keras, and other pivotal libraries like XGBoost, LightGBM, and CatBoost, which are integral for efficient model development and deployment. The book delves into various neural network architectures, providing readers with a solid foundation in understanding and applying these models. Beginning with the basics of the Perceptron and its application in digit classification, it progresses to more complex structures such as multilayer perceptrons for financial forecasting, radial basis function networks for air quality prediction, and convolutional neural networks (CNNs) for image classification. Additionally, the book covers recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) and gated recurrent units (GRUs), which are crucial for time-series analysis and sequential data applications. Supervised machine learning algorithms are meticulously explained, with practical examples to illustrate their application. The book covers logistic regression and its use in predicting sports outcomes, decision trees for plant classification, random forests for traffic prediction, and support vector machines for house price prediction. Gradient boosting machines and their applications in genomics, AdaBoost for bioinformatics data classification, and extreme gradient boosting (XGBoost) for churn prediction are also discussed, providing readers with a robust toolkit for various predictive tasks. Unsupervised learning algorithms are another significant focus of the book, introducing readers to techniques for uncovering hidden patterns in data. Hierarchical clustering for gene expression data analysis, principal component analysis (PCA) for climate predictions, and singular value decomposition (SVD) for signal denoising are thoroughly explained. The book also explores applications like robot navigation and network security, demonstrating the versatility of these techniques. Natural language processing (NLP) is comprehensively covered, highlighting its fundamental concepts and various applications. The book discusses the overview of NLP, its fundamental concepts, and its diverse applications such as chatbots, virtual assistants, clinical NLP applications, and social media analytics. Detailed sections on text pre-processing, syntactic analysis, machine translation, text classification, named entity recognition, and sentiment analysis equip readers with the knowledge to build sophisticated NLP models. The final chapters of the book explore generative AI, including generative adversarial networks (GANs) for image generation, variational autoencoders for vibrational encoder training, and autoregressive models for time series forecasting. It also delves into Markov chain models for text generation, Boltzmann machines for pattern recognition, and deep belief networks for financial forecasting. Special attention is given to the application of recurrent neural networks (RNNs) for generation tasks, such as wind power plant predictions and battery range prediction, showcasing the practical implementations of generative AI in various fields.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770046527},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10850537},}@INPROCEEDINGS{10422572,
  author={Wang, Xuhong and Wang, Ding and Chen, Liang and Wang, Fei-Yue and Lin, Yilun},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Building Transportation Foundation Model via Generative Graph Transformer}, 
  year={2023},
  volume={},
  number={},
  pages={6042-6047},
  abstract={In recent years, researchers have made notable advancements in various disciplines using large-scale foundation models. However, foundation models in the transportation system have not received adequate attention. To address this gap, we propose the Generative Graph Transformer (GGT), a transportation foundation model (TFM) that leverages graph structure and dynamic graph generation algorithms. The primary objective of our TFM is to capture participant behavior and interaction in the transportation system, at various scales, and establish a large-scale neural network to comprehend the entire system. The GGT-based TFM can overcom challenges of structural complexity and model accuracy in conventional traffic models. This approach holds promise for addressing complex traffic issues by utilizing up-to-date real traffic data. To demonstrate the capabilities of GGT, a simulation experiment was conducted.},
  keywords={Heuristic algorithms;Transportation;Traffic control;Transformers;Data models;Vehicle dynamics;Testing},
  doi={10.1109/ITSC57777.2023.10422572},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{5596680,
  author={Xue, Ya and Hu, Xiao and Yan, Weizhong and Qiu, Hai},
  booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Learning mixtures of experts with a hybrid generative-discriminative algorithm}, 
  year={2010},
  volume={},
  number={},
  pages={1-8},
  abstract={The hierarchical mixtures of experts (HME) model is a flexible model that stochastically partitions the feature space into sub-regions, in which simple surfaces can be fitted to data. However, there are issues with model selection during the learning of the HME model using Expectation-Maximization (EM) inference. In addition, the EM algorithm suffers from the well-known problem of local minima. In this paper, we present a hybrid generative-discriminative approach that inherits the flexibility of the HME while decomposing the learning process into a few simple steps. The proposed algorithm solves the model-selection problem and empirical experiments on public benchmark datasets show its advantages in classification accuracy and efficiency.},
  keywords={Hybrid power systems},
  doi={10.1109/IJCNN.2010.5596680},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{11094645,
  author={Fu, Jiahui and Gong, Yue and Wang, Luting and Zhang, Shifeng and Zhou, Xu and Liu, Si},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Generative Map Priors for Collaborative BEV Semantic Segmentation}, 
  year={2025},
  volume={},
  number={},
  pages={11919-11928},
  abstract={Collaborative perception aims to address the constraint of single-agent perception by exchanging information among multiple agents. Previous works primarily focus on collaborative object detection, exploring compressed transmission and fusion prediction tailored to sparse object features. However, these strategies are not well-suited for dense features in collaborative BEV semantic segmentation. Therefore, we propose CoGMP, a novel Collaborative framework that leverages Generative Map Priors to achieve efficient compression and robust fusion. CoGMP introduces two key innovations: Element Format Feature Compression (EFFC) and Structure Guided Feature Fusion (SGFF). Specifically, EFFC leverages map element priors from codebook to encode BEV features as discrete element indices for transmitted information compression. Meanwhile, SGFF utilizes a diffusion model with structural priors to coherently integrate multi-agent features, thereby achieving consistent fusion predictions. Evaluations on the OPV2V dataset show that CoGMP achieves a 6.89/7.64 Road/Lane IoU improvement and a 32-fold reduction in communication volume.},
  keywords={Technological innovation;Computer vision;Semantic segmentation;Collaboration;Object detection;Diffusion models;Feature extraction;Pattern recognition;collaborative perception;bev semantic segmentation},
  doi={10.1109/CVPR52734.2025.01113},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10522752,
  author={Sun, Guolong and Xiong, Zhitong and Yuan, Yuan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Detail-Preserving and Diverse Image Translation for Adverse Visual Object Detection}, 
  year={2024},
  volume={34},
  number={10},
  pages={9139-9152},
  abstract={The effectiveness of object detection is significantly hampered in challenging nighttime or rainy scenarios. This is due to the severe domain shifts between daytime and adverse-visual images. Previous methods have demonstrated that using image-to-image translation methods for data augmentation can effectively address domain shifts, but they may still fail in preserving image objects when faced with extreme adverse images like rainy nights. In addition, achieving diversity in the generated results remains challenging. To this end, we propose a Progressive Adverse Image Translation (PAIT) framework that tackles domain shifts by generating diverse and detail-preserving images. The main contributions of this paper are as follows. 1) We propose a novel PAIT framework, which incorporates an iterative mapping module and a slicing layer. This framework enables the progressive generation of increasingly challenging images in a fine-to-coarse manner. 2) To preserve the details of the images, we innovatively introduce an iterative mapping module to generate smooth style transform curves. 3) To enhance the diversity of synthesized images, a simple but efficient end-to-end optimization method is proposed. 4) We found a strong correlation between the style diversity of augmented images and the performance of the detection model through a quantitative analysis, highlighting the crucial role of style diversity in enhancing the model’s generalizability. Our framework achieves state-of-the-art performance on multiple challenging visual datasets, surpassing the current state-of-the-art methods by 27%(+8.0AP). Moreover, our approach and modules can be easily extended to different detectors and other domain adaptation methods, making it a versatile solution for object detection in adverse visual environments. Our code will be available at https://github.com/ssunguotu/Diverse-Aug.},
  keywords={Visualization;Object detection;Adaptation models;Training;Task analysis;Circuits and systems;Image edge detection;Object detection;domain adaptation;image-to-image translation;generative adversarial network},
  doi={10.1109/TCSVT.2024.3398145},
  ISSN={1558-2205},
  month={Oct},}@INPROCEEDINGS{10409327,
  author={Krause, Jonas and De Souza Inacio, Andrei and Lopes, Heitor Silvério},
  booktitle={2023 IEEE Latin American Conference on Computational Intelligence (LA-CCI)}, 
  title={Language-focused Deepfake Detection Using Phonemes, Mouth Movements, and Video Features}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The potential implications of Artificial Intelligence (AI) and Deep Learning (DL) algorithms in generating highly realistic deepfake videos have raised concerns regarding the reliability of our human senses. In response to this challenge, we propose a deepfake detection system based on phonemes, the transcribed text, associated mouth movements, and video-extracted features. As a proof-of-concept, we develop a deepfake detection system specifically designed for the Portuguese language, employing three presidential candidates from the 2022 Brazilian elections. Additionally, we introduce a unique dataset comprising real and fake videos involving these three individuals and deliberately blending their identities. The extracted features consolidate relevant attributes, which we utilized to train multiple classification algorithms. Notably, our computational models demonstrate satisfactory performance when authenticating or detecting fake videos containing at least one of the trained phonemes from the Portuguese language. Hence, we conclude that deepfake detection is feasible, primarily due to the absence of natural expressions, particularly in non-English language deepfake videos. Furthermore, developing individual-guided deepfake detection systems may facilitate the authentication of videos featuring celebrities or politicians during future online events.},
  keywords={Deepfakes;Computational modeling;Mouth;Feature extraction;Classification algorithms;Reliability;Artificial intelligence;Deepfake;Language-focused;Phoneme-based.},
  doi={10.1109/LA-CCI58595.2023.10409327},
  ISSN={2769-7622},
  month={Oct},}@BOOK{10948557,
  author={Mukherjee, Amit and Saladi, Adithya and Casalaina, Marco},
  booktitle={Azure OpenAI Essentials: A practical guide to unlocking generative AI-powered innovation with Azure OpenAI},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Build innovative, scalable, and ethical AI solutions by harnessing the full potential of generative AI with this exhaustive guideKey FeaturesExplore the capabilities of Azure OpenAI’s LLMsCraft end-to-end applications by utilizing the synergy of Azure OpenAI and Cognitive ServicesDesign enterprise-grade GenAI solutions with effective prompt engineering, fine-tuning, and AI safety measuresPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionFind out what makes Azure OpenAI a robust platform for building AI-driven solutions that can transform how businesses operate. Written by seasoned experts from Microsoft, this book will guide you in understanding Azure OpenAI from fundamentals through to advanced concepts and best practices. The book begins with an introduction to large language models (LLMs) and the Azure OpenAI Service, detailing how to access, use, and optimize its models. You'll learn how to design and implement AI-driven solutions, such as question-answering systems, contact center analytics, and GPT-powered search applications. Additionally, the chapters walk you through advanced concepts, including embeddings, fine-tuning models, prompt engineering, and building custom AI applications using LangChain and Semantic Kernel. You'll explore real-world use cases such as QnA systems, document summarizers, and SQLGPT for database querying, as well as gain insights into securing and operationalizing these solutions in enterprises. By the end of this book, you'll be ready to design, develop, and deploy scalable AI solutions, ensuring business success through intelligent automation and data-driven insights.What you will learnUnderstand the concept of large language models and their capabilitiesInteract with different models in Azure OpenAI using APIs or web interfacesUse content filters and mitigations to prevent harmful content generationDevelop solutions with Azure OpenAI for content generation, summarization, semantic search, NLU, code and image generation and analysisIntegrate Azure OpenAI with other Azure Cognitive services for enhanced functionalityApply best practices for data privacy, security, and prompt engineering with Azure OpenAIWho this book is forThis book is for software developers, data scientists, AI engineers, ML engineers, system architects, LLM engineers, IT professionals, product managers, and business professionals who want to learn how to use Azure OpenAI to create innovative solutions with generative AI. To fully benefit from this book, you must have both an Azure subscription and Azure OpenAI access, along with knowledge of Python.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805122654},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10948557},}
