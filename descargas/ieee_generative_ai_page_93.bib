@INPROCEEDINGS{11016448,
  author={Ferreira, José Manuel Martins},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={AI-Assisted Assessment: A Dual Perspective on Effective Usage Plans for Students and Teachers}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper focuses upon assessing the use of Google's NotebookLM for AI-assisted grading of a project assignment weighing 50 % of the final grades obtained by a class of 30 students in a Hardware/Software Codesign course that belongs to a master program in Computer Science. The work complements an existing strategy for AI-assisted learning, shifting the focus from the students' to the teachers' side. NotebookLM's potential contribution to improve teaching activities is briefly considered, followed by the presentation of an experiment where this tool was used to support the teacher's assessment work, and of the students' perception towards AIassisted grading. The paper closes with a brief reflection upon the impact of these emerging technologies upon the wider higher-education sector.},
  keywords={Computer science;Graphics;Generative AI;Writing;Feature extraction;Reflection;Internet;Artificial intelligence;Engineering education;Recruitment;AI-assisted grading;AI strategies;teaching and learning models;generative AI in education},
  doi={10.1109/EDUCON62633.2025.11016448},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11131755,
  author={Glavaš, A. and Staščik, A. and Jukić, R.},
  booktitle={2025 MIPRO 48th ICT and Electronics Convention}, 
  title={Evaluation of Lesson Plan Generated by ChatGPT 4.0: Focus Group}, 
  year={2025},
  volume={},
  number={},
  pages={668-673},
  abstract={In this paper, we examine possible use of generative AI in teaching. The authors generated a lesson plan for one teaching unit in primary education using ChatGPT 4.0. To minimize the influence, the authors included only the learning outcome, subject, grade level and lesson duration in the prompt. In this study a heterogeneous focus group was conducted with 5 primary school teachers. Teachers analyzed and evaluated AI-generated lesson plan along with 4 other lesson plans created by 4 different teachers. Firstly, teachers evaluated and ranked them without knowing that among given lesson plans there is one generated by AI completely. Afterwards, they were informed that one of the lesson plans had been generated by AI and requested to identify which one it was. They put ChatGPT generated lesson plan on the first place and couldn't recognize which one was generated by ChatGPT. Despite all the shortcomings (e.g. accuracy of terminology), the results suggest that generative AI could provide good ideas and be useful for lesson planning, which is classified as one of the most time-consuming tasks for teachers. However, for AI to be effective and useful so that teacher can use it to create good lesson plan it requires specific skills and competencies such as prompt engineering, didactic-methodical, pedagogical and content-knowledge for critical evaluation and selection of the proposed content. Finally, considering the possible great contribution in the field of teaching it is necessary to further research the field of application of AI teaching purposes.},
  keywords={Hands;Accuracy;Terminology;Education;Chatbots;Planning;Prompt engineering;Usability;chatGPT;lesson plan;teaching;AI in education},
  doi={10.1109/MIPRO65660.2025.11131755},
  ISSN={1847-3938},
  month={June},}@INPROCEEDINGS{10931953,
  author={Tomar, Swati and Jain, Kirti},
  booktitle={2024 International Conference on Communication, Control, and Intelligent Systems (CCIS)}, 
  title={An Efficient Deep Learning Technique for Facial Expression Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Automatic facial expression recognition is one of the most important issues in computer vision and it has its use in areas such as HCI and emotion-based systems. For facial expression recognition, this paper presented a Bidirectional Long Short-Term Memory (BiLSTM) deep learning technology. The proposed model is to address the temporal structure of sequential facial features and provided improved performance in the temporal dependency of facial expressions. We utilize a selected dataset, perform preprocessing on the facial images, BacYN feature extraction, and optimize BiLSTM for expression recognition. Based on a large number of experiments and assessments, our method proves successful and can be successfully used in realizing applications that require a detailed analysis of facial emotions.},
  keywords={Deep learning;Accuracy;Image recognition;Face recognition;Computational modeling;Bidirectional long short term memory;Feature extraction;Robustness;Intelligent systems;Facial features;BiLSTM;Facial expression;Recognition;Machine;Deep;Artificial Intelligence},
  doi={10.1109/CCIS63231.2024.10931953},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11046083,
  author={Alamiri, Deimah and Alfadhli, Rimah and Almutairi, Hajar and Alhabshi, Rahimah and Almutairi, Nour and Eleyan, Alaa},
  booktitle={2025 6th International Conference on Bio-engineering for Smart Technologies (BioSMART)}, 
  title={Applying Deep Learning to MRI Image Analysis for Brain Tumor Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Automated and accurate brain tumor classification from MRI scans is a promising application of deep learning. This paper presents a YOLOv11-based deep learning model for detecting and classifying three tumor types: glioma, meningioma, and pituitary. Performance evaluation demonstrates the model's strong capability in tumor detection and classification, particularly with meningioma and pituitary tumors showing higher precision than glioma. Validation curves indicate steady reduction in loss functions across epochs, signifying effective learning and convergence. The model was trained and validated on a structured dataset, achieving a high accuracy performance of 98.9%. Deep learning-based tumor classification can facilitate early detection, assist radiologists, and improve clinical decision-making process.},
  keywords={Deep learning;YOLO;Accuracy;Explainable AI;Magnetic resonance imaging;Decision making;Brain tumors;Brain modeling;Market research;Convergence;Deep Learning;Convolutional Neural Networks;MRI Classification;Brain tumor Detection;Artificial Intelligence},
  doi={10.1109/BioSMART66413.2025.11046083},
  ISSN={2831-4352},
  month={May},}@INPROCEEDINGS{10862684,
  author={Shehada, Dina and Turky, Ayad and Rabie, Tamer and Hussain, Abir},
  booktitle={2024 IEEE 12th Conference on Systems, Process & Control (ICSPC)}, 
  title={Enhanced Lightweight Facial Emotion Recognition Systems for Visually Impaired People}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Facial expressions are a vital component of human communication, conveying emotional information that enhances the social experience. However, for individuals with visual impairments, perceiving and interpreting facial expressions can be challenging, hindering their ability to engage fully in social interactions. This paper presents an enhanced facial emotion recognition system specifically designed to aid visually impaired individuals. Two enhanced approaches for facial emotion recognition systems are proposed. The first approach consolidates negative emotions (anger, fear, sadness, disgust) into a single class, while the second approach incorporates diverse training datasets from real-world environments. The negative emotion consolidation approach achieved $\mathbf{9 0. 9 \%}$ and $\mathbf{9 4. 7 \%}$ accuracy on the FER2013 and CK+ datasets, respectively. While, the diverse dataset integration approach achieved $\mathbf{8 4. 9 \%}$ and $\mathbf{7 0. 6 \%}$ accuracy on FER2013 and CK+ datasets, respectively. This is a significant improvement when benchmarked with other approaches available in the literature. The feasibility of the enhanced proposed systems as assistive technology for the visually impaired indicated its ability to be applied in real-world applications.},
  keywords={Training;Emotion recognition;Visualization;Accuracy;Face recognition;Visual impairment;Diversity reception;Robustness;Convolutional neural networks;Compounds;Convolutional Neural Network;Artificial intelligence;Blind;Visually Impaired;Emotion;Facial Detection},
  doi={10.1109/ICSPC63060.2024.10862684},
  ISSN={2769-7916},
  month={Dec},}@INPROCEEDINGS{10672681,
  author={Kaushik, Pratham and Singh Gill, Kanwarpartap and Chattopadhyay, Saumitra and Singh, Mukesh},
  booktitle={2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT)}, 
  title={Probing Aberrant Heart Rhythms Utilizing CNN Autoencoders for Enhanced ECG Anomaly Recognition}, 
  year={2024},
  volume={1},
  number={},
  pages={820-824},
  abstract={An application of Convolutional Neural Network (CNN) Autoencoders for anomaly identification in electrocardiogram (ECG) data is investigated in this study using the PTB Diagnostic ECG Database. Heartbeats affected by cardiac abnormalities and normal heartbeats make up the two groups of 14,552 samples in all. Transposed convolution has been used to greatly enhance model performance. This work highlights the need of early cardiac issue detection and introduces a CNN Autoencoder model specifically made to effectively encode and decode ECG data, therefore allowing for the identification of abnormal patterns. Construction of a strong Autoencoder with trained encoder and decoder components to lower reconstruction errors is the process. The astonishing F1 score (65.40%), recall (89.81%), precision (55.23%), and accuracy (76.93%) of the model are demonstrated by the assessment measures. This paper emphasises the importance of early anomaly identification in ECG data using deep learning methods. Higher diagnostic skills and improved patient outcomes are very much possible with this approach.},
  keywords={Heart beat;Databases;Convolution;Buildings;Medical services;Electrocardiography;Data models;Artificial Intelligence;Deep Learning;Anomaly detection;Model Training;Autoencoder},
  doi={10.1109/ICCPCT61902.2024.10672681},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10641756,
  author={Monaco, Manilo and Licciardi, Giorgio A. and Battagliere, Maria L. and Guarini, Rocchina and Cimino, Mario G.C.A. and Candela, Laura},
  booktitle={IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium}, 
  title={A Machine-Learning Approach for Generating Synthetic Prisma Hyperspectral Images from Multispectral Data}, 
  year={2024},
  volume={},
  number={},
  pages={3659-3662},
  abstract={The scarcity of a sufficiently large and representative hyperspectral image dataset is a substantial obstacle to the effective development of algorithms for remote sensing applications. Hyperspectral images can provide rich spectral information for various tasks, such as land cover classification, vegetation monitoring, and environmental assessment. However, the limited availability of diverse and well-annotated hyperspectral datasets hinders the development and optimization of these models in this domain. For this purpose, the generation of synthetic hyperspectral images has emerged as a pivotal area of research.This paper aims to introduce a preliminary analysis of various AI-based methodologies specifically crafted to generate synthetic PRISMA hyperspectral images derived from Sentinel-2 data. By exploring innovative approaches, this study aims to develop novel techniques for creating synthetic datasets, providing valuable insights into the potential of synthetic hyperspectral imagery for algorithm training and evaluation in the absence of extensive real-world hyperspectral datasets.},
  keywords={Training;Satellites;Urban areas;Vegetation mapping;Machine learning;Spatial databases;Distortion measurement;Hyperspectral imagery;PRISMA;Artificial Intelligence;synthetic images},
  doi={10.1109/IGARSS53475.2024.10641756},
  ISSN={2153-7003},
  month={July},}@INPROCEEDINGS{10594280,
  author={Cai, Haowen and Lin, Wei},
  booktitle={2024 IEEE International Workshop on Radio Frequency and Antenna Technologies (iWRF&AT)}, 
  title={AI-Assisted Automatic Design Methods for Antennas and RF Circuits Based on Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={140-142},
  abstract={Radio-frequency (RF) antennas and circuits are essential components to applications such as wireless communication, satellite, radar, and the Internet of Things. Manual design process for antennas and circuits are dependent on expert knowledge that proves to be time-consuming and thus more efficient and autonomous design methods are preferred. This paper introduces two AI-assisted automatic design approaches employing deep learning (DL), specifically deep neural networks (DNNs) for antenna design and deep reinforcement learning (DRL) for circuit design. The DNNs predict antenna parameters and radiation gain, significantly reducing simulation time. RF coupler is adopted as the example for circuit design that utilizes DRL with a deep Q-network (DQN), dynamically selecting element types and parameter values to achieve desired power dividing ratios. The presented methods showcase outstanding efficiency and accuracy compared to traditional approaches, offering a significant advancement in automatic design processes for antennas and RF circuits.},
  keywords={Radio frequency;Learning systems;Wireless communication;Accuracy;Design methodology;Spaceborne radar;Satellite broadcasting;Antenna;artificial intelligence;coupler;reinforcement learning;substrate integrated waveguide;neural networks},
  doi={10.1109/iWRFAT61200.2024.10594280},
  ISSN={},
  month={May},}@INPROCEEDINGS{11115707,
  author={Hirpara, Priyanshu and Valangar, Hardi and Kachhadiya, Vishwa and Chauhan, Uttam},
  booktitle={2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Deepfake Detection: Demodulate Synthetic Videos Using Deep Learning Models}, 
  year={2025},
  volume={},
  number={},
  pages={01-06},
  abstract={A deepfake detection system that uses machine learning (ML) and deep learning (DL) models to detect manipulated videos and images is presented in the study. Being aware of such synthetic content is crucial considering the emergence of deepfake technology, which might alter photos, videos, and audio for malevolent objectives including fraud, extortion, and disinformation. Deepfake technology has been applied to solve various real-time problems but is also exploited for unethical and illegal purposes. As a result, developing research and detection models is crucial to prevent its misuse. We proposed a CNN-LSTM hybrid model for analysis of cropped images to improve the performance of fake video detection. The suggested method focuses on identifying fake videos using the Celeb-DF dataset, which consists of 1203 videos (795 fake, 408 real). Moreover, the benefits and drawbacks of the various deepfake detection techniques are examined. The paper indicates potential improvements in model accuracy through more datasets and improved architectures, and it emphasizes the significance of sophisticated detection techniques to mitigate the negative consequences of deepfakes. With cropped video frames and deep learning techniques, the model's accuracy increased from 79.06% with the original dataset to $\mathbf{8 6. 8 2 \%}$ with cropped videos.},
  keywords={Deep learning;Deepfakes;Analytical models;Accuracy;Computational modeling;Computer architecture;Real-time systems;Fraud;Faces;Deepfake Detection;Deep Learning;Artificial Intelligence;Face Manipulation;CNN-LSTM},
  doi={10.23919/INDIACom66777.2025.11115707},
  ISSN={},
  month={April},}@INPROCEEDINGS{10569871,
  author={Mlinarić, D. and Dončević, J. and Brčić, M. and Botički, I.},
  booktitle={2024 47th MIPRO ICT and Electronics Convention (MIPRO)}, 
  title={Revolutionizing Software Development: Autonomous Software Evolution}, 
  year={2024},
  volume={},
  number={},
  pages={224-228},
  abstract={This paper discusses a software development method focused on creating a self-adapting and evolving system using AI and ML techniques. The goal is to reduce the need for manual software updates, offering a solution that continuously adapts to changing requirements. To achieve this goal, an AI-based update model is presented, and a possible system is discussed. Use case example demonstrate the applicability of the update model in real-world scenarios. As a cloud-based solution, this method could ensure scalability and broad applicability across various industry sectors.},
  keywords={Adaptation models;Codes;Scalability;Manuals;Games;Software;Safety;automatic software development;autonomous systems;software evolution;artificial intelligence},
  doi={10.1109/MIPRO60963.2024.10569871},
  ISSN={2623-8764},
  month={May},}@INPROCEEDINGS{10835035,
  author={Zhang, Shuya and Yang, Yi},
  booktitle={2024 International Conference on Electronics and Devices, Computational Science (ICEDCS)}, 
  title={Research on library resource optimization algorithm based on reinforcement learning}, 
  year={2024},
  volume={},
  number={},
  pages={522-526},
  abstract={This paper aims to explore the application and optimization effect of context-aware deep reinforcement learning (CDR-L) and resource management system (RMS) integrated model in library resource management. The CDR-L RMS model innovatively integrates collaborative filtering mechanism and deep duplicative strategy gradient (DDPG) algorithm, aiming to realize accurate recommendation of library books and efficient dynamic adjustment of inventory management, so as to meet the diversified and personalized needs of users, optimize resource allocation and reduce operating costs. Firstly, this paper analyzes the challenges existing in current library resource management. Then, the design principle and architecture of the CDR-L RMS model are introduced in detail, which dynamically adjusts the recommendation strategy and inventory optimization decision, in order to achieve the goal of improving user satisfaction, increasing the circulation rate of books and improving the efficiency of resource use. The experimental results show that compared with traditional resource management methods, CDR-L RMS model can significantly increase the amount of books borrowed.},
  keywords={Costs;Scientific computing;Heuristic algorithms;Collaborative filtering;Inventory management;Dynamic scheduling;Deep reinforcement learning;Libraries;Resource management;Optimization;reinforcement learning;Artificial intelligence;RFPP model;CF algorithm},
  doi={10.1109/ICEDCS64328.2024.00099},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11042758,
  author={Gao, Mingyang},
  booktitle={2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)}, 
  title={DSCAT: Dual Smoothing-constrained Adaptive Step Adversarial Training}, 
  year={2025},
  volume={},
  number={},
  pages={720-724},
  abstract={Adversarial training has attracted considerable attention due to its effectiveness in safeguarding models against adversarial attacks in recent years. Among these methods, adversarial training with learnable attack strategies has stood out for its strong theoretical foundation and remarkable empirical performance. In this paper, we propose a novel adaptive attack strategy based on a single influencing factor for adversarial example generation: Dual Smoothing Constrained Adaptive Step-size Adversarial Training (DSCAT). DSCAT employs an entropy-constrained mechanism to smooth the adversarial example generation process, while simultaneously utilizing a perturbation intensity judgment and clipping layer to further regularize the generated adversarial examples. During training, the interplay between step-size adjustment and perturbation clipping effectively mitigates the “lagging” decision phenomenon observed in learnable attack strategies. This not only facilitates a smoother training process but also ensures that the model can make precise judgments when handling data near the decision boundary. Experimental evaluations on benchmark datasets demonstrate that DSCAT significantly outperforms other popular methods, substantially enhancing model robustness under a variety of attack scenarios.},
  keywords={Training;Adaptation models;Smoothing methods;Perturbation methods;Benchmark testing;Network security;Robustness;Entropy;Data models;Software engineering;Adversarial Training;Robustness;Network Security;Artificial Intelligence},
  doi={10.1109/AEMCSE65292.2025.11042758},
  ISSN={},
  month={May},}@INPROCEEDINGS{10937610,
  author={Lenk, Leon and Christian, Asher and Alanis, Savannah and Bhusari, Renuka and DeSoto, Zachary and Guo, Daniel and Iziumska, Iryna and Kharbanda, Jishan and Pai, Jonathan and Shih, William and Qiao, Cathleen},
  booktitle={2024 IEEE MIT Undergraduate Research Technology Conference (URTC)}, 
  title={Deep Learning for Steganographic Image Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Steganography is often difficult to detect, especially when applied to images, as alterations to the image are minimal and often imperceptible to the human eye. To address these obstacles, we trained EfficientNetV2-S, a Convolutional Neural Network (CNN) model optimized for efficiency, for steganography detection. Using a CNN allows for flexibility as it can train on subtle features created by steganographic methods and differentiate between them and clean images. We find that our fine-tuned model is able to differentiate between clean images and certain classes of steganographically altered images with an accuracy of up to 98 percent. Notably, our model is able to distinguish between four methods of steganographic alteration and clean images with a 66.29 percent accuracy, demonstrating a new capability in the field of steganalysis.},
  keywords={Deep learning;Steganography;Computer vision;Accuracy;Computational modeling;Data security;Information security;Convolutional neural networks;Image classification;Testing;steganography;information security;data security;artificial intelligence;computer vision;machine learning},
  doi={10.1109/URTC65039.2024.10937610},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10235715,
  author={Li, Lingyu and Shen, Tianwen},
  booktitle={2023 4th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)}, 
  title={RATSR: A Region Adaptive Transformer for Image Super-Resolution Based on Heat Feature Map}, 
  year={2023},
  volume={},
  number={},
  pages={34-37},
  abstract={In an endeavor to develop a state-of-the-art model in the realm of image acceleration super-resolution (SR), we devised a sophisticated transformer-based classifier that accounts for regional features. Our innovative transformer, RATSR, seamlessly integrates features derived from the classifier into the SR process, while introducing a groundbreaking heatmap concept. This novel approach enables the image features to substantially enhance the attentional span of the super-resolution model, better utilizing regional information. Moreover, we conducted an in-depth exploration of the similarities and disparities between feature matching and feature fusion, optimizing components that are less perceptible to global information. Through rigorous theoretical analysis, we assessed the efficacy of our approach and proposed a comprehensive model architecture accompanied by a robust training strategy. Concurrently, we substantiated its effectiveness by employing extensive experimentation, and implemented ablation studies to fine-tune the model's modules. The culmination of our research effort is evidenced in the experimental results, which reveal that our cutting-edge image super-resolution model attains exceptional performance without necessitating a substantial increase in the number of parameters.},
  keywords={Heating systems;Training;Adaptation models;Information science;Analytical models;Superresolution;Transformers;Artificial Intelligence;Image Processing;Computer Vision;Deep Learning;Image Super-resolution;Vision Transformer;Introduction},
  doi={10.1109/ISPDS58840.2023.10235715},
  ISSN={},
  month={July},}@INPROCEEDINGS{10386748,
  author={Liu, Wenzhao and Meng, Dechao},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation}, 
  year={2023},
  volume={},
  number={},
  pages={4497-4502},
  abstract={Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We’ve set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/},
  keywords={TV;Semantics;Refining;Music;Focusing;Production;Games;image to music generation;dual-path;artificial intelligence},
  doi={10.1109/BigData59044.2023.10386748},
  ISSN={},
  month={Dec},}@ARTICLE{8666767,
  author={Deng, Cheng and Yang, Erkun and Liu, Tongliang and Li, Jie and Liu, Wei and Tao, Dacheng},
  journal={IEEE Transactions on Image Processing}, 
  title={Unsupervised Semantic-Preserving Adversarial Hashing for Image Search}, 
  year={2019},
  volume={28},
  number={8},
  pages={4032-4044},
  abstract={Hashing plays a pivotal role in nearest-neighbor searching for large-scale image retrieval. Recently, deep learning-based hashing methods have achieved promising performance. However, most of these deep methods involve discriminative models, which require large-scale, labeled training datasets, thus hindering their real-world applications. In this paper, we propose a novel strategy to exploit the semantic similarity of the training data and design an efficient generative adversarial framework to learn binary hash codes in an unsupervised manner. Specifically, our model consists of three different neural networks: an encoder network to learn hash codes from images, a generative network to generate images from hash codes, and a discriminative network to distinguish between pairs of hash codes and images. By adversarially training these networks, we successfully learn mutually coherent encoder and generative networks, and can output efficient hash codes from the encoder network. We also propose a novel strategy, which utilizes both feature and neighbor similarities, to construct a semantic similarity matrix, then use this matrix to guide the hash code learning process. Integrating the supervision of this semantic similarity matrix into the adversarial learning framework can efficiently preserve the semantic information of training data in Hamming space. The experimental results on three widely used benchmarks show that our method not only significantly outperforms several state-of-the-art unsupervised hashing methods, but also achieves comparable performance with popular supervised hashing methods.},
  keywords={Semantics;Hash functions;Binary codes;Image reconstruction;Generators;Gallium nitride;Generative adversarial networks;Hashing;image search;adversarial learning;deep learning},
  doi={10.1109/TIP.2019.2903661},
  ISSN={1941-0042},
  month={Aug},}@ARTICLE{8718595,
  author={Cabrera, Diego and Sancho, Fernando and Long, Jianyu and Sánchez, René-Vinicio and Zhang, Shaohui and Cerrada, Mariela and Li, Chuan},
  journal={IEEE Access}, 
  title={Generative Adversarial Networks Selection Approach for Extremely Imbalanced Fault Diagnosis of Reciprocating Machinery}, 
  year={2019},
  volume={7},
  number={},
  pages={70643-70653},
  abstract={At present, countless approaches to fault diagnosis in reciprocating machines have been proposed, all considering that the available machinery dataset is in equal proportions for all conditions. However, when the application is closer to reality, the problem of data imbalance is increasingly evident. In this paper, we propose a method for the creation of diagnoses that consider an extreme imbalance in the available data. Our approach first processes the vibration signals of the machine using a wavelet packet transform-based feature-extraction stage. Then, improved generative models are obtained with a dissimilarity-based model selection to artificially balance the dataset. Finally, a Random Forest classifier is created to address the diagnostic task. This methodology provides a considerable improvement with 99% of data imbalance over other approaches reported in the literature, showing performance similar to that obtained with a balanced set of data.},
  keywords={Fault diagnosis;Machinery;Data models;Gallium nitride;Task analysis;Time-frequency analysis;Radio frequency;Imbalanced data;GAN;model selection;random Forest;reciprocating machinery},
  doi={10.1109/ACCESS.2019.2917604},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9054379,
  author={Shin, Jamin and Xu, Peng and Madotto, Andrea and Fung, Pascale},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Generating Empathetic Responses by Looking Ahead the User’s Sentiment}, 
  year={2020},
  volume={},
  number={},
  pages={7989-7993},
  abstract={An important aspect of human conversation difficult for machines is conversing with empathy, which is to understand the user's emotion and respond appropriately. Recent neural conversation models that attempted to generate empathetic responses either focused on conditioning the output to a given emotion, or incorporating the current user emotional state. However, these approaches do not factor in how the user would feel towards the generated response. Hence, in this paper, we propose Sentiment Look-ahead, which is a novel perspective for empathy that models the future user emotional state. In short, Sentiment Look-ahead is a reward function under a reinforcement learning framework that provides a higher reward to the generative model when the generated utterance improves the user's sentiment. We implement and evaluate three different possible implementations of sentiment look-ahead and empirically show that our proposed approach can generate significantly more empathetic, relevant, and fluent responses than other competitive baselines such as multitask learning.},
  keywords={Conferences;Natural languages;Reinforcement learning;Signal processing;Acoustics;Speech processing;Natural Language Processing;Dialogue Systems;Empathetic Chatbots;Sentiment Look-ahead},
  doi={10.1109/ICASSP40776.2020.9054379},
  ISSN={2379-190X},
  month={May},}@INPROCEEDINGS{10447981,
  author={Byun, Dong-Min and Lee, Sang-Hoon and Hwang, Ji-Sang and Lee, Seong-Whan},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Midi-Voice: Expressive Zero-Shot Singing Voice Synthesis via Midi-Driven Priors}, 
  year={2024},
  volume={},
  number={},
  pages={12622-12626},
  abstract={Recently, singing voice synthesis (SVS) models have shown significant progress with generative models. However, previous SVS models inaccurately predict prior and fundamental frequency (F0) for unseen speakers, resulting in a low-quality generated singing voice. To address these issues, in this paper, we propose MIDI-Voice for expressive singing voice synthesis and robust zero-shot singing voice style transfer. We employ a MIDI-based prior to a score-based diffusion model for better singing voice style adaptation. We first generate a MIDI-driven prior from the musical score, and this only includes the note information, not speaker information resulting in high-quality singing voice style adaptation. We also propose a DDSP-based MIDI-style prior for synthesizing a more expressive singing voice and for singing style adaptation, although it requires additional information from the audio. The experimental results show that MIDI-Voice outperforms the previous models in synthesizing an expressive singing voice, and also the superiority in zero-shot singing voice style transfer performance.},
  keywords={Adaptation models;Predictive models;Signal processing;Acoustics;Data mining;Speech processing;Singing voice synthesis;Zero-shot singing voice synthesis;MIDI-driven prior;Diffusion models},
  doi={10.1109/ICASSP48485.2024.10447981},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10879751,
  author={Son, Young-Han and Shin, Dong-Hee and Lee, Deok-Joong and Kam, Tae-Eui},
  booktitle={2025 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={Molecular Optimization with Mamba-Based GFlowNet}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Molecular discovery involves the identification and design of novel chemical compounds with desired properties. Recently, this field has gained significant attention across various scientific domains for its potential to drive advancements in drug development and materials science. We propose an improved Generative Flow Network (GFlowNet) architecture for molecular optimization by replacing the traditional Gated Recurrent Unit (GRU) encoder with Mamba, which employs State Space Models (SSMs) to capture long-term dependencies more effectively. Experiments on single and multi-objective molecular optimization tasks demonstrate that our Mamba-based GFlowNet achieves superior sample efficiency and higher performance compared to existing methods, effectively generating high-quality molecular candidates within a limited number of trials.},
  keywords={Drugs;Materials science and technology;Electric potential;Memory management;Logic gates;Chemical compounds;Optimization;Molecular optimization;GFlowNets;Mamba},
  doi={10.1109/ICEIC64972.2025.10879751},
  ISSN={2767-7699},
  month={Jan},}@INPROCEEDINGS{10929884,
  author={Jeong, Seokjue and Lee, Sunghan and Koh, Jeonghwan and An, Hyungchan and Jeong, In cheol},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Eye Image and EOG Signal Conversion via Autoencoders for Human-Machine Interaction}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Converting between EOG signals and eye images allows for adaptive gaze-based systems, enabling real-time visualization of eye movements in healthcare and low-cost gaze-tracking in consumer electronics. This study presents a bidirectional mapping framework between eye images and electrooculography (EOG) signals, enabling intuitive and hands-free human-machine interaction (HMI). A generative autoencoder was trained over 7000 epochs to map 2-D eye images to 1-D EOG signals and back, using a 2-step decoder structure for enhanced accuracy. The training utilized a dataset of synchronized EOG and eye image data from 30 participants. Experimental results showed that the model successfully reconstructed key EOG patterns and eye images, achieving an average image correlation of $0.9582 (\pm 0.0068)$ and EOG correlation values of $0.6825(\pm 0.3272)$ for horizontal and $0.6526 (\pm 0.3000)$ for vertical signals, with minor noise noted in fine EOG details. These findings suggest that the proposed model can reliably transform between EOG signals and eye images. It has the potential to provide a promising basis for accessible, gaze-based HMI systems in consumer electronics. Future work will focus on refining detail accuracy and multimodal integration.},
  keywords={Human computer interaction;Electrooculography;Correlation;Accuracy;Translation;Autoencoders;Medical services;Real-time systems;Consumer electronics;Image reconstruction;Autoencoder;electrooculography (EOG);human-machine interaction (HMI);reconstruction},
  doi={10.1109/ICCE63647.2025.10929884},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10016338,
  author={Liu, Qide and Chu, Jielei and Yu, Hua and Wang, XinLei and Li, Tianrui},
  booktitle={2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={VQ-ViCNet: Strengthen Unique Features Comparison Autoencoder with Embedding Space for Covid-19 Image Classification}, 
  year={2022},
  volume={},
  number={},
  pages={464-468},
  abstract={In this paper, we propose a new novel coronavirus pneumonia image classification model based on the combination of Transformer and convolutional network(VQ-ViCNet), and present a vector quantization feature enhancement module for the inconspicuous characteristics of lung medical image data. This model extracts the local latent layer features of the image through the convolutional network, and learns the deep global features of the image data through the Transformer’s multi-head self attention algorithm. After the calculation of convolution and attention, the features learned by the Transformer Encoder are enhanced by the vector quantization feature enhancement module and able to better complete the final downstream tasks. This model performs better than convolutional architectures, pure attention architectures and generative models on all 6 public datasets.},
  keywords={COVID-19;Vector quantization;Computational modeling;Lung;Computer architecture;Feature extraction;Transformers;Transformer;Self attention;Vector Quantization;Covid-19;convolutional},
  doi={10.1109/CCIS57298.2022.10016338},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10547979,
  author={Lai, Shih-Lun and Chen, Pin-Chuan and Ma, Ching-Wen},
  booktitle={2024 10th International Conference on Applied System Innovation (ICASI)}, 
  title={Compositional Conditional Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={377-379},
  abstract={Generative model often struggle to produce data beyond the training distribution. To address this, we explored various methods for unseen data generation, eventually focusing on compositional zero-shot image generation. By guiding the generation model with compositional class labels, we achieved better control over the generation process. Our model can generate unseen images whose compositional labels are not appear in the training set. While large language models like GPT-4 and image generation models like DALL-E offer similar zero-shot generation capabilities, our research emphasizes domain-specific zero-shot generation using smaller models. Through a series of tasks, we demonstrated the effectiveness of compositional zero-shot image generation across various complexities, showcasing its potential in contemporary machine learning.},
  keywords={Training;Technological innovation;Image synthesis;Zero-shot learning;Process control;Focusing;Data models;Diffusion Model;Image Synthesis;Compositional Zero-shot Learning},
  doi={10.1109/ICASI60819.2024.10547979},
  ISSN={2768-4156},
  month={April},}@INPROCEEDINGS{11084463,
  author={Park, Chanyeong and Jang, Junbo and Lee, Jiyoon and Yoon, Jaehong and Baek, Minju and Paik, Joonki},
  booktitle={2025 IEEE International Conference on Image Processing (ICIP)}, 
  title={VIDA: Unsupervised Visible-to-Infrared Domain Adaptation for Object Detection Using Large Vision Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={1426-1431},
  abstract={In autonomous driving, reliable object detection across varied environmental conditions is essential, particularly when transitioning between spectral domains like visible light (RGB) and infrared (IR). This paper introduces a novel, fully unsupervised approach for RGB-to-IR domain adaptive object detection. By utilizing generative models to synthesize IR images from RGB inputs, our method eliminates the need for direct IR data collection. The proposed method falls under unsupervised domain adaptation. It generates IR images from RGB inputs to train a domain adaptive object detection model, leveraging a Large Vision-Language Model (LVLM) to transfer the target domain style using text prompts. By generating IR images solely from RGB inputs, the approach eliminates the need for expensive and time-consuming IR data collection, making it highly efficient. This enhances the robustness of object detection across spectral domains, improving vehicle safety in challenging environments. Extensive experiments demonstrate significant improvements in IR detection accuracy, all achieved without direct IR data during training, offering a cost-effective and scalable solution.},
  keywords={Training;Adaptation models;Image processing;Vehicle safety;Object detection;Data collection;Robustness;Data models;Spectral analysis;Autonomous vehicles;Object Detection;Domain Adaptation;Vision-Language Model;RGB;Infrared},
  doi={10.1109/ICIP55913.2025.11084463},
  ISSN={2381-8549},
  month={Sep.},}@INPROCEEDINGS{11092295,
  author={Zhao, Ya and Xu, Ying and Long, Taotao and Wang, Linzhi},
  booktitle={2025 7th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Capturing Pre-service Science Teachers’ Collaborative Discourse patterns in Collaborative Refection: a network analytic approach}, 
  year={2025},
  volume={},
  number={},
  pages={181-185},
  abstract={Collaborative reflection has been recognized as an integral theoretical part of lesson study (LS) to improve teachers’ professional development. However, current research revealed that it is a challenge for facilitating collaborative reflection in lesson study due to its its conceptual ambiguity and intangible quality. Considering the ALACT model has been demonstrated to be able to help promote teachers’ collaborative reflection in a large amount of existing research, this study adopted the ALACT model to structure pre-service science teachers’ collaborative reflection in LS. Due to the dynamic and complicated process of collaborative reflection, epistemic network analysis (ENA) approach was employed to trace the development process of 14 pre-service science teachers’ collaborative discourse patterns in two collaborative reflection activities in LS. Data analysis results revealed that the pre-service science teachers paid more attention on posing problems and introducing multiple voices. And they demonstrated more active in "Generative Orientations" in the first collaborative reflection after micro-teaching, and "Support and Critiquing" in the second collaborative reflection after classroom teaching in primary school. Suggestions are provided for improving pre-service science teachers’ collaborative abilities in a scaffolded collaborative reflection in LS.},
  keywords={Computer science;Analytical models;Data analysis;Education;Collaboration;Network analyzers;Reflection;Data models;lesson study;collaborative reflection;teachers’ collaborative discourse;epistemic network analysis},
  doi={10.1109/CSTE64638.2025.11092295},
  ISSN={},
  month={April},}@ARTICLE{9284628,
  author={Muhammad, Khan and Ullah, Amin and Lloret, Jaime and Ser, Javier Del and de Albuquerque, Victor Hugo C.},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Deep Learning for Safe Autonomous Driving: Current Challenges and Future Directions}, 
  year={2021},
  volume={22},
  number={7},
  pages={4316-4336},
  abstract={Advances in information and signal processing technologies have a significant impact on autonomous driving (AD), improving driving safety while minimizing the efforts of human drivers with the help of advanced artificial intelligence (AI) techniques. Recently, deep learning (DL) approaches have solved several real-world problems of complex nature. However, their strengths in terms of control processes for AD have not been deeply investigated and highlighted yet. This survey highlights the power of DL architectures in terms of reliability and efficient real-time performance and overviews state-of-the-art strategies for safe AD, with their major achievements and limitations. Furthermore, it covers major embodiments of DL along the AD pipeline including measurement, analysis, and execution, with a focus on road, lane, vehicle, pedestrian, drowsiness detection, collision avoidance, and traffic sign detection through sensing and vision-based DL methods. In addition, we discuss on the performance of several reviewed methods by using different evaluation metrics, with critics on their pros and cons. Finally, this survey highlights the current issues of safe DL-based AD with a prospect of recommendations for future research, rounding up a reference material for newcomers and researchers willing to join this vibrant area of Intelligent Transportation Systems.},
  keywords={Roads;Task analysis;Safety;Automobiles;Accidents;Vehicles;Lane detection;Autonomous driving (AD);artificial intelligence;deep learning (DL);decision making;vehicular safety;vehicular technology;intelligent sensors},
  doi={10.1109/TITS.2020.3032227},
  ISSN={1558-0016},
  month={July},}@ARTICLE{10111025,
  author={Li, Heqing and Li, Xinde and Zhang, Zhentong and Hu, Chuanfei and Dunkin, Fir and Ge, Shuzhi Sam},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={ESUAV-NI: Endogenous Security Framework for UAV Perception System Based on Neural Immunity}, 
  year={2024},
  volume={20},
  number={1},
  pages={732-743},
  abstract={Unmanned aerial vehicles (UAVs) represent an essential component of advanced intelligent equipment that can be used as an aerial perception system by installing various sensors such as vision, hearing, touch, taste, and smell to achieve intelligently integrated perception of environments. However, these perception system with environmental information may be threatened by various internal and external attacks, causing a great challenge to the security of the UAV. The original security system relied on an expert knowledge base to prevent attacks, but the weaknesses of lacking proactivity and flexibility are gradually exposed. The strong resistance and survivability of biological systems can be used to fill this capability gap and provide new ideas for the security of the UAV perception system. Therefore, an endogenous security framework (ESUAV-NI) based on the neural system and immune system is proposed in this article. Through breeding artificial intelligence (AI) vaccines and distributed neural hierarchical control, we achieve the security protection for the UAV perception system. Moreover, we evaluated the AI vaccine breeding approach in the ESUAV-NI by conducting extensive experiments on internal threats and external aerial imagery camouflage data, respectively. The results show that the proposed approach has a superior performance for the UAV perception system.},
  keywords={Security;Immune system;Autonomous aerial vehicles;Antibodies;Control systems;Neurons;Nervous system;Artificial intelligence (AI) vaccines;endogenous security;security threats;unmanned aerial vehicles (UAVs)},
  doi={10.1109/TII.2023.3271443},
  ISSN={1941-0050},
  month={Jan},}@ARTICLE{9753685,
  author={Hao, Wangli and Guan, He and Zhang, Zhaoxiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={VAG: A Uniform Model for Cross-Modal Visual-Audio Mutual Generation}, 
  year={2025},
  volume={36},
  number={3},
  pages={4196-4208},
  abstract={Considering both audio and visual modalities is helpful for understanding a video. In the face of harsh environmental interference or signal packet loss, automatically compensating for audio and vision is a challenging task. We propose a dynamic cross-modal visual-audio mutual generation model (VAMG), which includes audio to visual conversion, visual to audio conversion, audio self-generation, and visual self-generation. VAMG jointly optimizes modal reconstruction and adversarial constraints, effectively solving the problems of structural alignment and signal compensation in incomplete videos. We conducted an instrument-oriented and pose-oriented cross-modal audio-visual mutual generation experiment on the sub-University of Rochester Musical Performance dataset to verify the effectiveness of the model.},
  keywords={Task analysis;Instruments;Visualization;Image reconstruction;Generators;Decoding;Generative adversarial networks;Cross modality;cross-modal generation;mutual generation;visual and audio},
  doi={10.1109/TNNLS.2022.3161314},
  ISSN={2162-2388},
  month={March},}@INPROCEEDINGS{9812396,
  author={Mo, Fan and Ji, Xiaoqiang and Qian, Huihuan and Xu, Yangsheng},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)}, 
  title={A User-customized Automatic Music Composition System}, 
  year={2022},
  volume={},
  number={},
  pages={640-645},
  abstract={This paper introduces an intelligent system which composes music following the users' instructions. Current auto-matic music generation models are lack of stability. Meanwhile, they cannot satisfy the preference of different people. To overcome these challenges, we train a Transformer-based neural network to generate short music segments using a dataset. A user can compose music pieces by interacting with a well-trained generator. Our system collects the user's feedback during the interactions, and fine-tunes the neural network to optimize the generator. After a large number of interactions, our system can learn the musical taste of the user and customize a personal automatic music composer for him or her. Our work enhances the application value of generative models significantly, which enables people to compose music with the assistance of artificial intelligence.},
  keywords={Automation;Neural networks;Music;Entertainment industry;Transformers;Generators;Stability analysis},
  doi={10.1109/ICRA46639.2022.9812396},
  ISSN={},
  month={May},}@ARTICLE{10979418,
  author={Jiang, Fenlong and Huo, Xinlong and Zhang, Mingyang and Gong, Maoguo and Pu, Yan and Zhou, Yu and Zhao, Wei and Guan, Ziyu},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={D3PM: Dual-Stream Denoising Diffusion Probabilistic Model for Change Detection in Multimodal Remote Sensing Images}, 
  year={2025},
  volume={63},
  number={},
  pages={1-15},
  abstract={Detecting land cover changes from multitemporal and multimodal remote sensing images acquired by different sensors at the same location is a complex yet highly valuable task. Recently, diffusion models, exemplified by the denoising diffusion probabilistic model (DDPM), have garnered significant attention for their remarkable performance and straightforward architecture. These models excel in image generation, distribution modeling, and feature extraction, making them highly promising for advancing multimodal change detection (MCD). In this article, we propose a dual-stream DDPM (D3PM) to address the challenges of MCD. Specifically, D3PM leverages DDPM to design two distinct processing streams, one for each image modality. The first stream employs an unconditional DDPM, whose denoising encoder-decoder network can achieve robust feature extraction. The second stream employs a conditional DDPM to facilitate modal translation, enabling the extracted features to align with the characteristics of the other modality, thereby improving cross-modality comparability. To further enhance performance, we constructed a CD task branch based on the decoder features of the two DDPMs across multiple denoising time steps. In addition, we designed a collaborative learning optimization strategy with asynchronous time steps, fostering cross-task knowledge sharing and mutual enhancement while preserving the integrity of individual task learning. Experimental results on multiple public datasets demonstrate the effectiveness and superiority of the proposed D3PM, which achieves efficient modal transformation and alignment, mitigates modal heterogeneity interference, and significantly improves detection performance.},
  keywords={Feature extraction;Diffusion models;Noise reduction;Translation;Training;Streams;Multitasking;Data models;Data mining;Computer vision;Change detection (CD);denoising diffusion probabilistic model (DDPM);multimodal remote sensing images;multitask learning},
  doi={10.1109/TGRS.2025.3564959},
  ISSN={1558-0644},
  month={},}@ARTICLE{10916737,
  author={Zhang, Sijia and Gong, Maoguo and Lin, Yiming and Li, Hao and Gao, Yuan and Zhang, Yihong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dual Distillation Fusion for Weakly Supervised Anomaly Detection in Surveillance Videos}, 
  year={2025},
  volume={35},
  number={8},
  pages={7512-7525},
  abstract={Anomaly detection in surveillance videos aims to differentiate anomalies from regular events by discriminative representations, which has gathered considerable attention due to its significant effect to public security. However, most existing works are limited in the lack of annotated samples, and lots of approaches find it challenging to avoid the well-reconstruction of anomalous data. To alleviate these issues, we propose a dual distillation fusion framework for weakly supervised anomaly detection. We reformulate the anomaly detection problem into two steps, namely filtering anomalies and inpainting normal patterns. Each step corresponds to one branch of the dual distillation. Specifically, the dual distillation comprises the contrastive distillation module and the inpainting distillation module. The contrastive distillation optimizes the encoder to filter out abnormal features and capture key normal features, while the inpainting distillation refines the decoder to inpaint normal patterns on the encoded features. The contrastive distillation module and the inpainting distillation module are optimized iteratively in a self-training manner with video-level labeled data. Moreover, a joint optimization module is devised to effectively fuse the distilled encoder and decoder, thereby collectively improving the anomaly detection performance. During the training phase, we take into account the diversity of normal samples by selecting pseudo normal and abnormal samples with high confidence from abnormal videos. These selected samples, along with original normal frames, are then fed into the subsequent training iterations to enhance the distinguishing ability of the model. Experimental results show that our proposed method performs competitively on five benchmark datasets.},
  keywords={Anomaly detection;Videos;Training;Feature extraction;Data models;Training data;Decoding;Optimization;Knowledge engineering;Computational modeling;Dual distillation;anomaly detection;weak supervision},
  doi={10.1109/TCSVT.2025.3549144},
  ISSN={1558-2205},
  month={Aug},}@ARTICLE{10251686,
  author={Xie, Charles and Ding, Xiaotong and Jiang, Rundong},
  journal={IEEE Computer Graphics and Applications}, 
  title={Using Computer Graphics to Make Science Visible in Engineering Education}, 
  year={2023},
  volume={43},
  number={5},
  pages={99-106},
  abstract={Science plays a crucial role in engineering. But science tends to be obscure to students, especially when they are overwhelmed by complex engineering design challenges that involve many variables. This article shows how computer graphics can be used to visualize science concepts and operationalize inquiry practices in engineering design to support integrated learning and teaching of science and engineering. Based on these graphical capabilities, generative design driven by evolutionary computation can also be visually illustrated to give students a glimpse into how artificial intelligence is transforming engineering design. The article provides real-world examples in the field of sustainable energy engineering based on Aladdin, an open-source design and analysis Web app. It also presents evidence of learning from pilot tests at culturally diverse high schools. Science educators interested in incorporating engineering design into their lesson plans may find this article helpful.},
  keywords={Industries;Visualization;Codes;Affordances;Evolutionary computation;Design tools;Artificial intelligence},
  doi={10.1109/MCG.2023.3298386},
  ISSN={1558-1756},
  month={Sep.},}@INBOOK{10953025,
  author={Lopez-Lira, Alejandro},
  booktitle={The Predictive Edge: Outsmart the Market using Generative AI and ChatGPT in Financial Forecasting}, 
  title={Understanding the Stock Market}, 
  year={2024},
  volume={},
  number={},
  pages={3-25},
  abstract={Summary <p>The stock market is crucial in modern financial markets. It helps generate wealth, reflects economic health, and drives growth. This chapter explores the stock market's function and history. It examines different financial markets, each with a distinct role in the global financial ecosystem and some investment vehicles. The chapter explains portfolio management and trading strategies and discusses how to build and manage a diversified investment portfolio. It overviews various investment approaches, from passive investment to value and growth investing. Understanding the basics of portfolio management and trading strategies is fundamental to investing in the stock market. One of the core principles of portfolio management is diversification, which involves spreading investments across distinct assets or asset classes to mitigate risk. Value investing is an active strategy involving buying undervalued stocks at lower prices than their intrinsic value. The chapter also discusses the fundamentals of trading, which involve buying and selling stocks through a broker for profit.</p>},
  keywords={Stock markets;Investment;Companies;Economics;Instruments;Portfolios;Economic indicators;Business;Currencies;Artificial intelligence},
  doi={10.1002/9781394308286.ch1},
  ISSN={},
  publisher={Wiley},
  isbn={9781394242733},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953025},}@INPROCEEDINGS{10873161,
  author={Yin, Xuanyi},
  booktitle={2024 International Conference on Artificial Intelligence, Deep Learning and Neural Networks (AIDLNN)}, 
  title={Deep drug molecule generation model based on skeleton structure guidance and hybrid attention mechanism}, 
  year={2024},
  volume={},
  number={},
  pages={107-112},
  abstract={In this paper, we propose a deep generative model based on skeleton and considering stereochemical information of molecules, which characterizes molecules in the form of molecular diagrams, and generates new molecules by adding atoms and bonds sequentially to the skeleton diagrams using molecular skeleton and physicochemical properties as constraints. The model is capable of controlling multiple physicochemical properties of the molecule when generating the molecule, and has a multi-objective optimization function. In terms of model architecture: we use CVAE containing GRU to build the model, which consists of three parts: encoder (inference network), a priori network and decoder (generation network). In addition, in order to improve the efficiency and accuracy of the model, we combine a hybrid attention mechanism in the model: self-attention mechanism and external attention mechanism. The attention mechanism can automatically find the most informative feature points in the feature graph. Specifically, the attention mechanism can learn a task-oriented weighted feature graph and use it for subsequent representation learning. The validation results of the model performance show that: in terms of validity, uniqueness and novelty, the model has better validity and novelty than the pair-group model and good generalization ability; in terms of diversity, the diversity of the new molecules generated by the model is better than that of the molecules in the test set.},
  keywords={Drugs;Deep learning;Representation learning;Solid modeling;Attention mechanisms;Shape;Neural networks;Atoms;Skeleton;Optimization;Ddeep learning;skeletal structure;CVAE;self-attention mechanism;external attention mechanisms},
  doi={10.1109/AIDLNN65358.2024.00025},
  ISSN={},
  month={Sep.},}@INBOOK{10952552,
  author={Lopez-Lira, Alejandro},
  booktitle={The Predictive Edge: Outsmart the Market using Generative AI and ChatGPT in Financial Forecasting}, 
  title={How AI Is Shaping Our Economic Future}, 
  year={2024},
  volume={},
  number={},
  pages={191-210},
  abstract={Summary <p>In this book's conclusion, the authors begin by summarizing their learning journey, emphasizing the core insights and knowledge the readers acquired, and reflecting on the practical takeaways. Then, they explore AI's potential future developments. Finally, the authors learn how to continue participating in the ongoing conversation about AI and stay updated on the latest developments. Developing a ChatGPT&#x2010;driven trading strategy begins with understanding AI capabilities in sentiment analysis. ChatGPT can analyze vast amounts of news and extract trading signals by identifying market sentiment shifts. The book also explores the future of AI in financial forecasting, including deep learning, reinforcement learning, and the fusion of AI with blockchain and IoT. These advancements promise greater accuracy, efficiency, and breadth in financial decision&#x2010;making, positioning AI as an indispensable finance tool.</p>},
  keywords={Artificial intelligence;Chatbots;Biological system modeling;Adaptation models;Large language models;Investment;Finance;Ethics;Predictive models;Data models},
  doi={10.1002/9781394308286.ch10},
  ISSN={},
  publisher={Wiley},
  isbn={9781394242733},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952552},}@INBOOK{10951614,
  author={Lopez-Lira, Alejandro},
  booktitle={The Predictive Edge: Outsmart the Market using Generative AI and ChatGPT in Financial Forecasting}, 
  title={ChatGPT in Action: Practical Applications}, 
  year={2024},
  volume={},
  number={},
  pages={157-173},
  abstract={Summary <p>Integrating AI tools like ChatGPT into investment strategies in financial markets represents a significant leap. In this chapter, the authors explore ChatGPT's role in financial forecasting and investment decision&#x2010;making. They also explore how these insights can seamlessly integrate into traditional and quantitative investment approaches. The authors study how risk management, a critical component of any investment strategy, benefits from ChatGPT's predictive capabilities. By analyzing market sentiment trends, ChatGPT can provide early warnings about potential negative shifts in the market, allowing us to take preemptive measures to safeguard our investments. The authors examine the role of ChatGPT in automated trading systems. These systems make investment decisions algorithmically, based on various data inputs, including sentiment analysis. ChatGPT's ability to analyze market mood in real time and with high accuracy can significantly enhance the effectiveness of these systems, leading to more informed and timely trading decisions.</p>},
  keywords={Chatbots;Investment;Risk management;Sentiment analysis;Portfolios;Market research;Social networking (online);Artificial intelligence;Real-time systems;Predictive models},
  doi={10.1002/9781394308286.ch8},
  ISSN={},
  publisher={Wiley},
  isbn={9781394242733},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951614},}@INBOOK{10951013,
  author={Baker, Pam},
  booktitle={Generative AI For Dummies}, 
  title={Upholding Responsible AI Standards in GenAI Use}, 
  year={2025},
  volume={},
  number={},
  pages={241-254},
  abstract={Summary <p>This chapter helps the readers to navigate the exciting yet challenging landscape of GenAI without sacrificing ethical standards and quality. To ensure the originality and quality of GenAI creations, it's essential to implement strategies that can verify the uniqueness and accuracy of the content produced. The foundation of GenAI's output quality lies in the data it's trained on. GenAI models should be evaluated regularly against quality benchmarks, which can include both automated metrics and human evaluation to gauge the relevance, coherence, and originality of the outputs. Applying journalism ethics to GenAI&#x2010;generated content is a sound and useful way to consistently make sure its responses meet high standards and comply with ethical rules. Sticking to the principles of truth and integrity is more crucial than ever when using GenAI tools like ChatGPT for reporting information or in content generation. The AI movement emphasizes the importance of inclusivity in AI development.</p>},
  keywords={Watermarking;Metadata;Artificial intelligence;Standards;Ethics;Reviews;Predictive models;Plagiarism;Copyright protection;Accuracy},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394270767},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951013},}@INBOOK{10955671,
  author={Subramanian, Shreyas},
  booktitle={Large Language Model-Based Solutions: How to Deliver Value with Cost-Effective Generative AI Applications}, 
  title={Model Selection and Alternatives}, 
  year={2024},
  volume={},
  number={},
  pages={89-122},
  abstract={Summary <p>This chapter delves into the intricacies of model selection, focusing on the trade&#x2010;offs between compact, nimble models and their larger counterparts, the emergence of domain&#x2010;specific models, and the criticality of inference hyperparameter optimization. It first discusses the development and application of compact and nimble models, emphasizing their importance in certain contexts. The chapter then explores domain&#x2010;specific models, highlighting how tailoring models to specific industries can enhance performance and relevance. It delves into the optimization of inference hyperparameters, a crucial step in refining model performance regardless of size. The success of models such as Mistral and Orca 2 has significant implications for the future of language model development. It demonstrates the potential of smaller models in scenarios where a balance between efficiency and capability is crucial. The chapter shows how general&#x2010;purpose models when prompted well with dynamic few shots can also perform domain&#x2010;specific models.</p>},
  keywords={Quantization (signal);Computational modeling;Data models;Costs;Artificial intelligence;Analytical models;Transformers;Adaptation models;Accuracy;Real-time systems},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394240746},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10955671},}@INBOOK{10950674,
  author={Khan, Ian},
  booktitle={The Quick Guide to Prompt Engineering: Generative AI Tips and Tricks for ChatGPT, Bard, Dall-E, and Midjourney}, 
  title={Prompt Engineering across Industry}, 
  year={2024},
  volume={},
  number={},
  pages={63-81},
  abstract={Summary <p>Prompt engineering, an integral component of the burgeoning AI domain, offers novel possibilities to the world of creative writing. With the rise of AI, prompt engineering emerges as a game changer, offering businesses a competitive edge through nuanced, AI&#x2010;guided solutions. This chapter delves into how prompt engineering is reshaping various business operations. With the advent of AI, and especially prompt engineering, the media landscape is undergoing a transformative shift, leveraging technology for enhanced storytelling and informed reportage. The convergence of AI with prompt engineering is opening doors to a more streamlined, personalized, and data&#x2010;driven approach in patient care and medical research. With the merging of prompt engineering and AI, urban planning and public policy are undergoing significant enhancements, pushing the envelope for smarter cities. In the world of finance and economics, prompt engineering melds with AI to offer a paradigm shift in decision&#x2010;making, risk management, and market analysis.</p>},
  keywords={Prompt engineering;Artificial intelligence;Business;Law;Writing;Navigation;Data analysis;Urban planning;Solid modeling;Resource management},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394243341},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950674},}@ARTICLE{10634040,
  author={Zhang, Songyang and Choi, Brian and Ouyang, Feng and Ding, Zhi},
  journal={IEEE Communications Magazine}, 
  title={Physics-Inspired Machine Learning for Radiomap Estimation: Integration of Radio Propagation Models and Artificial Intelligence}, 
  year={2024},
  volume={62},
  number={8},
  pages={155-161},
  abstract={Radiomap captures the geometrical distribution of radio frequency signal power. As an important tool to qualitatively and quantitatively describe radio propagation behavior and spectrum occupancy, radiomap has found broad applications in deployment and configuration of modern wireless and Internet-of-things networks. Practically, a high-resolution radiomap can be reconstructed from partial or sparse observations collected by mobile devices or sensors. To leverage the power of radiomap, efficient radiomap reconstruction from sparse samples has emerged as an urgent challenge. To capture the underlying data statistics, and also to estimate the physical radio frequency models, this work introduces three types of physics-inspired machine learning approaches to radiomap reconstruction. The experimental results demonstrate the potential of integrating data-driven artificial intelligence with model-based radio propagation behavior for radiomap reconstruction.},
  keywords={Wireless communication;Representation learning;Wireless sensor networks;Accuracy;Estimation;Radio propagation;Data models},
  doi={10.1109/MCOM.001.2300782},
  ISSN={1558-1896},
  month={August},}@INPROCEEDINGS{10317853,
  author={Tarabin, Mohamad and Alketbi, Mariam Mohamed and Alfalasi, Hamad Rashed and Alsmirat, Mohammad and Sharrab, Yousef},
  booktitle={2023 Fourth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)}, 
  title={Detecting Distracted Drivers Using Convolutional Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={59-66},
  abstract={Driver distraction has become a major concern for road safety as distracted driving is a leading cause of car accidents, resulting in a large number of injuries and fatalities every year. This project aims to address the issue of driver distraction as a leading cause of traffic accidents worldwide. Utilizing various image datasets, the project explores the use of Convolutional Neural Networks (CNNs), specifically the DarkNet53 model, to classify whether drivers are focused on the road or distracted. A collection of publicly available datasets and a new dataset consisting of 4024 images produced by the authors were used in training the model, with image augmentation being used to significantly enhance the model performance. The training and testing processes were conducted using MATLAB and different techniques such as transfer learning and layer freezing were utilized to improve the training process. The real-time performance of the DarkNet53 model was also evaluated against other models and it was shown that the DarkNet53 had adequate performance while maintaining a high level of accuracy. Furthermore, the model misclassifications were evaluated using confusion matrices and different statistics such as the F1 score were reported. The study concludes with a recommendation for further development of datasets to train more accurate models without sacrificing real-time performance. The datasets should be generalized, including different lighting conditions and camera angles.},
  keywords={Training;Transfer learning;Mathematical models;Real-time systems;Road safety;Convolutional neural networks;Vehicles;Distracted Driver;Convolutional Neural Networks;CNN;Computer Vision},
  doi={10.1109/IDSTA58916.2023.10317853},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10596205,
  author={Ammous, Donia and Kammoun, Fahmi and Masmoudi, Nouri},
  booktitle={2024 IEEE International Conference on Advanced Systems and Emergent Technologies (IC_ASET)}, 
  title={Artificial Intelligence in Age Estimation: a Comparative Study}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Age estimation is a curcial learning problem in computer vision. Many larger and deeper CNNs (convolutional neural network) have been proposed with promising performance, such as AlexNet, VggNet, GoogLeNet and ResNet. However, some models are not practical for the embedded/mobile devices. Age estimation is labeling a face image with exact real age or age group: In this paper, we present a thorough analysis of recent research in age estimation. We present the common algorithms used for age estimation, existing models, and how they compare with each other. We also compare the performance of different AI (Artificial Intelligence) systems and how they are evaluated.},
  keywords={Performance evaluation;Deep learning;Computer vision;Machine learning algorithms;Computational modeling;Estimation;Prediction algorithms;Age estimation;Facial image;Models;Feature;Classification;computer vision;deep learning;face detection},
  doi={10.1109/IC_ASET61847.2024.10596205},
  ISSN={},
  month={April},}@INPROCEEDINGS{10889811,
  author={Xing, Ke and Dong, Yanjie and Hu, Xiping and Leung, Victor C. M. and Deen, M. Jamal and Guo, Song},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Privacy-Aware Federated Fine-Tuning of Large Pretrained Models With Just Forward Propagation}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={With the extraordinary success of generative artificial intelligence, large pretrained models (LPMs) have been widely used to achieve human-level performance. Despite the one-shot capability, it is always preferred to fine-tune the LPMs for domain-specific downstream tasks. Therefore, the federated learning system is leveraged to fine-tune the large pretrained models enabling concurrrently use multiple distributed clients as well as their local datasets. While the first-order fine-tuning methods suffer from high computational and memory costs due to the backward propagation, we are motivated to propose a federated zeroth-order fine-tuning method with only forward propagation. Moreover, we also leverage differential privacy to further preserve the data privacy of local clients. Experimental results illustrate that our proposed federated zeroth-order method can reduce the memory and retain a similar testing accuracy over the state-of-the-art benchmarks.},
  keywords={Differential privacy;Costs;Generative AI;Federated learning;Computational modeling;Graphics processing units;Signal processing;Numerical models;Servers;Speech processing;Federated learning;zeroth-order optimization;differential privacy;parameter fine-tuning},
  doi={10.1109/ICASSP49660.2025.10889811},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10940154,
  author={Chitra, N. Thulasi and Gogula, Sreenivasulu and Satapathy, Sukla},
  booktitle={2025 International Conference on Electronics and Renewable Systems (ICEARS)}, 
  title={A Survey of Advanced Algorithms and Experimental Approaches for Kidney Abnormality Detection in Ultrasound Imaging}, 
  year={2025},
  volume={},
  number={},
  pages={1838-1845},
  abstract={Kidney abnormalities, such as cysts, tumors, and stones, are critical health concerns requiring early and precise detection. This study reviews advanced algorithms and experimental approaches for kidney abnormality detection in ultrasound imaging, emphasizing deep learning (DL) methodologies and explainable AI (XAI) techniques. Contributions include exploring CNN-based segmentation frameworks like SDFNet and U-Net, multi-scale feature fusion, and transfer learning to enhance diagnostic accuracy. Data augmentation and adversarial training are employed to address dataset bias, while Grad-CAM and CAManim provide interpretability, fostering clinician trust. Results demonstrate significant improvements in diagnostic performance, with sensitivity and specificity exceeding 95% and Dice coefficients surpassing 95% on diverse clinical datasets. The findings highlight the transformative potential of DL to automate kidney abnormality detection, reducing dependency on operator expertise while improving accuracy and efficiency. Challenges such as dataset scarcity and computational complexity persist, but future research on multimodal imaging, CNN optimization, and emerging technologies like quantum machine learning presents promising opportunities. This study serves as a guide for advancing diagnostic tools, paving the way for next-generation solutions in clinical applications.},
  keywords={Deep learning;Training;Image segmentation;Ultrasonic imaging;Accuracy;Explainable AI;Transfer learning;Imaging;Kidney;Tumors;Kidney abnormalities;deep learning;ultrasound imaging;explainable AI;segmentation frameworks},
  doi={10.1109/ICEARS64219.2025.10940154},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11136244,
  author={Kanam, Sathvik and Mazumder, Debarshi and Al-Asady, Heba Abdul-Jaleel and Kumaraswamy, I. and M, Dhanamalar},
  booktitle={2025 International Conference on Intelligent Computing and Knowledge Extraction (ICICKE)}, 
  title={Comprehensive Analysis of Glaucoma Detection using Machine Learning and Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Glaucoma is an insidious and progressive eye condition that finally end up causing blindness in the patient. Traditional methods of screening are conspicuous for their dependence on the judgment of a clinician, which possibly miss the finer early signs. With the evolution of AI, specifically deep learning, researchers now utilize retinal fundus and OCT images to construct more objective and accurate diagnostic apparatuses. This review seeks to introduce some of the recent approaches involving machine learning, deep neural network architectures like U-Net, ResNet, and DenseNet, and Explainable AI tools such as SHAP and Grad-CAM, to highlight the affected regions of the eye so that the decision-making of the AI becomes more explainable and trustworthy. Although various papers are demonstrating high accuracy and promising results, problems remain in terms of limited datasets, variability of image quality, and computational intensity. Nevertheless, these AI-based methods provide a glimmer of hope for the future of the faster, earlier, and more accessible detection of glaucoma, particularly in deprived areas where routine eye care accessible.},
  keywords={Glaucoma;Deep learning;Training;Accuracy;Computational modeling;Computer architecture;Blindness;Retina;Load modeling;Testing;computer-aided diagnosis;deep learning;glaucoma;image classification;image segmentation;machine learning;medical imaging;retinal fundus imaging},
  doi={10.1109/ICICKE65317.2025.11136244},
  ISSN={},
  month={June},}@INPROCEEDINGS{10988433,
  author={Soni, Dhruv Kumar and Taneja, Ashu and Kumar, Komuravelly Sudheer and Salami, Zaeid Ajsan and Goel, Mohit Kumar},
  booktitle={2025 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={Automated Detection of Poultry Diseases Using Deep Learning and Computer Vision}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Early, accurate detection of chicken diseases is necessary for promoting sustainable farming techniques, enhancing poultry health, and lowering of economic losses. This paper presents a deep learning-based method using transfer learning with the EfficientNetB5 architecture to classify chicken health issues into four categories: healthy, Disease A, Disease B, and Disease C using photo data. Using a powerful training pipeline and thorough evaluation, the model not only achieved remarkable overall accuracy of 97% by surpassing 93% across all classes but also high precision, recall, and F1-scores. Closely inspection hhhkof the confusion matrix and classification metrics confirmed that the method could effectively separate visually similar but complex disease kinds. Analyzing the deployment alternatives of this model also addressed concerns such hardware constraints, scalability, and the need of real-time inference to enable useful use in chicken farms. This work presents a noninvasive, efficient diagnostic method to help early intervention and disease control in chicken production as well as showing how well advanced convolutional neural networks address significant agricultural challenges. The findings of this study suggest to a transformational route towards adding AI-driven solutions in agriculture, therefore guaranteeing better animal welfare, more farm productivity, and a sustainable future for chicken health management.},
  keywords={Deep learning;Visualization;Accuracy;Animals;Scalability;Computational modeling;Transfer learning;Convolutional neural networks;Diseases;Farming;Chicken disease detection;deep learning;EfficientNetB5;transfer learning;poultry health management;agricultural AI;precision agriculture;convolutional neural networks;disease diagnosis;animal health},
  doi={10.1109/ESCI63694.2025.10988433},
  ISSN={2996-1815},
  month={March},}@INPROCEEDINGS{10821814,
  author={Deng, Yushan and Jiang, Yuqiu and Hong, Yi},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={DiffLGE: Diffusion-Based Image Generation for Synthesizing Late Gadolinium Enhancement Scans}, 
  year={2024},
  volume={},
  number={},
  pages={6990-6997},
  abstract={In recent years, late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging synthesis using deep learning has gained attention for generating high-quality LGE images without relying on gadolinium-based contrast agents (GBCA). However, traditional methods like GANs struggle with issues such as unstable training and mode collapse, making them less reliable for clinical applications. Diffusion models have emerged as a promising alternative, offering more stable training and the capability to generate high-resolution, realistic images. In this study, we address the challenge of effectively integrating multiple noninvasive imaging modalities for LGE synthesis by proposing DiffLGE, a novel method based on diffusion models that leverages both CINE and T1 mapping data. Utilizing the stable diffusion framework, we condition the generative network on CINE and T1 mapping inputs to achieve controlled and stable synthesis. Furthermore, we integrate a VideoMAE encoder to extract and enhance CINE features, which are often challenging to incorporate due to their dynamic nature. Trained on a private dataset of 240 samples from 83 subjects, our approach outperforms existing image generation methods, producing high-quality LGE images and offering a more reliable and accurate solution for LGE image synthesis.},
  keywords={Training;Deep learning;Contrast agents;Image synthesis;Biological system modeling;Magnetic resonance;Gadolinium;Feature extraction;Diffusion models;Reliability;LGE Image Generation;Conditional Diffusion Model;CINE and T1 mapping images},
  doi={10.1109/BIBM62325.2024.10821814},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10776221,
  author={Xie, Zhige and Xu, Hao and Yang, Chenguang and Lei, Zhao and Li, Jiaxin and Liu, Aizhi},
  booktitle={2024 International Conference on New Trends in Computational Intelligence (NTCI)}, 
  title={Research on Training Scenario Assisted Design Method Based on Domain-Adapted Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={150-154},
  abstract={In recent years, the generative artificial intelligence technology has developed rapidly, showing great application potential in the field of training. Training scenario design requires familiarity with relevant training concepts, equipment performance, force deployment, force action and other information, and it is difficult and inefficient to rely solely on manual work. In view of this issue, we propose training scenario assisted design method based on domain-adapted language model. First, we build the training scenario question and answer database to fine-tune the large language model, then use chain of thought and retrieval-augmented to accelerate the process of training scenario design. The experimental results show that the proposed method can quickly generate the training scenario summary document and assist the relevant personnel to complete the simulation scenario design efficiently.},
  keywords={Training;Generative AI;Design methodology;Computational modeling;Large language models;Force;Manuals;Market research;Software;Personnel;domain-adapted language model;natural language processing;training scenario design;chain of thought},
  doi={10.1109/NTCI64025.2024.10776221},
  ISSN={},
  month={Oct},}@ARTICLE{10288434,
  author={Tara, Andrei and Turesson, Hjalmar K. and Natea, Nicolae and Kim, Henry M.},
  journal={IEEE Access}, 
  title={An Evaluation of Storage Alternatives for Service Interfaces Supporting a Decentralized AI Marketplace}, 
  year={2023},
  volume={11},
  number={},
  pages={116919-116931},
  abstract={Given the exploding interest in generative AI and the concern that a few companies like Microsoft will monopolize access to such models, we address this centralization risk in the context of a DApp that matches buyers and sellers of various AI services. A key question for a decentralized marketplace is where and how to store the metadata that specifies the services’ properties in human and machine-readable formats. Having one or a few actors controlling access to that data constitutes undesirable centralization. We explore data storage alternatives to ensure decentralization, equitable match-making, and efficiency. Classifying decentralized storage alternatives as simple peer-to-peer replication, replication governed by a permissionless consensus, and replication governed by a private consensus, we select an exemplar for each category: IPFS, Tendermint Cosmos and Hyperledger Fabric. We conduct experiments on performance and find that read and write speeds are fastest for IPFS, about two times slower for Tendermint and slowest for Hyperledger. Writing using IPFS and Tendermint takes significantly longer than reading, and finally, specifically with IPFS, write speeds strongly depend on configuration. Given these results and the properties of the storage technologies, we conclude that simple peer-to-peer storage is the best option for the proposed AI marketplace.},
  keywords={Artificial intelligence;Ontologies;Peer-to-peer computing;Blockchains;Distributed ledger;Resource description framework;Fabrics;Decentralized applications;Storage management;Blockchain;decentralized AI decentralized storage;distributed ontology;semantic models},
  doi={10.1109/ACCESS.2023.3326418},
  ISSN={2169-3536},
  month={},}@INBOOK{10951486,
  author={Grill, Andrew},
  booktitle={Digitally Curious: Your Guide to Navigating the Future of AI and All Things Tech}, 
  title={FROM TURING TO TRANSFORMERS}, 
  year={2025},
  volume={},
  number={},
  pages={29-38},
  abstract={Summary <p>This chapter reviews what has come so far and make some broad predictions about what will likely happen over the next few months. It presents a short history lesson about artificial intelligence (AI), from Turing to transformers. The phrase &#x201c;artificial intelligence&#x201d; describes the field of computer science focused on creating machines that can mimic human&#x2010;like behaviour. What makes an AI more or less advanced, or a specific tool better or worse at a given task, is how it &#x201c;thinks&#x201d;. In machine learning, the AI tool needs to organise the data before it can do anything with it. Deep learning, on the other hand, is a subset of machine learning designed to let a machine train itself to do a task. Generative AIs are a category of AI tools that can apply advanced techniques like deep learning to create things.</p>},
  keywords={Artificial intelligence;Deep learning;Chatbots;History;Speech recognition;Automobiles;Transformers;Navigation;Internet;Companies},
  doi={10.1002/9781394309399.ch3},
  ISSN={},
  publisher={Wiley},
  isbn={9781394217014},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951486},}@ARTICLE{10960301,
  author={Haffar, Rami and Sánchez, David and Domingo-Ferrer, Josep},
  journal={IEEE Access}, 
  title={Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks}, 
  year={2025},
  volume={13},
  number={},
  pages={63827-63840},
  abstract={Human facial data offers valuable potential for tackling classification problems, including face recognition, age estimation, gender identification, emotion analysis, and race classification. However, recent privacy regulations, particularly the EU General Data Protection Regulation, have restricted the collection and usage of human images in research. As a result, several previously published face data sets have been removed from the internet due to inadequate data collection methods and privacy concerns. While synthetic data sets have been suggested as an alternative, they fall short of accurately representing the real data distribution. Additionally, most existing data sets are labeled for just a single task, which limits their versatility. To address these limitations, we introduce the Multi-Task Face (MTF) data set, designed for various tasks including face recognition and classification by race, gender, and age, as well as for aiding in training generative networks. The MTF data set comes in two versions: a non-curated set containing 132,816 images of 640 individuals, and a manually curated set with 5,246 images of 240 individuals, meticulously selected to maximize their classification quality. Both data sets were ethically sourced, using publicly available celebrity images in full compliance with copyright regulations. Along with providing detailed descriptions of data collection and processing, we evaluated the effectiveness of the MTF data set in training five deep learning models across the aforementioned classification tasks, achieving up to 98.88% accuracy for gender classification, 95.77% for race classification, 97.60% for age classification, and 79.87% for face recognition with the ConvNeXT model. Both MTF data sets can be accessed through the following link. https://github.com/RamiHaf/MTF_data_set},
  keywords={Faces;Data models;Training;Ethics;Artificial intelligence;Regulation;General Data Protection Regulation;Face recognition;Europe;Law;Face images;image data set;image classification;deep learning},
  doi={10.1109/ACCESS.2025.3559310},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10062441,
  author={Zhao, Dongdong and Wang, Zhao and Li, Huanhuan and Xiang, Jianwen},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={GAN-Based Privacy-Preserving Unsupervised Domain Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={117-126},
  abstract={In recent years, the rapid development of deep learning is attributed to the large amount of labeled data brought by the digital age. When there is no labeled data available in some application scenarios, domain adaptation can be used to transfer knowledge from the source domain with labeled data to the target domain without labeled data. In the process of domain adaptation, the target client requires direct access to the source data or model, which would lead to the risk of privacy leakage, e.g., Membership Inference Attacks (MIA). Attackers can collect the prediction vector of the model through black-box access to the source model, and then infer an individual’s membership in the source training dataset. To deal with this privacy issue, we propose a GAN-based Privacy-Preserving Unsupervised Domain Adaptation Framework. Specifically, the target client learns a conditional generator, sends the intermediate results perturbed by differential privacy to the source client, and the source client uses the source model to provide guidance for the generator so that the generator can generate the data corresponding to the input label that is the same as the data distribution in the target domain. We evaluate the performance of our proposed method on digital dataset and office-31dataset, which are popular domain adaptation benchmark datasets, and verify the security by the accuracy and F1-score of Membership Inference Attacks.},
  keywords={Training;Adaptation models;Differential privacy;Visualization;Resists;Predictive models;Benchmark testing;Domain adaptation;Generative adversarial networks;Differential privacy;Privacy protection amplifiers},
  doi={10.1109/QRS57517.2022.00022},
  ISSN={2693-9177},
  month={Dec},}@INPROCEEDINGS{11093057,
  author={Tang, Zichen and Yao, Yuan and Cui, Miaomiao and Bo, Liefeng and Yang, Hongyu},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior}, 
  year={2025},
  volume={},
  number={},
  pages={348-358},
  abstract={Text-guided 3D human generation has advanced with the development of efficient 3D representations and 2D-lifting methods like Score Distillation Sampling (SDS). However, current methods suffer from prolonged training times and often produce results that lack fine facial and garment details. In this paper, we propose GaussianIP, an effective two-stage framework for generating identity-preserving realistic 3D humans from text and image prompts. Our core insight is to leverage human-centric knowledge to facilitate the generation process. In stage 1, we propose a novel Adaptive Human Distillation Sampling (AHDS) method to rapidly generate a 3D human that maintains high identity consistency with the image prompt and achieves a realistic appearance. Compared to traditional SDS methods, AHDS better aligns with the human-centric generation process, enhancing visual quality with notably fewer training steps. To further improve the visual quality of the face and clothes regions, we design a View-Consistent Refinement (VCR) strategy in stage 2. Specifically, it produces detail-enhanced results of the multi-view images from stage 1 iteratively, ensuring the 3D texture consistency across views via mutual attention and distance-guided attention fusion. Then a polished version of the 3D human can be achieved by directly perform reconstruction with the refined images. Extensive experiments demonstrate that GaussianIP outperforms existing methods in both visual quality and training efficiency, particularly in generating identity-preserving results. Our code is available at: https://github.com/silence-tang/GaussianIP.},
  keywords={Training;Visualization;Computer vision;Three-dimensional displays;Face recognition;Avatars;Clothing;Rendering (computer graphics);Video recording;Image reconstruction;computer vision;generative model;3d human generation},
  doi={10.1109/CVPR52734.2025.00041},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10401610,
  author={Yang, Liu and Sun, Shaoxin and Qin, Zongchen and Shan, Ge and Han, Zhijie},
  booktitle={2023 38th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={Automatic Generation Analysis Method of Automobile Chassis Electronic Control System Based on NLP-intelligent Control Condition}, 
  year={2023},
  volume={},
  number={},
  pages={514-519},
  abstract={In recent years, the development direction of intelligent control has gradually shifted towards humanoid models. Intelligent control based on human-machine interaction is increasingly applied to chassis electronic control systems, and in the process of innovation in new energy vehicles in the field of artificial intelligence, the speech recognition mode of the chassis electronic control system is further trending towards specialization and intelligence. This article uses multi-modal entity word recognition technology to obtain various commands for intelligent control and management from users. In the process of fuzzy search, relatively accurate word command results are obtained, and instruction search is carried out according to the results of multi-modal entity word recognition, thereby achieving real-time dynamic control of the vehicle by the user. In the process of entity word recognition, data reasoning functions for text word entries are added to provide effective instructions within the user’s operational range, thereby completing the human-machine interaction process of the automotive chassis electronic control system.},
  keywords={Human computer interaction;Visualization;Natural languages;Control systems;Real-time systems;Automobiles;Intelligent control;multimodal;fuzzy search;named entity recognition;intelligent control;human-machine interaction},
  doi={10.1109/YAC59482.2023.10401610},
  ISSN={2837-8601},
  month={Aug},}@ARTICLE{8574054,
  author={Du, Changde and Du, Changying and Huang, Lijie and He, Huiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning}, 
  year={2019},
  volume={30},
  number={8},
  pages={2310-2323},
  abstract={Neural decoding, which aims to predict external visual stimuli information from evoked brain activities, plays an important role in understanding human visual system. Many existing methods are based on linear models, and most of them only focus on either the brain activity pattern classification or visual stimuli identification. Accurate reconstruction of the perceived images from the measured human brain activities still remains challenging. In this paper, we propose a novel deep generative multiview model for the accurate visual image reconstruction from the human brain activities measured by functional magnetic resonance imaging (fMRI). Specifically, we model the statistical relationships between the two views (i.e., the visual stimuli and the evoked fMRI) by using two view-specific generators with a shared latent space. On the one hand, we adopt a deep neural network architecture for visual image generation, which mimics the stages of human visual processing. On the other hand, we design a sparse Bayesian linear model for fMRI activity generation, which can effectively capture voxel correlations, suppress data noise, and avoid overfitting. Furthermore, we devise an efficient mean-field variational inference method to train the proposed model. The proposed method can accurately reconstruct visual images via Bayesian inference. In particular, we exploit a posterior regularization technique in the Bayesian inference to regularize the model posterior. The quantitative and qualitative evaluations conducted on multiple fMRI data sets demonstrate the proposed method can reconstruct visual images more accurately than the state of the art.},
  keywords={Visualization;Brain modeling;Functional magnetic resonance imaging;Image reconstruction;Decoding;Bayes methods;Deep neural network (DNN);image reconstruction;multiview learning;neural decoding;variational Bayesian inference},
  doi={10.1109/TNNLS.2018.2882456},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{9735278,
  author={Karimi, Davood and Gholipour, Ali},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Improving Calibration and Out-of-Distribution Detection in Deep Models for Medical Image Segmentation}, 
  year={2023},
  volume={4},
  number={2},
  pages={383-397},
  abstract={Convolutional neural networks (CNNs) have proved to be powerful medical image segmentation models. In this study, we address some of the main unresolved issues regarding these models. Specifically, training of these models on small medical image datasets is still challenging, with many studies promoting techniques such as transfer learning. Moreover, these models are infamous for producing overconfident predictions and for failing silently when presented with out-of-distribution (OOD) test data. In this article, for improving prediction calibration we advocate for multitask learning, i.e., training a single model on several different datasets, spanning different organs of interest and different imaging modalities. We show that multitask learning can significantly improve model confidence calibration. For OOD detection, we propose a novel method based on spectral analysis of CNN feature maps. We show that different datasets, representing different imaging modalities and/or different organs of interest, have distinct spectral signatures, which can be used to identify whether or not a test image is similar to the images used for training. We show that our proposed method is more accurate than several competing methods, including methods based on prediction uncertainty and image classification.},
  keywords={Biomedical imaging;Image segmentation;Training;Predictive models;Data models;Calibration;Multitasking;Convolutional neural networks;multitask learning;out-of-distribution (OOD) detection;segmentation;uncertainty},
  doi={10.1109/TAI.2022.3159510},
  ISSN={2691-4581},
  month={April},}@INPROCEEDINGS{9878956,
  author={Wu, Chao and Ge, Wenhang and Wu, Ancong and Chang, Xiaobin},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification}, 
  year={2022},
  volume={},
  number={},
  pages={20206-20216},
  abstract={To learn camera-view invariant features for person Re-IDentification (Re-ID), the cross-camera image pairs of each person play an important role. However, such cross-view training samples could be unavailable under the ISo-lated Camera Supervised (ISCS) setting, e.g., a surveillance system deployed across distant scenes. To handle this challenging problem, a new pipeline is introduced by synthesizing the cross-camera samples in the feature space for model training. Specifically, the feature encoder and generator are end-to-end optimized under a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint learning procedure raises concern on the stability of generative model training. Therefore, a new feature generator, σ-Regularized Conditional Variational Autoencoder (σ-Reg. CVAE), is proposed with theoretical and experimental analysis on its robustness. Extensive experiments on two ISCS person Re-ID datasets demonstrate the superiority of our CCSFG to the competitors. 11https://github.com/ftd-Wuchao/CCSFG},
  keywords={Training;Representation learning;Visualization;Surveillance;Pipelines;Cameras;Generators;Biometrics; Recognition: detection;categorization;retrieval; Representation learning},
  doi={10.1109/CVPR52688.2022.01960},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9023996,
  author={Baek, Jae-Yong and Yoo, Yong-Sang and Bae, Seung-Hwan},
  journal={IEEE Access}, 
  title={Generative Adversarial Ensemble Learning for Face Forensics}, 
  year={2020},
  volume={8},
  number={},
  pages={45421-45431},
  abstract={The recent advance of synthetic image generation and manipulation methods allows us to generate synthetic face images close to real images. On the other hand, the importance of identifying the synthetic face images increases more and more to protect personal privacy from those. Although some deep learning-based image forensic methods have been developed recently, it is still challenging to distinguish synthetic images generated by recent image generation and manipulation methods such as the deep fake, face2face, and face swap. To resolve this challenge, we propose a novel generative adversarial ensemble learning method. We train multiple discriminative and generative networks based on the adversarial learning. Compared to the conventional adversarial learning, our method is however more focused on improving the discrimination ability rather than image generation one. To this end, we improve the discriminabilty by ensembling outputs from different two discriminators. In addition, we train two generators in order to generate general and hard synthetic images. By ensemble learning of all the generators and discriminators, we improve the discriminators by using the generated synthetic face images, and improve the generators by passing the combined feedback of the discriminators. On the FaceForensics benchmark challenge, we thoroughly evaluate our methods by comparing the recent methods. We also provide the ablation study to prove the effectiveness and usefulness of our method.},
  keywords={Face;Generators;Feature extraction;Transform coding;Image generation;Machine learning;Image coding;Digital image forensics;generative adversarial ensemble learning;deep learning;synthetic image detection;face image},
  doi={10.1109/ACCESS.2020.2968612},
  ISSN={2169-3536},
  month={},}@ARTICLE{11021410,
  author={Farooq, Muhammad Ali and Yao, Wang and Corcoran, Peter},
  journal={IEEE Access}, 
  title={ChildDiffusion: Unlocking the Potential of Generative AI and Controllable Augmentations for Child Facial Data Using Stable Diffusion and Large Language Models}, 
  year={2025},
  volume={13},
  number={},
  pages={96616-96634},
  abstract={Ensuring the availability of child facial datasets is essential for advancing AI applications, yet legal, ethical, and data scarcity concerns pose significant challenges. Current generative models such as StyleGAN excel at producing synthetic facial data but struggle with temporal consistency, control over output attributes, and diversity in rendered features. These limitations underscore the need for a more robust and adaptable framework. In this research, we propose the ChildDiffusion framework, designed to generate photorealistic child facial data using diffusion models. The framework integrates intelligent augmentations via short text prompts, employs various image samplers, and leverages ControlNet for enhanced model conditioning. Additionally, we have used large language models (LLMs) to provide complex textual guidance to enable precise image-to-image transformations, facilitating the curation of diverse, high-quality datasets. The model was validated by generating child faces with varied ethnicities, facial expressions, poses, lighting conditions, eye-blinking effects, accessories, hair colors, and multi-subject compositions. To exemplify its potential, we open-sourced a dataset of 2.5k child facial samples across five ethnic classes, which underwent rigorous qualitative and quantitative evaluations. Further, we fine-tuned a Vision Transformer model to classify child ethnicity as a downstream task, demonstrating the framework’s utility. This research advances generative AI by addressing data scarcity and ethical challenges, showcasing how diffusion models can produce realistic child facial data while ensuring compliance with privacy standards. The versatile ChildDiffusion framework offers broad potential for machine learning applications, serving as a valuable tool for AI innovation. The project website, along with the complete ChildRace dataset and the fine-tuned model, is available at (https://mali-farooq.github.io/childdiffusion/).},
  keywords={Diffusion models;Faces;Data models;Text to image;Image synthesis;Computational modeling;Buildings;Adaptation models;Training;Pipelines;T21;stable diffusion;synthetic data;GAN’s;generative AI;diffusion models},
  doi={10.1109/ACCESS.2025.3575964},
  ISSN={2169-3536},
  month={},}@ARTICLE{9762727,
  author={Sivakumar, Seshadri and Sivakumar, Shyamala},
  journal={IEEE Access}, 
  title={Activation Function Modulation in Generative Triangular Recurrent Neural Networks}, 
  year={2022},
  volume={10},
  number={},
  pages={45709-45725},
  abstract={Autonomous generation of time series is challenging because the network must capture short-term features while tracking long-term time dependencies. This paper introduces the modulation of the activation function slopes of the upper-lower triangular recurrent neural networks (ULTRNNs) for dynamic variation of memory through a secondary recurrent network with its own independent states. A zigzag propagation algorithm for weight updates is proposed that accounts for the dynamic interaction of the states between the ULTRNN and the secondary network. A novel training method is proposed that distributes the eigenvalues of the closed-loop system around the unit circle in the complex z-plane to ensure that the network behaves as a nonlinear oscillator with an output that neither collapses nor saturates but continues to emulate the target. Examples encompassing the Lorenz series, Santa Fe laser data, kolam patterns, electrocardiogram (ECG) signals, stock pricing data, and smart grid data are presented to demonstrate that the proposed approach is highly effective in the generative modeling of complex periodic, chaotic, and nonstationary time series. The qualitative and quantitative performance of the ULTRNN obtained with the proposed activation-function modulation technique is comparable to that of state-of-the-art techniques including feedforward networks and generative adversarial networks, but with far fewer trainable parameters and shorter computation times.},
  keywords={Modulation;Training;Stability analysis;Eigenvalues and eigenfunctions;Time series analysis;Heuristic algorithms;Computational modeling;Triangular RNN;activation function slope modulation;generative networks;closed-loop training},
  doi={10.1109/ACCESS.2022.3170042},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8761026,
  author={Hieida, Chie and Horii, Takato and Nagai, Takayuki},
  booktitle={2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, 
  title={Toward Empathic Communication: Emotion Differentiation via Face-to-Face Interaction in Generative Model of Emotion}, 
  year={2018},
  volume={},
  number={},
  pages={66-71},
  abstract={In this paper, a model of emotions is proposed based on various neurological and psychological findings. The proposed model consists of three layers: the external/internal appraisal layer, the prediction/decision-making layer, and the emotional memory layer. We implement the proposed model by integrating some deep learning modules such as recurrent attention model, convolutional long short-term memory, and deep deterministic policy gradient. We set a “facial expression” task simulating mother-child interactions and verified emotion differentiation during the task. We also examine the trained model in the “still face” experiment. A claim in this study is that it is a very important step for the constructive approach to compare the proposed model with real human subjects in the same experiment that was carried out in the psychological studies.},
  keywords={Appraisal;Predictive models;Reinforcement learning;Decision making;Robots;Face;Brain modeling;Model of Emotion;decision-making;still face experiment},
  doi={10.1109/DEVLRN.2018.8761026},
  ISSN={2161-9484},
  month={Sep.},}@INPROCEEDINGS{10692031,
  author={Peñas, Reynaldo Ted L. and Cajote, Rhandley D.},
  booktitle={2024 2nd World Conference on Communication & Computing (WCONF)}, 
  title={Philippine Region-Based Food and Nutrition Cross-Reference Using Fine-Tuned Generative ChatBot}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper describes the development of a Large Language Model Chatbot for Philippine Food and Nutrition knowledge base. The objective is to associate the region or province of origin of any available food item in the country to the nutrients contained and the nutritional benefits to the human body, considering the build of the knowledge base coming from 30 web sites or sources. Three models have been used to test and evaluate the performance in accomplishing the task. In the training and fine-tuning efficiency, the Meta Llama 2 7B HF model is the most efficient when varying the training epochs from 1 through 5; but based on evaluation metrics of BERTScore, ROUGE Score and GLEU, the Meta Llama 3 7B Instruct model obtained the highest scores, and the highest acceptance score when tested on 100 pairs of question-and-answer with ground truths. Further demonstration of the chatbot revealed that the objective has been achieved in most query attempts during the testing phase of the development.},
  keywords={Training;Measurement;Large language models;Computational modeling;Knowledge based systems;Hafnium;Chatbots;Nutrients;Web sites;Testing;Food;nutrition;large language model;Philippines;chatbot},
  doi={10.1109/WCONF61366.2024.10692031},
  ISSN={},
  month={July},}@ARTICLE{10620289,
  author={Oh, Jangmin},
  journal={IEEE Access}, 
  title={Developing a Model for Extracting Actual Product Names From Order Item Descriptions Using Generative Language Models}, 
  year={2024},
  volume={12},
  number={},
  pages={122695-122701},
  abstract={This study proposes a cost-effective method for developing a model that extracts actual product names from order item descriptions in food delivery requests processed by a delivery agency platform. First, we utilize GPT-4, a state-of-the-art generative language model, to generate actual product names corresponding to the original order item descriptions, creating an annotated dataset. Next, we process this dataset into a Named Entity Recognition (NER) format to generate a labeled dataset. Based on this, we fine-tune Korean and multilingual pre-trained language models to develop models for extracting actual product names. Experimental results show that the Korean model achieves the highest performance, with an F1 score of 0.8750 or higher on a 10% holdout dataset, securing a product name extraction model that meets the desired level of accuracy. The developed model has been successfully integrated into the food delivery platform’s data analysis process, where it is being utilized for various data management tasks, such as building a master product name table and enhancing the retrieval quality of similar products. This study confirms the effectiveness of an approach that leverages generative language models for annotated dataset creation and fine-tuning of pre-trained language models for developing models tailored to domain-specific problems.},
  keywords={Hidden Markov models;Task analysis;Labeling;Chatbots;Transformers;Prompt engineering;Encoding;Brand management;Product development;Annotated dataset;delivery agency platform;extracting actual product names;generative language model;labeled dataset},
  doi={10.1109/ACCESS.2024.3437197},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10466102,
  author={Garg, Anshumaan and Sharma, Dolly},
  booktitle={2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)}, 
  title={Generative AI for Software Test Modelling with a focus on ERP Software}, 
  year={2023},
  volume={},
  number={},
  pages={187-193},
  abstract={Generative AI in this context refers to the type of artificial intelligence that can generate content or give new information based on patterns it has learned. In the case of software testing, it refers to the use of generative AI to model or for the creation of test scenarios, test cases or objects for ERP (Enterprise Resource Planning) software. The special focus on ERP software means that generative AI-based techniques have been particularly designed and optimized for the purpose of software testing. It does take into account the unique features and complexity of the ERP systems which allows for more effective and accurate testing. The problem with the existing chatbots is that they are not integrated with generative AI and the training is either not properly done or the data used for training is biased. The objective of this work is to develop a chatbot integrated with the generative AI-based framework and develop training data to cater to user needs. Methods and tools used in this approach are the OpenAI's API used for integrating chatbot with the generative AI-based software, Postman API has also been used to send and receive API requests and prompts and completions to be generated using Python code. The result of this approach is that a chatbot has been developed which develops test cases and scenarios, requests sent and received successfully and prompts and completions have been successfully generated using Python code. To state it simply, generative AI for software test modelling with a focus on ERP software means creating test cases and scenarios using AI and generating them automatically which helps testers ensure that the software is working correctly and meets the needs of the business operations.},
  keywords={Training;Software testing;Codes;Generative AI;Training data;Software quality;Chatbots},
  doi={10.1109/ICAICCIT60255.2023.10466102},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10831787,
  author={Lima, Caroline Da Conceição and Salazar, Herbert and Lima, Yuri and Barbosa, Carlos Eduardo and Argôlo, Matheus and Lyra, Alan and de Souza, Jano Moreira},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Generative AI Impact on the Future of Work: Insights from Software Development}, 
  year={2024},
  volume={},
  number={},
  pages={3887-3892},
  abstract={Recent Artificial Intelligence advancements raise concerns about the future of work, particularly technological unemployment. Studies show automation's impact, but tools like ChatGPT disrupt even traditionally secure professions like programming. In this study, we reevaluate AI's effects using a method to assess the impact of Generative AI technologies on occupations to understand the potential effects of Generative AI systems on software development work. Valuable insights were obtained by gathering the view of a group of workers, primarily composed of developers who are starting their careers, regarding the impact of these technologies on the tasks they perform to provide a comprehensive understanding of the implications of Generative AI for software development. Results show that all programming tasks performed by these workers would experience some impact by Generative AI -65% of the tasks being considerably impacted, 12% moderately impacted, and 18% minimally impacted. This analysis highlights the substantial influence of Generative AI technologies on software development, mainly affecting those in the early stages of their career. The results of this work contribute to the academic community with valuable information. Policymakers can also use this information, as this work provides a comprehensive view of the impacts of Generative AI on software developers, considering their direct impact on job tasks.},
  keywords={Automation;Generative AI;Engineering profession;Chatbots;Software;Unemployment;Programming profession;Cybernetics;Software development management},
  doi={10.1109/SMC54092.2024.10831787},
  ISSN={},
  month={Oct},}@ARTICLE{9229088,
  author={Zhang, Guoling and Wang, Xiaodan and Li, Rui and Song, Yafei and He, Jiaxing and Lai, Jie},
  journal={IEEE Access}, 
  title={Network Intrusion Detection Based on Conditional Wasserstein Generative Adversarial Network and Cost-Sensitive Stacked Autoencoder}, 
  year={2020},
  volume={8},
  number={},
  pages={190431-190447},
  abstract={In the field of intrusion detection, there is often a problem of data imbalance, and more and more unknown types of attacks make detection difficult. To resolve above issues, this article proposes a network intrusion detection model called CWGAN-CSSAE, which combines improved conditional Wasserstein Generative Adversarial Network (CWGAN) and cost-sensitive stacked autoencoders (CSSAE). First of all, the CWGAN network that introduces gradient penalty and L2 regularization is used to generate specified minority attack samples to reduce the class imbalance of the training dataset. Secondly, the stacked autoencoder is used to intelligently extract the deep abstract features of the network data. Finally, a cost-sensitive loss function is constructed to give a large misclassification cost to a minority of attack samples. Thus, effective detection of network intrusion attacks can be realized. The experimental results based on KDDTest+, KDDTest-21, and UNSW-NB15 datasets show that the CWGAN-CSSAE network intrusion detection model improves the detection accuracy of minority attacks and unknown attacks. In addition, the method in this article is compared with other existing intrusion detection methods, excellent results have been achieved in performance indicators such as accuracy and F1 score. The accuracy on the above datasets reached 90.34%, 80.78% and 93.27% respectively. The accuracy of U2R on the KDDTest+ and KDDTest-21 datasets both reached 42.50%. The accuracy of R2L on the KDDTest+ and KDDTest-21 datasets reached 54.39% and 52.51%, respectively. And the F1 score on the above datasets reached 91.01%, 87.18% and 93.99% respectively.},
  keywords={Feature extraction;Network intrusion detection;Deep learning;Data models;Generative adversarial networks;Training;Intrusion detection;conditional Wasserstein GAN;stacked autoencoder;imbalanced classification;cost-sensitive;regularization;deep learning},
  doi={10.1109/ACCESS.2020.3031892},
  ISSN={2169-3536},
  month={},}@ARTICLE{9082695,
  author={Dai, Qiang and Cheng, Xi and Qiao, Yan and Zhang, Youhua},
  journal={IEEE Access}, 
  title={Agricultural Pest Super-Resolution and Identification With Attention Enhanced Residual and Dense Fusion Generative and Adversarial Network}, 
  year={2020},
  volume={8},
  number={},
  pages={81943-81959},
  abstract={The growth of the most significant field crops such as rice, wheat, maize, and soybean are influenced because of various pests. And crop production is decreased due to various categories of insects. Deep learning technologies significantly increased the efficiency of identifying and controlling agricultural pests attack. However, agricultural pests images obtained are often obscure and unclear because of the sparse density of cameras deployed in the real farmland. This always makes pests difficult to recognize and monitor. Additionally, the existing classification and segmentation methods are not satisfying for the identification of low-resolution images because they are pre-trained on the clear and high-resolution datasets. Therefore, it is crucial to restore and upscale the obtained low-resolution pest images in order to improve classification accuracy and the recall rate of the instance segmentation. In this paper, we propose a generative adversarial network (GAN) with quadra-attention and residual and dense fusion mechanisms to transform low-resolution pest images. Compared with previous state-of-the-art PSNR-oriented super-resolution methods, our proposed method is more powerful in image reconstruction and achieves the state of the art performance. The experiment results show that after reconstructing with our proposed gan, the recall rate increased by 182.89% and classification accuracy also improved a lot. Besides, our proposed method could decrease the density of the camera layout in the agricultural Internet of Things (IOT) monitor systems and the cost of infrastructure, which is practical for real-world applications.},
  keywords={Agriculture;Insects;Image segmentation;Deep learning;Generative adversarial networks;Diseases;Agricultural pests;super-resolution;classification;object instance segmentation;deep learning;quadra-attention;residual and dense fusion},
  doi={10.1109/ACCESS.2020.2991552},
  ISSN={2169-3536},
  month={},}@ARTICLE{8792068,
  author={Liu, Jialun and Li, Wenhui and Pei, Hongbin and Wang, Ying and Qu, Feng and Qu, You and Chen, Yuhao},
  journal={IEEE Access}, 
  title={Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-Identification}, 
  year={2019},
  volume={7},
  number={},
  pages={114021-114032},
  abstract={In this paper, we study the domain adaptive person re-identification(re-ID) problem: train a re-ID model on the labeled source domain and test it on the unlabeled target domain. It's known challenging due to the feature distribution bias between the source domain and target domain. The previous methods directly reduce the bias by image-to-image style translation between the source and the target domain in an unsupervised manner. However, these methods only consider the rough bias between the source domain and the target domain but neglect the detailed bias between the source domain and the target camera domains (divided by camera views), which contain critical factors influencing the testing performance of re-ID model. In this work, we particularly focus on the bias between the source domain and the target camera domains. To overcome this problem, a multi-domain image-to-image translation network, termed Identity Preserving Generative Adversarial Network (IPGAN) is proposed to learn the mapping relationship between the source domain and the target camera domains. IPGAN can translate the styles of images from the source domain to the target camera domains and generate many images with styles of target camera domains. Then the re-ID model is trained with the translated images generated by IPGAN. During the training of the re-ID model, we aim to learn the discriminative feature. We design and train a novel re-ID model, termed IBN-reID, in which Instance and Batch Normalization block (IBN-block) are introduced. Experimental results on Market-1501, DukeMTMC-reID and MSMT17 show that the images generated by IPGAN are more suitable for cross-domain re-ID. Very competitive re-ID accuracy is achieved by our method.},
  keywords={Cameras;Adaptation models;Generative adversarial networks;Task analysis;Semantics;Training;Feature extraction;Person re-identification;domain adaptation;style transfer;unsupervised learning},
  doi={10.1109/ACCESS.2019.2933910},
  ISSN={2169-3536},
  month={},}@ARTICLE{9204705,
  author={Yang, Chuan and Wang, Zhenghong},
  journal={IEEE Access}, 
  title={An Ensemble Wasserstein Generative Adversarial Network Method for Road Extraction From High Resolution Remote Sensing Images in Rural Areas}, 
  year={2020},
  volume={8},
  number={},
  pages={174317-174324},
  abstract={Road extraction from high resolution remote sensing (HR-RS) images is an important yet challenging computer vision task. In this study, we propose an ensemble Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) method called E-WGAN-GP for road extraction from HR-RS images in rural areas. The WGAN-GP model modifies the standard GANs with Wasserstein distance and gradient penalty. We add a spatial penalty term in the loss function of the WGAN-GP model to solve the class imbalance problem typically in road extraction. Parameter experiments are undertaken to determine the best spatial penalty and the weight term in the new loss function based on GaoFen-2 dataset. In addition, we execute an ensemble strategy in which we first train two WGAN-GP models using the U-Net and BiSeNet as generator respectively, and then intersect their inferred outputs to yield better road vectors. We train our new model with GaoFen-2 HR-RS images in rural areas from China and also the DeepGlobe Road Extraction dataset. Compared with the U-Net, BiSeNet, D-LinkNet and WGAN-GP methods without ensemble, our new method makes a good trade-off between precision and recall with F1-score = 0.85 and IoU = 0.73.},
  keywords={Roads;Gallium nitride;Feature extraction;Generative adversarial networks;Image segmentation;Computer architecture;Image resolution;Deep Learningwi;Road Extraction;High Resolution Remote Sensing;Wasserstein GAN;Ensemble Learning},
  doi={10.1109/ACCESS.2020.3026084},
  ISSN={2169-3536},
  month={},}@ARTICLE{9766340,
  author={Rüttgers, Mario and Jeon, Soohwan and Lee, Sangseung and You, Donghyun},
  journal={IEEE Access}, 
  title={Prediction of Typhoon Track and Intensity Using a Generative Adversarial Network With Observational and Meteorological Data}, 
  year={2022},
  volume={10},
  number={},
  pages={48434-48446},
  abstract={To save lives and reduce damage from the destructive impacts of a typhoon, an accurate and fast forecast method is highly demanded. Particularly, predictions for short lead times, known as nowcasting, rely on fast forecasts allowing immediate emergency plannings in the affected areas. In this paper, we propose a generative adversarial network that operates on a single graphics processing unit, to predict both the track and intensity of typhoons for short lead times within fractions of a second. To investigate the effects of meteorological variables on typhoon forecasts, we conducted a parameter study for 6-h track predictions. The results of the study indicate that learning velocity, temperature, pressure, and humidity along with satellite images have positive effects on prediction accuracy. To address the limited access to observational data and facilitate predictions for 12-h intervals, we replaced satellite images with reanalysis data of the total cloud cover and vorticity fields. This replacement led to an increase in data from 76 to 757 typhoons, and it reduced the error of the 6-h track forecasts by 23.5%. The best combination of the parameter study yields track predictions in intervals of 6 and 12 h with the corresponding averaged absolute errors of 44.5 and 68.7 km. Typhoon intensities are predicted by extracting information from generated velocity fields with averaged hit rates of 87.3% and 83.2% for 6- and 12-h interval forecasts, respectively. For typhoons after 1994, tracks and intensities for 12-h intervals are compared to forecasts from the Joint Typhoon Warning Center and Regional Specialized Meteorological Center Tokyo.},
  keywords={Tropical cyclones;Satellites;Generative adversarial networks;Weather forecasting;Generators;Predictive models;Training;Typhoon track prediction;typhoon intensity prediction;deep learning;nowcasting},
  doi={10.1109/ACCESS.2022.3172301},
  ISSN={2169-3536},
  month={},}@ARTICLE{10214234,
  author={Majeed, Abdul and Hwang, Seong Oun},
  journal={IEEE Access}, 
  title={CTGAN-MOS: Conditional Generative Adversarial Network Based Minority-Class-Augmented Oversampling Scheme for Imbalanced Problems}, 
  year={2023},
  volume={11},
  number={},
  pages={85878-85899},
  abstract={This paper proposes a novel data augmentation scheme called the conditional generative adversarial network minority-class-augmented oversampling scheme (CTGAN-MOS) for solving class imbalance problems. Our methodology encompassed six key steps: data engineering using sophisticated pre-processing techniques, identifying the type of vulnerabilities present in the data, curating good quality synthetic data using the CTGAN model, the intelligent fusion of real and synthetic data, noise removal from the augmented data using coin-throwing algorithm, and building classifiers with the high-quality augmented data. Our scheme maintains higher structural similarity (data truthfulness) between the original and the resampled data by intelligently adding high-quality samples only to the minority class, whereas some augmentation techniques add records to the majority class, leading to poor-quality resampled data. Our scheme removes noisy samples from the data, which has remained unexplored in the CTGAN-based data augmentation. Furthermore, it augments data by adding fewer records compared to existing schemes, while offering comparable performance. Experiments are conducted on benchmark datasets to prove the feasibility of the proposed CTGAN-MOS in realistic scenarios. Results prove the improvement by CTGAN-MOS over existing state-of-the-art (SOTA) techniques in terms of accuracy, recall, precision,  $F_{1}$  score, and  $G$ -mean score. Specifically, the CTGAN-MOS has yielded accuracy values of 100% and 99.83% on two datasets which are higher than recent SOTA techniques. On average, it has yielded the 22.58% and 29.47% improvements w.r.t.  $G$ -mean score on two different datasets. On average, it adds 8.26% and 26.01% fewer records than the existing SOTA methods in the two datasets. Lastly, our scheme yields highly balanced confusion matrices compared to recent SOTA data augmentation techniques.},
  keywords={Generative adversarial networks;Data models;Synthetic data;Data augmentation;Costs;Predictive models;Noise measurement;Training data;Sampling methods;Imbalanced problem;data augmentation;machine learning;classifiers;noise;majority class;minority class;model training;samples;intelligent fusion;data truthfulness;data engineering},
  doi={10.1109/ACCESS.2023.3303509},
  ISSN={2169-3536},
  month={},}@ARTICLE{8918328,
  author={Wang, Wenhui and Wang, Anna and Ai, Qing and Liu, Chen and Liu, Jinglu},
  journal={IEEE Access}, 
  title={AAGAN: Enhanced Single Image Dehazing With Attention-to-Attention Generative Adversarial Network}, 
  year={2019},
  volume={7},
  number={},
  pages={173485-173498},
  abstract={Due to the atmospheric scattering and absorption, hazy weather often occurs in our everyday life, thus reducing the visibility of scenes. Single image dehazing is considered as an ill-posed and challenging problem in computer vision. To restore visibility in inclement weather, we propose an attention-to-attention generative adversarial network (AAGAN) whose motivation is the human visual perceptual mechanism. More specifically, a dense channel attention model is embedded into the encoder. Moreover, its output is projected forward to the corresponding multiscale spatial attention model in the decoder. Both attention models form an attention-to-attention mechanism to implement attention projection, thus capturing global feature dependencies of the whole network. Besides, we analyze the dehazing mechanism based on the atmospheric scattering model, and then utilize an improved RaLSGAN to recover more realistic texture information and enhance visual contrast for different hazy scenes. Finally, in order to improve the visual performance of image restoration, we remove all the instance normalization layers to avoid unnecessary artifacts, and then introduce spectral normalization for all the convolution layers to stabilize the entire training process. Qualitative assessments and analyses demonstrate that our proposed approach can achieve remarkable dehazing performance on both synthetic and real-world scenes against previous state-of-the-art methods.},
  keywords={Atmospheric modeling;Visualization;Image restoration;Scattering;Task analysis;Generative adversarial networks;Meteorology;Image dehazing;GAN;attention projection;image restoration},
  doi={10.1109/ACCESS.2019.2957057},
  ISSN={2169-3536},
  month={},}@ARTICLE{9144563,
  author={Shen, Haocheng and Chen, Jingkun and Wang, Ruixuan and Zhang, Jianguo},
  journal={IEEE Access}, 
  title={Counterfeit Anomaly Using Generative Adversarial Network for Anomaly Detection}, 
  year={2020},
  volume={8},
  number={},
  pages={133051-133062},
  abstract={Anomaly detection aims to detect anomaly with only normal data available for training. It attracts considerable attentions in the medical domain, as normal data is relatively easy to obtain but it is rather difficult to have abnormal data especially for some rare diseases, making training a standard classifier challenging or even impossible. Recently, generative adversarial networks (GANs) become prevalent for anomaly detection and most existing GAN-based methods detect outliers by the reconstruction error. In this paper, we propose a novel framework called adGAN for anomaly detection using GAN. Unlike existing GAN-based methods, adGAN is a discriminative model, which uses the fake data generated from GAN as an abnormal class, and then learns a boundary between normal data and simulated abnormal data. Thus it is able to output the anomaly scores directly similar as one-class SVM (OCSVM), without any reconstruction process. We explicitly design adGAN with two key elements, i.e., fake pool generation and concentration loss. The fake pool is created by incrementally collecting the fake data produced by intermediate-state GAN, which are likely surrounding the normal data distribution. The concentration loss is innovatively introduced to penalize large standard deviations of discriminator outputs for normal data, aiming to make the distribution of normal data more compact and more likely to be separated from the distribution of the potential abnormal data. The trained discriminator is finally used as an anomaly detector. We evaluated adGAN on three datasets, including ab-MNIST for synthetic anomaly detection, the ISIC’2016 for skin lesion detection, and the BraTS’2017 for brain lesion detection. The extensive experiments demonstrate that adGAN is consistently superior to its competitors on all three datasets.},
  keywords={Anomaly detection;Gallium nitride;Generators;Training;Image reconstruction;Biomedical imaging;Generative adversarial networks;Anomaly detection;concentration loss;fake pool;GAN},
  doi={10.1109/ACCESS.2020.3010612},
  ISSN={2169-3536},
  month={},}@ARTICLE{9932604,
  author={Khan, Wasif and Zaki, Nazar and Ahmad, Amir and Masud, Mohammad Mehedy and Ali, Luqman and Ali, Nasloon and Ahmed, Luai A.},
  journal={IEEE Access}, 
  title={Mixed Data Imputation Using Generative Adversarial Networks}, 
  year={2022},
  volume={10},
  number={},
  pages={124475-124490},
  abstract={Missing values are common in real-world datasets and pose a significant challenge to the performance of statistical and machine learning models. Generally, missing values are imputed using statistical methods, such as the mean, median, mode, or machine learning approaches. These approaches are limited to either numerical or categorical data. Imputation in mixed datasets that contain both numerical and categorical attributes is challenging and has received little attention. Machine learning-based imputation algorithms usually require a large amount of training data. However, obtaining such data is difficult. Furthermore, no considerate work has been conducted in the literature that focuses on the effects of the training and testing size with increasing amounts of missing data. To address this gap, we proposed that increasing the amount of training data will improve imputation performance. We first used generative adversarial network (GAN) methods to increase the amount of training data. We considered two state-of-the-art GANs (tabular and conditional tabular) to add synthetic samples using observed data with different synthetic sample ratios. We then used three state-of-the-art imputation models that can handle mixed data: MissForest, multivariate imputation by chained equations, and denoising auto encoder (DAE). We proposed robust experimental setups on four publicly available datasets with different training-testing data divisions that have increasing missingness ratios. Extensive experimental results show that incorporating synthetic samples with training data achieves better performance compared to the baseline methods for mixed data imputation in both categorical and numerical variables, especially for large missingness ratios.},
  keywords={Training data;Generative adversarial networks;Generators;Statistics;Data models;Machine learning algorithms;Prediction algorithms;Sequential analysis;Multivariate regression;Mixed data imputation;missing data;GANs;miss forest;MICE;denoising auto encoders},
  doi={10.1109/ACCESS.2022.3218067},
  ISSN={2169-3536},
  month={},}@ARTICLE{9340238,
  author={Mi, Ying and Yuan, Shihua and Li, Xueyuan and Zhou, Junjie},
  journal={IEEE Access}, 
  title={Dense Residual Generative Adversarial Network for Rapid Rain Removal}, 
  year={2021},
  volume={9},
  number={},
  pages={24848-24858},
  abstract={Single-image rain removal always was one of the difficulties in the environment perception task. Usually, it has two paths to solve this problem: data-driven solutions and model-based solutions. Due to the benefits of convenient, learning features automatically and rapidly, data driven solutions has attracted tremendous interests. However, the time consumed per frame is still hard to match the requirement of high real-time performance, especially for high speed unmanned platform. In this article, we propose a fast dense residual generative adversarial network (FDRN), which can remove rain and reduce computation time consumption, the de-raining time of each frame only consumes 0.02s. We enhanced the data of original rainy images, put it into the generator network which is composed of long short-term memory networks (LSTM) and a newly designed dense residual network (DRN). The feature map in generator and discriminator is extracted to calculate the loss function and guide the direction of training. We selected 1500 pairs of synthetic images from existed datasets to train our network. And in order to test our method's de-raining ability realistically, we also selected 147 real-world rainy images from existed datasets. Experiments on both synthetic and real-world rainy images demonstrate that the proposed method can achieve competitive results to some existing methods in performance and effectiveness.},
  keywords={Rain;Generators;Generative adversarial networks;Task analysis;Residual neural networks;Feature extraction;Visualization;Rain removal;FDRN;dense residual network;time consuming},
  doi={10.1109/ACCESS.2021.3055527},
  ISSN={2169-3536},
  month={},}@ARTICLE{9963540,
  author={Wu, Jianlong and Zhang, Weiwei and Wu, Minghui and Cui, Guohua and Wang, Xiaolan and Gong, Jun},
  journal={IEEE Access}, 
  title={Distributed Multi-Attention Generative Adversarial Network for Surrounding Vehicles Trajectories Prediction Based On Comprehensive Social Repulsion}, 
  year={2022},
  volume={10},
  number={},
  pages={125254-125263},
  abstract={Trajectories prediction for vehicles is a key task in autonomous driving. This task is complicated by the presence of social interactions between vehicles and their physical constraints with the scene. From dominant and concealed characteristics of the driving scenarios, this paper proposes a distributed multi-attention generative adversarial network named DTMA-GAN. Firstly, temporal and spatial attention heads are used in the framework of DTMA-GAN to extract scene information, driving features of self-vehicles and social interaction features among vehicles. These features help the model learn the position in the large scene and capture the most salient parts of the image associated with the path. Secondly, a subset of the Euclidean space is modeled using the lane environment and a collision-free set of states is constructed to generate a safe driving area via the environment occupancy points set. Finally, the multiple trajectories with confidence generated by GAN are combined with this candidate driving area, and an accurate, reasonable, and long-term vehicle trajectory is selected using non-maximum suppression. And we use publicly available NGSIM and Argoverse datasets for model evaluation. Experimental results show that compared with CS-LSTM, GRIP and other excellent algorithms, this algorithm can effectively reduce the Root Mean Square Error for trajectory prediction in dynamic environments. And ablation experiments show that all the module components are effective.},
  keywords={Trajectory;Feature extraction;Generators;Generative adversarial networks;Predictive models;Hidden Markov models;Autonomous driving;Ablation experiments;automated driving;GAN;LSTM;vehicle trajectory prediction},
  doi={10.1109/ACCESS.2022.3224481},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10903828,
  author={Taylor, Zachary and Sharma, Akriti and Upadhyay, Kritagya},
  booktitle={2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC)}, 
  title={Examining the Threat Landscape of Generative AI: Attack Vectors and Mitigation Strategies for LLMs}, 
  year={2025},
  volume={},
  number={},
  pages={01014-01020},
  abstract={Generative Pretrained Transformers (GPTs) represent a transformative leap in artificial intelligence, transitioning from analytical systems to those capable of creating new content across various media. This advancement, rooted in the Distributional Hypothesis of Natural Language Processing (NLP), enables AI to produce human-like text by learning the nuances of language. However, the reliance on large-scale datasets and advanced algorithms brings forth significant security and privacy concerns. This paper explores the evolution of GPT models, their applications, and the associated vulnerabilities. We address the issue of misplaced trust in AI-generated information, highlighting potential impacts on critical sectors such as healthcare and finance. Furthermore, we examine the ethical dilemmas and unforeseen repercussions of biased content generation. By conducting an extensive literature review and analyzing real-world case studies, we identify gaps in existing research and propose comprehensive mitigation strategies. Our comprehensive review categorizes various types of attacks on GPT models, offering practical recommendations to enhance the security and reliability of GPT-based systems in critical applications.},
  keywords={Prevention and mitigation;Computational modeling;Medical services;Media;Transformers;Natural language processing;Vectors;Security;Reliability;Systematic literature review;Cybersecurity;ChatGPT;AI;Generative Pretrained Transformer;Generative AI;Natural Language Processing (NLP);Large Language Modeling (LLM)},
  doi={10.1109/CCWC62904.2025.10903828},
  ISSN={},
  month={Jan},}@ARTICLE{10207016,
  author={Han, Kyunghoon and Koo, Heejoon and Jung, Sunghee and Park, Hyung-Bok and Hong, Youngtaek and Shim, Hackjoon and Jeon, Byunghwan and Chang, Hyuk-Jae},
  journal={IEEE Access}, 
  title={Reconstruction of Partially Broken Vascular Structures in X-Ray Images via Vesselness-Loss-Based Multi-Scale Generative Adversarial Networks}, 
  year={2023},
  volume={11},
  number={},
  pages={86335-86350},
  abstract={Coronary artery procedures are primarily performed based on X-ray angiography images. However, coronary arteries in X-ray images are often partially broken, complicating diagnoses and procedures owing to lack of visibility. In this paper, we propose a fully automatic method to restore locally broken parts of coronary arteries in X-ray images without using any external information, such as computed tomography images. To this end, we design a new multi-scale generative adversarial network and a vesselness-loss function. The proposed method is optimized for focus on elongated structures and can be utilized in various clinical applications. The proposed method is evaluated and compared with four other existing methods using the performance metrics, PSNR, MSE, and SSIM, and the result shows 34.3, 0.18, and 0.91 averages, respectively for each metric. Based on the performance result, the blocked regions are plausibly reconstructed into such original shapes of blood vessels, which can aid in image-based guiding catheter manipulation during coronary artery procedures. Eventually, the proposed method can be utilized in various clinical applications, e.g., image-based planning and guidance of coronary procedures and prior simulation of results.},
  keywords={X-ray imaging;Arteries;Image segmentation;Image reconstruction;Solid modeling;Computed tomography;Biomedical imaging;Angiography;Coronary arteriosclerosis;Generative adversarial networks;Coronary artery;X-ray angiography;procedure guidance},
  doi={10.1109/ACCESS.2023.3301568},
  ISSN={2169-3536},
  month={},}@ARTICLE{9537764,
  author={Liu, Jun and Liu, Xiaoyang and Feng, Yanjun},
  journal={IEEE Access}, 
  title={Generative Adversarial Network for Multi Facial Attributes Translation}, 
  year={2021},
  volume={9},
  number={},
  pages={129375-129384},
  abstract={Recently, Generative Adversarial Network (GAN) based approaches are applied in facial attribute translation. However, many tasks, i.e. multi facial attributes translation and background invariance, are not well handled in the literature. In this paper, we propose a novel GAN-based method that aims to get the target image that performs better within modifying one or more facial attributes in a single model. The model generator learns multi-points by inputting a re-coded transfer vector, ensuring the single model could learn multiple attributes simultaneously. It also optimizes the cycle loss to enhance the efficiency of transferring multi attributes. Moreover, the method uses the adaptive parameter to improve the calculation method of the loss function of the residual image. The results are also compared with the StarGAN v2, which is the current state-of-the-art model to prove the effectiveness and advancedness. Experiments show that our method has a satisfactory performance in multi facial attributes translation.},
  keywords={Generators;Facial features;Generative adversarial networks;Training;Adaptation models;Image color analysis;GAN;deep learning;facial attribute translation;loss optimization},
  doi={10.1109/ACCESS.2021.3112895},
  ISSN={2169-3536},
  month={},}@ARTICLE{10213992,
  author={Jung, Jiyoung and Moon, HeeJoon and Yu, Geunhyeok and Hwang, Hyoseok},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Generative Perturbation Network for Universal Adversarial Attacks on Brain-Computer Interfaces}, 
  year={2023},
  volume={27},
  number={11},
  pages={5622-5633},
  abstract={Deep neural networks (DNNs) have successfully classified EEG-based brain-computer interface (BCI) systems. However, recent studies have found that well-designed input samples, known as adversarial examples, can easily fool well-performed deep neural networks model with minor perturbations undetectable by a human. This paper proposes an efficient generative model named generative perturbation network (GPN), which can generate universal adversarial examples with the same architecture for non-targeted and targeted attacks. Furthermore, the proposed model can be efficiently extended to conditionally or simultaneously generate perturbations for various targets and victim models. Our experimental evaluation demonstrates that perturbations generated by the proposed model outperform previous approaches for crafting signal-agnostic perturbations. We demonstrate that the extended network for signal-specific methods also significantly reduces generation time while performing similarly. The transferability across classification networks of the proposed method is superior to the other methods, which shows our perturbations' high level of generality.},
  keywords={Perturbation methods;Brain modeling;Electroencephalography;Convolution;Kernel;Deep learning;Training;Adversarial attack;brain computer interfaces;EEG classification;universal adversarial perturbation},
  doi={10.1109/JBHI.2023.3303494},
  ISSN={2168-2208},
  month={Nov},}@ARTICLE{10557508,
  author={Tian, Ye and Ji, Luke and Hu, Yiwei and Ma, Haiping and Wu, Le and Zhang, Xingyi},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={An Evolutionary Multitasking Algorithm for Efficient Multiobjective Recommendations}, 
  year={2025},
  volume={6},
  number={3},
  pages={518-532},
  abstract={Represented by evolutionary algorithms and swarm intelligence algorithms, nature-inspired metaheuristics have been successfully applied to recommender systems and amply demonstrated effectiveness, in particular, for multiobjective recommendation. Owing to the population-based search paradigm, these algorithms can produce a number of recommendation lists, making diverse tradeoffs between multiple metrics and meeting the requirements of accuracy, novelty, diversity, and other user preferences. However, these algorithms are criticized for the low efficiency of the optimization process, especially when the number of users is large. To address this issue, this article proposes an evolutionary multitasking-based recommendation method, where each task corresponds to a user and all the tasks are optimized simultaneously, thus highly improving the efficiency of recommendation. To enhance the convergence speed, all the users are divided into multiple populations according to the similarity between their preferences, where each population evolves with internal knowledge transfer between users, and all the populations evolve with external knowledge transfer between populations. Experimental results on various datasets verify that the proposed method can better balance between multiple metrics than classical and deep neural network-based recommendation methods and exhibits significantly higher efficiency than evolutionary multiobjective optimization-based recommendation methods.},
  keywords={Optimization;Multitasking;Task analysis;Measurement;Statistics;Sociology;Accuracy;Evolutionary multitasking;knowledge transfer;multiobjective optimization;recommender systems},
  doi={10.1109/TAI.2024.3414289},
  ISSN={2691-4581},
  month={March},}@INPROCEEDINGS{9170984,
  author={Liu, Wei and Su, Jian and Mao, Zhenyu and Jin, Peipei and Huang, Yu and Dou, Chengfeng and Zhou, Limei and Shang, Yuwei},
  booktitle={2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)}, 
  title={Research on Text Classification Method of Distribution Network Equipment Fault based on Deep Learning}, 
  year={2020},
  volume={},
  number={},
  pages={11-16},
  abstract={With the development of science and technology, the intelligent distribution network is making rapid progress and the demand for power supply is also increasing. The classification and analysis of the power outage causes in distribution network is helpful to the prediction of power failure. There is a large gap in the proportion of categories of the sample data of power grid outage causes, and serious category imbalance will lead to the deviation of classification results. In order to classify the original data more reasonably, the sample value added technology based on generative adversary network is used to expand the category data with less data in this paper. In order to realize automatic classification of power grid outage causes, the method of text data preprocessing and the text classification model based on deep learning are studied. Finally, the experimental results show the influence of sample capacity and classification accuracy.},
  keywords={Deep learning;Cloud computing;Power measurement;Power supplies;Conferences;Text categorization;Distribution networks;Distribution network;Text classification;Generative adversary network;Data preprocessing;Deep learning},
  doi={10.1109/CSCloud-EdgeCom49738.2020.00012},
  ISSN={},
  month={Aug},}@INBOOK{10287838,
  author={Wong, Kelvin K. L.},
  booktitle={Cybernetical Intelligence: Engineering Cybernetics with Machine Intelligence}, 
  title={Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={333-365},
  abstract={Deep learning is a subset of machine learning that uses neural networks with numerous layers in order to learn from complex data. Neural network models are an essential component of deep learning. Deep Learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems. Deep learning frameworks are software libraries that allow developers to build and train deep neural networks. Deep learning has been widely applied in cybernetical intelligent control systems to improve the accuracy, efficiency, and adaptability of various control tasks. This chapter covers the concept of deep learning and its applications in the field of intelligent control. It starts with an overview of deep learning and its various methods, including Convolutional Neural Networks, Recurrent Neural Networks, Generative Adversarial Networks, and Deep Belief Networks. As deep learning continues to evolve and mature, it is likely to become an even more integral part of cybernetical intelligent control systems.},
  keywords={Deep learning;Biological neural networks;Mathematical models;Neurons;Convolutional neural networks;Neural networks;Robots},
  doi={10.1002/9781394217519.ch16},
  ISSN={},
  publisher={IEEE},
  isbn={9781394217496},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10287838},}@INPROCEEDINGS{10665241,
  author={Borrison, Reuben and Aleksy, Markus and Dix, Marcel},
  booktitle={2024 IEEE 19th Conference on Industrial Electronics and Applications (ICIEA)}, 
  title={Building Metadata Normalization Using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The escalating global emissions attributable to commercial buildings call for the integration of digital technologies to enhance energy efficiency and occupant comfort. Cyber-physical control systems (CPCS) can collect data from various sensors and actuators in the building to provide valuable insights to building owners. However, the unstructured metadata text fields in these systems pose a challenge in leveraging artificial intelligence and machine learning solutions for building management. This paper proposes a solution using pre-trained large language models for building metadata normalization that addresses inconsistencies in the point text metadata across buildings. The evaluation of this solution is performed using two publicly-available CPCS datasets, revealing its ability to structure the unstructured natural language metadata without fine tuning or training from scratch.},
  keywords={Training;Generative AI;Scalability;Buildings;Natural languages;Metadata;Sensor phenomena and characterization;Smart Building;Generative AI (GenAI);Large Language Model (LLM);Building Metadata Normalization;BRICK schema},
  doi={10.1109/ICIEA61579.2024.10665241},
  ISSN={2158-2297},
  month={Aug},}@INPROCEEDINGS{10947611,
  author={Laguarta, Jesús Rincón and Jiménez, Virginia Yagüe and Blanco, José Luis and González, Icíar},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Generative Images of Microscopy Cell Arrangements Using AI Conditional Training}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Advances in artificial intelligence-driven techniques are poised to revolutionize our understanding of cellular biology. Synthetic imaging capabilities elevate the precision and efficiency of analytical methods in cellular microscopy. These advancements have largely benefited cell segmentation, dynamics, and interaction analysis. In contrast, only a limited number of contributions have considered conditional generation. We present an innovative image-to-image translation scheme to generate high-quality cell microscopy images from controlled patterns. Our method accommodates both manual and automated annotated data. It uses a U-Net structure trained on patterns generated from actual cell microscopy images. We evaluated the efficacy of this approach using two distinct datasets, demonstrating its capacity to generate large-scale, highfidelity cellular images. The proposed methodology has significant potential for data augmentation in machine learning applications and elucidating complex cellular dynamics, including migration patterns and proliferation rates. Our results open the door to high-resolution dynamic analysis of cellular processes.},
  keywords={Training;Image segmentation;Pathology;Translation;Generative AI;Microscopy;Medical treatment;Self-supervised learning;Biology;Medical diagnostic imaging;Generative AI;cell microscopy images;cell arrangements;conditional training},
  doi={10.1109/BIBM62325.2024.10947611},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10979245,
  author={Yang, Ling and Chen, Zhenghao and Wang, Kaisiyuan and Zhou, Luping},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Improving CXR Bone Suppression by Exploiting Domain-level and Instance-level Information}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={For chest X-ray image (CXR) analysis, effective bone structure suppression is essential for uncovering lung abnormalities and facilitating accurate clinical diagnoses. While recent deep generative models, to some extent, improve the reconstruction quality of bone-suppressed CXRs, they often fall short in delivering substantial improvements in downstream diagnosis tasks. This limitation is attributed to a narrow focus on instance-specific details, neglecting broader domain-level knowledge, which hampers bone-suppression effectiveness. In response to these challenges, our proposed framework adopts a novel approach that integrates both instance-level and domain-level information. To capture instance information, our model employs a hybrid approach using both cross-covariance attention blocks (CABs) to underscore relevant image information and a followed Vision Transformers (ViTs) encoder for image feature embedding. To capture domain information, we introduce multi-head codebook attention (MCA) which leverages codebook structure with multi-head attention mechanism to capture global, domain-level information specific to the bone-suppressed CXR domain, thereby refining the synthesis process. During optimization, our two-stage training scheme involves a MCA learning stage that encapsulates the domain of bone-suppressed CXRs in MCA through a ViT-based GAN model, and a synthesis stage that employs the learned codebook to generate bone-suppressed CXRs from the original ones, enhancing instance synthesis through domain insights. Moreover, the incorporation of CABs further refines pixellevel instance information. Extensive experiments demonstrate the superior performance of our approach, improving PSNR by 8.36% and SSIM by 2.7% for bone suppression while boosting lung disease classification by 2.8% and 4.2% on two datasets and segmentation by 1.5%.},
  keywords={Bones;Biomedical imaging;Feature extraction;Codes;Decoding;Data mining;X-ray imaging;Image reconstruction;Attention mechanisms;Transformers;Bone Suppression;Chest X-ray;Pneumonia;Transformers;Generative Model;Soft Quantization},
  doi={10.1109/TMI.2025.3564894},
  ISSN={1558-254X},
  month={},}@ARTICLE{10921715,
  author={He, Zhitao and Chen, Yongyi and Chen, Ankang and Zhang, Dan and Zhang, Hui and Zhang, Jingbing},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={GAN-enabled U-shaped Network for Adversarial Attack Generation for Autonomous Unmanned Vehicles}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the rapid development of autonomous unmanned vehicle (AUV) technology, the requirements for sensor data integrity and reliability are increasing. Data-driven intrusion detection systems (IDSs) have been widely used as an effective module of defense to ensure AUV security. Once the IDS is destroyed, the impact on the normal driving of the AUV will be unimaginable. Unlike traditional research that seeks to improve the detection accuracy of IDS, this paper discusses the challenges posed by adversarial attacks on autonomous vehicles from the perspective of attackers. In this paper, an generative adversarial-enabled U-shaped network (GEUN) is proposed to generate adversarial sensor attacks in AUVs and reveal vulnerabilities in existing IDSs. Specifically, GEUN consists of a shared feature extraction encoder, an adversarial anomaly reconstruction decoder, a discriminator, and a detector. The feature extraction encoder is used to add multi-scale information to the input to strengthen the multi-scale feature extraction of sensor signals by the network. With the help of discriminators and detectors, the adversarial anomaly reconstruction decoder is used to generate anomalies based on the extracted multi-scale vehicle information. Through experimental comparison with other methods, it is shown that the proposed GEUN can effectively attack various common IDS, which provides guidance for future research and development in the field of security of AUVs.},
  keywords={Feature extraction;Autonomous vehicles;Security;Training;Neural networks;Safety;Detectors;Transportation;Intrusion detection;Accuracy;Autonomous unmanned vehicle (AUV);intrusion detection system (IDS);generative adversarial networks (GANs);U-net;intelligent transportation system (ITS)},
  doi={10.1109/TASE.2025.3550305},
  ISSN={1558-3783},
  month={},}@INPROCEEDINGS{10378381,
  author={Wu, Xiaoshi and Sun, Keqiang and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Human Preference Score: Better Aligning Text-to-image Models with Human Preference}, 
  year={2023},
  volume={},
  number={},
  pages={2096-2105},
  abstract={Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/alignsd-web/.},
  keywords={Measurement;Adaptation models;Computer vision;Computational modeling;Predictive models;Artificial intelligence;Tuning},
  doi={10.1109/ICCV51070.2023.00200},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{9733175,
  author={Xu, Bingrong and Zeng, Zhigang and Lian, Cheng and Ding, Zhengming},
  journal={IEEE Transactions on Image Processing}, 
  title={Few-Shot Domain Adaptation via Mixup Optimal Transport}, 
  year={2022},
  volume={31},
  number={},
  pages={2518-2528},
  abstract={Unsupervised domain adaptation aims to learn a classification model for the target domain without any labeled samples by transferring the knowledge from the source domain with sufficient labeled samples. The source and the target domains usually share the same label space but are with different data distributions. In this paper, we consider a more difficult but insufficient-explored problem named as few-shot domain adaptation, where a classifier should generalize well to the target domain given only a small number of examples in the source domain. In such a problem, we recast the link between the source and target samples by a mixup optimal transport model. The mixup mechanism is integrated into optimal transport to perform the few-shot adaptation by learning the cross-domain alignment matrix and domain-invariant classifier simultaneously to augment the source distribution and align the two probability distributions. Moreover, spectral shrinkage regularization is deployed to improve the transferability and discriminability of the mixup optimal transport model by utilizing all singular eigenvectors. Experiments conducted on several domain adaptation tasks demonstrate the effectiveness of our proposed model dealing with the few-shot domain adaptation problem compared with state-of-the-art methods.},
  keywords={Adaptation models;Training;Numerical models;Couplings;Feature extraction;Deep learning;Automation;Few-shot learning;domain adaptation;optimal transport;data augmentation},
  doi={10.1109/TIP.2022.3157139},
  ISSN={1941-0042},
  month={},}@ARTICLE{9751076,
  author={Mohanty, Anwesha and Sutherland, Alistair and Bezbradica, Marija and Javidnia, Hossein},
  journal={IEEE Access}, 
  title={Skin Disease Analysis With Limited Data in Particular Rosacea: A Review and Recommended Framework}, 
  year={2022},
  volume={10},
  number={},
  pages={39045-39068},
  abstract={Recently, the rapid advancements in Deep Learning and Computer Vision technologies have introduced a new and exciting era in the field of skin disease analysis. However, there are certain challenges in the roadmap towards developing such technologies for real-life applications that must be investigated. This study considers one of the key challenges in data acquisition and computation, viz. data scarcity. Data scarcity is a central problem in acquiring medical images and applying machine learning techniques to train Convolutional Neural Networks for disease diagnosis. The main objective of this study is to explore the possible methods to deal with the data scarcity problem and to improve diagnosis with small datasets. The challenges in data acquisition for a few lamentably neglected skin conditions such as rosacea are an excellent instance to explore the possibilities of improving computer-aided skin disease diagnosis. With data scarcity in mind, the possible techniques explored and discussed include Generative Adversarial Networks, Meta-Learning, Few-Shot classification, and 3D face modelling. Furthermore, the existing studies are discussed based on skin conditions considered, data volume and implementation choices. Some future research directions are recommended.},
  keywords={Skin;Diseases;Medical diagnostic imaging;Medical diagnosis;Computer vision;Faces;Three-dimensional displays;Artificial intelligence;dermatology;generative adversarial networks;image analysis;meta-learning;neural network;rosacea;skin disease diagnosis;teledermatology},
  doi={10.1109/ACCESS.2022.3165574},
  ISSN={2169-3536},
  month={},}@ARTICLE{9435367,
  author={Vo, Duc My and Le, Thao Phuong and Nguyen, Duc Manh and Lee, Sang-Woong},
  journal={IEEE Access}, 
  title={BoostNet: A Boosted Convolutional Neural Network for Image Blind Denoising}, 
  year={2021},
  volume={9},
  number={},
  pages={115145-115164},
  abstract={Deep convolutional neural networks and generative adversarial networks currently attracted the attention of researchers because it is more effective than conventional representation-based methods. However, they have been facing two serious problems in the trade-off between noise removal, artifacts, and preserving low-contrast features and high-frequency details. In particular, deep convolutional neural networks might fail to remove strong noise in regions with higher noise levels while completely erasing low-contrast features and high-frequency details. By contrast, compared with conventional deep convolutional neural networks, generative adversarial networks might be better in balancing between erasing different types of noise and recovering texture details. However, they often generate fake details and unexpected artifacts in the image owing to the instability of their discriminator during training. In this study, we explored an innovative strategy for handling the serious problems of image denoising. With this strategy, we propose a novel boosting generative adversarial network (BoostNet) that not only combines all advantages of a generative adversarial sub-network and a deep convolutional neural network, it also successfully avoids the serious problems caused by the corruption and instability of training. BoostNet is developed by integrating a stand-alone deep convolutional neural network and a robust generative adversarial network into an ensemble network, which can effectively boost the denoising performance. We conducted several experiments using challenging datasets of additive white Gaussian noise and real-world noisy images. The experimental results show that our proposed method is superior to other state-of-the-art denoisers in terms of quantitative metrics and visual quality. Our source codes and datasets for BoostNet are available at https://github.com/ZeroZero19/BoostNet.git.},
  keywords={Training;Noise reduction;Generative adversarial networks;Generators;Convolutional neural networks;PSNR;Noise measurement;Generative adversarial networks;deep convolutional neural networks;image denoising;BoostNet;fluorescence microscopy images;blind denoising;Drosophila},
  doi={10.1109/ACCESS.2021.3081697},
  ISSN={2169-3536},
  month={},}@ARTICLE{11006101,
  author={Chen, Yuehan and Zhang, Jiqing and Li, Yafeng and Li, Yudong and Tang, Haoming and Wang, Huibing and Fu, Xianping},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Fusion-based Channel-wise Isotropic Convergent Real-time Underwater Image Enhancement}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Existing underwater image enhancement (UIE) methods typically prioritize improving image quality at the expense of algorithmic efficiency. In this paper, we propose a fusion-based, channel-wise isotropic convergent UIE method designed for real-time performance. The proposed approach comprises three key modules: (i) a non-linear transformation module that corrects color casts and aligns the pixel distribution with the gray-world assumption (GWA); (ii) a channel-wise isotropic convergence scheme that reduces intensity distribution disparities across channels, promoting balanced convergence; and (iii) a patch-based enhancement strategy that divides the image into smaller patches to better capture local features and improve adaptability to non-uniform degradation. Moreover, certain critical steps in our method are optimized to achieve O(1) time complexity, allowing it to meet real-time requirements. Extensive experiments validate the effectiveness of each module in the proposed method, showcasing its superiority when compared to the existing state-of-the-art (SOTA) approaches. Code has been released at https://github.com/JohnChenS/FCICE_UnderwaterImageEnhancement.},
  keywords={Image color analysis;Real-time systems;Degradation;Image restoration;Image enhancement;Training;Autonomous underwater vehicles;Imaging;Generative adversarial networks;Fuses;Underwater imaging;Low-Level vision;Realtime underwater image enhancement;Real-time Underwater image restoration},
  doi={10.1109/TCSVT.2025.3570651},
  ISSN={1558-2205},
  month={},}@ARTICLE{11172283,
  author={Zhang, Jiajing and Dai, Bingze and Guan, Haotian and Lee, Wei-Ning},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={MSAF: A Multi-level Sensing Adversarial Framework for Signal Recovery in Synthetic Aperture Ultrasound}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Synthetic aperture ultrasound imaging achieves two-way focusing and produces high-quality image formation by coherently combining returning echoes from successive single-element or sub-aperture transmissions across the entire transducer array. To depict transient or fast tissue dynamics, a prominent line of research efforts involves employing a limited number of sub-apertures to approximate full-aperture scans, enabling full-view imaging at high frame rates. However, existing end-to-end approaches predominantly operate in the image domain, which inherently discards phase information. Some studies use radio-frequency or in-phase and quadrature (IQ) domains, but they analyze spatial features frame by frame, thus not fully leveraging the temporal information. Motivated by high-dimensional data inherent to synthetic apertures, this study proposes a multi-level sensing adversarial framework (MSAF) to recover full IQ signals from sparse sub-aperture transmissions. MSAF utilizes sparse apertures by spatiotemporal sensing and constrains the recovered signals by global-attribute sensing. MASF was evaluated on large in vivo human echocardiography datasets and on in vivo unseen echocardiographic views and abdominal images as zero-shot testing sets. Experimental results demonstrated high-fidelity IQ signal recovery, achieving a peak signal-to-noise ratio of 32.80 dB, a structural similarity index of 0.88, a contrast-to-noise ratio of 2.92 dB, and a contrast ratio of 15.46 dB, outperforming all tailored benchmark models. Therefore, MSAF enables sparse-aperture transmissions with high signal quality and data efficiency, making it a potential approach for functional ultrasound imaging.},
  keywords={Ultrasonic imaging;Apertures;Sensors;Generators;Spatiotemporal phenomena;Spatial resolution;Radio frequency;RF signals;Image quality;Generative adversarial networks;Adversarial Learning;In-phase and quadrature Signal;Radio-frequency;Synthetic Aperture;Ultrafast Ultrasound},
  doi={10.1109/JBHI.2025.3610664},
  ISSN={2168-2208},
  month={},}@ARTICLE{11125494,
  author={Li, Linfeng and Qing, Chunmei and Tan, Junpeng and Jin, Jianxiu and Xu, Xiangmin},
  journal={IEEE Transactions on Multimedia}, 
  title={Artistic Style Transfer via Fine-grained Text Guidance and Contrastive Semantics Similarity}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Due to the development of text-image multimodal methods, text is used to guide the style transfer of images, which has attracted growing attention. Notably, The existing text-guided image style methods are limited to expressing specific artistic style through simple text. It can only accept coarse-grained text input such as “Van Gogh” and “White Cloud”, and cannot understand fine-grained text input such as “The Night Café by Vincent van Gogh”. To this end, this paper proposes a novel artistic style transfer network based on the fine-grained text guidance and the contrastive semantics similarity, named as TCStyler. It can accept images or texts as style guidance, which is more suitable for fine-grained content understanding stylization. In this network, to address the issue of text-image cross-modal discrepancy, the residual attention feature mapper (RAFM) is introduced to constrain the differences between different modalities in feature space. Then, the global cascading style-sharing module (GCSM) is proposed for performing content-style feature fusion and image-text modality fusion by adopting a global feature-sharing strategy. Furthermore, the contrastive semantics similarity loss is designed to address the problem of multimodal universality. Quantitative and visualization experiments demonstrate that our TCStyler can handle fine-grained artistic text inputs and maintain consistency in the style transfer results guided by different modalities.},
  keywords={Semantics;Image synthesis;Generative adversarial networks;Electronic mail;Transformers;Training;Text to image;Feature extraction;Diffusion models;Attention mechanisms;Artistic Style transfer;Fine-grained Text Guidance;Cross-modal Discrepancy;Contrastive Semantics Similarity},
  doi={10.1109/TMM.2025.3599050},
  ISSN={1941-0077},
  month={},}@ARTICLE{11111679,
  author={Marchetti, Francesco and Picano, Benedetta and Seidenari, Lorenzo and Fantacci, Romano},
  journal={IEEE Internet of Things Journal}, 
  title={Foundation Forecasting in IoE Networks: When Generative AI Meets Programmable Edge Nodes}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Artificial intelligence (AI)-native edge networks are promising solutions to seamlessly integrate AI into modern network architectures, promoting intelligent and hyper-flexible behavior in self-adaptation and reconfiguration of hybrid networks. AI-native edge nodes have to promptly react to any change in network conditions and to be linked to Internet of Everything (IoE) deployed in etherogeneous communication domains. This paper deals with a next-generation programmable edge node capable of being employed in different domains in a unified and flexible manner. With the aim to achieve a low re-configuration overhead in such herogeneous contexts, this paper proposes the integration of a generative-AI module within an edge node exploiting foundation models for efficient general-purpose time-series prediction, without involving overhead and costs due to models trained from scratch and overcoming data scarcity. This permits to manage IoE networks deployed in both homogeneous and heterogeneous domains, i.e., aqua, ground and air, autonomously, without the need for adjustments from the outside. As foundation models, we focused on Chronos and TimesFM, in both the zero-shot and fine-tuning learning paradigms. Finally, performance results in terms of prediction accuracy, training, and inference time are provided and compared with those achieved by the state-of-the-art recurrent neural networks trained from scratch and baseline alternatives. The obtained results corroborate the potential of foundation models as key enablers of native AI networks, achieving good accuracy despite the absence of a training phase (i.e., in zero-shot mode) or with limited training (fine-tuning), w.r.t. alternatives.},
  keywords={Foundation models;Training;Artificial intelligence;Forecasting;Time series analysis;Accuracy;Internet of Things;Computer architecture;Systematic literature review;Signal to noise ratio;foundation models;AI native networks;forecasting},
  doi={10.1109/JIOT.2025.3595465},
  ISSN={2327-4662},
  month={},}@INPROCEEDINGS{11162291,
  author={Tong, Wen and Huo, Wei and Lejkin, Thierry and Penhoat, Joel and Peng, Chenghui and Pereira, Carlos and Wang, Fei and Wu, Shaoyun and Yang, Lu and Shi, Yuanming},
  booktitle={2025 IEEE International Conference on Communications Workshops (ICC Workshops)}, 
  title={A-Core: A Novel Framework of Agentic AI in the 6G Core Network}, 
  year={2025},
  volume={},
  number={},
  pages={1104-1109},
  abstract={With the rapid advancement of generative artificial intelligence (GenAI), its integration into next-generation wireless networks is poised to address challenges faced by various stakeholders. To facilitate GenAI’s interaction with dynamic environments, agentic AI and multi-agent systems are gaining prominence. This paper introduces a novel framework, named A-Core, to integrate agentic AI with 6G core networks (CNs), playing a pivotal role in managing connectivity and generating customized services. A-Core leverages a network-wide CN large model (CN-LM), generative network (GN) instances, and multiple collaborative GenAI-based agents with specialized roles for different stages of service creation to autonomously generate customized services. A case study on smart cities demonstrates A-Core’s potential to improve service delivery efficiency. Finally, we discuss key issues in design and implementation of A-Core, outlining future research directions.},
  keywords={6G mobile communication;Training;Visualization;Solid modeling;Three-dimensional displays;Generative AI;Conferences;Wireless networks;Collaboration;Stakeholders;6G;Generative AI;Agentic AI;Core Network;Multi-Agents;Generative Network},
  doi={10.1109/ICCWorkshops67674.2025.11162291},
  ISSN={2694-2941},
  month={June},}@INPROCEEDINGS{9522830,
  author={Jo, Eunsung and Sim, Jae-Young},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Multi-Scale Selective Residual Learning for Non-Homogeneous Dehazing}, 
  year={2021},
  volume={},
  number={},
  pages={507-515},
  abstract={As the particles in hazy medium cause the absorption and scattering of light, the images captured under such environment suffer from quality degradation such as low contrast and color distortion. While numerous single image dehazing methods have been proposed to reconstruct clean images from hazy images, non-homogeneous dehazing has been rarely studied. In this paper, we design an end-to- end network to remove non-homogeneous dense haze. We employ the selective residual blocks to adaptively improve the visibility of resulting images, where the input feature and the residual feature are combined with fully trainable weights. Experimental results including the ablation study demonstrate that the proposed method is a promising tool for non-homogeneous dehazing that enhances the contrast of hazy images effectively while restoring colorful appearance faithfully.},
  keywords={Degradation;Computer vision;Image color analysis;Conferences;Scattering;Tools;Distortion},
  doi={10.1109/CVPRW53098.2021.00062},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{9882217,
  author={Chen, Yao and Zhang, Biao and Li, Zhen and Qiao, Yuanyuan},
  booktitle={2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML)}, 
  title={Ship Detection with Optical Image Based on Attention and Loss Improved YOLO}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Object detection is a critical research topic in computer vision. As a subtask of object detection, Ship detection has important research significance. Most ship detection research is based on SAR (Synthetic Aperture Radar) images in the existing research. However, compared with SAR images, optical images have more image feature information, which can assist the algorithm to better learn ship features. In addition, the research of optical image ship detection has more important commercial value. This paper conducts optical image ship detection experiments, applies the YOLO algorithm to the ship detection task, introduces the attention mechanism to transform the residual block in the DarkNet-53 network, and achieves more excellent performance. At the same time, this paper optimizes the recently proposed CIoU loss function which is better than the ln−norm loss function and presents the AIoU loss function on this basis. By increasing the area penalty term, the AIoU loss improves performance while converging speed compared with CIoU loss. It is applied to ship detection in the optical image and compared with GIoU, DIoU, and CIoU loss functions, and it has achieved better results than them in the bounding box regression of ship detection.},
  keywords={Optical losses;Object detection;Transforms;Optical imaging;Adaptive optics;Radar polarimetry;Pattern recognition;ship detection;attention mechanism;residual block;loss function;boundingbox regression},
  doi={10.1109/PRML56267.2022.9882217},
  ISSN={},
  month={July},}@INPROCEEDINGS{9748537,
  author={Dinh, Tuan Le and Lee, Suk-Hwan and Kwon, Seong-Geun and Kwon, Ki-Ryong},
  booktitle={2022 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={Cell Nuclei Segmentation in Cryonuseg dataset using Nested Unet with EfficientNet Encoder}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Cell Nuclei Segmentation is one of the important stages in clinical research. Many applications in medical treatment, drug discoveries, and disease diagnosis can be benefited from cell nuclei segmentation results. Although it plays a significant role, the task of cell nuclei segmentation seems tedious and time-consuming because of the large number of cells in one histopathology image and the manual nature of the task. The segmentation task of nuclei cells can be automated by computer programs which help to save lots of time and work for histopathologists and also produce stable results, preventing mistakes made by humans. Recently, with the emergence of deep learning methods, many segmentation tasks were done by neural nets with superb output and outperforming many traditional methods. In this paper, we run experiments with CryonuSeg dataset using Nested Unet, a variant of the Unet neural net for medical and biomedical image segmentation and combine the model with EfficientNet as the encoder of the network. The modified network results surpassed many other state-of-the-art techniques on medical image segmentation tasks with the Dice score = 0.929, AJI=0.604, and PQ=0.503.},
  keywords={Image segmentation;Histopathology;Microprocessors;Neural networks;Training data;Medical treatment;Computer architecture;Cell Nuclei Segmentation;Nested Unet;Unet;EfficientNet;medical image analysis},
  doi={10.1109/ICEIC54506.2022.9748537},
  ISSN={2767-7699},
  month={Feb},}@INPROCEEDINGS{9761706,
  author={Wu, Jianghao and Gu, Ran and Dong, Guiming and Wang, Guotai and Zhang, Shaoting},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)}, 
  title={FPL-UDA: Filtered Pseudo Label-Based Unsupervised Cross-Modality Adaptation for Vestibular Schwannoma Segmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Automatic segmentation of Vestibular Schwannoma (VS) from Magnetic Resonance Imaging (MRI) will help patient management and improve clinical workflow. This paper aims to adapt a model trained with annotated ceT1 images to segment VS from hrT2 images, without annotations of the latter. The proposed method is named as Filtered Pseudo Label-based Unsupervised Domain Adaptation (FPL-UDA) and consists of three components: 1) an image translator converting hrT2 images to pseudo ceT1 images, where a two-stage translation strategy is proposed to deal with images with VS in various sizes, 2) a pseudo label generator trained with ceT1 images to provide pseudo labels for the pseudo ceT1 images, where a GAN-based data augmentation method is proposed to deal with the domain gap between them, and 3) a final segmentor trained with hrT2 images and the corresponding pseudo labels, where an uncertainty-based filtering is used to select high-quality pseudo labels to improve the segmentor’s robustness. Experimental results with a public VS dataset showed that our method achieved an average Dice of 81.52% for VS segmentation from hrT2 images, which outperformed existing unsupervised cross-modality adaptation methods.},
  keywords={Image segmentation;Adaptation models;Filtering;Annotations;Magnetic resonance imaging;Magnetic separation;Robustness;Domain adaption;image translation;uncertainty estimation;vestibular schwannoma;pseudo labels},
  doi={10.1109/ISBI52829.2022.9761706},
  ISSN={1945-8452},
  month={March},}
