@INPROCEEDINGS{10050070,
  author={Zheng, Yuyu and Liu, Leyuan},
  booktitle={2022 International Conference on Intelligent Education and Intelligent Research (IEIR)}, 
  title={Rapid Screening of Children With Autism Spectrum Disorders Through Face Image Classification}, 
  year={2022},
  volume={},
  number={},
  pages={266-271},
  abstract={Autism spectrum disorders (ASD) impact the development of children’s language, motor, and expression abilities, causing great adverse effects on children’s growth. The incidence of autism screening is still quite poor, nevertheless, due to the traditional method’s time and financial requirements for child guardians. If symptoms of autism are detected early, children with autism usually return to normal development after effective medical intervention. Furthermore, the likelihood of accurately identifying children with autism grows if deep learning is used to recognize face images of autistic children. In this study, the dataset of autistic children’s faces in the Kaggle database [1] is selected to classify the typically developing children and autistic children through the face recognition model. On model selection, VGG19 [1], VGG16 [2], ResNet18 [3], ResNet101 [4], and DenseNet161 [5] are candidates. After training, among the five models, ResNet101 and DenseNet161 have better performance, and the recall rate of ResNet101 is higher in these two networks.},
  keywords={Training;Deep learning;Pediatrics;Autism;Image recognition;Databases;Face recognition;autism;children;classification;face recognition},
  doi={10.1109/IEIR56323.2022.10050070},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10078539,
  author={Kim, Sung-Jin and Lee, Dae-Hyeok and Choi, Yeon-Woo},
  booktitle={2023 11th International Winter Conference on Brain-Computer Interface (BCI)}, 
  title={CropCat: Data Augmentation for Smoothing the Feature Distribution of EEG Signals}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Brain-computer interface (BCI) is a communication system between humans and computers reflecting human intention without using a physical control device. Since deep learning is robust in extracting features from data, research on decoding electroencephalograms by applying deep learning has progressed in the BCI domain. However, the application of deep learning in the BCI domain has issues with a lack of data and overconfidence. To solve these issues, we proposed a novel data augmentation method, CropCat. CropCat consists of two versions, CropCat-spatial and CropCat-temporal. We designed our method by concatenating the cropped data after cropping the data, which have different labels in spatial and temporal axes. In addition, we adjusted the label based on the ratio of cropped length. As a result, the generated data from our proposed method assisted in revising the ambiguous decision boundary into apparent caused by a lack of data. Due to the effectiveness of the proposed method, the performance of the four EEG signal decoding models is improved in two motor imagery public datasets compared to when the proposed method is not applied. Hence, we demonstrate that generated data by CropCat smooths the feature distribution of EEG signals when training the model.},
  keywords={Deep learning;Computers;Training;Smoothing methods;Feature extraction;Brain modeling;Electroencephalography;computer interface;electroencephalogram;data augmentation;motor imagery},
  doi={10.1109/BCI57258.2023.10078539},
  ISSN={2572-7672},
  month={Feb},}@INPROCEEDINGS{10212455,
  author={S, Anusree P and You, Wonsang},
  booktitle={2023 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC)}, 
  title={A Deep Learning Approach to Generating Flattened CBCT Volume Across Dental Arch From 2D Panoramic X-ray for 3D Oral Cavity Reconstruction}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Panoramic radiography is the most popular imaging modality in dentistry. On the other hand, cone-beam computed tomography (CBCT) is usually utilized to review three-dimensional dental structures in high resolution, however, it is less utilized in dental clinics due to its high cost and radiation exposure. An automated method for reconstructing 3D oral cavity structure from panoramic X-ray will be beneficial for patients to reduce expenses for dental care without radiation exposure. As an initial step to the 3D shape reconstruction of the oral cavity, a flattened CBCT 3D volume across the dental arch can be generated from a 2D panoramic X-ray image using a deep learning method. Existing deep learning models are almost based on the encoder-decoder architecture, but their performance has not been well elucidated. In this paper, we evaluated the applicability of attention U-net for reconstructing the flattened CBCT 3D volume from a panoramic X-ray. In our quantitative and qualitative evaluations, the attention U-net outperformed the auto-encoder and led to enhanced perceptual quality.},
  keywords={Deep learning;Radiography;Computers;Three-dimensional displays;Image resolution;Costs;Shape;CBCT;Panoramic X-ray;3D dental reconstruction},
  doi={10.1109/ITC-CSCC58803.2023.10212455},
  ISSN={},
  month={June},}@INPROCEEDINGS{10007974,
  author={Wang, Chen and He, Zhaofeng and Wang, Caiyong and Tian, Qing},
  booktitle={2022 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={Generating Intra- and Inter-Class Iris Images by Identity Contrast}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Iris recognition is one of the most accurate and reliable biometric technologies. However, due to the high collection costs and privacy of the iris, it is difficult to build a large-scale iris image database for training iris recognition models. This paper proposes a novel iris image generation algorithm which can produce numerous intra- and inter-class iris images. By using contrastive learning, we disentangle identity-related features (e.g., iris texture, left or right eye) and condition-variant features (e.g., pupil size, iris exposure ratio) in the generated images. This disentanglement facilitates identity control over synthetic iris images. Since the iris has the multi-degree-of-freedom (MDOF) topology and high-entropy texture, we specially design the dual-channel input protocol to separate the topology and texture of the iris, so that the generator can infer multi-condition iris images while maintaining the unique texture details. Extensive experiments demonstrate that the proposed approach achieves impressive performance in both image quality and identity representation.},
  keywords={Training;Image quality;Protocols;Image synthesis;Biological system modeling;Topology;Reliability},
  doi={10.1109/IJCB54206.2022.10007974},
  ISSN={2474-9699},
  month={Oct},}@INPROCEEDINGS{9191275,
  author={Deng, Ye and Wang, Jinjun},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={Image Inpainting Using Parallel Network}, 
  year={2020},
  volume={},
  number={},
  pages={1088-1092},
  abstract={Due to the lack of contextual information and the difficulty to directly learn the distribution of a complete image, existing image inpainting methods always use a two-stages approach to make plausible prediction for missing pixels in a coarse-to-fine manner. In this paper, we propose a novel inpainting method with two parallel pipelines. The first pipeline is a standard image completion path that takes the corrupted image as input and outputs the predicted complete image. The second pipeline exists only during the training phase that inputs a complementary image of the corrupted one and still outputs the same complete image. The two pipelines operate simultaneously, and they share identical encoder and most parameters in the decoder. Furthermore, inspired by VAE, random Gaussian noise are added to the features not only to improve the robustness of the model but also to enable generating diverse and plausible results. We evaluated our model on several public datasets and demonstrated that the proposed method outperforms several state-of-the-arts approaches.},
  keywords={Pipelines;Image reconstruction;Training;Gaussian noise;Gallium nitride;Standards;Decoding;Image Completion;Parallel Pipelines;Complementary Images;Random Noise},
  doi={10.1109/ICIP40778.2020.9191275},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10446941,
  author={Kim, Dohoon and Shin, Minwoo and Ryu, Jaeseok and Lim, Heunseung and Paik, Joonki},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Pu-Edgeformer++: An Advanced Hierarchical Edge Transformer for Arbitrary-Scale Point Cloud Upsampling using Distance Fields}, 
  year={2024},
  volume={},
  number={},
  pages={6230-6234},
  abstract={Despite of pre-processing the raw point cloud is important, limited research has been conducted on learning-based approaches to point cloud upsampling. PU-EdgeFormer [1] model stands out for its exceptional performance, which is attributed to its unique inductive biases that seamlessly blend both local and global characteristics of point clouds through the integration of graph convolution and transformer mechanisms. In this paper, we present EdgeFormer++, an advanced hierarchical edge transformer module designed for arbitrary-scale point cloud upsampling. Our module employs crossattention and dense connections to integrate information from both feature and input point cloud while maintaining the structural elements of graph convolutions and transformers. Experimental results, both qualitative and quantitative, indicate that our proposed method outperforms existing techniques in point cloud upsampling. The official source code is accessible at https://github.com/dohoon2045/PU-EdgeFormer2.},
  keywords={Point cloud compression;Manifolds;Measurement;Convolutional codes;Convolution;Source coding;Computer architecture;Point cloud upsampling;graph convolution;vision transformer;implicit neural representation},
  doi={10.1109/ICASSP48485.2024.10446941},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9102867,
  author={Zhu, Dandan and Chen, Yongqing and Han, Tian and Zhao, Defang and Zhu, Yucheng and Zhou, Qiangqiang and Zhai, Guangtao and Yang, Xiaokang},
  booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Ransp: Ranking Attention Network For Saliency Prediction On Omnidirectional Images}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Various convolutional neural network (CNN)-based methods have shown the ability to boost the performance of saliency prediction on omnidirectional images (ODIs). However, these methods are limited by sub-optimal accuracy, because not all the features extracted by the CNN model are not useful for the final fine-grained saliency prediction. Features are redundant and have negative impact on the final fine-grained saliency prediction. To tackle this problem, we propose a novel Ranking Attention Network for saliency prediction (RANSP) of head fixations on ODIs. Specifically, the part-guided attention (PA) module and channel-wise feature (CF) extraction module are integrated in a unified framework and are trained in an end-to-end manner for fine-grained saliency prediction. To better utilize the channel-wise feature map, we further propose a new Ranking Attention Module (RAM), which automatically ranks and selects these maps based on scores for fine-grained saliency prediction. Extensive experiments are conducted to show the effectiveness of our method for saliency prediction of ODIs.},
  keywords={Feature extraction;Head;Predictive models;Random access memory;Two dimensional displays;Task analysis;Omnidirectional images;saliency prediction;ranking attention;part-guided attention;channel-wise feature map},
  doi={10.1109/ICME46284.2020.9102867},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{10353502,
  author={Varun Prakash, J M and Kodipalli, Ashwini and Rao, Trupthi and Kumaraswamy, S},
  booktitle={2023 4th IEEE Global Conference for Advancement in Technology (GCAT)}, 
  title={Comparative Study on the Analysis of the Performance of Transfer Learning and the Customized Convolutional Neural Network to Detect Eyewear}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper, we propose a method for eyewear detection using Convolutional Neural Networks (CNNs). The objective of the study is to accurately classify images into two classes: “Glasses” and “No Glasses The proposed approach is evaluated on a dataset comprising a total of 5000 images, with a training set consisting of 1947 images in the “Glasses” class and 1554 images in the “No Glasses” class. The testing set includes 834 images in the “Glasses” class and 665 images in the “No Glasses” class.We evaluate the eyewear detection performance using various evaluation metrics, including accuracy, precision, and recall. The outcomes demonstrate how well the suggested CNN-based technique for eyeglasses detection works. The trained model achieves an overall accuracy of 99.17%, with a precision of 0.76% and a recall of 0.44% on the testing set. The discovery advances the field of computer vision and creates new opportunities for improving eyeglasses detection methods through further additional study.},
  keywords={Training;Measurement;Computer vision;Transfer learning;Glass;Computer architecture;Feature extraction;Convolutional Neural Network;Eyewear detection;Computer Vision;Feature Extraction;Accuracy Evaluation;Image Classification},
  doi={10.1109/GCAT59970.2023.10353502},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10444444,
  author={Kim, Minki and Kim, Junyeong},
  booktitle={2024 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={A Study on Automatic Open-Domain Dialogue Evaluation Metrics}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={Given the extensive use of conversational systems in daily life, there arises a need for an automated evaluation system tailored to such conversations. In light of this, an array of studies is underway to develop automatic evaluation metrics. Unlike conventional approaches, contemporary methodologies incorporate techniques that leverage the contextual nuances of conversation systems. In conclusion, it points out that recently explored methodologies are better suited for assessing conversations compared to classical methods; however, it is worth noting that these methodologies may also possess inherent limitations.},
  keywords={Measurement;Correlation;Catalysts;Deep architecture;Oral communication;Probabilistic logic;Consumer electronics;open-domain dialogue;automatic evaluation;evaluation metrics},
  doi={10.1109/ICCE59016.2024.10444444},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10164457,
  author={Wang, Jingjing and Li, Xi},
  booktitle={2023 6th International Symposium on Autonomous Systems (ISAS)}, 
  title={Cross-Attention Network for Cross-View Image Geo-Localization}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The task of cross-view geo-location is to get a corresponding image from a dataset of Global Positioning System (GPS) labeled aerial-view images, given a ground-view query image with an unknown location. This task presents challenges due to the significant differences in viewpoint and appearance between the two types of images. To overcome these challenges, we have developed a novel attention-based method that leverages a key localization cue. The cross-attention-based Swap Encoder Module (SEM) is proposed, which effectively aligns features by directing the network’s focus towards relevant information. Additionally, we employ an Image Proposal Network (IPN) to ensure consistent inputs of both aerial and ground-view images that correspond, during both training and validation phases. Experimental results show that our proposed network significantly outperforms existing benchmarking CVUSA dataset, with significant improvements for top-1 recall from 61.4% to 71.45%, and for top-10 from 90.49% to 92.30%.},
  keywords={Location awareness;Training;Visualization;Autonomous systems;Image processing;Benchmark testing;Proposals;cross-view geo-localization;cross-attention;machine vision and image processing},
  doi={10.1109/ISAS59543.2023.10164457},
  ISSN={},
  month={June},}@INPROCEEDINGS{10448464,
  author={Yin, Wang and Lu, Peng and Peng, Xujun},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={COLORFLOW: A Conditional Normalizing Flow for Image Colorization}, 
  year={2024},
  volume={},
  number={},
  pages={2735-2739},
  abstract={Image colorization is an ill-posed task, as objects within grayscale images can correspond to multiple colors, motivating researchers to establish a one-to-many relationship between objects and colors. Previous work mostly could only create an insufficient deterministic relationship. Normalizing flow can fully capture the color diversity from natural image manifold. However, classical flow often overlooks the color correlations between different objects, resulting in generating unrealistic color. To solve this issue, we propose a conditional normalizing flow, named ColorFlow, to jointly learn the one-to-many relationships between objects and colors, and the color correlations between different objects within image. To represent these color correlations in flow, we design a color distribution predictor to estimate the global color histogram of grayscale image as global tones, which is utilized as the mean value of flow’s latent variables. Experiments results show that ColorFlow outperforms state-of-the-art methods.},
  keywords={Manifolds;Histograms;Correlation;Image color analysis;Gray-scale;Signal processing;Acoustics;Image understanding;Normalizing flow;Colorization},
  doi={10.1109/ICASSP48485.2024.10448464},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10690135,
  author={D, Renny Harlin and Gupta, Kritesh Kumar},
  booktitle={2024 IEEE Recent Advances in Intelligent Computational Systems (RAICS)}, 
  title={Neural Networks Assisted Vigenère Cipher Encryption of Text}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={With the recent advent of the digital interface in our day-to-day life, internet usage for a variety of purposes such as bank transactions, social media, communication and cloud storage has risen exponentially. Since the internet is surfed worldwide, the odds of the data being exposed to cyber criminals are significantly high. This establishes a strong rationale for devising an automated framework for end-to-end encryption for securing the data. In the present study, we utilized the computational efficiency of long short-term memory (LSTM) networks to predict the accurate vigenère cipher. The synthetic dataset with 10000 samples consisting of the alphabetical plain text and corresponding Vigenère cipher based on the fixed key “KEY” is utilized to train, test and validate the deep learning model. The validation of the LSTM model depicted exceptional accuracy, which is further reflected while performing different experiments of encrypting the plain text in the final stage of this study.},
  keywords={Deep learning;Ciphers;Accuracy;Social networking (online);Computational modeling;Neural networks;Encryption;Vigenère cipher;vigenère encryption using neural network;Recurrent Neural Network;LSTM},
  doi={10.1109/RAICS61201.2024.10690135},
  ISSN={2769-5565},
  month={May},}@INPROCEEDINGS{10221991,
  author={Kim, Min-jae and Kwon, Subin and Ahn, Byung-hyun and Ha, Euntaek and Choi, Yeonhee and Paik, Joonki},
  booktitle={2023 IEEE International Conference on Image Processing (ICIP)}, 
  title={Adaptive Camouflage Pattern Generation to Different Environments Via Content-Aware Style Transfer}, 
  year={2023},
  volume={},
  number={},
  pages={1645-1649},
  abstract={Visual camouflage is an effective means of protecting valuable assets that are vulnerable to theft, espionage, or other forms of malicious activity. To overcome the limitation of standardized camouflage patterns in certain environments, we need an innovative approach that adapts the camouflage pattern to the specific surroundings of the asset to be concealed. In this paper, we present a novel camouflage image generation method whose results change by the circumstances around the asset to be concealed using the style transfer. In order to remove the influence of stuff and objects that are not advantageous for camouflage in the surrounding, we propose a novel mechanism that introduces the contents-aware information into the calculation of style representation. Experimental results in various situations, including the snowy natural scene, show that the proposed method provides excellent adaptive camouflage outcomes while effectively suppressing conspicuous elements in the surrounding.},
  keywords={Visualization;Image segmentation;Image recognition;Image synthesis;Image color analysis;Neural networks;Pattern recognition;Camouflage;Style transfer;Convolutional neural networks (CNNs);Panoptic segmentation},
  doi={10.1109/ICIP49359.2023.10221991},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10065606,
  author={Shen, Wendi and Yang, Genke},
  booktitle={2022 International Conference on Mechanical, Automation and Electrical Engineering (CMAEE)}, 
  title={An error neighborhood-based detection mechanism to improve the performance of anomaly detection in industrial control systems}, 
  year={2022},
  volume={},
  number={},
  pages={25-29},
  abstract={Anomaly detection for devices (e.g, sensors and actuators) plays a crucial role in Industrial Control Systems (ICS) for security protection. The typical framework of deep learning-based anomaly detection includes a model to predict or reconstruct the state of devices and a detection mechanism to determine anomalies. The majority of anomaly detection methods use a fixed threshold detection mechanism to detect anomalous points. However, the anomalies caused by cyberattacks in ICSs are usually continuous anomaly segments. In this paper, we propose a novel detection mechanism to detect continuous anomaly segments. Its core idea is to determine the start and end times of anomalies based on the continuity characteristics of anomalies and the dynamics of error. We conducted experiments on the two real-world datasets for performance evaluation using five baselines. The F1 score increased by 3.8% on average in the SWAT dataset and increased by 15.6% in the WADI dataset. The results show a significant improvement in the performance of baselines using an error neighborhood-based continuity detection mechanism in a real-time manner.},
  keywords={Integrated circuits;Performance evaluation;Mechanical sensors;Industrial control;Sensor phenomena and characterization;Predictive models;Real-time systems;industrial control system;anomaly detection;detection mechanism;unsupervised learning},
  doi={10.1109/CMAEE58250.2022.00013},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10980979,
  author={Rue, Nejung and Na, Inye and Lee, Ho Yun and Park, Hyunjin},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Misalignment-Aware MRI-to-CT Synthesis for Lung Segmentation on MRI}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Lung MRI is increasingly utilized for its radiation-free nature and ability to reflect lung function, but weak lung signals make accurate segmentation difficult. In addition, the lack of labeled datasets, as manual labeling is labor-intensive, poses a challenge for lung MRI segmentation. Noting that pre-trained lung CT segmentation models are widely available, we propose a novel framework that applies MRI-to-CT translation using a diffusion model to address the issue. The synthesized CT can be easily applied to the pre-trained lung CT segmentation model. Our approach also resolves misalignment issues caused by differences in acquisition principles and breathing techniques (free breathing vs. breath-hold) between MRI and CT, while effectively capturing MRI's structural details and CT's lung-specific information to enhance segmentation accuracy. The proposed method surpasses existing techniques in both quantitative metrics, such as Dice coefficient and Hausdorff distance, and qualitative evaluations, particularly in difficult-to-segment MRI slices. This research sets a new standard for lung MRI segmentation, reducing the need for MRI-specific labels and paving the way for future advancements in the field. Our code is available at https://github.com/Nejung-Rue/MR2CTforLungSeg.},
  keywords={Measurement;Image segmentation;Translation;Accuracy;Lungs;Magnetic resonance imaging;Computed tomography;Diffusion models;Labeling;Standards;Lung MRI segmentation;MRI-to-CT translation;Misalignment awareness;3-channel diversity},
  doi={10.1109/ISBI60581.2025.10980979},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{10879688,
  author={Lee, Joonwoo and Uk-Jin Lee, Scott},
  booktitle={2025 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={Exploration of Diffusion-Based Test Case Generation of Autonomous Driving Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-3},
  abstract={Autonomous driving systems demand extensive testing to guarantee safety and reliability. However, real-world testing is often costly and limited in variety. This paper investigates the use of diffusion models to generate synthetic driving images as a means of augmenting test datasets for autonomous driving systems. Leveraging their capability to produce diverse and highly realistic images, diffusion models can simulate various driving conditions, including rare or challenging scenarios that are difficult to replicate. By incorporating these synthetic images into the testing pipeline, self-driving systems can be assessed across a wider range of conditions, enhancing their robustness and safety.},
  keywords={Deep learning;Technological innovation;Text to image;Transforms;Diffusion models;Data models;Robustness;Safety;Autonomous vehicles;Testing;Autonomous Driving;Diffusion;Test Generation},
  doi={10.1109/ICEIC64972.2025.10879688},
  ISSN={2767-7699},
  month={Jan},}@INPROCEEDINGS{10222280,
  author={Qiu, Ji and Lu, Peng and Peng, Xujun},
  booktitle={2023 IEEE International Conference on Image Processing (ICIP)}, 
  title={Learning to Draw Through A Multi-Stage Environment Model Based Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1240-1244},
  abstract={Machine drawing has gradually become a hot research topic in computer vision and robotics domains recently. However, decomposing a given target image from raster space into an ordered sequence and reconstructing those strokes is a challenging task. In this work, we focus on the drawing task for the images in various styles where the distribution of stroke parameters differs. We propose a multi-stage environment model based reinforcement learning (RL) drawing framework with fine-grained perceptual reward to guide the agent under this framework to draw details and an overall outline of the target image accurately. The experiments show that the visual quality of our method slightly outperforms SOTA method in nature and doodle style, while it outperforms the SOTA approaches by a large margin with high efficiency in sketch style.},
  keywords={Visualization;Computer vision;Reinforcement learning;Task analysis;Image reconstruction;Vision;Reinforcement Learning;Drawing},
  doi={10.1109/ICIP49359.2023.10222280},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10448415,
  author={Elabid, Zakaria and Busby, Daniel and Hadid, Abdenour},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows}, 
  year={2024},
  volume={},
  number={},
  pages={7020-7024},
  abstract={Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and simulations. Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering. However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions. This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework. We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations. By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques. Moreover, by leveraging the power of neural networks, the computational cost associated with numerical simulations is substantially reduced. We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models.},
  keywords={Knowledge engineering;Solid modeling;Computational modeling;Oils;Neural networks;Production;Predictive models;Physics-informed neural networks;knowledge based learning;reservoir engineering;deep learning;porous media},
  doi={10.1109/ICASSP48485.2024.10448415},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10695330,
  author={Chen, Xiaoxiao and Yu, Cheng},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)}, 
  title={Advanced Human-Guided Prompts for Enhancing Text-to-Image Synthesis via Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={534-537},
  abstract={In this work, we propose a novel method for text-to-image generation, combining the techniques of stable diffusion and large language model. Our approach improves prompt representation using large language model, with key tokens based on defects in the observed images. These key tokens draws inspiration from prompt learning. We improve synthesized images by using better prompts with key tokens from a large language model. Experiments show that ourapproach achieves better performance compared to existing methods.},
  keywords={Information science;Large language models;Computational modeling;Text to image;Feeds;Text-to-image;stable diffusion;large language model;prompt learning;image generation},
  doi={10.1109/CISAT62382.2024.10695330},
  ISSN={},
  month={July},}@INPROCEEDINGS{11086056,
  author={He, Siqi and Xing, Jianping and Zhang, Jingfang and Sun, Kaiping and Bai, Yu and Li, Jiachang},
  booktitle={2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL)}, 
  title={Research on Dehazing Algorithm Based on Spatial Pyramid Attention}, 
  year={2025},
  volume={},
  number={},
  pages={651-654},
  abstract={To address the issue that existing image dehazing methods using convolutional neural networks (CNNs) as the backbone fail to utilize the deep detail features of hazy images, leading to suboptimal dehazing performance of trained models, this paper innovatively integrates network structure improvement and a two-stage feature fusion strategy based on a single-image dehazing method using Detail-Enhanced Convolution (DEConv) and Content-Guided Attention (CGA). For network architecture optimization, the improvement involves serially fusing the Spatial Pyramid Attention (SPA) module with the Detail Enhancement Attention Block (DEAB) to construct a hierarchical feature extraction system. The two-stage feature fusion establishes a cascade mechanism of cross-level attention mixing and multi-scale attention enhancement. These methods enhance the model's ability to capture multi-scale structural information and accurately perceive haze concentration distributions.},
  keywords={Deep learning;Image dehazing;Technological innovation;Computer vision;Convolution;Network architecture;Feature extraction;Convolutional neural networks;Optimization;Periodic structures;Image Dehazing;SPA;DEConv;Two-Stage Feature Fusion;Multi-Scale Feature Extraction;CNN},
  doi={10.1109/CVIDL65390.2025.11086056},
  ISSN={},
  month={May},}@INPROCEEDINGS{10887743,
  author={Lv, Qincheng and Liu, Xiaofeng and Li, Jie and Ni, Rongrong and Xue, Pujun and Song, Siyang},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Hierarchical Multimodal Decoupling-Fusion Framework for offline Multiple Appropriate Facial Reaction Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Facial reactions convey crucial emotional information and coordinating interpersonal relationships in human dyadic interactions. While existing Multiple Appropriate Facial Reaction Generation (MAFRG) methods focus on generating multiple reasonable facial reactions, none of these approaches combines 2D and 3D facial behaviour information nor account for the influence of individuals’ facial identities, leading to inconsistencies in the generated facial reactions and limited capability in capturing subtle variations in facial depth and expression dynamics. This paper proposes a novel Hierarchical Multimodal Decoupling-Fusion (HMDF) framework that decouples 3D facial identity from expression behaviors, eliminating identity-based interference in the reaction generation process, which are integrated with audio-visual features through a cross-attention mechanism. Experiments show that our framework achieved the enhanced diversity and synchrony in the generated facial reactions.},
  keywords={Three-dimensional displays;Interference;Signal processing;Acoustics;Speech processing;Multiple Appropriate Facial Reaction Generation;Facial identity decoupling;2D and 3D facial behaviour fusion},
  doi={10.1109/ICASSP49660.2025.10887743},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10864498,
  author={Ruan, Dan-Ling and Liu, Jian-Wei},
  booktitle={2024 China Automation Congress (CAC)}, 
  title={Diffusion Modeling and Transformer Knowledge Distillation Techniques for Time Series Imputation}, 
  year={2024},
  volume={},
  number={},
  pages={3103-3108},
  abstract={In recent years, big data analysis has gained popularity, but data are often missing during collection, storage, etc., which affects the effectiveness of data analysis. Therefore, this paper proposes a Time Series Imputation method based on Diffusion model and Knowledge Distillation (TSI-DIKD). The diffusion model imputation missing values in time series and is combined with a Transformer structure to capture the intricate dependencies present in the time series data. Knowledge distillation is also used in the reverse process of the diffusion model to improve imputation accuracy. We conducted experiments on healthcare and environmental data to compare the performance of the model with different percentages of missing values. The results show that our model outperforms other models on all three evaluation metrics.},
  keywords={Accuracy;Computational modeling;Time series analysis;Noise reduction;Medical services;Predictive models;Diffusion models;Transformers;Imputation;Data models;diffusion modeling;Transformer;knowledge distillation},
  doi={10.1109/CAC63892.2024.10864498},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10191100,
  author={Seo, Youjin and Jeong, ByeongChang and Yoon, Yeji and Kim, Daegyeom and Min, JuHong and Han, Cheol E.},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={3D Super Resolution for Non-Isotropic Medical Image Through Multi-Input 3D ResUnet}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Fluid-attenuated inversion recovery imaging (FLAIR) is a magnetic resonance (MR) method that is frequently utilized to diagnose brain lesions. However, it usually provides transverse section imaging in high resolution but with very low resolution in the other axis. This non-isotropy huddles its wide utilization in research including machine learning. In this study, we applied a deep-learning based super resolution (SR) technique to convert non-isotropic FLAIR images into isotropic one. We proposed a multi-input 3D ResUnet that uses both FLAIR and T1-weighted MR images as its inputs. As a result, our proposed model successfully reconstructed isotropic FLAIR images (SSIM: 0.9947 $\pm 0.0009)$, and out-performed the FLAIR only single-input 3D ResUnet.},
  keywords={Solid modeling;Three-dimensional displays;Superresolution;Magnetic resonance;Machine learning;Lesions;Medical diagnostic imaging;Super Resolution;multi-input 3D ResUnet;Fluid-attenuated inversion recovery (FLAIR) imaging;non-isotropy},
  doi={10.1109/IJCNN54540.2023.10191100},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11064527,
  author={Sun, ShuJiao and Chen, Li},
  booktitle={2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={Research on De-fogging Algorithm Based on Adaptive Sky Recognition and Accurate Atmospheric Light Estimation}, 
  year={2025},
  volume={},
  number={},
  pages={54-58},
  abstract={Aiming at the problem that the traditional dark-channel defogging algorithm is prone to the distortion of the sky region color and the miscalculation of the atmospheric light value due to white objects when processing foggy images of urban roads, a defogging algorithm that can adaptively identify the sky region and realize the accurate estimation of the atmospheric light value is proposed. First, in the sky recognition stage, an image that enhances the detailed features of the image by reducing the luminance component is selected as the processing object, and the improved Canny edge detection method is used to enhance the edge gradient of the sky region, and combined with the maximum interclass variance method (OTSU) and the adaptive parameter, the algorithmic model for adaptive recognition of sky regions is constructed to realize the accurate segmentation of the sky region in the foggy image. Then, according to whether the sky region is detected or not, different atmospheric light estimation strategies are applied: if there is a sky region, the quadtree method is applied directly to estimate the atmospheric light value in the region; if there is no sky region, the luminance threshold is calculated by using the minimum error method, and based on the threshold, the bright regions in the image are extracted and the luminance is reduced, and then the quadtree method is applied to the processed image for the estimation of the atmospheric light value. Subsequently, a coarse transmittance map is generated and the transmittance is optimized by bootstrap filtering. Finally, by subjectively and objectively evaluating the original haze image and the restored image, including the comparison of several quality metrics such as Universal Quality Index (UQI), Structural Similarity (SSIM), and Peak Signal-to-Noise Ratio (PSNR), the experimental results show that the proposed algorithm outperforms the other classical methods in terms of dehazing performance, and exhibits a superior image restoration effect.},
  keywords={Image segmentation;Accuracy;PSNR;Roads;Image edge detection;Atmospheric modeling;Estimation;Distortion;Image restoration;Object recognition;image defogging;dark channel prior;sky region identification;atmospheric light value estimation;image segmentation},
  doi={10.1109/NNICE64954.2025.11064527},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11085198,
  author={Singal, Pulkit and Saha, Sudipan},
  booktitle={2025 International Conference on Next Generation Information System Engineering (NGISE)}, 
  title={Task Arithmetic for Multi-Domain Visual Learning}, 
  year={2025},
  volume={1},
  number={},
  pages={1-5},
  abstract={Task Arithmetic is a novel approach that enables efficient fine-tuning of pre-trained models by manipulating weight spaces through arithmetic operations, such as addition and subtraction, without altering the original parameters. Although extensively explored in NLP, its application in multi-domain computer vision remains less explored. This study focuses on leveraging Task Arithmetic models for domain adaptation in image classification tasks. By modeling domain shifts through transformations in representation spaces, it enables robust generalization across domains while reducing reliance on large labeled datasets. Unlike conventional domain adaptation methods that require simultaneous access to source and target domain data, Task Arithmetic removes this dependency, providing a modular and scalable alternative. This work marks the first successful application of Task Arithmetic for visual domain adaptation, demonstrating its potential as a cost-effective and interpretable solution.},
  keywords={Adaptation models;Visualization;Computer vision;Computational modeling;Vectors;Next generation networking;Knowledge transfer;Arithmetic;Image classification;Information systems;Task arithmetic;Task vector;Domain adaptation;Modular learning;Image classification;Knowledge transfer;Computer vision;Deep learning},
  doi={10.1109/NGISE64126.2025.11085198},
  ISSN={},
  month={March},}@INPROCEEDINGS{10036616,
  author={Fan, Zhixiang and Qian, Pengjiang},
  booktitle={2022 21st International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)}, 
  title={Intelligent judgment of rotating machinery based on multi-scale parallel network and attention mechanism}, 
  year={2022},
  volume={},
  number={},
  pages={91-94},
  abstract={The primary problem solved in rotating machinery fault diagnosis is how to effectively extract fault features from the vibration signals with noise. To extract fault features accurately, this study proposes a multi-scale parallel convolutional neural network fault recognition algorithm, which can carry out feature fusion. The above method combines empirical feature extraction (e.g., fast Fourier transform) to enrich feature information, which can effectively implement deep learning. The effectiveness and reliability of the method are verified through example studies on JNU, SEU and PU rolling bearing experimental data sets. The algorithm has the higher classification capability and diagnostic accuracy compared with four common deep learning algorithms.},
  keywords={Deep learning;Vibrations;Fault diagnosis;Fast Fourier transforms;Rolling bearings;Distributed databases;Feature extraction;component;Mechanical intelligent diagnosis;Deep neural network;Multi-scale module;Attention mechanism},
  doi={10.1109/DCABES57229.2022.00040},
  ISSN={2473-3636},
  month={Oct},}@INPROCEEDINGS{9871795,
  author={Li, Chenguang and Yang, Hongjun and Cheng, Long and Huang, Fubiao},
  booktitle={2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={A Time-Series Augmentation Method Based on Empirical Mode Decomposition and Integrated LSTM Neural Network}, 
  year={2022},
  volume={},
  number={},
  pages={333-336},
  abstract={Adequate patients' data have always been critical for disease assessment. However, large amounts of patient data are often difficult to collect, especially when patients are required to complete a series of assessment movements. For example, assessing the hand motor function of stroke patients or Parkinson's patients requires patients to complete a series of evaluation movements, and it is often difficult for patients to complete each group of actions multiple times, resulting in a small amount of data. To solve the problem of insufficient data quantity, this study proposes a data augmentation method based on empirical mode decomposition and integrated long short-term memory neural network (EMD-ILSTM). The method mainly consists of two parts: one is to decompose the raw signal by the method of EMD, and the other is to use LSTM for data augmentation of the decomposed signal. Then, the method is tested on the public dataset named Ninaweb, and the test results show that the classification accuracy can be improved by 5.2% by using the augmented data for classification tasks. Finally, clinical trials are conducted to verify that after dimensionality reduction, the augmented data and raw data have smaller intra-class distances and larger inter-class distances, indicating that data augmentation is effective.},
  keywords={Dimensionality reduction;Empirical mode decomposition;Stroke (medical condition);Clinical trials;Biology;Task analysis;Biological neural networks},
  doi={10.1109/EMBC48229.2022.9871795},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10792679,
  author={Zhao, ChenYu and Han, Yi and Ouyang, Ningkang and Zhao, YiFan},
  booktitle={2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)}, 
  title={Unsupervised Rail Surface Defect Detection Method}, 
  year={2024},
  volume={9},
  number={},
  pages={293-297},
  abstract={Railroad travel has become an essential part of modern life, therefore, rail surface defect detection has become a problem that cannot be ignored. However, most of the currently existing methods often require a large number of defect samples for learning, which are labor-intensive and very difficult to obtain. In order to alleviate the data dependence of the detection model, this paper proposes an unsupervised method. Specifically, we use Transformer’s attention mechanism to construct the encoder-decoder structure and train it only on normal samples with the purpose of reconstruction, so that those regions that cannot be reconstructed will be regarded as defects at the time of detection. To prevent the model from generalizing its generalization ability to those unknown defective samples, i.e., to avoid overgeneralization, we introduce a noise perturbation in the training so that the model focuses on the reconstruction of normal features. Experiments show that the proposed unsupervised approach outperforms a host of supervised models with low data requirements and can effectively address the current challenges of track defect detection.},
  keywords={Rails;Surface reconstruction;Perturbation methods;Noise;Transformers;Data models;Decoding;Image reconstruction;Unsupervised learning;Defect detection;Rail Surface Defect Detection;Unsupervised Learning;Autoencoder;Transformer},
  doi={10.1109/ICIIBMS62405.2024.10792679},
  ISSN={2189-8723},
  month={Nov},}@INPROCEEDINGS{10810212,
  author={Wang, Haoming and Gao, Shu},
  booktitle={2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)}, 
  title={Decomposition-driven Multi-level Feature Fusion Framework for Infrared and Visible Image Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={618-621},
  abstract={This paper proposes a novel infrared and visible image fusion method based on the feature decomposition and the Multi-level Feature Fusion. Firstly, modal features are initially extracted and decomposed into unique features with disparate foreground information and common features with similar background information to better discern the background to be maintained and the foreground to be emphasized. Then, the multi-level features are extracted and fused, thus the target image contains detailed and semantic information in low-level and high-level features respectively. Finally, the fused multi-level feature is reconstructed to the target image. Experiments on public datasets M3FD and MSRS show that the proposed method is better than other latest methods.},
  keywords={Semantics;Brightness;Feature extraction;Data mining;Image fusion;Image reconstruction;Infrared and visible image fusion;Decomposition-driven;Multi-level Fusion},
  doi={10.1109/ISCEIC63613.2024.10810212},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10490819,
  author={Zhang, Yanming and Li, Xianlong},
  booktitle={2023 7th International Conference on Electrical, Mechanical and Computer Engineering (ICEMCE)}, 
  title={Database Multi-Connection Query Optimization Based on Improved Snake Optimization Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={946-950},
  abstract={In the face of a large amount of data and complex multi-connection query requirements, Finding a quick way to search for the best query execution plan is a research hotspot in improving database query speed. This paper proposes a database multi-connection query optimization method based on the improved snake optimization algorithm, An iterative chaotic mapping is utilized to initialize the population and enhance the population's diversity; A multi-armed bandit strategy is employed for the dynamic selection of population behavior modes; furthermore, an opposites-learning strategy is employed to update the optimal solution and population acquired at each iteration, thereby enhancing the algorithm's self-learning capability. Experimental results show that the algorithm proposed in this paper can find a better query execution plan in a shorter time.},
  keywords={Databases;Heuristic algorithms;Query processing;Simulation;Sociology;Iterative algorithms;Statistics;component;multi-connected query optimization;snake optimization algorithm;query execution plan},
  doi={10.1109/ICEMCE60359.2023.10490819},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10181689,
  author={Wang, Chen and Hao, Chao and Wang, Guijin and Su, Nan},
  booktitle={2023 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Texture-Shape Optimized GAT for 3D Face Reconstruction}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={3D face reconstruction is widely used in face recognition research, online makeup, etc. However, texture and shape distortion regions usually exist in the reconstruction results. This paper proposes a novel framework named Texture-Shape optimized GAT for 3D face Reconstruction (TSGAT-3D), including data preprocessing and training phases. In the data preprocessing phase, we adopt the styleGAN2 to convert single-view images to multi-view images set. In the training phase, we design a novel Texture-Shape optimized Graph Attention Network, which can learn the facial prior knowledge from the multi-view images set, to improve the details of the initially reconstructed faces based on the auto-encoder module. This network can aggregate the features of face vertices according to the correlation of vertices, thereby improving the accuracy of the 3D reconstructed faces. Furthermore, we present a view loss function for this framework to constrain the shape and texture of the reconstructed face. Extensive experiments conducted on CelebA and Bosphorus show that the reconstruction results of our proposed method are closer to the real 3D faces.},
  keywords={Training;Knowledge engineering;Three-dimensional displays;Correlation;Shape;Face recognition;Data preprocessing;3D face reconstruction;GAN;Graph attention network;Normalized Chamfer Distance},
  doi={10.1109/ISCAS46773.2023.10181689},
  ISSN={2158-1525},
  month={May},}@INPROCEEDINGS{9648586,
  author={Shakhovska, Nataliya and Natiahlyi, Andrii},
  booktitle={2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)}, 
  title={The System for Eliminating Blurring of Text Images Using Convolutional Neural Networks}, 
  year={2021},
  volume={1},
  number={},
  pages={93-96},
  abstract={Image blur degrades the quality of human perception of images and significantly complicates their subsequent analysis using computer vision systems. Real blurs tend to have unknown, spatially complex, and unique blur kernels and usually are even more complicated by noise and other artifacts. The above factors determine the relevance of developing, analyzing, and improving deblurring methods based on convolutional neural networks. A thorough study of existing solutions in the field of machine learning and convolutional neural networks for restoring blurry images was conducted. This work provides a detailed description of the key elements of convolutional neural networks, describes the loss function used for training image recovery models, and provides an overview of perception loss. As a result of the thesis, a new architecture of deep convolutional neural network called CNN was proposed, which, while having 25 times fewer parameters, achieves similar results to the CNN-L15 network on PSNR, SSIM metrics, and during visual comparison of resulting images. Also, modifications to the CNN-L15 network architecture and usage of perceptual loss function are proposed, which help the network achieve significantly better results on metrics, and subjective visual clarity of images, while being trained the same number of epochs},
  keywords={Measurement;Training;Visualization;Neural networks;Computer architecture;Network architecture;Image restoration;convolutional neural networks;machine learning;image deblurring;perceptual loss},
  doi={10.1109/CSIT52700.2021.9648586},
  ISSN={2766-3639},
  month={Sep.},}@INPROCEEDINGS{10337589,
  author={Fkih, Hèdi and Kallel, Abdelaziz and Chtourou, Zied},
  booktitle={2023 International Conference on Cyberworlds (CW)}, 
  title={Super-Resolution of UAVs Thermal Images Guided by Visible Images}, 
  year={2023},
  volume={},
  number={},
  pages={40-45},
  abstract={Thermal imaging of unmanned aerial vehicles (UAVs) can sometimes suffer from a lack of information due to their small sizes. Therefore, the ability to understand and analyze such images of drones will be limited. Nevertheless, high-resolution (HR) visible images are often available and could be useful for improving the resolution of thermal images from UAVs. In recent years, Deep learning has been increasingly used in several computer vision tasks such as super-resolution (SR), where it has shown promising results for image resolution enhancement as it allows for creating high-quality detailed images. In this paper, we propose a Guidance Super-Resolution Network (GSRNet), that improves the spatial resolution of thermal UAVs images by taking advantage of the textures of the visible images. We adapt a Convolutional Neural Network (CNN) model that has an encoder-decoder architecture to translate visible images into thermal images as well as an auto-attention mechanism to allow the network to selectively focus on relevant structures of the image while ignoring irrelevant parts. Moreover, to preserve the low frequency information such as the brightness level and the body that are present in the low-resolution (LR) thermal image, we propose to merge the letter image with the translated one, such that the obtained HR thermal image when downsampled it equals the original LR one. Experimental results on the custom UAVs image dataset prove the higher performance of the proposed model on both qualitative and quantitative evaluations when compared to several state-of-the-art (SOTA) methods.},
  keywords={Surveillance;Superresolution;Imaging;Computer architecture;Feature extraction;Probabilistic logic;Convolutional neural networks;UAVs;Thermal Image;Super-Resolution;Image Translation;U-Net Architecture;Self-Attention},
  doi={10.1109/CW58918.2023.00016},
  ISSN={2642-3596},
  month={Oct},}@INPROCEEDINGS{9602498,
  author={Chen, Fan and Zheng, Jingjing and Tang, Yutao},
  booktitle={2021 33rd Chinese Control and Decision Conference (CCDC)}, 
  title={Modified LSTM-CNN Model for Arrhythmia Classification With Mixed Handcrafted Features}, 
  year={2021},
  volume={},
  number={},
  pages={1858-1862},
  abstract={In this paper, we propose a novel machine learning approach for the classification problem of cardiac arrhythmias. First, we develop a combined model consisting of long short-term memory (LSTM) networks and convolutional neural networks (CNNs) to extract the deep features of ECG signals. Then, this LSTM-CNN model is augmented by mixed handcrafted features, including Heart Rate Variability (HRV) and morphological features. The final model is trained and validated over the MIT-BIH arrhythmia dataset. It can achieve a classification performance with an average accuracy of 99.58%, an average sensitivity of 99.42%, and average specificity of 99.62%. Experimental results demonstrate the effectiveness of the proposed approach.},
  keywords={Sensitivity;Neural networks;Machine learning;Electrocardiography;Feature extraction;Convolutional neural networks;Heart rate variability;Arrhythmia;LSTM-CNN;Expert-augmentation;Handcrafted feature},
  doi={10.1109/CCDC52312.2021.9602498},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{10830670,
  author={Zhang, Xinyue and Zhao, Yuxuan and Man, Ka Lok and Smith, Jeremy S. and Jung, Young-Ae and Yue, Yutao},
  booktitle={2024 International Conference on Platform Technology and Service (PlatCon)}, 
  title={Temporal Improvement of Video Traffic Anomaly Detection: A Positioning Paper}, 
  year={2024},
  volume={},
  number={},
  pages={155-159},
  abstract={Internet of Things (IoT) technology can provide real-time public facilities data to Intelligent Transportation Systems (ITS), especially the surveillance cameras' video data. With the popularity of video surveillance systems, Video Anomaly Detection (VAD) has become more important in society and traffic management. However, the traditional, highly manual-dependent VAD method and the neural network model-based VAD method (such as Recurrent Neural Networks, RNN) face a significant challenge in temporal stream processing. This paper analyses current specific temporal challenges and proposes a Transformer-based spatial-temporal VAD model to alleviate the influence of temporal limitations. With the development of Transformer models, global information consideration and long-term relationship building have become more accessible, and the processing of temporal information in video data has become more efficient.},
  keywords={Recurrent neural networks;Transportation;Manuals;Streaming media;Transformers;Video surveillance;Real-time systems;Internet of Things;Anomaly detection;Faces;Video Anomaly Detection (VAD);Deep Learning (DL);Intelligent Transportation System (ITS)},
  doi={10.1109/PlatCon63925.2024.10830670},
  ISSN={2766-4198},
  month={Aug},}@INPROCEEDINGS{10647365,
  author={Kim, Gahyeon and Vien, An Gia and Nguyen, Duong Hai and Lee, Chul},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={Feature Decomposition Transformers for Infrared and Visible Image Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={2662-2668},
  abstract={We propose an infrared and visible image fusion algorithm using modality-shared and modality-specific feature decomposition transformers. First, the proposed algorithm extracts multiscale shallow features of infrared and visible images. Then, we develop modality-shared and modality-specific feature decomposition transformers that decompose the features into common and complementary components for each modality. For better decomposition, we develop a decomposition loss by constraining the common features to be correlated while the complementary features are uncorrelated. Finally, the reconstruction block generates the fused image by combining the common and complementary features. Experimental results show that the proposed algorithm significantly outperforms conventional algorithms on several datasets.},
  keywords={Feature extraction;Transformers;Image fusion;Image reconstruction;Visible and infrared image fusion;feature decomposition;contrastive learning;transformer},
  doi={10.1109/ICIP51287.2024.10647365},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10929902,
  author={Choi, Hyun-Tae and Hong, Byung-Woo},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Inpainting the Degraded Area with Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={In recent years, diffusion models have gained prominence for their ability to generate high-quality images by reversing a stochastic diffusion process. Denoising Diffusion Probabilistic Models (DDPMs) have shown state-of-the-art performance in image synthesis, but their focus on removing Gaussian noise limits their application in cases where only partial image information is available. In this paper, we propose a novel adaptation to the DDPM framework that replaces the conventional denoising process with an inpainting process. Our approach progressively removes random pixels, setting them to zero at each timestep instead of adding noise. The reverse process reconstructs the original image by filling in the missing pixels. To demonstrate the feasibility of this method, we conducted a prototype experiment using a subset of the CelebA-HQ dataset, training the model on 128 images. Initial results indicate that our method is capable of reconstructing images effectively, even with limited data, suggesting potential for future work in image restoration and enhancement tasks. However, challenge such as low diversity during sampling process remains, which will be addressed in subsequent research.},
  keywords={Training;Degradation;Image synthesis;Noise reduction;Stochastic processes;Prototypes;Diffusion models;Sampling methods;Image restoration;Image reconstruction;deep learning;image generation},
  doi={10.1109/ICCE63647.2025.10929902},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10845311,
  author={Haidarh, Mosa and Mu, Caihong and Liu, Yi},
  booktitle={2024 12th International Conference on Information Systems and Computing Technology (ISCTech)}, 
  title={A Hybrid CNN-Swin Transformer Module for Hyperspectral Image Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Hyperspectral image (HSI) classification stands as a pivotal mission in remote sensing applications, demanding intricate fusion of spectral and spatial information for accurate analysis. As a result of substantial variations between the spatial and spectral characteristics in the HSIs, the extraction of local and global spectral-spatial characteristics is still not fully completed. To face this challenge and leverage the advancements in deep learning, this paper presents a novel approach integrating the strengths of both 3D and 2D convolutional neural networks (CNNs) and the Swin Transformer for robust HSI classification, called CNN-Swin. The 3D-CNN leverages the inherent three-dimensional structure of HSIs to capture intricate spectral-spatial features, which are crucial for accurate classification, and the 2D-CNN to know more about spatial representation. The Swin Transformer, with its group attention and sliding window mechanisms, effectively learns long-range spectral dependencies while preserving global information to improve the representation of features. The calculation of sliding window attention can consider the contextual information of various windows, which is to be able to keep the global features of the HSI and enhance the outcomes of classification. Our suggested model is evaluated in the experiments utilizing three benchmark hyperspectral datasets: Indian Pines (IP), Pavia University (PU), and Salinas Scene (SA). Experiments outcomes illustrate that our suggested model can accomplish test accuracies of 99.91%, 99.98%, and 100.0% on the IP, PU, and SA datasets, respectively. The outcomes illustrate that our proposed model outperforms several state-of-the-art classification techniques for HSIs.},
  keywords={Training;Solid modeling;Accuracy;Three-dimensional displays;Transformers;Feature extraction;IP networks;Convolutional neural networks;Standards;Hyperspectral imaging;deep learning;hyperspectral image classification;3D-CNN;2D-CNN;Swin Transformer},
  doi={10.1109/ISCTech63666.2024.10845311},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9768544,
  author={Suganyadevi, S. and Renukadevi, K. and Balasamy, K. and Jeevitha, P.},
  booktitle={2022 First International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)}, 
  title={Diabetic Retinopathy Detection Using Deep Learning Methods}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetes mellitus causes diabetic retinopathy (DR), which is the primary source of blindness worldwide. Initial identification and cure are required to postpone or avert visual degradation and loss. In this regard, various artificial-intelligence-powered approaches for detecting and classifying diabetic retinopathy on fundus retina pictures have been proposed by the scientific community. This paper will mostly examine existing early DR diagnostic tools to determine their merits and drawbacks. Although pictures from fluorescein angiography, colour fundus medical images or visual lucidity tomography angiography are used for early diagnosis. Only colour fundus medical images are included in this study. It is possible to categorise the early DR detection methods described in this paper as either classical image processing, traditional machine learning, or deep learning. The issues that must be addressed in creating such efficient, effective and resilient methods for initial detection of DR systems are discussed in length in this study, as is the substantial opportunity for future research in this field.},
  keywords={Deep learning;Visualization;Retinopathy;Image color analysis;Angiography;Tomography;Retina;diabetes mellitus;diabetic retinopathy;fundus retina;machine learning;deep learning},
  doi={10.1109/ICEEICT53079.2022.9768544},
  ISSN={},
  month={Feb},}@ARTICLE{10945848,
  author={Himdi, Hanen and Zamzami, Nuha and Najar, Fatma and Alrehaili, Mada and Bouguila, Nizar},
  journal={IEEE Access}, 
  title={Arabic Fake News Dataset Development: Humans and AI-Generated Contributions}, 
  year={2025},
  volume={13},
  number={},
  pages={62234-62253},
  abstract={The extensive use of social media platforms has promoted the rapid spread of fake news on the internet, such as fake reviews, rumors, and propaganda. Although these terminologies have different objectives, they share the aim of causing harm in the form of fake news. This study presents an Arabic fake news detection framework to overcome the widespread fake news phenomenon. The proposed framework introduces the first Arabic fake news dataset compiled by passing through strict guidelines to produce fake articles composed by humans and the generative pre-trained transformer (GPT). First, we performed human-based experiments to evaluate the ability of humans to distinguish real news articles from fake news articles. Our findings reveal that humans could roughly identify half of the fake articles from humans or GPT, raising concerns about their ability to detect fake news. This highlights the growing concern surrounding fake news, especially because GPT demonstrates the ability to generate fake news that closely resembles human-created content, further amplifying the issue. To address this issue, we performed the same task using Deep Learning (DL) and transformer-based methods with different word embeddings. Across all the employed models, the study revealed that the innovative transformer-based model, ARBERT, outperformed the DL models, reaching an accuracy of 78% in classifying real and fake news generated by humans and GPT. The findings suggest effective techniques for addressing and resolving this issue.},
  keywords={Fake news;Biological system modeling;Accuracy;Transformers;Social networking (online);COVID-19;Training;Sports;Generative Pre-trainer transformer;Deep learning;Arabic fake news detection;Okaz dataset;deep learning (DL);transformers;GPT-generated fake news;natural language processing (NLP)},
  doi={10.1109/ACCESS.2025.3556376},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10913179,
  author={Careli, Álvaro S. and Boas, Evandro C. Vilas and Teixeira, Eduardo H. and Silva, Elaine C. C. and Aquino, Guilherme P. and Figueiredo, Felipe A. P.},
  booktitle={2024 International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)}, 
  title={Deep Learning Segmentation Models Evaluation for Deforestation Monitoring Embedded Systems}, 
  year={2024},
  volume={},
  number={},
  pages={274-278},
  abstract={This work evaluates deep learning segmentation models to propose a deforestation monitoring embedded system. The approach stands for environmental monitoring using remote sensing imagery, edge computing, and a deep learning segmentation model. Thus, the performance of you only look once architecture version 8 (YOLOv8) and Mask Region-based convolutional neural networks (Mask R-CNN) embedded in Raspberry Pi Model 4 regarding Intersection over Union (IoU), mean Average Precision (mAP), and time per image processing metrics is compared. The models are combined with a pixel-based algorithm that analyzes the temporal segmented images to define their forest area percentage for deforestation monitoring and detection. The results demonstrate YOLOv8x model achieved an IoU of 0.762, with a time per image of 0.4777 seconds, while Mask R-CNN R101 FPN 3x obtained an IoU of 0.763, with a time per image of 0.2669 seconds. The average times for YOLOv8 ranged from 0.0434 to 0.4777 seconds, and for Mask R-CNN from 0.1969 to 0.2669 seconds. Finally, this work proposes evaluating the model's performance when working with generative AI models Dall-e, Craiyon, and Tess-AI to create a synthetic dataset to augment the initial one with synthetic samples and improve the model's training with a large dataset. The Dall-e has been shown to outperform the others regarding the IoU metric, which was suggested to augment datasets with synthetic samples.},
  keywords={Deforestation;Measurement;Deep learning;Training;Image segmentation;Embedded systems;Generative AI;Computational modeling;Environmental monitoring;Synthetic data;Environmental Monitoring;Deforestation;Mask R-CNN;Segmentation;YOLOv8},
  doi={10.1109/ICICYTA64807.2024.10913179},
  ISSN={},
  month={Dec},}@INBOOK{10287808,
  author={Wong, Kelvin K. L.},
  booktitle={Cybernetical Intelligence: Engineering Cybernetics with Machine Intelligence}, 
  title={The Structure of Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={69-81},
  abstract={A neural network is a machine learning model that is inspired by the structure and function of the brain. It is a network of interconnected nodes, which are called artificial neurons that process information and make predictions. The Perceptron is a simple linear binary classifier algorithm introduced in the 1950s as one of the earliest models of artificial neural networks. A recurrent neural network (RNN) is a type of neural network designed to handle sequential data. RNNs can be used for various applications such as language modeling, speech recognition, and time&#x2010;series forecasting. Markov Neural Network (MNN) is a type of neural network that combines neural networks with Markov models. MNNs can be used for various applications such as image segmentation, speech recognition, and natural language processing. A Generative Adversarial Network is composed of two parts: a generator and a discriminator.},
  keywords={Neurons;Biological neural networks;Training;Feature extraction;Multilayer perceptrons;Mathematical models;Data models},
  doi={10.1002/9781394217519.ch4},
  ISSN={},
  publisher={IEEE},
  isbn={9781394217496},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10287808},}@INPROCEEDINGS{10581158,
  author={Saeed, Nuaima},
  booktitle={2024 International Conference on Engineering & Computing Technologies (ICECT)}, 
  title={Translating Words to Pixels: An Effective Text-to-Image System Utilizing Upsampling and CLIP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative AI and text-based image synthesis are being pursued with more interest than ever and adopt a growing audience. This work offers a new way of producing images within a broad range of quality levels, deducing the extra value of the increase resolution. The system applies sophisticated technologies such as diffusion and upscaling models that are guided by the pre-training of the Contrastive Language-Image PreTraining (CLIP). However, the model shines primarily in its ability to generate visually cohesive and artistic images which are a good illustration of each specified textual prompt. Upsampling technique resolved the major translation problems by generating more high resolution and detailed resultant images of base images. Beyond that, the system also thoroughly illustrated its diversity by accurately producing a comprehensive collection of images based on the text input. This work would largely be a demonstration of the suitability of it for different fields like digital art, advertising, and content creation. Moreover, it delimits the chances of producing innovative and unique content. Future development of many other applications of the system will include the improvement of the system handling more complex textual descriptions, the addition of user-interactive generation features, the optimization of system performance and expanding to multimodal content generation. Our results here only represent a small but important step in this exciting direction. By means of this survey we enrich further knowledges on the field and its farther issues.},
  keywords={Surveys;Image resolution;Image synthesis;Generative AI;System performance;Text to image;Digital art;CLIP;Upsampling;Generator;Image Generation},
  doi={10.1109/ICECT61618.2024.10581158},
  ISSN={},
  month={May},}@INPROCEEDINGS{11016577,
  author={Liu, ShuChang and Pan, MingHui and Yang, YeHan},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={I-LEAD: A Digital-Intelligence-Powered Ecosystem for Innovation and Entrepreneurship Education}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Generative artificial intelligence and large model agents are revolutionizing the higher education, influencing everything from talent development frameworks and teaching methodologies to knowledge acquisition processes and research paradigms. Meanwhile, in the context of digital transformation and technological innovation, data as a production factor and emerging productive forces are reshaping the requirements and demands for cultivating high-level interdisciplinary engineering talents. This paper draws on the joint educational achievements between Beijing University of Posts and Telecommunications and Queen Mary University of London, focusing on the coconstruction and sharing of experimental resources, as well as innovation-driven entrepreneurship education. It introduces a digital-intelligence-powered educational platform aimed at fostering internationally-minded, innovative, and outstanding talents. The platform's core philosophy, development strategy, functional modules, and technical framework are detailed. The wide recognition and interest among stakeholders further validate its potential to support the development of a cross-disciplinary, cross-professional, and cross-national ecosystem for innovation and entrepreneurship education. As a key outcome of the 20th Anniversary Development Conference of Joint Education of our two universities, I-LEAD is a platform designed to cultivate students' comprehensive innovative capabilities. Leveraging large language models (LLMs) and multi-agent technology, we have developed BUPT iMentor, an intelligent agent for innovation and entrepreneurship guidance; and BUPT EnPower, a cultivation assistant for personalized longlife learning. By LLMs with a robust knowledge based augmented generation and fine tuning, this tool effectively addresses common student challenges during innovation & entrepreneurship projects, such as idea generation and validation, access to relevant learning resources. I-LEAD provides comprehensive support, including customized course creation, problem-solving guidance, real-time interactive Q&A, and learning progress monitoring. This empowers students to independently plan their learning journeys and holistically enhance their academic and innovative skills. The effectiveness and practicality of the platform have been validated through a questionnaire survey. Therefore, we are extensively gathering feedback and continuously optimizing the platform's services. The teacher-student collaborative learning represents the future of higher education. This student-centered education system provides a platform for that.},
  keywords={Technological innovation;Large language models;Knowledge acquisition;Education;Ecosystems;Entrepreneurship;Production;Real-time systems;Telecommunications;Monitoring;innovation and entrepreneurship education;large language model agents;interdisciplinary talent cultivation;digital and intelligent empowerment},
  doi={10.1109/EDUCON62633.2025.11016577},
  ISSN={2165-9567},
  month={April},}@ARTICLE{10490262,
  author={Hong, Danfeng and Zhang, Bing and Li, Xuyang and Li, Yuxuan and Li, Chenyu and Yao, Jing and Yokoya, Naoto and Li, Hao and Ghamisi, Pedram and Jia, Xiuping and Plaza, Antonio and Gamba, Paolo and Benediktsson, Jon Atli and Chanussot, Jocelyn},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SpectralGPT: Spectral Remote Sensing Foundation Model}, 
  year={2024},
  volume={46},
  number={8},
  pages={5227-5244},
  abstract={The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS Big Data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via multi-target reconstruction; and 4) trains on one million spectral RS images, yielding models with over 600 million parameters. Our evaluation highlights significant performance improvements with pretrained SpectralGPT models, signifying substantial potential in advancing spectral RS Big Data applications within the field of geoscience across four downstream tasks: single/multi-label scene classification, semantic segmentation, and change detection.},
  keywords={Data models;Task analysis;Computational modeling;Transformers;Image reconstruction;Three-dimensional displays;Visualization;Artificial intelligence;deep learning;downstream;foundation model;progressive;remote sensing;spectral data;tensor masked modeling;transformer},
  doi={10.1109/TPAMI.2024.3362475},
  ISSN={1939-3539},
  month={Aug},}@ARTICLE{11026826,
  author={Jiang, Xiaoyu and Zheng, Chen and Zhuo, Yue and Kong, Xiangyin and Ge, Zhiqiang and Song, Zhihuan and Xie, Min},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Advancing Industrial Data Augmentation in AIGC Era: From Foundations to Frontier Applications}, 
  year={2025},
  volume={74},
  number={},
  pages={1-22},
  abstract={In the field of intelligent manufacturing and industrial big data, data-driven industrial intelligence models (IIMs) based on machine learning have become indispensable for modern industrial systems. While IIMs are renowned for their effective learning capabilities, a critical challenge persists: their performance is severely compromised by the substantial quality gap between raw industrial data and model-ready data. To address this core issue, industrial data augmentation (IDA) has emerged as a transformative solution, yet existing research lacks systematic frameworks and implementation guidelines. This article presents the first comprehensive survey establishing IDA as an independent research domain. We propose a novel taxonomy categorizing IDA methods by transformation-based, interpolation-based, and distribution estimation-based approaches. Beyond methodology analysis, we conduct a systematic review of frontier IDA applications spanning key performance indicator (KPI) prediction, anomaly monitoring, fault diagnosis, and defect detection. Significantly, an open-source IDA toolbox implementing 20 IDA algorithms is introduced to facilitate ongoing development and application, available at https://github.com/3uchen/IdaLy. Finally, this article highlights the current challenges and future prospects of the IDA, seeking to motivate and steer further research in this area.},
  keywords={Predictive models;Training;Data augmentation;Big Data;Protection;Data privacy;Artificial intelligence;Data-driven modeling;Technological innovation;Manufacturing;Anomaly monitoring;fault diagnosis;industrial data augmentation (IDA);industrial intelligence model (IIM);key performance indicator (KPI) prediction},
  doi={10.1109/TIM.2025.3572162},
  ISSN={1557-9662},
  month={},}@ARTICLE{8843965,
  author={Li, Nan and Zhang, Xiaoguang and Zhang, Chunlong and Guo, Huiwen and Sun, Zhe and Wu, Xinyu},
  journal={IEEE Access}, 
  title={Real-Time Crop Recognition in Transplanted Fields With Prominent Weed Growth: A Visual-Attention-Based Approach}, 
  year={2019},
  volume={7},
  number={},
  pages={185310-185321},
  abstract={Crop recognition is one of the key processes for robotic weeding in precision agriculture, which remains an open problem due to the unstructured field environment and the wide variety of plant species. It becomes especially challenging when the weeds are prominent and overlap with the crop plants. This paper presents a novel method for recognizing crop plants of field images with a high weed presence. This method segments crop plants from overlapped weeds based on the visual attention mechanism of the human visual system using a convolutional neural network. The network utilizes ResNet-10 as backbone, while introducing side outputs and short connections for multi-scale feature fusion. The Adaptive Affinity Fields method is adopted to improve the segmentation at object boundaries and for fine structures. To train and test the network, a field image dataset has been created which consists of 788 color images with manually segmented annotations. The images are captured under challenging conditions with extremely high weed pressure. The experimental results show that the proposed method can accurately segment crops from weeds and soil, with mean absolute errors less than 0.005 and F-measure scores exceeding 97%. In terms of efficiency, the proposed method can process up to 169 images per second when accelerated by a NVIDIA RTX 2080Ti graphics processing unit (GPU), and operate at approximately 5.6 Hz in a Jetson TX2 embedded computer. The results indicate that the proposed method has the potential to provide an efficient solution for recognizing crop plants, even in the presence of severe weed growth. The code and the dataset are available at https://github.com/ZhangXG001/Real-Time-Crop-Recognition.},
  keywords={Agriculture;Visualization;Feature extraction;Robots;Image segmentation;Computational modeling;Soil;Precision agriculture;weed control;crop recognition;visual attention;convolutional neural networks},
  doi={10.1109/ACCESS.2019.2942158},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10392776,
  author={Mhatre, Aatmaj},
  booktitle={2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)}, 
  title={Detecting the presence of social bias in GPT-3.5 using association tests}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Bias in AI is becoming a rising concern as more and more systems rely on generative artificial intelligence. The bias in the training data used to train generative AI is reflected in its applications such as writing, drawing, advising and decision-making. Since the advent of GPT-3, a growing number of people have been relying on it. Content generated by GPT-3 has the potential to influence people on multiple levels. If this content is biased only towards a certain proportion of the population, it may have grave consequences including legal and ethical concerns. The aim of this study is to detect the associations in large language generative models. This study presents a test which can be used as an effective medium to test high-level bias in chat-based GPT models such as ChatGPT. This test measures word associations and is similar in essence to the Implicit Association Test (IAT) which is used to measure bias in humans. The test gives insights into the bias of the model with respect to the explicit associations found by querying the model. In this study, we use this test to detect gender-career and racial bias in the ChatGPT-3.5 model. We prove that the model is negatively biased towards women and people of the Arabic race.},
  keywords={Ethics;Generative AI;Law;Computational modeling;Weapons;Sociology;Decision making;bias;GPT;association test;WEAT;IAT},
  doi={10.1109/ICACTA58201.2023.10392776},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11101730,
  author={Culafic, Igor and Popovic, Tomo and Jovovic, Ivan and Cakic, Stevan},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Real-time Image Generation Utilizing ARM SBC Architecture}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Real-time image generation using artificial intelligence has traditionally been constrained by computational limitations on edge devices. This paper presents a novel implementation of real-time image generation on the Jetson Orin Nano platform using Stable Diffusion models. We demonstrate the feasibility of running computationally intensive generative $A I$ models on resource-constrained ARM-based systems while maintaining acceptable performance metrics. Our system processes live camera input and generates corresponding AIenhanced images in real-time. Unlike prior implementations focused on desktop systems, our work provides innovative solutions for ARM architecture, enabling practical applications in real-time environments while achieving frame rates of 2-6 FPS at $512 \times 512$ resolution. Through comprehensive performance analysis, we identify optimal operating parameters and critical thresholds for maintaining image quality while maximizing processing speed. The results demonstrate successful deployment of state-of-the-art generative models on edge devices, providing valuable insights for future edge AI applications in computer vision and real-time image processing. These results highlight the potential of low-power systems to contribute to decentralized highperformance computing frameworks. This opens new pathways for deploying advanced generative models in latency-sensitive, infrastructure-limited, or privacy-critical environments.},
  keywords={Computer vision;Image synthesis;Generative AI;Image edge detection;Computational modeling;Computer architecture;Edge AI;Real-time systems;Optimization;Testing;edge computing;real-time image generation;stable diffusion;CUDA optimization;computer vision},
  doi={10.1109/ISAS66241.2025.11101730},
  ISSN={},
  month={June},}@ARTICLE{9084137,
  author={Wang, Wei-Yen and Hsu, Min-Jie and Yu, Li-An and Chien, Yi-Hsing and Hsu, Chen-Chien},
  journal={IEEE Access}, 
  title={Deep Learning-Based Hypothesis Generation Model and Its Application on Virtual Chinese Calligraphy-Writing Robot}, 
  year={2020},
  volume={8},
  number={},
  pages={87243-87251},
  abstract={In recent years, a tremendous amount of effort has been devoted to modeling the cognition of human brain, particularly hypothesis generation process. Most research of the hypothesis generation model is probability-based. However, computation of human brains is still neuron-based instead of calculating the probability. As an attempt to solve this problem in this paper, we propose a novel neuron-based hypothesis generation model, called hypothesis generation net, to model human cognition, including how to make decisions and how to do actions. Basically, the proposed hypothesis generation model consists of two parts, i.e., a hypothesis model and an evaluation model. When these two models interact, the system is able to generate hypotheses to solve complex tasks based on historical experiences. To validate the feasibility of the proposed hypothesis generation model, we show a virtual robot with its cognition system can learn how to write Chinese calligraphy in a simulation environment, where an image-to-action translation via a cognitive framework is proposed to learn the pattern of Chinese characters. Based on the proposed deep thinking and learning mechanism, the virtual robot is able to write Chinese calligraphy well, which is a difficult task requiring extremely complicated motions, through thinking and practicing according to a human writing sample.},
  keywords={Robot kinematics;Cognition;Brain modeling;Computational modeling;Writing;Biological neural networks;Hypothesis generation model;deep neural networks;Chinese calligraphy;image-to-action translation},
  doi={10.1109/ACCESS.2020.2991767},
  ISSN={2169-3536},
  month={},}@INBOOK{10950563,
  author={Carpenter, Perry},
  booktitle={FAIK: A Practical Guide to Living in a World of Deepfakes, Disinformation, and AI-Generated Deceptions}, 
  title={Bias, Data Poisoning, &amp; Output Oddities}, 
  year={2025},
  volume={},
  number={},
  pages={59-78},
  abstract={Summary <p>Generative artificial intelligence (AI) possesses something very much like the ability to imagine&#x2014;the ability to dream, to combine information in unexpected and sometimes downright bizarre ways. AI systems are trained on vast amounts of data&#x2014;from billions to trillions of datapoints. Hallucinations are one of the main superpowers of generative AI. That creativity allows them to generate images, write poems, help with blogs, become a brainstorming partner, and do well on Alternative Use and Remote Association tests. Correcting for that bias is where the concept of alignment comes in. In the context of AI, alignment refers to the process of ensuring that AI systems act in ways that are consistent with human values, goals, and ethical standards. Even worse, training data can be purposely polluted by bad actors&#x2014;a tactic known as data poisoning. Left unchecked, data poisoning can cause AI systems to learn harmful biases, make incorrect predictions, or even expose private training data.</p>},
  keywords={Chatbots;Artificial intelligence;Generative AI;Weaving;Oral communication;Law;Image edge detection;Accuracy;Writing;Training data},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394299904},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950563},}@ARTICLE{1624355,
  author={Mun Wai Lee and Cohen, I.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A model-based approach for estimating human 3D poses in static images}, 
  year={2006},
  volume={28},
  number={6},
  pages={905-916},
  abstract={Estimating human body poses in static images is important for many image understanding applications including semantic content extraction and image database query and retrieval. This problem is challenging due to the presence of clutter in the image, ambiguities in image observation, unknown human image boundary, and high-dimensional state space due to the complex articulated structure of the human body. Human pose estimation can be made more robust by integrating the detection of body components such as face and limbs, with the highly constrained structure of the articulated body. In this paper, a data-driven approach based on Markov chain Monte Carlo (DD-MCMC) is used, where component detection results generate state proposals for 3D pose estimation. To translate these observations into pose hypotheses, we introduce the use of "proposal maps," an efficient way of consolidating the evidence and generating 3D pose candidates during the MCMC search. Experimental results on a set of test images show that the method is able to estimate the human pose in static images of real scenes.},
  keywords={Humans;Face detection;Biological system modeling;Image databases;Information retrieval;Image retrieval;Content based retrieval;State-space methods;Robustness;Monte Carlo methods;Three-dimensional human pose estimation from static images;body parts detector;data driven Markov chain Monte Carlo;generative models.},
  doi={10.1109/TPAMI.2006.110},
  ISSN={1939-3539},
  month={June},}@ARTICLE{4633366,
  author={Liu, Xiaoming},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Discriminative Face Alignment}, 
  year={2009},
  volume={31},
  number={11},
  pages={1941-1954},
  abstract={This paper proposes a discriminative framework for efficiently aligning images. Although conventional Active Appearance Models (AAMs)-based approaches have achieved some success, they suffer from the generalization problem, i.e., how to align any image with a generic model. We treat the iterative image alignment problem as a process of maximizing the score of a trained two-class classifier that is able to distinguish correct alignment (positive class) from incorrect alignment (negative class). During the modeling stage, given a set of images with ground truth landmarks, we train a conventional Point Distribution Model (PDM) and a boosting-based classifier, which acts as an appearance model. When tested on an image with the initial landmark locations, the proposed algorithm iteratively updates the shape parameters of the PDM via the gradient ascent method such that the classification score of the warped image is maximized. We use the term Boosted Appearance Models (BAMs) to refer to the learned shape and appearance models, as well as our specific alignment method. The proposed framework is applied to the face alignment problem. Using extensive experimentation, we show that, compared to the AAM-based approach, this framework greatly improves the robustness, accuracy, and efficiency of face alignment by a large margin, especially for unseen data.},
  keywords={Active appearance model;Fitting;Iterative algorithms;Robustness;Face detection;Active shape model;Optimization methods;Testing;Boosting;Magnesium compounds;Face;alignment;boosting;active appearance models;AAM;boosted appearance models;BAM;image alignment;gradient descent;landmark;generative versus discriminative model.;face;alignment;Active Appearance Models;boosting},
  doi={10.1109/TPAMI.2008.238},
  ISSN={1939-3539},
  month={Nov},}@ARTICLE{5109419,
  author={Kokkinos, Iasonas and Maragos, Petros},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Synergy between Object Recognition and Image Segmentation Using the Expectation-Maximization Algorithm}, 
  year={2009},
  volume={31},
  number={8},
  pages={1486-1501},
  abstract={In this work, we formulate the interaction between image segmentation and object recognition in the framework of the Expectation-Maximization (EM) algorithm. We consider segmentation as the assignment of image observations to object hypotheses and phrase it as the E-step, while the M-step amounts to fitting the object models to the observations. These two tasks are performed iteratively, thereby simultaneously segmenting an image and reconstructing it in terms of objects. We model objects using Active Appearance Models (AAMs) as they capture both shape and appearance variation. During the E-step, the fidelity of the AAM predictions to the image is used to decide about assigning observations to the object. For this, we propose two top-down segmentation algorithms. The first starts with an oversegmentation of the image and then softly assigns image segments to objects, as in the common setting of EM. The second uses curve evolution to minimize a criterion derived from the variational interpretation of EM and introduces AAMs as shape priors. For the M-step, we derive AAM fitting equations that accommodate segmentation information, thereby allowing for the automated treatment of occlusions. Apart from top-down segmentation results, we provide systematic experiments on object detection that validate the merits of our joint segmentation and recognition approach.},
  keywords={Object recognition;Image segmentation;Expectation-maximization algorithms;Active appearance model;Object detection;Fitting;Iterative algorithms;Evolution (biology);Equations;Image reconstruction;Image segmentation;object recognition;Expectation Maximization;Active Appearance Models;curve evolution;top--down segmentation;generative models.},
  doi={10.1109/TPAMI.2008.158},
  ISSN={1939-3539},
  month={Aug},}@INPROCEEDINGS{9319051,
  author={Aliman, Nadisha-Marie and Kester, Leon},
  booktitle={2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={Malicious Design in AIVR, Falsehood and Cybersecurity-oriented Immersive Defenses}, 
  year={2020},
  volume={},
  number={},
  pages={130-137},
  abstract={Advancements in the AI field unfold tremendous opportunities for society. Simultaneously, it becomes increasingly important to address emerging ramifications. Thereby, the focus is often set on ethical and safe design forestalling unintentional failures. However, cybersecurity-oriented approaches to AI safety additionally consider instantiations of intentional malice - including unethical malevolent AI design. Recently, an analogous emphasis on malicious actors has been expressed regarding security and safety for virtual reality (VR). In this vein, while the intersection of AI and VR (AIVR) offers a wide array of beneficial cross-fertilization possibilities, it is responsible to anticipate future malicious AIVR design from the onset on given the potential socio-psycho-technological impacts. For a simplified illustration, this paper analyzes the conceivable use case of Generative AI (here deepfake techniques) utilized for disinformation in immersive journalism. In our view, defenses against such future AIVR safety risks related to falsehood in immersive settings should be transdisciplinarily conceived from an immersive co-creation stance. As a first step, we motivate a cybersecurity-oriented procedure to generate defenses via immersive design fictions. Overall, there may be no panacea but updatable transdisciplinary tools including AIVR itself could be used to incrementally defend against malicious actors in AIVR.},
  keywords={Artificial intelligence;Information integrity;Videos;Safety;Media;Journalism;Computer crime;AI Safety;VR;AI;Immersive Journalism;Disinformation;HCI;Design Fiction;Psychology;Cybersecurity},
  doi={10.1109/AIVR50618.2020.00031},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10356527,
  author={Ding, Silong and Mao, Yingchi and Cheng, Yong and Pang, Tianfu and Shen, Lijuan and Qi, Rongzhi},
  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={ECIFF: Event Causality Identification based on Feature Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={646-653},
  abstract={Event causality identification is an important task in natural language processing. However, this task is highly challenging due to the high dependency of event context, text semantic ambiguity and insignificant causality features between text events. These issues lead to the low precision of causal relationship identification between events. We propose an Event Causality Identification Based on Feature Fusion (ECIFF) to improve the causality identification precision between events by integrating the context, semantics, and syntax of natural language. Firstly, we utilize BERT to capture the contextual features of events in natural language, enhancing the contextual embedding of events in different contexts. Secondly, based on an adversarial generative graph representation method, ECIFF learns a massive amount of causal relationships in the CauseNet, which can enhance the semantic representation of causes and effects of events. Next, we exploit the shortest dependency path to shorten the length of sentences and inductively learn all possible syntactic dependency relationships. Finally, the contextual, semantic and syntactic features are fused to synthetically determine the causal relationships among events. The experimental results indicate that our proposed approach significantly outperforms the state-of-the-art method LSIN: on the CTBank dataset, the precision, recall and F1-score of our approach are improved by 1.6%, 3.2% and 2.4%; on the ESL dataset, the precision, recall and F1-score of our approach are improved by 4.0%, 4.7% and 4.3%.},
  keywords={Semantics;Syntactics;Feature extraction;Natural language processing;Task analysis;Artificial intelligence;Natural Language Processing;Causality Identification;Feature Extraction;Feature Fusion},
  doi={10.1109/ICTAI59109.2023.00101},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10605533,
  author={Lyu, Huanbo and Herring, Daniel and Wang, Lingfeng and Ninic, Jelena and Andrews, James and Li, Miqing and Kočvara, Michal and Spill, Fabian and Wang, Shuo},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Multi-Objective Optimization for Flexible Building Space Usage}, 
  year={2024},
  volume={},
  number={},
  pages={932-939},
  abstract={Globally, more than 30% of the world’s energy consumption arises in buildings. Optimization of buildings is a key opportunity for reducing energy consumption and carbon emissions, improving operational efficiency and occupant well-being and comfort. While building generative design and control systems have received considerable research attention, optimizing space utilization, particularly for flexible spaces is an underdeveloped research area that is relevant to existing buildings. Flexible spaces, for example, rooms with movable walls, are increasingly common in modern building designs where space requirements are dynamic. In this paper, a novel space usage optimization framework is proposed, including a practical task formulation that enables room reallocation, combination and removal, a machine learning model for energy cost estimation (XGBoost) based on real sensor data and a multi-objective optimization component to minimize energy consumption and maximize room thermal comfort simultaneously (NSGA-II). Its effectiveness is tested and discussed through two representative problem scenarios. Our case studies show that we can reduce energy cost substantially by around 40% in comparison with the original space usage setting, while additionally improving thermal comfort for the occupants. This work shows great potential of using AI techniques for optimizing building space usage.},
  keywords={Energy consumption;Costs;Buildings;Machine learning;Learning (artificial intelligence);Aerospace electronics;Thermal sensors;Building Optimization;Building Space Management;Flexible Space Utilization;Energy Optimization;Multi-Objective Optimization;Machine Learning},
  doi={10.1109/CAI59869.2024.00172},
  ISSN={},
  month={June},}@INPROCEEDINGS{560475,
  author={McCluskey, T.L. and Kitchin, D.E. and Porteous, J.M.},
  booktitle={Proceedings Eighth IEEE International Conference on Tools with Artificial Intelligence}, 
  title={Object-centred planning: lifting classical planning from the literal level to the object level}, 
  year={1996},
  volume={},
  number={},
  pages={346-353},
  abstract={A great deal of emphasis in classical AI planning research has been placed on search-control issues in plan generation, while the issue of knowledge representation and acquisition of models for use with classical planning engines has been largely ignored. Work in knowledge-based planning, on the other hand, is often associated with 'scruffy' AI, there being no standard representation languages with associated formal semantics for encoding domain models. In this paper we describe a method to create a planning domain model which preserves the domain independence, generality and 'clean' properties of generative planners to which the model can be attached. Our method is based an lifting the level of domain representation from the literal-centred, to the object-centred. This object-centred method has the advantage that it naturally allows for the creation of a supporting tools environment to help in (i) the creation and validation of a precise planning model, and (ii) the speed-up of plan generation.},
  keywords={Encoding;Artificial intelligence;Engines;Mathematics;Knowledge representation;Mathematical model;Computational complexity;Software engineering;Testing},
  doi={10.1109/TAI.1996.560475},
  ISSN={1082-3409},
  month={Nov},}@ARTICLE{10838616,
  author={Guo, Ying and Li, Bingxin and Zhen, Kexin and Liu, Jie and Li, Gaolei and Wang, Qi and Liu, Yong-Jin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Consistency-Heterogenity Balanced Fake News Detection via Cross-Modal Matching}, 
  year={2025},
  volume={6},
  number={7},
  pages={1787-1796},
  abstract={Generating synthetic content through generative AI (GAI) presents considerable hurdles for current fake news detection methodologies. Many existing detection approaches concentrate on feature-based multimodal fusion, neglecting semantic relationships such as correlations and diversities. In this study, we introduce an innovative cross-modal matching-driven approach to reconcile semantic relevance (text–image consistency) and semantic gap (text–image heterogeneity) in multimodal fake news detection. Unlike the conventional paradigm of multimodal fusion followed by detection, our approach integrates textual modality, visual modality (images), and text embedded within images (auxiliary modality) to construct an end-to-end framework. This framework considers the relevance of contents across different modalities while simultaneously addressing the gap in structures, achieving a delicate balance between consistency and heterogeneity. Consistency is fostered by evaluating intermodality correlation via pairwise-similarity scores, while heterogeneity is addressed by employing cross-attention mechanisms to account for intermodality diversity. To achieve equilibrium between consistency and heterogeneity, we employ attention-guided enhanced modality interaction and similarity-based dynamic weight assignment to establish robust frameworks. Comparative experiments conducted on the Chinese Weibo dataset and the English Twitter dataset demonstrate the effectiveness of our approach, surpassing the state-of-the-art by 7% to 13%.},
  keywords={Feature extraction;Fake news;Semantics;Visualization;Correlation;Social networking (online);Artificial intelligence;Blogs;Data mining;Accuracy;Cross-modal matching;multimodal fake news detection;text–image consistency;text–image heterogeneity},
  doi={10.1109/TAI.2025.3527921},
  ISSN={2691-4581},
  month={July},}@INPROCEEDINGS{10864206,
  author={Wang, Teng and Xue, Zhenyu},
  booktitle={2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={An Efficient Training Framework for Chinese Text Classification: Integrating Large Language Models and Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={825-829},
  abstract={This study proposes an efficient model training framework aimed at improving the performance of hierarchical classification of Chinese text in low-computation environments. By leveraging the generative capabilities of large language models for data annotation and augmentation, a high-quality dataset is constructed. Knowledge distillation techniques are applied to transfer knowledge from a complex teacher model to a lightweight student model, effectively reducing computational resources. Additionally, a reinforcement learning strategy is introduced to further optimize the student model, enabling it to surpass the teacher model in both accuracy and inference efficiency, providing a novel solution for natural language processing and other resource-constrained tasks.},
  keywords={Training;Accuracy;Annotations;Computational modeling;Large language models;Text categorization;Reinforcement learning;Learning (artificial intelligence);Data models;Natural language processing;data labeling;knowledge distillation;data augmentation;reinforcement learning},
  doi={10.1109/ICAICE63571.2024.10864206},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11105963,
  author={Wang, Rensong and Zhang, Jie and Ren, Zhiwen and Zhang, Weiming and Yu, Nenghai},
  booktitle={2025 11th International Conference on Computing and Artificial Intelligence (ICCAI)}, 
  title={SignaShield: Guarding Your Signature via Font Style Watermarking}, 
  year={2025},
  volume={},
  number={},
  pages={393-400},
  abstract={Image-based signatures, which represent a person’s unique handwriting or writing style in image form, have become increasingly common in modern digital transactions. These signatures are widely used for authentication and authorization, offering convenience and efficiency across various domains. However, their growing prevalence also introduces significant security risks. Attackers may exploit these image-based signatures to forge identities, commit financial fraud, or gain unauthorized access. The risks are further exacerbated by the advancement of AI models, which leverage data-driven techniques to replicate handwriting styles with remarkable accuracy, making such attacks increasingly feasible. To address it, watermarking presents a straightforward and practical solution. However, existing methods typically embed watermarks across the entire image, making them ineffective against AI models, which can isolate and replicate the style without preserving the watermark.In this paper, we propose SignaShield, the first watermarking method specifically designed to protect image-based signatures by embedding watermarks directly into style features. Our approach integrates a watermarking module into existing style transfer networks, leveraging their generative capabilities to produce high-quality, watermarked signature images. This design enables the recovery of the original watermark even when attackers attempt to imitate styles using watermarked images. Additionally, we identify and exploit redundancy in the latent space of style transfer networks, allowing for seamless and inconspicuous watermark embedding. Extensive experiments and ablation studies validate the effectiveness of SignaShield, demonstrating superior synthesis quality, robust watermark extraction, and strong style preservation.},
  keywords={Training;Authorization;Digital images;Redundancy;Finance;Watermarking;Writing;Robustness;Fraud;Artificial intelligence;Watermarking;Font Style;Handwritten Text Generation},
  doi={10.1109/ICCAI66501.2025.00068},
  ISSN={},
  month={March},}@INBOOK{10950622,
  author={Smith, Christie and Monahan, Kelly},
  booktitle={Essential: How Distributed Teams, Generative AI, and Global Shifts Are Creating a New Human-Powered Leadership}, 
  title={Soft Skills Are Power Skills}, 
  year={2025},
  volume={},
  number={},
  pages={117-135},
  abstract={Summary <p>Technology can only take people so far without the adoption of a new kind of leadership, and soft skills will be the differentiator for organizations in the war for skills and the creation of greater business value. The abuse of power and the neglect of soft skills in leadership can have devastating consequences. The CEOs cited within the research acknowledged that a lot of their job was focused on the hard skills of running a business, and with machine learning and generative artificial intelligence on the rise, many felt that these technologies could likely do these tasks at least as well, if not better. Udemy's power skills trend analysis discovered that because of today's shifting work models, such as distributed work, flattened organizational structures, and the rise of the alternative workforce, soft skills must be referred to in a more substantive way.</p>},
  keywords={Leadership;Productivity;Heart;Generative AI;Cultural differences;Companies;Space technology;Socioeconomics;Reviews;Machine learning},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394276608},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950622},}@INPROCEEDINGS{11100306,
  author={Jansen, Andrew Nicholas and Suwandy, Jacky and Muliono, Yohan and Nadia and Junior, Franz Adeta},
  booktitle={2025 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={The Precision of Model to Detect AI-generated Text: Comparison on AI Text Detectors and Impact of Paraphrasing}, 
  year={2025},
  volume={},
  number={},
  pages={796-803},
  abstract={The swift advancement of generative technologies such as LLMs has resulted in a variety of new challenges, especially in detecting AI-generated text. This study will evaluate 10 different AI models in distinguishing Indonesian AI-generated and human-generated text and the impact of paraphrasing on model detection outcomes. Each model will be trained using a dataset consisting of 224 paragraphs divided evenly between AI and human writing with each paragraph containing approximately 100-200 words. The models are validated using a paraphrased version of all 224 training dataset paragraphs, generated by Quillbot. The evaluation results show that partially paraphrased human-generated text and AI-generated text continue to be classified as their original categories. Three models in particular — indobenchmark/indobert-base-p1, indobenchmark/indobert-base-p2, and indobenchmark/indobert-large-p2 — demonstrate high accuracy in detecting the validation dataset. While all models can generally differentiate between Indonesian AI-generated and human-generated text, they still face challenges when dealing with paraphraser, especially paraphrased human-generated text.},
  keywords={Training;Accuracy;Text detection;Detectors;Writing;Communications technology;Fourth Industrial Revolution;Artificial intelligence;Faces;BERT;IndoBERT;AI text detection;Model comparison;Paraphrasing impact},
  doi={10.1109/IAICT65714.2025.11100306},
  ISSN={2834-8249},
  month={July},}@ARTICLE{10235947,
  author={Li, Zhongya and Dong, Boyu and Li, Guoqiang and Jia, Junlian and Sun, Aolong and Shen, Wangwei and Xing, Sizhe and Shi, Jianyang and Chi, Nan and Zhang, Junwen},
  journal={Journal of Optical Communications and Networking}, 
  title={Attention-assisted autoencoder neural network for end-to-end optimization of multi-access fiber-terahertz communication systems}, 
  year={2023},
  volume={15},
  number={9},
  pages={711-725},
  abstract={We propose an end-to-end (E2E) fiber-terahertz (THz) integrated communication system based on an attention-assisted multi-access autoencoder (AMAE) neural network. The AMAE neural network comprises artificial neural networks (ANNs) that function as transmitters (T-ANNs), channel models, and receivers (R-ANNs) for multiple users. By connecting the computational graph of multiple T-ANNs and R-ANNs, we jointly optimize the AMAE to facilitate E2E multi-access communication. Attention mechanisms guide the optimization process to achieve fair and efficient power allocation and orthogonality among different users. We experimentally evaluated the performance of our proposed E2E framework in a 60 Gbit/s multi-channel (1, 5, and 10 km) fiber-THz hybrid system. The results indicate that our AMAE approach outperforms the conventional single-carrier quadrature amplitude modulation scheme by over 3 dB in receiver sensitivity and 11 Gbit/s in capacity under the 20% soft-decision forward error correction threshold in the same-channel back-to-back condition. Additionally, under the performance balance constraint, our approach achieves a transmission speed of 60 Gbit/s within a 10 GHz bandwidth in the multi-channel setting.},
  keywords={Optimization;Training;Communication systems;Wireless communication;Optical transmitters;6G mobile communication;Artificial intelligence},
  doi={10.1364/JOCN.492770},
  ISSN={1943-0639},
  month={Sep.},}@ARTICLE{9086834,
  author={Israel, Steven A. and Sallee, Philip and Tanner, Franklin and Goldstein, Jonathan and Zabel, Shane},
  journal={IEEE Potentials}, 
  title={Applied Machine Learning Strategies}, 
  year={2020},
  volume={39},
  number={3},
  pages={38-42},
  abstract={Recent advances in machine learning (ML), fueled by new frameworks and algorithms, more powerful computing architectures, scalable cloud-based services, and availability of large-scale data sets, have enabled scientists and engineers to tackle more complex problems than ever before. Computer hardware has made tremendous leaps in processing power, bit depth, caching, and storage. Graphics processing units, developed originally for the gaming industry, provide a parallel processing capability that is ideally suited to computer vision (CV) and the simulation of artificial neural networks. The expansion to cloud-based services allows researchers virtually unlimited scaling of resources to tackle problems having millions of input attributes, with output domains up to thousands of potentially nonexclusive classes.},
  keywords={Training;Training data;Gallium nitride;Neural networks;Tools;Automobiles;Generators},
  doi={10.1109/MPOT.2019.2927899},
  ISSN={1558-1772},
  month={May},}@INBOOK{10790430,
  author={Arthi, K. and Sankaradass, Veeramalai and Parveen, Nikhat and Muralidharan, J.},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={7 Methods of cross-validation and bootstrapping}, 
  year={2024},
  volume={},
  number={},
  pages={145-166},
  abstract={A statistical method called cross-validation is used to determine a machine learning model’s parameters and gauge its correctness. It is a type of resampling method where the dataset is arbitrarily split into training and test sets. Using different subsets of the same data, the model is tested and trained. Comparing the projected values from the testing data with the actual values of the same data allows one to evaluate the model’s performance.},
  keywords={Data models;Training;Accuracy;Testing;Machine learning;Predictive models;Tuning;Computational modeling;Statistical analysis;Reliability},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790430},}@INBOOK{10880524,
  author={Gupta, Anuj and Kumar, Vikas and Nakhale, Aryan},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={A Comprehensive Review of Cardiac Image Analysis for Precise Heart Disease Diagnosis Using Deep Learning Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={275-295},
  abstract={Summary <p>Coronary illness is a leading cause of mortality worldwide. Prompt discovery and precise forecasts of coronary disease can help in forestalling and dealing with sickness. Lately, profound learning strategies, such as convolutional neural networks (CNN), have shown promising outcomes in different clinical imaging errands, including coronary illness expectation. This review proposes a CNN&#x2010;based approach for coronary illness expectation utilizing cardiovascular pictures. The proposed model comprises numerous convolutional layers, pooling, and completely associated layers. The model is prepared on an enormous dataset of cardiovascular pictures and related marks for coronary illness presence or nonattendance. The prepared model is assessed on a different test dataset to evaluate its exactness, responsiveness, explicitness, and other execution measurements. Our outcomes show that the proposed CNN&#x2010;based approach accomplishes high exactness and outflanks customary machine learning strategies for coronary illness expectation. The proposed model can be utilized as a device to help doctors in the early determination and treatment of coronary illness.</p>},
  keywords={Solid modeling;Electrocardiography;Heart beat;Arrhythmia;Diseases;Tagging;Prediction algorithms;Nearest neighbor methods;Medical services;Measurement},
  doi={10.1002/9781394280735.ch14},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880524},}@INBOOK{10880615,
  author={Sivaraman, R and Praveena, S and Naresh Kumar, H},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Sustainable Agriculture Through Advanced Crop Management: VGG16&#x2010;Based Tea Leaf Disease Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={121-133},
  abstract={Summary <p>This research employs the VGG16 convolutional neural network architecture coupled with transfer learning to propose a novel method for predicting diseases in tea leaves. Transfer learning leverages knowledge acquired from training on a large&#x2010;scale dataset, such as ImageNet, to perform a related task, such as identifying illnesses in tea leaves. By fine&#x2010;tuning the pre&#x2010;trained VGG16 model on a dataset specific to tea leaf diseases, pertinent features unique to these illnesses were captured, while benefiting from the generalization capabilities of the pre&#x2010;trained model. This approach optimizes computational resources and enhances model performance by utilizing the learned representations of the pre&#x2010;trained model. The attained accuracy of 92% underscores the efficacy of the proposed method in disease prediction, which is crucial for effective disease management in tea cultivation. Furthermore, the precise categorization of illnesses demonstrates the model's ability to distinguish between various types of tea leaf diseases, offering valuable insights for targeted treatment strategies. The model underwent training and validation using a dataset comprising 174 images for validation and 711 images for training, encompassing eight classes of tea leaf diseases. Subsequently, the trained model was evaluated on a separate dataset of 528 images, achieving a test accuracy score of 86.17%. This research represents a significant stride toward harnessing modern technology for sustainable agricultural practices and crop management.</p>},
  keywords={Diseases;Stress;Accuracy;YOLO;Reliability;Plant diseases;Monitoring;Deep learning;Visualization;Surveys},
  doi={10.1002/9781394280735.ch7},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880615},}@INBOOK{10880597,
  author={Sharma, Parth and Kumar, Sohan and Falor, Tanay and Dabral, Om and Upadhyay, Abhinav and Gupta, Rishik and Singh Andotra, Vanshika},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Leveraging Process Mining for Enhanced Efficiency and Precision in Healthcare}, 
  year={2025},
  volume={},
  number={},
  pages={471-488},
  abstract={Summary <p>In the realm of healthcare, optimizing processes is imperative for delivering timely and effective patient care. Process mining, a technique rooted in data and process science, offers a promising avenue for uncovering insights and streamlining operations within healthcare organizations. This chapter explores the applications of process mining in hospitals, focusing on its ability to identify inefficiencies, optimize resource allocation, and enhance overall operational efficiency. Furthermore, it discusses the challenges faced in healthcare operations and proposes solutions to bridge the gap between visible and invisible problems. The chapter concludes with recommendations for leveraging process mining alongside innovative technologies like robotic process automation to revolutionize healthcare delivery and improve patient outcomes. Furthermore, process mining serves as a valuable tool for regulatory compliance and quality assurance in healthcare. By capturing and analyzing process data, organizations can ensure adherence to standards, protocols, and best practices, thereby mitigating risks and improving patient safety. In summary, the utilization of process mining in conjunction with cutting&#x2010;edge technologies and quality improvement methodologies offers a comprehensive approach to transforming healthcare delivery. By fostering a culture of data&#x2010;driven decision&#x2010;making and continuous improvement, healthcare organizations can enhance efficiency, effectiveness, and patient&#x2010;centricity across the care continuum.</p>},
  keywords={Process mining;Medical services;Data models;Analytical models;Monitoring;Xenon;Transforms;Standards organizations;Standardization;Resource management},
  doi={10.1002/9781394280735.ch24},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880597},}@INBOOK{10880604,
  author={},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Index}, 
  year={2025},
  volume={},
  number={},
  pages={623-655},
  abstract={},
  keywords={},
  doi={10.1002/9781394280735.index},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880604},}@INBOOK{10790372,
  author={Venkataramana Sagar, G. and Ambareesh, S. and Augustine, P. John and Gayathri, R.},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={3 Classification and regression algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={53-86},
  abstract={Machine learning algorithms for predicting or categorizing data include classification and regression techniques. Regression algorithms are used to forecast a continuous numerical value, such as the anticipated value of a stock price. In contrast, classification algorithms predict a discrete label, such as whether something is a cat or a dog. The accuracy of the algorithm’s predictions and classifications determines how effective a prediction or classification is. Decision trees, support vector machines, k-nearest neighbors, and naive Bayes are examples of standard classification techniques. Regression methods often used include generalized, polynomial, and linear regression.},
  keywords={Data models;Predictive models;Classification algorithms;Prediction algorithms;Training;Testing;Logistic regression;Accuracy;Machine learning algorithms;Market research},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790372},}@ARTICLE{11123796,
  author={Dmitriev, Anton and Trofimov, Ilya and Burnaev, Evgeny and Barannikov, Serguei},
  journal={IEEE Access}, 
  title={Topological Alternatives for Precision and Recall in Generative Models}, 
  year={2025},
  volume={13},
  number={},
  pages={162273-162284},
  abstract={We introduce the Normalized Topological Divergence (NTD), a fully differentiable metric that simultaneously quantifies fidelity and diversity of generative models directly in raw pixel or spectrogram space, eliminating reliance on pretrained feature extractors. For two empirical distributions P (model) and Q (reference), NTD builds a Vietoris–Rips filtration over  $P \cup Q$  where distance matrix within Q is equal to 0. Extensive experiments on six vision and audio benchmarks: ImageNet-1k, CIFAR-10, MNIST, AFHQv2, AFHQ-Cat, LJSpeech-1, and Gaussian mixtures covering ten generator families show that NTD exposes blur, mode collapse, variance inflation and other generation artifacts. As a result, our metrics are domain-agnostic, provide a precision-recall trade-off, not offered by FID. It represent difference in variance better than density-coverage, TopP&R and P-Precision (Recall) while indicating problems of VAE-like generation more effectively. Moreover, it can be directly optimized by gradient descent algorithms.},
  keywords={Measurement;Manifolds;Linear matrix inequalities;Topology;Standards;Nearest neighbor methods;Estimation;Point cloud compression;Generators;Spectrogram;Topology;generative models;metrics},
  doi={10.1109/ACCESS.2025.3598704},
  ISSN={2169-3536},
  month={},}@INBOOK{10880576,
  author={Chaudhary, Poonam and Rani, Nikki and Aggarwal, Diksha and Sharma, Srishti},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Temporal Normalization and Brain Image Analysis for Early&#x2010;Stage Prediction of Attention Deficit Hyperactivity Disorder (ADHD)}, 
  year={2025},
  volume={},
  number={},
  pages={103-119},
  abstract={Summary <p>A neurological condition known as attention deficit hyperactivity disorder (ADHD) impacts a person's behavior. Concern has been intrigued by the growing rates of ADHD in kids and teenagers throughout the world, which calls for strategies for early detection and diagnosis. This chapter aims to analyze the different machine learning algorithms on the ADHD dataset to serve as a proof of concept of the feasibility of using machine learning models to produce a medically viable solution for the early detection of ADHD. It also discusses the Exploratory Data Analysis methods on the ADHD&#x2010;200 dataset as it provides a better overview of data by quickly analyzing and generating detailed reports of the dataset, saving both time and effort. The importance of temporal normalization on 4D NIfTI data and how it affects statistical assumptions, confounding factors, and comparability. Preprocessing, feature extraction, statistical analysis, cross&#x2010;validation, and interpretation/validation are some of the phases in this study of brain images for the identification of ADHD. The analyzed K&#x2010;nearest neighbors (accuracy: 0.92), random forest (score: 0.77), and linear regression (score: 0.765) algorithms show promising possibilities for analyzing the presented data. This study adds an understanding of ADHD and supports programs like the ADHD&#x2010;200 Sample, which encourages open data sharing to increase this field's scientific expertise.</p>},
  keywords={Functional magnetic resonance imaging;Random forests;Nearest neighbor methods;Logistic regression;Feature extraction;Blood;Training;Shape;Reviews;Reliability},
  doi={10.1002/9781394280735.ch6},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880576},}@INBOOK{10880580,
  author={Rotelli, Alessio and Iadanza, Ernesto},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Advancing Colorectal Cancer Diagnosis: Integrating Synthetic Data and Machine Learning for Microbiome Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={135-152},
  abstract={Summary <p>This work highlights the importance of the gut microbiota in colorectal health, especially during AP&#x2010;to&#x2010;CRC transition. Synthetic data augmentation enlarged and balanced a multidimensional OTU table for machine learning categorization. The OTU table refinement used SVM and LG for sample validation and several statistical tests to assure synthetic data realism. In addition, deep learning feature extraction with LRP identified 64 unique bacterial taxa that were assessed for their ability to distinguish AP and CRC samples in diverse datasets. Fusobacterium was important in LRP and SHAP studies, consistent with its connection with CRC. SHAP analysis using XGBoost discovered differentiated features like Parvimonas, Alistipes, and Ruminococcus in stool and biopsy datasets. Classifying the saliva dataset with 100% accuracy is noteworthy.</p>},
  keywords={Synthetic data;Support vector machines;Biological system modeling;Bacteria;Analytical models;Polynomials;Nutrients;Kernel;Genetics;Data models},
  doi={10.1002/9781394280735.ch8},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880580},}@INPROCEEDINGS{10940196,
  author={Rathish Babu, T. K. S and S, Chandramohan and Be, Hajira and H, Niroshini Infantia},
  booktitle={2025 International Conference on Electronics and Renewable Systems (ICEARS)}, 
  title={Smart Intelligence-Driven Deep Learning Framework for Reliable Detection and Accurate Classification of Skin Cancer Diseases}, 
  year={2025},
  volume={},
  number={},
  pages={1372-1378},
  abstract={The interest in skin cancer detection is growing remarkably because of the prevalence of this health condition worldwide. It improves the chances of survival and enhances general treatment outcomes when diagnosis has been done early. Most the traditional methods for skin lesion detection involve visual inspection performed by dermatologists; hence, it is time-consuming and, to some extent, subjective. Therefore, with the rise of deep learning-Artificial Intelligence, auto-detection of skin cancer has gained a foothold in improving diagnostic accuracy, hence treatment outcomes. The presented paper proposes a GAN-based approach for skin cancer detection by deep learning analysis of dermoscopic images, classifying them as malignant and benign. The proposed approach uses the generator and discriminator parts of GAN to obtain an effective discrimination between cancerous and non-cancerous lesions with high precision. The proposed method incorporates the native abilities of GANs for boosting diagnostic precision. GANs consist of two major components: a generator and a discriminator. These two adversaries collaborate with each other in a way to encourage the classification performance of the framework. The generator synthesizes the realistic data representations by augmenting the dataset with various variations of dermoscopic images. This helps not only to compensate for the insufficiency of annotated data but also to make the model generalize more robustly to diverse skin lesion patterns. At the same time, the discriminator learns to distinguish malignant from benign lesions with high accuracy by using the adversarial nature in GAN training. These results clearly indicate the effectiveness of the proposed GAN-based approach over state-of-the-art traditional deep learning models. GAN makes not only the detection system more robust but also reduces its dependency on large annotated datasets.},
  keywords={Deep learning;Measurement;Visualization;Accuracy;Generative adversarial networks;Generators;Skin;Lesions;Medical diagnosis;Skin cancer;Skin Cancer;Medical Imaging;Deep Learning;Classification;Disease Diagnosis;and Automated Detection},
  doi={10.1109/ICEARS64219.2025.10940196},
  ISSN={},
  month={Feb},}@INBOOK{10790416,
  author={Ashwin, M. and Shaik, Nazeer and Akram, Faiz and Wegari, Getachew Mamo},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={4 Clustering and association algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={87-108},
  abstract={This chapter provides an extensive examination of clustering and association algorithms, offering a comprehensive understanding of these essential unsupervised learning techniques in the field of machine learning. The chapter commences with an introductory section that underscores the significance of unsupervised learning, and presents a detailed overview of clustering and association algorithms. The various clustering algorithms covered in the following sections are k-means, hierarchical clustering, densitybased clustering (like DBSCAN), and Gaussian mixture models. Additionally, the chapter explores evaluation metrics for clustering, including both internal and external evaluation metrics, and offers advise on how to properly interpret the evaluation results. The chapter also examines dimensionality reduction methods, particularly principal component analysis and t-SNE, and clarifies how to use them in clustering scenarios. The Apriori algorithm, frequent itemset mining, association rule development, and methods for judging rule interestingness are all covered in the section on association rule learning. Along with explaining their individual uses, the chapter also goes into advanced clustering methods like density-based spatial clustering, mean-shift clustering, spectral clustering, and affinity propagation. The chapter also discusses association rule mining, ensemble clustering, and time series clustering in huge datasets. The chapter concludes with an overview of the core ideas and developments in unsupervised learning, along with a list of the field’s present research roadblocks and potential future paths.},
  keywords={Clustering algorithms;Unsupervised learning;Machine learning algorithms;Association rule learning;Shape;Recommender systems;Measurement;Decision making;Clustering methods;Buildings},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790416},}@INBOOK{10790750,
  author={Ramasamy, S. and Kantharaju, H. C. and Bindu Madhavi, N. and Haripriya, M. P.},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={8 Meta-learning through ensemble approach: bagging, boosting, and random forest strategies}, 
  year={2024},
  volume={},
  number={},
  pages={167-188},
  abstract={Meta-learning through ensemble approaches is an intriguing subfield of machine learning research. With this method, a more comprehensive learning model is created by combining many machine learning methods, including neural networks and support vector machines. By using an ensemble of models, meta-learning techniques are able to produce more robust results than individual algorithms alone. In addition, ensemble techniques are advantageous because they can easily be expanded to accommodate additional data sources or algorithms. This approach can also embed more knowledge from the data into a more powerful meta-model, which allows the system to generalize better and discover patterns more accurately. In short, metalearning through ensemble approaches is an effective and useful technique for tackling challenging problems in machine learning.},
  keywords={Ensemble learning;Predictive models;Metalearning;Machine learning algorithms;Machine learning;Data models;Bagging;Accuracy;Prediction algorithms;Boosting},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790750},}@INPROCEEDINGS{10959352,
  author={Liu, Ruonan and Wu, Shuai and Liu, Qiming and Guo, Dongsheng and Zhang, Weidong},
  booktitle={2024 IEEE 8th International Conference on Vision, Image and Signal Processing (ICVISP)}, 
  title={Rectified Flow-Based Generative Transfer Learning for System Fault Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={With the rapid development of new energy sources under the “dual carbon” background, the safety and stability of energy storage and power generation systems have attracted widespread attention. This paper investigates the challenges of insufficient training data and the limitations of existing generative transfer learning methods in the field of system fault detection. To address these issues, we propose a graph neural network model based on a generative transfer learning method utilizing the Rectified Flow algorithm for system fault detection. The Rectified Flow algorithm is applied to transfer learning to enhance the efficiency and effectiveness of system fault diagnosis, especially when training data is limited. The model leverages source domain data to train the target domain data, overcoming the data scarcity issue in the target domain. Our experimental results demonstrate that the proposed model outperforms traditional methods in terms of computational efficiency and model conciseness, showing remarkable performance in system fault detection. This study not only provides an effective solution to the data insufficiency problem in system fault detection but also offers valuable insights for further enhancing the performance of system fault diagnosis.},
  keywords={Fault diagnosis;Fault detection;Computational modeling;Transfer learning;Signal processing algorithms;Training data;Signal processing;Stability analysis;Safety;Power generation;system fault diagnosis;graph convolutional neural network;transfer learning;rectified flow algorithm},
  doi={10.1109/ICVISP64524.2024.10959352},
  ISSN={},
  month={Dec},}@ARTICLE{11039039,
  author={Jahan, Nusrat and Hasan, Mohammad Kamrul and Islam, Shayla and Nazri, Mohd Zakree Ahmad and Ariffin, Khairul Akram Zainol and Abbas, Huda Saleh and Alqahtani, Ali and Gohel, Hardik},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Game-Theoretic-GAI Approach for Computation Offloading and Resource Management for Mobile Edge Collaborative Vehicular Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={To counter challenges posed by emerging service needs, limited resources, and the imperative of real-time flexibility, the present research synthesizes game theory and Generative Artificial Intelligence (GAI). The designed architecture supports discreet and scalable resource allocation by combining a Stackelberg game framework with GAI-driven simulation and decision-support mechanisms. The algorithmic methodology of two stages allocates resources among vehicles and Mobile Edge Computing (MEC) servers in an efficient manner, optimizing the offloading ratio. The architecture dynamically adapts by changing network environments using GAI’s ability to simulate complex vehicular scenarios and weight trade-offs concerning latency, energy consumption, and computation efficiency. The resource allocation process is supported by an intelligent offloading decision-support module powered by GAI, complementing parameter optimization using Lagrangian optimization and Karush-Kuhn-Tucker (KKT) conditions. The performance of the designed framework is validated using extensive simulations. The results show the reductions in computational overhead, energy consumption, and latency compared to conventional methods, especially in cases of dynamic job intensity and communication scenarios. The Generative AI can improve the system scalability by allowing task offloading in resource-scarce cases. The result demonstrates the synergistic effectiveness of GAI and game-theoretical approaches in resolving real-time resource allocation challenges in vehicle-to-everything networks.},
  keywords={Resource management;Servers;Computational modeling;Vehicle dynamics;Generative AI;Optimization;Adaptation models;Scalability;Games;Real-time systems;Game-theoretic frameworks;generative AI;computation offloading;mobile edge computing (MEC);vehicular networks;next-generation ITS applications},
  doi={10.1109/TITS.2025.3577673},
  ISSN={1558-0016},
  month={},}@ARTICLE{10540177,
  author={Fan, Fu-You and Zhang, Lin and Dai, Yang},
  journal={IEEE Access}, 
  title={FEGAN: A Feature Extraction Based Approach for GAN Anomaly Detection and Localization}, 
  year={2024},
  volume={12},
  number={},
  pages={76154-76168},
  abstract={With the continuous advancement of industry 4.0 and intelligent manufacturing, there is a growing need for automation and intelligence in industrial production processes. Anomaly detection of industrial product surface images is a key technology in achieving this goal and has thus become a significant area of research. However, this endeavor still encounters challenges such as scarcity of abnormal samples, complexities in data labeling, and uncertainties stemming from unknown factors and randomness. To this end, we propose the Feature Extraction-based for Generative Adversarial Network (FEGAN), aimed at detecting and precisely localizing surface anomaly in industrial products. FEGAN focuses on the deep features of an image, and it builds a feature extraction network and an improved generative adversarial network based on VGG19, respectively. We also jointly determines the anomaly score through the deep feature space as well as the Euclidean distance in 2D image space to better identify and locate the anomaly. Furthermore, we introduce a novel Multi-scale Self-Enhancement (M-SE) strategy to bolster the model’s generalization capabilities. We conducted training and testing on the MVTec and Bottle-Cap public datasets. A plethora of experimental results indicate that the proposed method outperforms existing methods significantly in terms of anomaly detection. Additionally, through an evaluation of the model’s localization accuracy, we demonstrate that FEGAN exhibits certain competitive advantages.},
  keywords={Feature extraction;Image reconstruction;Anomaly detection;Training;Convolutional neural networks;Generative adversarial networks;Task analysis;Feature extraction;generative adversarial networks;surface anomaly},
  doi={10.1109/ACCESS.2024.3406438},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11009304,
  author={Zhang, Wenbo and Wei, Jun and Wang, Hua and Guo, Fanglin and Yang, Rong},
  booktitle={2025 2nd International Conference on Smart Grid and Artificial Intelligence (SGAI)}, 
  title={Research on Short and Long-Term Prediction of New Energy Generation Power Based on VAE-GAN}, 
  year={2025},
  volume={},
  number={},
  pages={336-343},
  abstract={New energy generation power is affected by multiple factors such as meteorological conditions, equipment status and grid dispatching demand, and its output shows strong volatility, intermittency and randomness, which brings great challenges to grid dispatching. In this paper, a new short and long-term prediction method combining Variational Autoencoder (V AE) and Generative Adversarial Nets (GAN) is proposed to solve the shortcomings of traditional models in accuracy and generalization ability. Firstly, key features highly related to power generation are extracted from multi-dimensional meteorological and load data through mutual information (MI) to ensure the stability of model input. Secondly, VAE is used to perform latent distribution learning and feature coding on historical data, and the future power sequence is generated by combining the adversarial training mechanism of GAN generator and discriminator, so as to continuously optimize the authenticity and accuracy of prediction results. The effectiveness of the proposed method is verified in short and long-term prediction tasks. Compared with BiLSTM, LightGBM, Informer and other mainstream models, the Mean Absolute Percentage Error (MAP E) of short-term prediction is reduced to 2.46%, and the coefficient of determination is increased to 0.977. The MAPE of long-term prediction is 3.37%, and the coefficient of determination is 0.961. The results show that the proposed method not only significantly improves the prediction accuracy and stability, but also has good generalization ability, which can provide reliable support for the scheduling optimization and planning decision-making of new energy generation power, and provides an innovative method exploration for new energy generation power prediction.},
  keywords={Accuracy;Autoencoders;Time series analysis;Predictive models;Power system stability;Feature extraction;Stability analysis;Optimization;Mutual information;Meteorology;new energy generation power;Variational Autoencoder;Generative Adversarial Nets;VAE-GAN},
  doi={10.1109/SGAI64825.2025.11009304},
  ISSN={},
  month={March},}@ARTICLE{10098158,
  author={Conze, Pierre-Henri and Andrade-Miranda, Gustavo and Singh, Vivek Kumar and Jaouen, Vincent and Visvikis, Dimitris},
  journal={IEEE Transactions on Radiation and Plasma Medical Sciences}, 
  title={Current and Emerging Trends in Medical Image Segmentation With Deep Learning}, 
  year={2023},
  volume={7},
  number={6},
  pages={545-569},
  abstract={In recent years, the segmentation of anatomical or pathological structures using deep learning has experienced a widespread interest in medical image analysis. Remarkably successful performance has been reported in many imaging modalities and for a variety of clinical contexts to support clinicians in computer-assisted diagnosis, therapy, or surgical planning purposes. However, despite the increasing amount of medical image segmentation challenges, there remains little consensus on which methodology performs best. Therefore, we examine in this article the numerous developments and breakthroughs brought since the rise of U-Net-inspired architectures. Especially, we focus on the technical challenges and emerging trends that the community is now focusing on, including conditional generative adversarial and cascaded networks, medical Transformers, contrastive learning, knowledge distillation, active learning, prior knowledge embedding, cross-modality learning, multistructure analysis, federated learning, or semi-supervised and self-supervised paradigms. We also suggest possible avenues to be further investigated in future research efforts.},
  keywords={Image segmentation;Medical diagnostic imaging;Market research;Shape;Transformers;Task analysis;Computer architecture;Artificial intelligence (AI);deep neural networks;medical imaging;semantic segmentation;vision transformers (ViTs)},
  doi={10.1109/TRPMS.2023.3265863},
  ISSN={2469-7303},
  month={July},}@INPROCEEDINGS{9284802,
  author={Cho, Woojin and Park, Gabyong and Woo, Woontack},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object}, 
  year={2020},
  volume={},
  number={},
  pages={251-259},
  abstract={We propose a 3D hand tracking system using bare-hand depth inpainting from an RGB-depth image for a hand interacting with an object. The effectiveness of most existing hand-object tracking methods is impeded by the insufficiency of data, which do not include hand data occluded by the object, and their reliance on the information inferred from assuming the specific object type. We generate a sufficiently accurate bare-hand depth image from a hand interacting with an object using a conditional generative adversarial network, which is trained using the synthesized 2D silhouettes of the object to learn the morphology of the hand. We evaluate the proposed approach using a hierarchical particle filter-based hand tracker and prove that our approach utilizing the bare-hand tracker in the hand-object interaction dataset achieve state-of-the-art performance. The generalization of our work will enable visual-tactile interaction that is more natural in various wearable augmented reality applications.},
  keywords={Geometry;Three-dimensional displays;Two dimensional displays;Morphology;Semisupervised learning;Object tracking;Augmented reality;Artificial intelligence;Computer vision;Computer vision problems;Tracking;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR50242.2020.00048},
  ISSN={1554-7868},
  month={Nov},}@ARTICLE{10114997,
  author={Han, Kyung-Hoon and Hong, Sungwook},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Ocean Salinity Retrieval and Prediction for Soil Moisture Active Passive Satellite Using Data-to-Data Translation}, 
  year={2023},
  volume={61},
  number={},
  pages={1-15},
  abstract={Sea surface salinity (SSS) is a key parameter in physical oceanography, global hydrological, biochemical cycles, and climate change studies. L-band radiometers onboard satellites, such as soil moisture active passive (SMAP), soil moisture, and ocean salinity (SMOS), have played a crucial role in monitoring and analyzing the global SSS in the past decades. This study presents a method to retrieve and predict the SMAP SSS through data-to-data translation (D2D) based on conditional generative adversarial networks (CGANs). The model was constructed from the polarized brightness temperatures (TBs), differences in the polarized TBs from the L-band of the SMAP satellite, and sea surface temperature (SST) and sea surface wind speed (SSW) from the European Center for Medium-Range Weather Forecasts data from April 2015 to July 2020, and applied to produce SMAP SSS. A comparison between the SMAP salinity and the D2D-generated SMAP salinity showed excellent agreement, evaluated through bias = 0.016, root mean square error (RMSE) = 0.173 in practical salinity unit (psu), and correlation coefficient (CC) = 0.985. Furthermore, a comparison between the D2D-generated and buoy-observed salinities showed good agreement (bias = −0.031 psu and RMSE = 0.196 psu, CC = 0.971). In addition, the results of the one-month prediction model were also in good agreement with the SMAP SSS (bias = 0.028 psu and RMSE = 0.218 psu, CC = 0.977). Consequently, the D2D-based model can be effectively used to generate SMAP SSS information and can be applied to various microwave satellites.},
  keywords={Climate change;Ocean temperature;Salinity (geophysical);Sea surface;Satellite broadcasting;Brightness temperature;Meteorology;Artificial intelligence;data-to-data translation (D2D);Pix2Pix;salinity;satellite remote sensing;soil moisture active passive (SMAP)},
  doi={10.1109/TGRS.2023.3272677},
  ISSN={1558-0644},
  month={},}@ARTICLE{10720015,
  author={Çetiner, Gökhan and Yayan, Uğur and Yazici, Ahmet},
  journal={IEEE Access}, 
  title={Mutation-Based White Box Testing of Deep Neural Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={160156-160174},
  abstract={Deep Neural Networks (DNNs) are used in many critical areas, such as autonomous vehicles, generative AI systems, etc. Therefore, testing DNNs is vital, especially for models used in critical areas. Mutation-based testing is a very successful technique for testing DNNs by mutating their complex structures. Deep Mutation Module was developed to address mutation-based testing and the robustness challenges of DNNs. It analyses the structures of DNNs in detail. It tests models by applying mutation to parameters and structures using its fault library. Testing DNN structures and detecting faults is a highly complex and open-ended challenge. The method proposed in this study applies mutations to DNN parameters to expose faults and weaknesses in the models, thereby testing their robustness. The paper focuses on mutation-based tests of an Reinforce Learning (RL) model developed for electric vehicle routing, a Long Short-Term Memory (LSTM) model developed for prognostic predictions, and a Transformer-based neural network model for electric vehicle routing tasks. The best mutation scores for the LSTM model were measured as 96%, 91.02%, 71.19%, and 68.77%. The test results for the RL model resulted in mutation scores of 93.20%, 72.13%, 77.47%, 79.28%, and 55.74%. The mutation scores of the Transformer model were 75.87%, 76.36%, and 74.93%. These results show that the module can successfully test the targeted models and generate mutants classified as “survived mutants” that outperform the original models. In this way, it provides critical information to researchers to improve the overall performance of the models. Conducting these tests before using them in real-world applications minimizes faults and maximizes model success.},
  keywords={Testing;Artificial neural networks;Robustness;Software testing;Long short term memory;Accuracy;Transformers;Predictive models;Libraries;Convolutional neural networks;Artificial neural networks;Reinforcement learning;Convolutional neural network;deep neural networks;long short-term memory;machine learning;mutation-based testing;reinforcement learning;transformers},
  doi={10.1109/ACCESS.2024.3482114},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10023840,
  author={Yu, Tong and Ling, Ying and Li, Xin and Bin, Dongmei and Yang, Chunyan and Wang, Zhiwen},
  booktitle={2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={Security posture assessment of power information networks based on adversarial deep learning and D-S evidence theory}, 
  year={2022},
  volume={},
  number={},
  pages={157-162},
  abstract={To address the shortcomings of over-reliance on expert experience in the construction of the basic probability assignment of traditional D-S evidence theory, a security posture assessment model for power information networks against deep learning and D-S evidence theory is proposed. Firstly, sample data are collected from different security detection devices and augmented using a generative adversarial network. Secondly, a genetic algorithm is used to optimize the back propagation neural network to obtain the basic probability assignments of different evidences. Finally, the D-S evidence theory is used to synthesize and calculate the trust degree matrices of different posture states according to the temporal order to successfully complete the posture assessment task. The proposed evaluation method is experimentally verified to be feasible and effective.},
  keywords={Backpropagation;Deep learning;Training;Evidence theory;Neural networks;Network security;Data models;Power internet of things;artificial intelligence technology;power information network security;network security posture assessment},
  doi={10.1109/YAC57282.2022.10023840},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9578610,
  author={Li, Dongze and Wang, Wei and Fan, Hongxing and Dong, Jing},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Exploring Adversarial Fake Images on Face Manifold}, 
  year={2021},
  volume={},
  number={},
  pages={5785-5794},
  abstract={Images synthesized by powerful generative adversarial network (GAN) based methods have drawn moral and privacy concerns. Although image forensic models have reached great performance in detecting fake images from real ones, these models can be easily fooled with a simple adversarial attack. But, the noise adding adversarial samples are also arousing suspicion. In this paper, instead of adding adversarial noise, we optimally search adversarial points on face manifold to generate anti-forensic fake face images. We iteratively do a gradient-descent with each small step in the latent space of a generative model, e.g. Style-GAN, to find an adversarial latent vector, which is similar to norm-based adversarial attack but in latent space. Then, the generated fake images driven by the adversarial latent vectors with the help of GANs can defeat main-stream forensic models. For examples, they make the accuracy of deepfake detection models based on Xception or EfficientNet drop from over 90% to nearly 0%, mean-while maintaining high visual quality. In addition, we find manipulating noise vectors n at different levels have different impacts on attack success rate, and the generated adversarial images mainly have changes on facial texture or face attributes.},
  keywords={Manifolds;Visualization;Privacy;Image forensics;Ethics;Computer vision;Face recognition},
  doi={10.1109/CVPR46437.2021.00573},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9302731,
  author={Lim, Yu-Quan and Chan, Chee Seng and Loo, Fung Ying},
  journal={IEEE MultiMedia}, 
  title={ClaviNet: Generate Music With Different Musical Styles}, 
  year={2021},
  volume={28},
  number={1},
  pages={83-93},
  abstract={Classically, the style of the generated music by deep learning models is usually governed by the training dataset. In this article, we improved this by proposing the continuous style embedding ${z}_{s}$zs to the general formulation of variational autoencoder (VAE) to allow users to be able to condition on the style of the generated music. For this purpose, we explored and compared two different methods to integrate ${z}_{s}$zs into the VAE. In the literature of conditional generative modeling, disentanglement of attributes from the latent space is often associated with better generative performance. In our experiments, we find that this is not the case with our proposed model. Empirically and from a musical theory perspective, we show that our proposed model can generate better music samples than a baseline model that utilizes a discrete style label. The source code and generated samples are available at https://github.com/daQuincy/DeepMusicvStyle.},
  keywords={Music;Training;Computer generated music;Decoding;Task analysis;Instruments;Context modeling;music synthesis;deep learning;style transfer},
  doi={10.1109/MMUL.2020.3046491},
  ISSN={1941-0166},
  month={Jan},}@INPROCEEDINGS{10451010,
  author={Shi, Jiangming and Yin, Xiangbo and Zhang, Demao and Qu, Yanyun},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Visible Embraces Infrared: Cross-Modality Person Re-Identification with Single-Modality Supervision}, 
  year={2023},
  volume={},
  number={},
  pages={4781-4787},
  abstract={Visible-infrared person re-identification (VI-ReID) has garnered significant attention due to its wide range of applications. However, a substantial performance gap still ex-ists between cross-modality and conventional ReID methods. Furthermore, existing VI-ReID models heavily rely on super-vised learning, necessitating a substantial amount of labeled infrared data. To overcome these limitations, we explore a “uni-to-cross” setting for VI-ReID, where the labels of infrared images are inaccessible. Our observation is that existing cross-modality datasets have a relatively small scale, while visible ReID datasets offer rich annotation. In this paper, we propose a disentangled generative solution for a “uni-to-cross” setting, which includes a disentangled representation learning module and a discriminative feature learning module. The former purifies the id-related information by utilizing a disentangled generative network. Meanwhile, the latter enables the network to learn from synthesized infrared images, effectively reducing the gap between the two modalities. Extensive experimental results on SYSU-MM01 and RegDB demonstrate that the proposed method gains impressive performance. Especially, our method achieves breakthrough improvement by a large margin of 10.8% and 10.4% in terms of mAP on SYSU-MM01 and RegDB datasets, respectively.},
  keywords={Representation learning;Automation;Image synthesis;Annotations;Design methodology;Benchmark testing;Data models;Cross-modality Person Re-identification;Disentangled Representation Learning;Semi-supervised Learning},
  doi={10.1109/CAC59555.2023.10451010},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{9231729,
  author={Subakan, Cem and Gasse, Maxime and Charlin, Laurent},
  booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={On the Effectiveness of Two-Step Learning for Latent-Variable Models}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Latent-variable generative models offer a principled solution for modeling and sampling from complex probability distributions. Implementing a joint training objective with a complex prior, however, can be a tedious task, as one is typically required to derive and code a specific cost function for each new type of prior distribution. In this work, we propose a general framework for learning latent variable generative models in a two-step fashion. In the first step of the framework, we train an autoencoder, and in the second step we fit a prior model on the resulting latent distribution. This two-step approach offers a convenient alternative to joint training, as it allows for a straightforward combination of existing models without the hustle of deriving new cost functions, and the need for coding the joint training objectives. Through a set of experiments, we demonstrate that two-step learning results in performances similar to joint training, and in some cases even results in more accurate modeling.},
  keywords={Hidden Markov models;Training;Data models;Gallium nitride;Standards;Task analysis;Minimization},
  doi={10.1109/MLSP49062.2020.9231729},
  ISSN={1551-2541},
  month={Sep.},}@ARTICLE{11059876,
  author={Cui, Shuhan and Nguyen, Huy H. and Le, Trung-Nghia and Lu, Chun-Shien and Echizen, Isao},
  journal={IEEE Access}, 
  title={LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase Image-Based Fact Verification}, 
  year={2025},
  volume={13},
  number={},
  pages={136505-136523},
  abstract={Amid the proliferation of forged images, notably the tsunami of deepfake content, extensive research has been conducted on using artificial intelligence (AI) to identify forged content in the face of continuing advancements in counterfeiting technologies. We have investigated the use of AI to provide the original authentic image after deepfake detection, which we believe is a reliable and persuasive solution. We call this “image-based automated fact verification,” a name that originated from a text-based fact-checking system used by journalists. We have developed a two-phase open framework that integrates detection and retrieval components. Additionally, inspired by a dataset proposed by Meta Fundamental AI Research, we further constructed a large-scale dataset that is specifically designed for this task. This dataset simulates real-world conditions and includes both content-preserving and content-aware manipulations that present a range of difficulty levels and have potential for ongoing research. This multi-task dataset is fully annotated, enabling it to be utilized for sub-tasks within the forgery identification and fact retrieval domains. This paper makes two main contributions: 1) We introduce a new task, ‘‘image-based automated fact verification,” and present a novel two-phase open framework combining “forgery identification” and “fact retrieval.” 2) We present a large-scale dataset tailored for this new task that features various hand-crafted image edits and machine learning-driven manipulations, with extensive annotations suitable for various sub-tasks. Extensive experimental results validate its practicality for fact verification research and clarify its difficulty levels for various sub-tasks.},
  keywords={Forgery;Artificial intelligence;Splicing;Feature extraction;Multitasking;Accuracy;Visualization;Transformers;Reliability;Digital images;Datasets;neural networks;forgery detection;image copy detection;fact verification},
  doi={10.1109/ACCESS.2025.3584395},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9397001,
  author={Paymode, Ananda S. and Magar, Shyamsundar P. and Malode, Vandana B.},
  booktitle={2021 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={Tomato Leaf Disease Detection and Classification using Convolution Neural Network}, 
  year={2021},
  volume={},
  number={},
  pages={564-570},
  abstract={Mostly development of country depends of growth of agriculture sectors. Now a day agriculture is facing lot of challenges like unavailability of labor, drastic climate change, uncertainty in rain, natural disaster, different diseases on plant leaf and crops, no fixed prices and unavailability of markets and many more. But as the world continuously increasing the demands of food and more production needed in next 50 years. There are huge numbers of threats in agriculture field. The use of artificial intelligence technology, found best for the all agriculture challenges. So our proposed research focus on detecting and classifying the accurate type of diseases occurred on leaf at early stage. Our research aims to address the problem using the Deep Learning (DL) techniques. The AgroDeep mobile application developed for collection of real database of agriculture leafs and crop. The real diseased leaf images collected and captured through our mobile application. The captured images uploaded over database. There are total six types of crops leaf images (tomato, grapes, soybean, sugarcane, cotton and onion) collected. The tomato diseased leaf selected for detection and classification. The techniques supported whether diseases affected on leaf or not with percentage of accuracy. The best Convolution Neural Network (CNN) algorithm suitable for these analysis. The CNN based model gave the highest accuracy of 97 % which is highest forever for real captured diseased images. Our research playing exquisite role in agriculture sector and farmers. The proposed research supported to increase food production in the agriculture. Ultimately it gives more profit in the farming sector which motivate the farmers for agriculture.},
  keywords={Climate change;Databases;Neural networks;Production;Agriculture;Mobile applications;Diseases;Deep Learning;Convolution Neural Network (CNN);Artificial Intelligence;AgroDeep},
  doi={10.1109/ESCI50559.2021.9397001},
  ISSN={},
  month={March},}@INPROCEEDINGS{9282324,
  author={Macas, Mayra and Wu, Chunming},
  booktitle={2020 IEEE Latin-American Conference on Communications (LATINCOM)}, 
  title={Review: Deep Learning Methods for Cybersecurity and Intrusion Detection Systems}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={As the number of cyber-attacks is increasing, cyber-security is evolving to a key concern for any business. Artificial Intelligence (AI) and Machine Learning (ML) (in particular Deep Learning - DL) can be leveraged as key enabling technologies for cyber-defense, since they can contribute in threat detection and can even provide recommended actions to cyber analysts. A partnership of industry, academia, and government on a global scale is necessary in order to advance the adoption of AI/ML to cybersecurity and create efficient cyber defense systems. In this paper, we are concerned with the investigation of the various deep learning techniques employed for network intrusion detection and we introduce a DL framework for cybersecurity applications.},
  keywords={Deep learning;Knowledge based systems;Network intrusion detection;Malware;Reliability;Computer security;Testing;Machine Learning;Artificial Intelligence;Deep Neural Networks;Cybersecurity;Intrusion Detection Systems},
  doi={10.1109/LATINCOM50620.2020.9282324},
  ISSN={2330-989X},
  month={Nov},}@INPROCEEDINGS{10193911,
  author={Saab, Christine and Zéhil, Gérard-Philippe},
  booktitle={2023 Fifth International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)}, 
  title={About Machine Learning Techniques in Water Quality Monitoring}, 
  year={2023},
  volume={},
  number={},
  pages={115-121},
  abstract={Water Quality Monitoring (WQM) faces significant challenges posed by emerging contaminants, non-point source pollutants, and climate change. The continued development of suitable sensing technologies that are likely to produce increasingly large amounts of data, also creates the need for accurate and efficient data analysis and modeling techniques. Artificial Intelligence is set to play a prominent role in performing analyses and predictions based on large datasets. This work hence reviews some leading Machine Learning (ML) approaches and applications in WQM. It also identifies emerging technique applications that can potentially enhance WQM significantly.},
  keywords={Analytical models;Climate change;Computational modeling;Training data;Machine learning;Water quality;Data models;water quality monitoring;artificial intelligence;machine learning},
  doi={10.1109/ACTEA58025.2023.10193911},
  ISSN={},
  month={July},}@INPROCEEDINGS{10693993,
  author={Jurczyk, Kristina and Riedl, Leonie and Dipp, Marcel and Heck, David and Gerhards, Ben and Popkov, Nikita Maksimovic and Schäfermeier, Bastian and Gildehaus, Luka and Marten, Frank and Berg, Sebastian Wende-von and Braun, Martin},
  booktitle={2024 International Conference on Smart Energy Systems and Technologies (SEST)}, 
  title={Comparing the Impact of AI-Based versus Standard Load Profiles in ANN State Estimation Training in a Real Distribution Grid}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Due to the increasing amount of renewable energy sources and new consumers in low voltage power grids, there is an increasing need for grid monitoring. For this purpose, there exist state estimation methods that can predict the unknown grid state. Historically, standard load profiles were used as input, but they may be outdated and not adequately represent small consumer dynamics. To improve the estimator, a novel artificial intelligence-based generator of load time series is used to generate small consumer load profiles. An estimation method is then trained on both standard- and novel load profiles to estimate the overall state of a realistic grid. The results of both runs are then compared with real grid measurements to determine the estimation error in each case. First results show that the overall estimation error is lower with novel synthetic load profiles. In line loading estimates for example, 52 % of upper quartiles were below a 5 % error with standard profiles, compared to 68 % of upper quartiles with novel profiles. Two topological errors in the grid model could also be identified.},
  keywords={Training;Estimation error;Systematics;Measurement uncertainty;Training data;State estimation;Forecasting;Standards;Optimization;Load modeling;Artificial Intelligence;State Estimation;monitoring LV grids;novel Synthetic Load Profiles},
  doi={10.1109/SEST61601.2024.10693993},
  ISSN={2836-4678},
  month={Sep.},}@ARTICLE{11173998,
  author={Cao, Yu and Ma, Shuhao and Zhang, Mengshi and Liu, Jindong and Huang, Jian and Zhang, Zhi-Qiang},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Neuro-Fuzzy Musculoskeletal Model-Driven Assist-as-Needed Control via Impedance Regulation for Rehabilitation Robots}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={In rehabilitation applications, encouraging patients to actively participate in training is essential for effective recovery. However, personalized control design in robot-assisted therapy remains challenging due to variations in patients' motor capabilities. To address this issue, this paper proposes an assist-as-needed (AAN) control framework that integrates a hybrid fuzzy-transformer neural network (HFTN) with a fuzzy echo state network (FESN)-based variable impedance controller to ensure personalized support and active engagement. The HFTN integrates fuzzy logic with transformer architectures in parallel paths, establishing a novel neuro-fuzzy musculoskeletal (MSK) model that maps surface electromyography (sEMG) signals to joint torque through combined uncertainty and temporal modeling for enhanced real-time estimation. The variable impedance controller constructs the stiffness and damping matrices of the robotic system through the FESN and develops an adaptive update law for the FESN output weights, effectively addressing instability issues in variable stiffness control. Furthermore, driven by physiologically estimated joint torques from the HFTN, the adaption of the FESN reservoir states enables real-time modulation of stiffness and damping, facilitating transitions between human-dominated and robot-dominated modes. This realizes the AAN concept, ensuring personalized and responsive assistance. Various experiments on an upper limb rehabilitation robot were conducted to validate the effectiveness of both the neuro-fuzzy MSK model and the AAN controller in delivering optimal assistance while promoting active user participation.},
  keywords={Impedance;Robots;Transformers;Training;Motors;Fuzzy logic;Torque;Computational modeling;Fuzzy systems;Computer architecture;Assist-as-needed control;hybrid fuzzy-transformer neural network;FESN-based impedance control;impedance regulation},
  doi={10.1109/TFUZZ.2025.3611266},
  ISSN={1941-0034},
  month={},}@INPROCEEDINGS{10046797,
  author={C, Vasanthakumar and S, Karthikeyan and V R, Raghunandhan and S, Sanjay and R, Tushar},
  booktitle={2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE)}, 
  title={A Study On Deep Fakes Detection Using Blinks and Tracker}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep learning, a type of artificial intelligence (AI), replicates how the human brain processes data when processing data for tasks including speech recognition, object detection, visual object recognition, language translation, and decision-making. Generative Adversarial Network (GAN) is an outstanding generative version which can be extensively utilized in diverse applications. Recent research has indicated that it's far feasible to achieve faux face photographs with an excessive visible excellence primarily based totally in this novel version. For instance, the example may be impacted by a person's orientation, age, the hour of the day, or level of personal wellbeing. The misuse of the fake faces in photo manipulation could lead to certain ethical, moral, and legal issues. Therefore, in this work we first recommend a Convolution Neural Network (CNN) based entirely technique to find fake face photos produced by the modern-day pleasant technique, and we provide practical evidences to indicate that the proposed technique can create high-satisfactory results with a median accuracy over 87.5% which was accomplished in previous papers and projects. In order to further support the logic of our method, we also provide comparison results assessed on a few variations of the suggested CNN architecture, including the excessive by skip filter, the variety of the layer agencies, and the activation function.},
  keywords={Ethics;Visualization;Face recognition;Speech recognition;Object detection;Generative adversarial networks;Generators;Deep Learning;Deep fake;GAN's;Cyber Security;CNN},
  doi={10.1109/ICATIECE56365.2022.10046797},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10605543,
  author={Rajasekar, Kishore and Loh, Randolph and Fok, Kar Wai and Thing, Vrizlynn L. L.},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Privacy preserving layer partitioning for Deep Neural Network models}, 
  year={2024},
  volume={},
  number={},
  pages={1129-1135},
  abstract={MLaaS (Machine Learning as a Service) has become popular in the cloud computing domain, allowing users to leverage cloud resources for running private inference of ML models on their data. However, ensuring user input privacy and secure inference execution is essential. One of the approaches to protect data privacy and integrity is to use Trusted Execution Environments (TEEs) by enabling execution of programs in secure hardware enclave. Using TEEs can introduce significant performance overhead due to the additional layers of encryption, decryption, security and integrity checks. This can lead to slower inference times compared to running on unprotected hardware. In our work, we enhance the runtime performance of ML models by introducing layer partitioning technique and offloading computations to GPU. The technique comprises two distinct partitions: one executed within the TEE, and the other carried out using a GPU accelerator. Layer partitioning exposes intermediate feature maps in the clear which can lead to reconstruction attacks to recover the input. We conduct experiments to demonstrate the effectiveness of our approach in protecting against input reconstruction attacks developed using trained conditional Generative Adversarial Network(c-GAN). The evaluation is performed on widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two datasets: ImageNet for Image classification and TON IoT dataset for cybersecurity attack detection.},
  keywords={Privacy;Cloud computing;Runtime;Computational modeling;Graphics processing units;Libraries;Hardware;enclave;model partition;private inference;Trusted execution environment;intel sgx;CNN},
  doi={10.1109/CAI59869.2024.00202},
  ISSN={},
  month={June},}@INPROCEEDINGS{8995245,
  author={Qin, Yingjie and Chen, Ming and Qi, Lizhe and Sun, Yunquan},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Focus Generator with Score Classification on Fabric Defect Detection}, 
  year={2019},
  volume={},
  number={},
  pages={1708-1714},
  abstract={Fabric defect detection plays an important role in the production of fabrics. Thanks to deep learning and large-scale datasets, popular object detection tasks have made great progress. RetinaNet has been widely used in object detection tasks in various fields, such as face detection, due to its flexibility and operability. It's high accuracy and detection results are derived from datasets with rich features. So limited by the small-scale fabric defect dataset, RetinaNet is difficult to apply to fabric defect detection task. In this paper, we propose an effective neural network approach to solve the problem of small-scale fabric dataset and apply RetinaNet to this task. To overcome the insufficient features because of the small-scale dataset, we first propose a generative model to add Gaussian noise on latent space, called focus generator, which can be controlled with defect instances to generate more data. Then we add a classification model to limit influence produced by the focus generator, called score classification. Finally, we merge the focus generator and score classification with an improved RetinaNet to achieve fabric defect detection, therefore, we name our model FSR. By the way, the operations of adding noise are different on the steps of training and testing. The experimental result shows that our proposed method can achieve better performance comparing to our baseline RetinaNet and finally achieve accuracy of 83.4% on our small-scale fabric defect dataset.},
  keywords={Deep learning;Training;Accuracy;Neural networks;Object detection;Production;Fabrics;Generators;Defect detection;Testing;Fabric defect Detection;Deep Learning;Focus Generator;RetinaNet},
  doi={10.1109/ICTAI.2019.00252},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{8463611,
  author={Zhang, Mingjin and Wang, Ruxin and Gao, Xinbo and Li, Jie and Tao, Dacheng},
  journal={IEEE Transactions on Image Processing}, 
  title={Dual-Transfer Face Sketch–Photo Synthesis}, 
  year={2019},
  volume={28},
  number={2},
  pages={642-657},
  abstract={Recognizing the identity of a sketched face from a face photograph dataset is a critical yet challenging task in many applications, not least law enforcement and criminal investigations. An intelligent sketched face identification system would rely on automatic face sketch synthesis from photographs, thereby avoiding the cost of artists manually drawing sketches. However, conventional face sketch-photo synthesis methods tend to generate sketches that are consistent with the artists'drawing styles. Identity-specific information is often overlooked, leading to unsatisfactory identity verification and recognition performance. In this paper, we discuss the reasons why conventional methods fail to recover identity-specific information. Then, we propose a novel dual-transfer face sketch-photo synthesis framework composed of an inter-domain transfer process and an intra-domain transfer process. In the inter-domain transfer, a regressor of the test photograph with respect to the training photographs is learned and transferred to the sketch domain, ensuring the recovery of common facial structures during synthesis. In the intra-domain transfer, a mapping characterizing the relationship between photographs and sketches is learned and transferred across different identities, such that the loss of identity-specific information is suppressed during synthesis. The fusion of information recovered by the two processes is straightforward by virtue of an ad hoc information splitting strategy. We employ both linear and nonlinear formulations to instantiate the proposed framework. Experiments on The Chinese University of Hong Kong face sketch database demonstrate that compared to the current state-of-the-art the proposed framework produces more identifiable facial structures and yields higher face recognition performance in both the photo and sketch domains.},
  keywords={Learning (artificial intelligence);Facial features;Face recognition;Face sketch-photo synthesis;dual-transfer;intra-domain transfer;inter-domain transfer},
  doi={10.1109/TIP.2018.2869688},
  ISSN={1941-0042},
  month={Feb},}
