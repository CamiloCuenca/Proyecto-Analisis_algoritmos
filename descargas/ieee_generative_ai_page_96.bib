@INPROCEEDINGS{10675746,
  author={Sudheerbabu, Gaadha and Ahmad, Tanwir and Truscan, Dragos and Vain, Jüri and Porres, Ivan},
  booktitle={2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Iterative Optimization of Hyperparameter-based Metamorphic Transformations}, 
  year={2024},
  volume={},
  number={},
  pages={13-20},
  abstract={Verification and validation of a software system to ensure compliance with the specification and intended functional behaviour often pose a challenge when it lacks an explicit test oracle. We present an efficient black-box metamorphic testing approach in which test cases are automatically generated based on metamorphic transformations. The hyperparameters of several metamorphic transformations are optimized on the fly using a generative AI with a feedback loop for optimal test generation and test suite minimization. The proposed method uses several combined metamorphic relations to define test inputs and to determine the test verdict. The feedback on test quality is evaluated based on the metamorphic relation’s fitness function and used to optimize the next iterations of test generation. The effectiveness of the proposed approach is evaluated on an industrial case study of a crane’s load position system which lacks an explicit test oracle. The experimental results confirm that optimizing the morphing transformations using the feedback loop improves the effectiveness of metamorphic test input generation. The outcome of the study shows that the approach can be potentially applied for functional safety verification in software systems with a test oracle problem.},
  keywords={Software testing;Feedback loop;Software algorithms;Optimization methods;Software systems;Minimization;Safety;Metamorphic testing;Verification and Validation;Artificial Intelligence;Software testing;Generative AI;Test Automation},
  doi={10.1109/ICSTW60967.2024.00016},
  ISSN={2159-4848},
  month={May},}@ARTICLE{11015746,
  author={Shen, Xiaobing and Zuo, Yu and Cobaleda, Diego Bernal and Martinez, Wilmar},
  journal={IEEE Transactions on Power Electronics}, 
  title={Iron Loss Extrapolation Predictions for High-Frequency Magnetic Components Using Denoising Diffusion Models}, 
  year={2025},
  volume={40},
  number={10},
  pages={15254-15264},
  abstract={This article introduces denoising diffusion probabilistic models to enhance core loss predictions in high-frequency magnetic components. Conventional methods, such as the Steinmetz equation, often fail to accurately capture the nonlinear dynamics and complex waveforms characteristic of high-frequency magnetic core losses. Previous approaches using multilayer perceptron, transfer learning, and generative adversarial networks have encountered issues, such as data boundaries and noise, adversely affecting prediction accuracy. Denoising diffusion probabilistic models overcome these challenges by improving both interpolation and extrapolation capabilities, achieving a minimum average relative error of 0.99% across all datasets provided by the MagNet challenge. Furthermore, denoising diffusion probabilistic models, first introduced into power electronics, support effective data augmentation, validated through the generation of high-quality core loss data for Nanocrystalline ring cores in three different sizes (R16, R25, R50). This robust modeling framework not only reduces prediction errors but also enhances dataset comprehensiveness, thereby facilitating the design and optimization of next-generation power electronic components.},
  keywords={Magnetic cores;Core loss;Training;Data models;Magnetic hysteresis;Frequency measurement;Voltage measurement;Mathematical models;Current measurement;Power electronics;Artificial intelligence;conditional generative adversarial networks;denoising diffusion probabilistic model;high-frequency magnetic cores;magnetic loss modeling},
  doi={10.1109/TPEL.2025.3573876},
  ISSN={1941-0107},
  month={Oct},}@ARTICLE{11082647,
  author={Zhang, Yong and Xu, Wenhao and Shen, Xiangyu and He, Zhichao and Xie, Nan},
  journal={IEEE Transactions on Power Delivery}, 
  title={Ultra-Low Power Electronic Current Transducer Based on 4-Level Encoding and Weak-Light Commutation}, 
  year={2025},
  volume={40},
  number={5},
  pages={2683-2695},
  abstract={The power supply solutions for the high-voltage side of active electronic current transducers (ECTs) suffer from dead zones in low currents, low energy conversion efficiency and so on. This paper proposes an ECT with a power consumption less than 4 milliwatts (mW) on the high-voltage side, which employs a battery-powered scheme to solve the problems. This scheme utilizes Light-emitting Diodes (LEDs) operating in a weak light mode with 1.8 V driving voltage and effective duty cycle of 0.16. A low-power driver, composed of double half-bridge, is adopted to merge two digital signals into a single 4-level one, halving the power consumption of the LEDs. A 4-level anti-interference decoding method integrating support vector machine and recurrent neural network is employed in the low voltage side. The experimental results demonstrate that the power consumption on the high-voltage side of ECT has a maximum value of 3.14 mW, with accuracy class of 0.2 S. The combined algorithm can correctly decode all the 4-level experimental datasets, as well as the expanded dataset generated by autoencoder and generative adversarial networks. Furthermore, it also exhibits excellent generalization capabilities and resistance to noise interference.},
  keywords={Light emitting diodes;High-voltage techniques;Power demand;Encoding;Circuits;Resistors;Fiber lasers;Coils;Semiconductor lasers;Mathematical models;Electronic current transducer;ultra-low power;4-level signals;machine learning;generative artificial intelligence},
  doi={10.1109/TPWRD.2025.3590164},
  ISSN={1937-4208},
  month={Oct},}@INPROCEEDINGS{11166810,
  author={M, Swathi Pai and Gaur, Poonam and V, Dankan Gowda and Kumari, Sabnam and Yogita and Abraham, Kochumol},
  booktitle={2025 3rd International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)}, 
  title={Deep Learning-Driven Facial Emotion Recognition with GANs for Optimizing Student Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1610-1617},
  abstract={Facial emotion recognition (FER) has emerged as a central tool in explaining the emotional process of human beings with significant ramifications to teaching and learning practice. This paper gives the introduction of such an edge-computing system, where a convolutional neural network (CNN) is utilized in classifying facial expressions and where the images generated by the Generative Adversarial Network (GAN) created synthetic images which are used to enhance the classification. The architecture aims at alleviating the limitations of traditional data by supplementing the training data with artifacts generated using GAN. Through ongoing monitoring of in real time of the emotional responses of students, the system provides flexible learning conditions adaptive to the emotional conditions, thus enabling engagement, motivation, and increased academic achievements. The empirical results display strong results related to emotion prediction achieving higher precision rates following the use of data augmentation based-GAN. Such findings highlight the promise of combining deep-learning and GANs in the creation of emotional intelligence systems in education to customise instructional content and teaching methods based on the needs and emotional responses.},
  keywords={Emotion recognition;Accuracy;Education;Diversity reception;Generative adversarial networks;Data augmentation;Real-time systems;Data models;Emotional responses;Convolutional neural networks;Facial Emotion Recognition;Deep Learning;GANs;Student Learning;Emotion Analysis;Educational Optimization;AI in Education},
  doi={10.1109/ICSCDS65426.2025.11166810},
  ISSN={},
  month={Aug},}@ARTICLE{10136197,
  author={Li, Chenyu and Zhang, Bing and Hong, Danfeng and Yao, Jing and Chanussot, Jocelyn},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={LRR-Net: An Interpretable Deep Unfolding Network for Hyperspectral Anomaly Detection}, 
  year={2023},
  volume={61},
  number={},
  pages={1-12},
  abstract={Considerable endeavors have been expended toward enhancing the representation performance for hyperspectral anomaly detection (HAD) through physical model-based methods and recent deep learning-based approaches. Of these methods, the low-rank representation (LRR) model is widely adopted for its formidable separation capabilities for background and target features, however, its practical applications are limited due to the reliance on manual parameter selection and subpar generalization performance. To this end, this article presents a new HAD baseline network, referred to as LRR-Net, which synergizes the LRR model with deep learning techniques. LRR-Net leverages the alternating direction method of multipliers (ADMM) optimizer to solve the LRR model efficiently and incorporates the solution as prior knowledge into the deep network to guide the optimization of parameters. Moreover, LRR-Net transforms the regularized parameters into trainable parameters of the deep neural network, thus alleviating the need for manual parameter tuning. Additionally, this article proposes a sparse neural network embedding to demonstrate the scalability of the LRR-Net framework. Empirical evaluations on eight distinct datasets illustrate the efficacy and superiority of the proposed approach compared to the state-of-the-art methods.},
  keywords={Hyperspectral imaging;Feature extraction;Deep learning;Optimization;Anomaly detection;Generative adversarial networks;Training;Alternating direction method of multipliers (ADMM);anomaly detection;artificial intelligence;deep unfolding;hyperspectral image;interpretability;low-rank representation (LRR);sparse representation},
  doi={10.1109/TGRS.2023.3279834},
  ISSN={1558-0644},
  month={},}@ARTICLE{10398264,
  author={Huang, Xumin and Li, Peichun and Du, Hongyang and Kang, Jiawen and Niyato, Dusit and Kim, Dong In and Wu, Yuan},
  journal={IEEE Network}, 
  title={Federated Learning-Empowered AI-Generated Content in Wireless Networks}, 
  year={2024},
  volume={38},
  number={5},
  pages={304-313},
  abstract={Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative AI models. Deploying AIGC services in wireless networks has been expected to enhance the user experience. However, the existing AIGC service provision suffers from several limitations, e.g., the centralized training in the pre-training, fine-tuning, and inference processes, especially their implementations in wireless networks with privacy preservation. Federated learning (FL), as a collaborative learning framework where the model training is distributed to cooperative data owners without the need for data sharing, can be leveraged to simultaneously improve learning efficiency and achieve privacy protection for AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim to enable users to generate diverse, personalized, and high-quality content. Furthermore, we conduct a case study of FL-aided AIGC fine-tuning by using the state-of-the-art AIGC model, i.e., stable diffusion model. Numerical results show that our scheme achieves advantages in effectively reducing the communication cost and training latency, and providing privacy protection. Finally, we highlight several major research directions and open issues for the convergence of FL and AIGC.},
  keywords={Data models;Computational modeling;Training;Transformers;Adaptation models;Generative adversarial networks;Federated learning;Deep learning;Federated learning;AIGC;wireless networks;deep learning;stable diffusion},
  doi={10.1109/MNET.2024.3353377},
  ISSN={1558-156X},
  month={Sep.},}@INPROCEEDINGS{8614099,
  author={Giacomo, Giovanni and Machado, Matheus and Drews, Paulo and Botelho, Silvia},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Sonar-to-Satellite Translation using Deep Learning}, 
  year={2018},
  volume={},
  number={},
  pages={454-459},
  abstract={Sonar images pose hindrances when being elucidated for applications such as underwater navigation and localization. On the other hand, satellite images are simpler to be interpreted, but require GPS that is unavailable underwater due to absorption phenomena. Thus, we propose a neural network capable of translating an acoustic image acquired underwater to a textured image. We called the process sonar-to-satellite translation. We adopted a state-of-the-art neural architecture on a dataset comprised of sonar data and their respective satellite images. The experimental results show our method can extract interesting features from acoustic images and generate an informative texture image.},
  keywords={Satellites;Generators;Acoustics;Generative adversarial networks;Sonar;Encoding;Sensors;Deep Learning;Acoustic Images;Artificial Intelligence;Neural Networks;Satellite Images},
  doi={10.1109/ICMLA.2018.00074},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10586611,
  author={Liu, Yang and Li, Xiaofei and Zhang, Jun and Hu, Shengze and Lei, Jun},
  booktitle={2024 3rd International Conference on Image Processing and Media Computing (ICIPMC)}, 
  title={DA-HFNet: Progressive Fine-Grained Forgery Image Detection and Localization Based on Dual Attention}, 
  year={2024},
  volume={},
  number={},
  pages={51-58},
  abstract={The increasing difficulty in accurately detecting forged images generated by AIGC(Artificial Intelligence Generative Content) poses many risks, necessitating the development of effective methods to identify and further locate forged areas. In this paper, to facilitate research efforts, we construct a DA-HFNet forged image dataset guided by text or image-assisted GAN and Diffusion model. Our goal is to utilize a hierarchical progressive network to capture forged artifacts at different scales for detection and localization. Specifically, it relies on a dual-attention mechanism to adaptively fuse multi-modal image features in depth, followed by a multi-branch interaction network to thoroughly interact image features at different scales and improve detector performance by leveraging dependencies between layers. Additionally, we extract more sensitive noise fingerprints to obtain more prominent forged artifact features in the forged areas. Extensive experiments validate the effectiveness of our approach, demonstrating significant performance improvements compared to state-of-the-art methods for forged image detection and localization. The code and dataset will be released in the future.},
  keywords={Location awareness;Image resolution;Fuses;Noise;Media;Feature extraction;Generative adversarial networks;AIGC;Forgery Image Detection;Hierarchical Network;Progressive Mechanism},
  doi={10.1109/ICIPMC62364.2024.10586611},
  ISSN={},
  month={May},}@INPROCEEDINGS{10541474,
  author={Gao, Shiwei and Guo, Zerong and Meng, Xin and Liu, Li and Xiong, Youzhi and Sun, Sanshan and Li, Hui},
  booktitle={2024 7th World Conference on Computing and Communication Technologies (WCCCT)}, 
  title={GAN-Assisted Sample Equalization for CNN-Based Human Peripheral Leukocyte Image Recognition and Classification}, 
  year={2024},
  volume={},
  number={},
  pages={324-329},
  abstract={Morphology analysis and the count of distinct cells in human peripheral blood are dominant methods to diagnose common blood diseases. In recent years, with the purpose of improving the efficiency of these methods, the convolutional neural network (CNN), a widely used model of artificial intelligence technology, has been utilized in computer-aided blood tests to recognize and classify blood cell images. However, due to the nature of a smaller number of leukocytes in one blood sample, the CNN model is generally trained by insufficient leukocyte images and thus shows lower accuracy on leukocytes. To address the above issue, we propose a generative adversarial network (GAN) assisted framework to increase leukocyte images and further enable the CNN model to improve the performance of human peripheral leukocyte image recognition and classification. We validate the effectiveness of the proposed framework through a comparative performance analysis of four CNN models trained by four image datasets deriving from different sample equalization methods. The numerical results show that the GAN-assisted sample equalization method is superior to making the CNN model converge fast and attain high recognition and classification accuracy.},
  keywords={Image recognition;Computational modeling;Cells (biology);Generative adversarial networks;Numerical models;Performance analysis;Convolutional neural networks;CNN;GAN;human peripheral leukocyte;sample equalization;data augmentation},
  doi={10.1109/WCCCT60665.2024.10541474},
  ISSN={},
  month={April},}@INPROCEEDINGS{10315999,
  author={Chen, Yen-Wen and Chiu, Yi-Wei and Liu, Yu-Sin and Huang, Chih-Yu and Shen, Yu-Chun},
  booktitle={2023 IEEE 14th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={AI Powered Multi-model Content Creation For Virtual Gallery Using Learning Machine}, 
  year={2023},
  volume={},
  number={},
  pages={0704-0709},
  abstract={Online digital advertising is currently a crucial marketing tool. Presently, the production of marketing content, including images, copy, and music, is still carried out using traditional manual methods. The purpose of this paper is to utilize AI technology to automatically generate diverse and contextually relevant initial drafts of marketing content. Users can gain inspiration from these drafts, resulting in significant time and manpower cost savings. Users only need to input a product image, product name, and select a music style through the webpage. Subsequently, the webpage will showcase AI-generated initial drafts of product copy with various backgrounds and arrangements. Taking inspiration from a museum concept, when users hover their mouse over the displayed product, they can simply press a designated key to play the generated music. This offers users a preliminary idea of the product in terms of imagery, copy, and music – all three aspects combined.},
  keywords={Industries;Presses;Production;Manuals;Machine learning;Mobile communication;Museums;AI-generation;GPT-2;digital advertisement;music},
  doi={10.1109/UEMCON59035.2023.10315999},
  ISSN={},
  month={Oct},}@ARTICLE{10443628,
  author={Dong, Lisha and Zhou, Yu and Jiang, Jianmin},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Dual-Clustered Conditioning Toward GAN-Based Diverse Image Generation}, 
  year={2024},
  volume={70},
  number={1},
  pages={2817-2825},
  abstract={Generative Artificial Intelligence (AI) has revolutionized image generation in the realm of consumer electronics, which has illustrated its significant impact on product development and user experiences. In this paper, we propose a class conditioned GAN with dual clustering to leverage correlations across both spatial and approximated discrete cosine transform (ADCT) domain towards improved diverse image generations. By analyzing the spectral bias from a frequency perspective through clustering in ADCT domain, the proposed achieves the advantage that class-conditioning provided by pixel clustering can be significantly strengthened and complemented by ADCT clustering. The sequential arrangement of ADCT clustering followed by pixel clustering not only optimizes their individual contribution and coordination, but also avoid the need to retrain the conditional generator and discriminator from scratch. Extensive experiments carried out illustrate that, in terms of FID and IS measurements as well as synthesized quality, integrity and diversity, our proposed achieves significant superiority against the existing state of the arts.},
  keywords={Discrete cosine transforms;Training;Generators;Frequency-domain analysis;Consumer electronics;Generative adversarial networks;Correlation;The class-conditional GAN;ADCT clustering;pixel clustering},
  doi={10.1109/TCE.2024.3367170},
  ISSN={1558-4127},
  month={Feb},}@ARTICLE{10839363,
  author={Liu, Mingming and Bossmann, Florian and Ma, Jianwei},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Kolmogorov-Arnold Networks for Semi-Supervised Impedance Inversion}, 
  year={2025},
  volume={22},
  number={},
  pages={1-5},
  abstract={Seismic impedance inversion plays a crucial role in obtaining underground physical properties and enhancing seismic exploration accuracy. In recent years, semi-supervised methods have significantly improved the efficiency and precision of impedance inversion. However, methods based on convolutional neural networks (CNNs) often struggle to effectively capture long-term dependencies in data, which can negatively impact the results of seismic acoustic impedance inversion for tasks with long-term characteristics. Therefore, this letter proposes a semi-supervised learning acoustic impedance inversion network that integrates the advanced deep learning techniques, including Kolmogorov-Arnold networks (KANs) and convolutional KAN. Through experimental comparisons of acoustic impedance fitting results, we demonstrate that KAN and convolutional KAN exhibit stronger fitting capabilities for long-term acoustic impedance data than traditional linear layers and CNN. This provides a novel method and strategy for establishing the mapping relationship between seismic data and wave impedance. Additionally, testing on the Marmousi2 model shows that the network incorporating these new deep learning methods improves the lateral continuity of the inversion profile and enhances the prediction accuracy in impedance inversion.},
  keywords={Impedance;Acoustics;Convolutional neural networks;Data models;Fitting;Predictive models;Attention mechanisms;Generative adversarial networks;Encoding;Deep learning;Artificial intelligence;impedance inversion;Kolmogorov-Arnold networks (KANs);semi-supervised learning},
  doi={10.1109/LGRS.2025.3529024},
  ISSN={1558-0571},
  month={},}@INPROCEEDINGS{10840601,
  author={Kaur, Tejinder and Kumar Gantayat, Pradosh and Das, Swatismita and Majhi, Madhusmita},
  booktitle={2024 4th International Conference on Technological Advancements in Computational Sciences (ICTACS)}, 
  title={Advanced Machine Learning Approaches to Combat Fake News in the Digital Era}, 
  year={2024},
  volume={},
  number={},
  pages={424-427},
  abstract={This paper aims to provide the sustainable design for the user to provide them with ability to check whether the information is fake or real by providing the accuracy of the news using Machine learning via various algorithms with the help of database, which will respond as whether the news is accurate or not. Fake news has become a prevalent issue in today's digital age, causing significant societal impacts by spreading misinformation. This paper explores the application of machine learning techniques for detecting fake news. The paper also addresses challenges such as data scarcity, model interpretability, and the dynamic nature of fake news. We conclude with recommendations for future research directions, emphasizing the need for real-time detection systems and cross-disciplinary collaboration to enhance the effectiveness of fake news detection using machine learning.},
  keywords={Accuracy;Social networking (online);Computational modeling;Machine learning;Streaming media;Transformers;Real-time systems;Data models;Fake news;Regression tree analysis;Machine Learning;Fake News;Naive Bayes;Decision Tree;Random Forest;Logistic Regression;Online Resources},
  doi={10.1109/ICTACS62700.2024.10840601},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10570928,
  author={Chen, Ming and Wang, Shangshang and Zhang, Tianyi and Shao, Ziyu and Yang, Yang},
  booktitle={2024 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={GNN-Aided Distributed GAN with Partially Observable Social Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The proliferation of edge computing has facilitated the edge-based artificial intelligence-generated content (AIGC) for ubiquitous and distributed end devices. To exemplify, we focus on the distributed implementation of one established instance, generative adversarial network (GAN), yielding the distributed GAN task. Practically speaking, this task usually is impeded by concerns including the unknown latency (of processing and transmission), the fairness requirement induced by heterogeneous distributed data and the limited energy budget of end devices. Besides, an often neglected factor is how to exploit feedback from networked end devices among which social ties indicate the flow of shared information. In practice, such social ties are partially observable to lack of exact knowledge of users, e.g., resulted from scarce historical data and privacy issues. Under this setting, we propose an online algorithm via integration of 1) online learning aided by graph neural network (GNN), aiming to recover social ties with GNN-based edge prediction, for accelerated learning of uncertainty and 2) online control to adaptively guarantee the constraints. We theoretically show that it not only achieves a sub-linear regret with guaranteed energy consumption and fairness but also leads to a superior global GAN. We also conduct simulations to justify its outperformance over online baselines.},
  keywords={Training;Energy consumption;Uncertainty;Image edge detection;Distributed databases;Generative adversarial networks;Prediction algorithms},
  doi={10.1109/WCNC57260.2024.10570928},
  ISSN={1558-2612},
  month={April},}@INPROCEEDINGS{11144040,
  author={Xia, Jianan and Zhu, Bing and Zhang, Huasheng and Li, Qishuang and Ren, Weilun and Dong, Baichuan and Song, Kangkang},
  booktitle={2025 6th International Conference on Electrical Technology and Automatic Control (ICETAC)}, 
  title={Overview of AIGC Models for Industry Defect Detection}, 
  year={2025},
  volume={},
  number={},
  pages={630-637},
  abstract={As a pivotal component of smart manufacturing, industrial defect detection has long faced core challenges such as the scarcity of defect samples and limited model generalization. AIGC (Artificial Intelligence Generated Content) models, leveraging their deep feature learning and data generation capabilities, provide a novel technical pathway to transcend conventional detection paradigms. This paper systematically reviews research progress and application scenarios of AIGC methodologies based on Generative Adversarial Networks (GAN), Transformer architectures, and Diffusion models in industrial defect detection. By analyzing technical frameworks encompassing high-fidelity defect generation, few-shot learning, multimodal fusion, and unsupervised anomaly detection, we reveal the unique advantages of AIGC models in enhancing detection accuracy and improving model generalization. This review aims to provide systematic references for theoretical innovation and practical implementation of AIGC technology in industrial quality inspection.},
  keywords={Reviews;Computational modeling;Computer architecture;Generative adversarial networks;Transformers;Diffusion models;Data models;Anomaly detection;Defect detection;Smart manufacturing;deep learning;AIGC models;fine-tuning techniques;defect detection},
  doi={10.1109/ICETAC65964.2025.11144040},
  ISSN={},
  month={June},}@INPROCEEDINGS{10881792,
  author={Verma, Palak and Kaur, Harkeerat},
  booktitle={2024 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)}, 
  title={MetaIClone: A Robust Iris Clone Detection Mechanism for the Metaverse}, 
  year={2024},
  volume={},
  number={},
  pages={319-324},
  abstract={Recent advancements in artificial intelligence have paved the way for the metaverse to become an imminent reality. Within this virtual world, people can buy and sell virtual assets, meet up with friends, purchase goods, and attend event, all while using biometric traits for secure and seamless authentication. Iris recognition has become a highly prevalent method for human authentication due to its robustness and effectiveness. Despite its widespread use, these systems are vulnerable to various security threats, particularly when attackers use biometric replicas to impersonate legitimate users-commonly known as biometric cloning. To counteract such cloning attempts, an anti-cloning subsystem is incorporated with the sensor, enabling intelligent differentiation between genuine and clone iris traits. In this research, we present MetaIClone, a hybrid iris clone detection (ICD) system designed for secure identification within the metaverse. MetaIClone integrates the powerful features of two handcrafted feature extractors namely histogram of oriented gradient (HoG) and binary statistical image feature (BSIF), with the extracted feature vectors subsequently classified using two distinct support vector machines (SVMs). The final decision is achieved through score-level fusion with a fine-tuned MobileNet V2 model. We evaluated MetaIClone using benchmark datasets, including iris liveness detection Notre Dame 2017 (LivDet ND17) and Notre Dame cosmetic lenses dataset 2015 (NDCLD-15). Additionally, we introduced a digital clone generation technique using a generative adversarial network (GAN) model and tested the system on this novel dataset. MetaIClone excels in known attack scenarios, achieving an average classification error rate (ACER) of $\mathbf{0. 0 2 \%}$, and performs well in unseen attack scenarios with an ACER of $\mathbf{0. 0 5 \%}$, delivering competitive results in crossdatabase evaluations. Overall, MetaIClone outperforms other ICD models, offering superior accuracy with high computational efficiency.},
  keywords={Support vector machines;Metaverse;Computational modeling;Cloning;Authentication;Benchmark testing;Feature extraction;Generative adversarial networks;Vectors;Iris recognition;metaverse;iris clone;clone detection mechanism;handcrafted feature;fine-tuned model;score-level fusion},
  doi={10.1109/MoSICom63082.2024.10881792},
  ISSN={},
  month={Dec},}@ARTICLE{9293109,
  author={Xia, Zi-Xiang and Lai, Wei-Cheng and Tsao, Li-Wu and Hsu, Lien-Feng and Hu Yu, Chih-Chia and Shuai, Hong-Han and Cheng, Wen-Huang},
  journal={IEEE Industrial Electronics Magazine}, 
  title={A Human-Like Traffic Scene Understanding System: A Survey}, 
  year={2021},
  volume={15},
  number={1},
  pages={6-15},
  abstract={Autonomous vehicles, also known as self-driving cars, have the capability to perceive the environment, locate its position, and safely drive to the destination without any human intervention. This field has made amazing improvements because of the advanced technologies and progress of the artificial intelligence (AI) field. While the existing surveys have addressed many topics, e.g., vehicle sensors, perception, and object detection, none of the existing works summarize the work studying the ability of human-like understanding, e.g., common sense reasoning. Therefore, in this article, we present a novel system flow for empowering autonomous vehicles to understand the traffic scene and summarize the state of the art research.},
  keywords={Cognition;Automobiles;Roads;Predictive models;Visualization;Three-dimensional displays;Traffic control},
  doi={10.1109/MIE.2020.2970790},
  ISSN={1941-0115},
  month={March},}@INPROCEEDINGS{10655454,
  author={Koley, Subhadeep and Bhunia, Ayan Kumar and Sekhri, Deeptanshu and Sain, Aneeshan and Chowdhury, Pinaki Nath and Xiang, Tao and Song, Yi-Zhe},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={It's All About Your Sketch: Democratising Sketch Control in Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={7204-7214},
  abstract={This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of “what you sketch is what you get”. A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.},
  keywords={Adaptation models;Adaptive systems;Navigation;Generative AI;Image retrieval;Process control;Streaming media},
  doi={10.1109/CVPR52733.2024.00688},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10121735,
  author={Zhao, Junhui and Mu, Huiqin and Zhang, Qingmiao and Zhang, Huan},
  journal={IEEE Internet of Things Journal}, 
  title={ResNet-WGAN-Based End-to-End Learning for IoV Communication With Unknown Channels}, 
  year={2023},
  volume={10},
  number={19},
  pages={17184-17192},
  abstract={An end-to-end learning framework is proposed to optimize each module jointly in the communication system. Recently, convolutional neural network (CNN) and conditional Generative Adversarial Network (cGAN) are used for end-to-end learning. However, deeper network layers will degrade the effect of CNN. cGAN suffers from unstable training and lacks generative diversity. In this article, we propose the end-to-end learning based on deep residual network (ResNet) and Wasserstein GAN (WGAN) for communication with unknown channels (ResNet-WGAN). First, ResNet is applied to solve the problem of network degradation to extract deeper data features. Second, for unknown channels, WGAN with conditional information is used to fit the channel effect to improve training stability and generative diversity. Finally, we present the simulation results of the ResNet-WGAN under additive white Gaussian noise (AWGN) channel, Rayleigh fading channel, and frequency selective channel. The results demonstrate that the ResNet-WGAN reduces the communication bit error rate (BER) and block error rate (BLER). In particular, this article applies ResNet-WGAN to the Internet of Vehicles (IoV) communication, and the results demonstrate that ResNet-WGAN is more effective.},
  keywords={Receivers;Transmitters;Training;Generative adversarial networks;Convolutional neural networks;Channel estimation;Generators;Deep residual network (ResNet);end-to-end learning;Internet of Vehicles (IoV);unknown channels;Wasserstein GAN (WGAN)},
  doi={10.1109/JIOT.2023.3274209},
  ISSN={2327-4662},
  month={Oct},}@INPROCEEDINGS{9797659,
  author={Xie, Jingyu},
  booktitle={2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Multi-Task Medical Image-to-Images Translation using Transformer for Chest X-Ray Radiography}, 
  year={2021},
  volume={},
  number={},
  pages={708-715},
  abstract={Chest X-ray is one of the main methods for screening chest diseases, which has the characteristics of low radiation dose, fast imaging and low cost. In order to better assist doctors in disease diagnosis, usually the X-ray bone suppression and organ segmentation are performed. Many research progress has been made in this field, but the accuracy of the above two tasks is still limited due to the inherent characteristics of medical images. Firstly, the shape of organs of different individuals varies greatly.So there are inevitable segmentation errors if the overall shape is not perceived. Generally, the boundary of organs is fuzzy, so it is prone to misclassification near the boundary. In addition, existing bone suppression methods still can't completely remove bone shadows. In this paper, we propose a deep learning model whose overall architecture is designed based on the pix2pix network. This model generates both bone suppression images and organ segmentation images.Aiming at the above three issues, we make some improvements. We innovatively use the Transformer structure to enhance the attention to the global context and enhance the perception of the overall shape of the organ in the feature extraction process. Also, we design a new loss function, which gives larger weight to the position error near the organ boundary in the later stage of network training. This loss function pays more attention to edge information and helps determine the position of organ boundaries. We evaluate the effectiveness of the model on the X-ray image dataset, and compare it with the latest algorithm comprehensively. We also evaluate the effectiveness of our improvements through ablation study which shows that our improvements are effective.},
  keywords={Deep learning;Training;Image segmentation;Shape;Image edge detection;Bones;Transformers;multitask deep learning;image-to-image translation;Transformer;Segmentation;Bone suppression},
  doi={10.1109/ICAICE54393.2021.00139},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10889163,
  author={Hao, Bowen and Zhou, Dongliang and Li, Xiaojie and Zhang, Xingyu and Xie, Liang and Wu, Jianlong and Yin, Erwei},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={LipGen: Viseme-Guided Lip Video Generation for Enhancing Visual Speech Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Visual speech recognition (VSR), commonly known as lip reading, has garnered significant attention due to its wide-ranging practical applications. The advent of deep learning techniques and advancements in hardware capabilities have significantly enhanced the performance of lip reading models. Despite these advancements, existing datasets predominantly feature stable video recordings with limited variability in lip movements. This limitation results in models that are highly sensitive to variations encountered in real-world scenarios. To address this issue, we propose a novel framework, LipGen, which aims to improve model robustness by leveraging speech-driven synthetic visual data, thereby mitigating the constraints of current datasets. Additionally, we introduce an auxiliary task that incorporates viseme classification alongside attention mechanisms. This approach facilitates the efficient integration of temporal information, directing the model’s focus toward the relevant segments of speech, thereby enhancing discriminative capabilities. Our method demonstrates superior performance compared to the current state-of-the-art on the lip reading in the wild (LRW) dataset and exhibits even more pronounced advantages under challenging conditions.},
  keywords={Visualization;Lips;Speech recognition;Speech enhancement;Signal processing;Data models;Robustness;Labeling;Video recording;Synthetic data;Generative model;lip reading;visual speech recognition;viseme labeling},
  doi={10.1109/ICASSP49660.2025.10889163},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10781922,
  author={Park, Ji-Ha and Lee, Seo-Hyun and Lee, Seong-Whan},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Towards EEG-based Talking-face Generation for Brain Signal-driven Dynamic Communication}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Research on decoding speech or generating images from human brain activity holds intriguing potential as neuroprosthesis for patients and innovative communication tools for general users. However, previous studies have been constrained in generating fragmented or abstract outputs, rendering them less applicable for serving as an alternative form of communication. In this paper, we propose an integrated framework that synthesizes speech from non-invasive speech-related brain signals and generates a talking-face that performs "lip-sync" using intermediate input decoded from brain signals. For realistic and dynamic brain signal-mediated communication, we generated a personalized talking-face by utilizing various forms of target data such as a real face or an avatar. Additionally, we performed a denoising process to enhance the quality of synthesized voices from brain signals, and to minimize unnecessary facial movements according to the noise. Therefore, clear and natural talking-faces, applicable to both real faces and avatars, could be generated from noisy brain signals, enabling dynamic communication. These findings serve as a pivotal contribution to the advancement of brain signal-driven face-to-face communication through the provision of integrated speech and visual interfaces. This represents a significant step towards the development of a more intuitive and dynamic brain-computer interface communication system.},
  keywords={Visualization;Communication systems;Avatars;Noise reduction;Noise;Rendering (computer graphics);People with disabilities;Electroencephalography;Noise measurement;Faces;brain–computer interface;electroencephalogram;generative adversarial network;speech synthesis;spoken speech;talking-face generation},
  doi={10.1109/EMBC53108.2024.10781922},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10097946,
  author={Wang, Dong and Xu, Tao and Zhang, Huatian and Shang, Fanhua and Liu, Hongying and Liu, Yuanyuan and Shen, Shengmei},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={PWPROP: A Progressive Weighted Adaptive Method for Training Deep Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={508-515},
  abstract={In recent years, adaptive optimization methods for deep learning have attracted considerable attention. AMSGRAD indicates that the adaptive methods may be hard to converge to optimal solutions of some convex problems due to the divergence of its adaptive learning rate as in ADAM. However, we find that AMSGRAD may generalize worse than ADAM for some deep learning tasks. We first show that AMSGRAD may not find a flat minimum. So how can we design an optimization method to find a flat minimum with low training loss? Few works focus on this important problem. We propose a novel progressive weighted adaptive optimization algorithm, called PWPROP, with fewer hyperparameters than its counterparts such as ADAM. By intuitively constructing a “sharp-flat minima” model, we show that how different second-order estimates affect the ability to escape a sharp minimum. Moreover, we also prove that PWPROP can address the non-convergence issue of ADAM and has a sublinear convergence rate for non-convex problems. Extensive experimental results show that PWPROP is effective and suitable for various deep learning architectures such as Transformer, and achieves state-of-the-art results.},
  keywords={Training;Deep learning;Adaptation models;Adaptive systems;Design methodology;Neural networks;Optimization methods;Deep Learning;Optimization;Flat Minima},
  doi={10.1109/ICTAI56018.2022.00081},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10604585,
  author={Ruan, Zanxi and Wei, Yingmei and Yuan, Yifei and Li, Yu and Guo, Yanming and Xie, Yuxiang},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Advances in Few-Shot Action Recognition: A Comprehensive Review}, 
  year={2024},
  volume={},
  number={},
  pages={390-398},
  abstract={Action recognition holds a key position in the field of video understanding. While methods based on deep learning have made significant strides in big data backgrounds, their application in few-shot scenarios for action recognition still faces numerous challenges. This review delves deeply into over 20 methods in the few-shot action recognition (FSAR) area from recent years. Initially, following the research's developmental trajectory, we systematically categorize the current mainstream methods for few-shot action recognition into three perspectives: memory networks, metric learning, and data augmentation. In order to ensure a fair comparison, we examine the performance of several approaches on multiple datasets. Lastly, we offer insights into the future development directions of this field. This review is the first review in the FSAR domain, summarizing a wealth of research outcomes, techniques, and theories. It offers researchers a cohesive perspective and illuminates potential new avenues for innovative studies.},
  keywords={Measurement;Deep learning;Reviews;Face recognition;Big Data;Data augmentation;Boosting;few-shot action recognition;few-shot learning;action recognition},
  doi={10.1109/ICAIBD62003.2024.10604585},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{10675205,
  author={Chen, Ziyi and Dai, Mingcheng and Cong, Yulai and Lu, Rongwei},
  booktitle={2024 5th International Conference on Electronic Communication and Artificial Intelligence (ICECAI)}, 
  title={Attribute-Controlled SAR Target Image Generation Driven by Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={462-468},
  abstract={Sufficient Synthetic Aperture Radar (SAR) target images are of great importance for the advancement of SAR systems. However, the necessity for professional annotation of SAR target images results in a high acquisition cost, which in turn restricts the availability of SAR images in practical applications and further impedes the development of SAR technology applications. In order to effectively expand the labeled SAR target image dataset, many scholars have devoted themselves to the research of GAN-based SAR image generation technology. However, few GAN-based SAR image generation methods can achieve controllable generation of multi-attribute combinations of SAR images, and the inherent instability during GAN training also limits the practicality and popularity of the methods. To overcome these limitations, we propose an innovative model named Attribute-controlled Diffusion Model (AcDM), which leverages the superior image generation capabilities of diffusion models to enhance both the quality and controllability of SAR image synthesis. Specifically, to achieve precise control over the azimuth and category attributes in SAR target image generation, we design a module called the Attribute Information Fusion Module (AIFM). This module facilitates an in-depth integration of SAR target images with their respective attribute information, enabling the generation of SAR target images with highly controllable attributes. The AcDM has proven to be highly effective in accomplishing SAR image generation tasks.},
  keywords={Training;Visualization;Costs;Image synthesis;Azimuth;Statistical analysis;Diffusion models;image generation;diffusion model;Transformer;attribute-controlled generation},
  doi={10.1109/ICECAI62591.2024.10675205},
  ISSN={},
  month={May},}@INPROCEEDINGS{9044246,
  author={Harada, Nami and Yamashita, Kazuya and Motomura, Yoichi and Kano, Yutaka},
  booktitle={2020 6th Conference on Data Science and Machine Learning Applications (CDMA)}, 
  title={Applying Statistical Approach to Topic Analysis for more Comprehensive and Appropriate Modeling}, 
  year={2020},
  volume={},
  number={},
  pages={13-18},
  abstract={Topic analysis is useful to construct probabilistic modeling and interpret real data. It is applied to data analysis in various fields. Topic analysis is expected to construct modeling appropriately and obtain interpretable results to assist our decision making about the field. More and more data of more and more fields give us many problems to solve with topic analysis. In this paper, we focus on treating complex data, constructing an appropriate model and interpreting data structure more meaningfully with topic analysis. To solve these problems, we suggest combinning topic analysis methods and statistical approaches. Finally, some results of experiments to study utility of our suggesting methods are given.},
  keywords={Matrix decomposition;Data models;Analytical models;Probabilistic logic;Principal component analysis;Semantics;Data structures;discriminative model, generative model, LDA, LSA, pLSA, principal component analysis, topic analysis},
  doi={10.1109/CDMA47397.2020.00008},
  ISSN={},
  month={March},}@INPROCEEDINGS{10928493,
  author={Hasan, Nowshad and Sharif, Md. Maskat and Rahman, Miskatur and Islam, Md Saiful and Khan, Md Tohedur Rahaman and Hoque, MD Jiabul and Bahar, Anamul},
  booktitle={2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)}, 
  title={Epileptic Seizure Prediction Using a Deep Hybrid CNN-GAN Model on EEG Data}, 
  year={2024},
  volume={},
  number={},
  pages={270-275},
  abstract={Epileptic seizure detection from EEG signals is essential for the diagnosis and treatment of neurological conditions. However, this task is difficult because of the complex and fluctuating characteristics of EEG data. To overcome these difficulties, this research employs a specialized CNN-GAN hybrid model designed to improve seizure prediction by generating realistic synthetic data. This approach addresses class imbalance and enhances feature learning. The process begins with preprocessing EEG datasets to remove noise and artifacts. Data augmentation follows, expanding the dataset and reducing the risk of overfitting. Segmentation then isolates important features to boost prediction accuracy. Feature extraction is achieved using a custom CNN model, and classification is performed with a GAN model. The model demonstrates high accuracy, with results of 99.95% on the BONN Dataset, 98.93% on the CHB-MIT Dataset and 98.80% on the SWEC-ETHZ Dataset. Overall, the custom CNN-GAN hybrid model significantly improves the accuracy of predicting epileptic seizures from EEG signals across various datasets.},
  keywords={Representation learning;Accuracy;Predictive models;Brain modeling;Data augmentation;Feature extraction;Electroencephalography;Data models;Robots;Synthetic data;Epileptic seizure;CNN-GAN;EEG;Data augmentation},
  doi={10.1109/RAAICON64172.2024.10928493},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10369483,
  author={Subramani, Neelakandan and Sardar, Suman Kalyan and Lee, Seul Chan},
  booktitle={2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Enhancing Human Emotion Recognition with Long Short-Term Memory (LSTM) and Adaptive Adam Optimization (AOA) of EEG Signals}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Electroencephalogram (EEG) data-based emotion recognition has acquired popularity in recent years. Because EEG signals are noisy, non-linear, and non-stationary, it is challenging to develop an intelligent framework capable of delivering high accuracy for emotion recognition. Human emotion recognition is a critical component of human-computer interaction and assessing emotional well-being. In this paper, we describe a new method for increasing the accuracy of human emotion recognition using EEG data. Adaptive Adam optimization is used to enhance the performance of Long Short-Term Memory (LSTM) neural networks by utilizing their power. Readings of the EEG, which record brain activity, provide crucial insight into emotional states. EEG data are utilized to extract intricate patterns and temporal connections using LSTM networks, which are well-known for their ability to efficiently characterize sequential data. To further optimize training and convergence, we provide the adaptive Adam Optimization Algorithm (AOA), which dynamically modifies learning rates during training to improve the model's ability to capture minute variations in EEG patterns. Experiment results from comprehensive evaluations on benchmark datasets indicate that our method is effective at improving the accuracy of identifying human emotions from EEG signals. The proposed model, LSTM-AOA, outperforms conventional methods and demonstrates exceptional proficiency in dealing with the complex temporal dependencies inherent to emotional states.},
  keywords={Training;Emotion recognition;Adaptation models;Databases;Benchmark testing;Brain modeling;Electroencephalography;Electroencephalogram;human emotion recognition;Long Short-Term Memory;Adam Optimization Algorithm;Neural Networks},
  doi={10.1109/RMKMATE59243.2023.10369483},
  ISSN={},
  month={Nov},}@ARTICLE{11006092,
  author={Wang, Pengyu and Lin, Hin Wang and Li, Jialu and Wang, Jiankun and Shi, Ling and Meng, Max Q.-H.},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={PierGuard: A Planning Framework for Underwater Robotic Inspection of Coastal Piers}, 
  year={2025},
  volume={22},
  number={},
  pages={15941-15952},
  abstract={Using underwater robots instead of humans for the inspection of coastal piers can enhance efficiency while reducing risks. A key challenge in performing these tasks lies in achieving efficient and rapid path planning within complex environments. Sampling-based path planning methods, such as Rapidly-exploring Random Tree* (RRT*), have demonstrated notable performance in high-dimensional spaces. In recent years, researchers have begun designing various geometry-inspired heuristics and neural network-driven heuristics to further enhance the effectiveness of RRT*. However, the performance of these general path planning methods still requires improvement when applied to highly cluttered underwater environments. In this paper, we propose PierGuard, which combines the strengths of bidirectional search and neural network-driven heuristic regions. We design a specialized neural network to generate high-quality heuristic regions in cluttered maps, thereby improving the performance of the path planning. Through extensive simulation and real-world ocean field experiments, we demonstrate the effectiveness and efficiency of our proposed method compared with previous research. Our method achieves approximately 2.6 times the performance of the state-of-the-art geometric-based sampling method and nearly 4.9 times that of the state-of-the-art learning-based sampling method. Our results provide valuable insights for the automation of pier inspection and the enhancement of maritime safety (https://youtu.be/A54AJ4bbX98). Note to Practitioners—The research presented in this paper focuses on enhancing the efficiency and safety of underwater inspections of coastal piers by utilizing underwater robots instead of human divers. One of the primary challenges in this context is the development of efficient path planning strategies within complex, dynamic environments. Our study investigates the use of sampling-based path planning methods, specifically RRT*, to improve the efficiency of these inspections. The key takeaway for practitioners is that integrating bidirectional search with neural heuristic regions in the path planning of underwater robots can significantly enhance inspection efficiency, reducing both the time required and the complexity of navigating hazardous environments. The method is particularly useful in environments where the terrain is difficult to model or constantly changing. By adopting these algorithms, practitioners can improve operational efficiency, enhance safety protocols, and reduce the overall cost of inspections. We recommend that practitioners looking to implement this approach invest in suitable robotic platforms capable of handling the complexities of underwater navigation. Additionally, integrating these path planning methods with real-time data acquisition systems will further enhance the effectiveness of inspections, allowing for more accurate and faster assessments of coastal infrastructure.},
  keywords={Path planning;Inspection;Planning;Robots;Autonomous underwater vehicles;Sea measurements;Safety;Neural networks;Oceans;Heuristic algorithms;Neural network;path planning;sampling-based algorithm;underwater vehicle},
  doi={10.1109/TASE.2025.3570694},
  ISSN={1558-3783},
  month={},}@INPROCEEDINGS{10849934,
  author={Yang, Ruolin and Li, Da and Zhang, Honggang and Song, Yi-Zhe},
  booktitle={2024 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
  title={SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Sketching is a uniquely human tool for expressing ideas and creativity. The animation of sketches infuses life into these static drawings, opening a new dimension for designers. Animating sketches is a time-consuming process that demands professional skills and extensive experience, often proving daunting for amateurs. In this paper, we propose a novel sketch animation model SketchAnimator, which enables adding creative motion to a given sketch, like "a jumping car". Namely, given an input sketch and a reference video, we divide the sketch animation into three stages: Appearance Learning, Motion Learning and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate sketch appearance information and motion dynamics from the reference video into the pre-trained T2V model. In the third stage, we utilize Score Distillation Sampling (SDS) to update the parameters of the Bézier curves in each sketch frame according to the acquired motion information. Consequently, our model produces a sketch video that not only retains the original appearance of the sketch but also mirrors the dynamic movements of the reference video. We compare our method with alternative approaches and demonstrate that it generates the desired sketch video under the challenge of one-shot motion customization.},
  keywords={Visual communication;Image processing;Dynamics;Animation;Diffusion models;Mirrors;Automobiles;Text to video;Creativity;Sketch animation;diffusion process;generative model;video generation;motion extraction},
  doi={10.1109/VCIP63160.2024.10849934},
  ISSN={2642-9357},
  month={Dec},}@INPROCEEDINGS{11011113,
  author={Ronchini, Francesca and Wu, Ho-Hsiang and Lin, Wei-Cheng and Antonacci, Fabio},
  booktitle={2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper investigates the design of effective prompt strategies for generating realistic datasets using Text-To-Audio (TTA) models. We also analyze different techniques for efficiently combining these datasets to enhance their utility in sound classification tasks. By evaluating two sound classification datasets with two TTA models, we apply a range of prompt strategies. Our findings reveal that task-specific prompt strategies significantly outperform basic prompt approaches in data generation. Furthermore, merging datasets generated using different TTA models proves to enhance classification performance more effectively than merely increasing the training dataset size. Overall, our results underscore the advantages of these methods as effective data augmentation techniques using synthetic data.},
  keywords={Training;Conferences;Merging;Signal processing;Data collection;Data augmentation;Data models;Acoustics;Speech processing;Synthetic data;Text-to-audio generative models;synthetic dataset;sound classification;data augmentation;prompt design},
  doi={10.1109/ICASSPW65056.2025.11011113},
  ISSN={},
  month={April},}@INPROCEEDINGS{10826716,
  author={Wang, Su and Cao, Mingwei},
  booktitle={2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={AutoFill: An Automatic Approach to Filling Panoramic Images}, 
  year={2024},
  volume={},
  number={},
  pages={335-342},
  abstract={Panoramic image filling stands as one of the foremost research topics in the field of image editing. The purpose of image stitching is to repair the blank area at the edge of the panoramic image, ensuring alignment with the original scene, and enhancing overall quality and usability. This paper proposes an efficient and high-precision automatic approach (called AutoFill) for filling panoramic images to defend the issue that missing edge information in panoramic images. The AutoFill method comprises the following key steps: first, calculate the binary image (Mask) for the input panoramic image; second, based on the binary image, calculate the area of the panoramic image that needs to be repaired; third, extract of pixel information from the neighborhood area surrounding the blank region to be repaired; and finally, use the extracted pixels to fill the blank area to produce the high-quality panoramic image. Multiple open-source data sets were used to evaluate the proposed AutoFill, and experimental results demonstrated its efficiency. Our method successfully completes the panoramic image filling task.},
  keywords={Computer vision;Image edge detection;Maintenance engineering;Filling;Image restoration;Data mining;Cultural differences;Biomedical image processing;Usability;Image stitching;panoramic images;image-filling;binary images;image stitching},
  doi={10.1109/PRAI62207.2024.10826716},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10928353,
  author={Rahman, Md Shahidur and Ahamed, Asif and Pranto, Md Nuruzzaman and Islam, Md Ariful and Al Masum, Abdullah and Al-Sakib, Abdullah and Rahman, S M Arafat and Haque, Rezaul and Rahman, Shafiur},
  booktitle={2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)}, 
  title={Effective Disease Recognition in Cucumbers: A Web-Based Application Using Transfer Learning Models}, 
  year={2024},
  volume={},
  number={},
  pages={59-64},
  abstract={Cucumber farming plays a crucial role in Bangladesh's agricultural economy, significantly contributing to vegetable production. However, diseases like Downy Mildew, Bacterial Wilt, Anthracnose, and Belly Rot threaten crop quality and lead to financial losses. Effective management of these diseases is crucial for maintaining high crop yields and ensuring economic stability. Current disease identification methods are labor-intensive, costly, and dependent on expert knowledge. To address these challenges, this study presents an automated disease categorization system using advanced transfer learning models, including MobileNet, ShuffleNet, InceptionResNet-v2, AlexNet, and Ensemble Transfer Learning (ETL). Utilizing a dataset of 8,000 well-annotated images, enhanced through preprocessing techniques like GLRM feature extraction and InfoMAX-GAN feature selection, the ETL model achieved a remarkable accuracy of 99.79% and an F1-score of 99.78%. We also developed a user-friendly web tool for real-time diagnostics, providing invaluable support to farmers and agricultural specialists. This tool enables rapid disease detection, leading to increased productivity and a transformative approach to cucumber disease management. By reducing the reliance on chemical interventions, it promotes sustainable farming practices and safeguards human and animal health.},
  keywords={Deep learning;Accuracy;Animals;Biological system modeling;Transfer learning;Feature extraction;Real-time systems;Chemicals;Diseases;Farming;cucumber disease;deep learning;ensemble learning;sustainable farming;agricultural technology},
  doi={10.1109/RAAICON64172.2024.10928353},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10941805,
  author={Jin, Zhiyang and Fu, Yusun and Huan, Yuehui and Xue, Jinliang and Wang, Chengguang},
  booktitle={2024 10th International Conference on Computer and Communications (ICCC)}, 
  title={A Model Based on LSTM and Data Augmentation for Network Traffic Prediction with Few Samples}, 
  year={2024},
  volume={},
  number={},
  pages={1438-1444},
  abstract={Utilizing artificial intelligence technologies, particularly deep learning models, for modeling and predicting time series has demonstrated significant efficacy and potential across various fields. In the domain of network traffic, models can predict future trends by leveraging features of learned traffic patterns, thereby facilitating rational allocation of network resources as well as assessing network security and preventing cyber threats. However, deep learning models often require large amounts of high-quality time series data for training, and network traffic data, being somewhat private, is generally not easily accessible. In the real world, effective modeling is often hindered by the lack of sufficient data. In this paper, we propose a method Transfer Data Augmentation(TransDA) for augmenting few samples of network traffic data based on network traffic characteristics and the concept of transfer learning. Additionally, we have combined Long Short-Term Memory(LSTM) with the newly proposed Kolmogorov-Arnold Networks(KAN), replacing the original Multi-Layer Perceptron(MLP), to better capture the nonlinear relationships between sequential data points. Experimental results indicate that our proposed model achieves better performance on few-sample network traffic datasets compared to the standard models.},
  keywords={Deep learning;Training;Computational modeling;Time series analysis;Transfer learning;Telecommunication traffic;Predictive models;Data augmentation;Data models;Long short term memory;network traffic prediction;small sample;low-resource condition;LSTM;KAN},
  doi={10.1109/ICCC62609.2024.10941805},
  ISSN={2837-7109},
  month={Dec},}@INPROCEEDINGS{10652683,
  author={Bekheet, Ahmed Ashraf and Ghoneim, Amr and Khoriba, Ghada},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={A Comprehensive Comparative Analysis of Deepfake Detection Techniques in Visual, Audio, and Audio-Visual Domains}, 
  year={2024},
  volume={},
  number={},
  pages={122-129},
  abstract={In recent years, the rise of social media platforms has made them vital channels for sharing news, where audio and visual content play a crucial role in enhancing the credibility of news content. However, significant Artificial Intelligence (AI) progress has introduced new techniques and tools for manipulating multimedia content. These advancements have made it easier to create fabricated digital media, leading to a harmful impact on sharing misinformation, especially in fake news. Consequently, an urgent need arises to explore prevailing methodologies for detecting fake images, audio, and videos, accompanied by a comprehensive exposition of their strengths and limitations. Our survey addresses these methodologies and conducts a rigorous comparative analysis of diverse approaches using various metrics and datasets. We categorize these approaches into visual-based, audio-based, and audio-visual-based deepfake detection methods, encompassing techniques employed across domains. Additionally, we examine notable datasets utilized in detecting image, video, and audio deepfakes, offering insights into their attributes and appropriateness for evaluation purposes. Our findings highlight the effectiveness and limitations of current detection methods, providing a roadmap for future research in multimodal deepfake detection. This includes exploring emerging facets of video manipulation, such as text overlays and motion patterns, investigating advanced deep learning architectures like Transformers, and emphasizing the need for extensive, diverse, and publicly accessible datasets to enhance the robustness and validation of detection methods.},
  keywords={Surveys;Measurement;Deepfakes;Visualization;Social networking (online);Neural networks;Streaming media;Audio-visual Deepfake Detection;Convolutional Neural Networks (CNNs);Recurrent Neural Networks (RNNs);Transformers;Mel Frequency Cepstral Coefficients (MFCC);Mel-spectrogram;Text-to-Speech Synthesis (TTS) Voice Conversion (VC)},
  doi={10.1109/IMSA61967.2024.10652683},
  ISSN={},
  month={July},}@INPROCEEDINGS{10246661,
  author={Li, Haitao and Wang, Jingyao and Liu, Bofan and Yin, Yaping and Chen, Zhe and Wang, Jianhua},
  booktitle={2023 4th International Conference on Mechatronics Technology and Intelligent Manufacturing (ICMTIM)}, 
  title={A Method for Generating Sonar Pulse Database Based on Improved DCGAN Network}, 
  year={2023},
  volume={},
  number={},
  pages={459-463},
  abstract={Sonar pulse recognition refers to the process of extracting features from underwater sonar pulse signals and classifying targets. The main process includes feature extraction and target classification. However, the marine environment is complex, with reverberation, multipath effects, and interference noise from other ships or marine organisms, which greatly reduces the effectiveness of traditional sonar pulse detection and recognition. Deep learning takes the artificial intelligence neural network as the backbone, and it consists of multiple processing layers to study data with different orders of magnitude. A deep learning network model can process structured and unstructured data, while avoiding automatic feature extraction by manual operation. When the amount of data reaches a certain level, deep learning methods can effectively achieve precise recognition of sonar pulses. However, due to the difficulty of underwater data collection and high experimental costs, the size of the sonar pulse recognition training set is a key issue affecting recognition. Starting from introducing the structure of deep neural networks, this paper proposes a method for generating sonar pulse recognition training sets based on DCGAN (Deep Convolutional Generative Adversarial Networks).},
  keywords={Training;Deep learning;Target recognition;Sonar;Artificial neural networks;Feature extraction;Data models;component;DCGAN;pulse sonar images;database generation},
  doi={10.1109/ICMTIM58873.2023.10246661},
  ISSN={},
  month={May},}@INPROCEEDINGS{10987486,
  author={Rodrigues, Anisha P and Shetty, Pratham S and Shet, Preethika and Cornelio, Priyal Mariam and Baliga, Samarth N and Fernandes, Roshan and Vijaya, P.},
  booktitle={2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)}, 
  title={Deep Learning in Forensic Sketch Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={45-50},
  abstract={When images of suspects are not accessible, forensic sketch analysis plays a crucial role in criminal investigations by giving investigators a visual representation of the subject. These sketches have typically been manually matched to databases or mugshots, which is a laborious and prone to human error process. This study investigates the use of cutting-edge deep learning methods to improve and automate forensic sketch recognition. In order to automate and improve sketch-to-photo recognition using public datasets, this work investigates sophisticated deep-learning techniques such as autoencoders, GANs, and CNNs. PCA and t-SNE are examples of dimensionality reduction algorithms that enhance feature representation and visualization. The outcomes indicate how well these models work to increase matching accuracy, providing law enforcement with a quicker and more dependable tool and highlighting AI’s potential to enhance criminal investigations and public safety.},
  keywords={Deep learning;Solid modeling;Visualization;PSNR;Law enforcement;Forensics;Autoencoders;Virtual reality;Data models;Signal resolution;Forensic;GAN;autoencoder;CNN;visualization},
  doi={10.1109/AIDE64228.2025.10987486},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10604071,
  author={Lu, Xijun and Lin, Chengde},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={High-Resolution Facial Portrait Generation System based on Adversarial CLIPs}, 
  year={2024},
  volume={},
  number={},
  pages={1043-1047},
  abstract={The portrait of a suspect is a crucial clue in the crime investigation process. Face simulation portrait technology is capable of creating a suspect’s portrait using eyewitnesses’ description of the suspect’s face. Traditional methods suffer from issues of inefficiency, inaccuracy, and lack of realism. To address these issues, we have developed a high-resolution face simulation portrait system utilizing adversarial CLIPs. The system is capable of synthesizing a face that accurately matches the provided description of the suspect’s facial features. Subsequently, these faces are reconstructed at a high-resolution level. The system is deployed on the web, enabling police officers to access high-resolution, authentic face portraits online.},
  keywords={Law enforcement;Faces;Facial features;Multi modal;Text-to-face synthesis;Generative Adversarial Networks;Super Resolution},
  doi={10.1109/ICCEA62105.2024.10604071},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{9721635,
  author={Nakada, Hidemoto and Asoh, Hideki},
  booktitle={2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={A Method to Generate Posed Person Image with few Context Images}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={We report a method to generate an image of an arbitrary person in an arbitrary pose, where the person’s characteristics are provided as a few sample images. The rapid development in image generation techniques using deep neural networks makes it possible to generate photo-realistic images. However, controlling the content of the generated images, such as a person’s clothing or pose, is still an open problem. The method we propose creates a pose invariant person representation using the attention mechanism and generates posed images by applying pose query on the representation. We evaluated the method using 3D rendered synthetic data and confirmed that the created person representation is pose-invariant, and we can render good quality images with the representation.},
  keywords={Deep learning;Three-dimensional displays;Image synthesis;Neural networks;Clothing;Data models;Information management;Image Generation;Posed Image;Generative Network},
  doi={10.1109/IMCOM53663.2022.9721635},
  ISSN={},
  month={Jan},}@ARTICLE{9942945,
  author={Fu, Kaicheng and Du, Changde and Wang, Shengpei and He, Huiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multi-View Multi-Label Fine-Grained Emotion Decoding From Human Brain Activity}, 
  year={2024},
  volume={35},
  number={7},
  pages={9026-9040},
  abstract={Decoding emotional states from human brain activity play an important role in the brain–computer interfaces. Existing emotion decoding methods still have two main limitations: one is only decoding a single emotion category from a brain activity pattern and the decoded emotion categories are coarse-grained, which is inconsistent with the complex emotional expression of humans; the other is ignoring the discrepancy of emotion expression between the left and right hemispheres of the human brain. In this article, we propose a novel multi-view multi-label hybrid model for fine-grained emotion decoding (up to 80 emotion categories) which can learn the expressive neural representations and predict multiple emotional states simultaneously. Specifically, the generative component of our hybrid model is parameterized by a multi-view variational autoencoder, in which we regard the brain activity of left and right hemispheres and their difference as three distinct views and use the product of expert mechanism in its inference network. The discriminative component of our hybrid model is implemented by a multi-label classification network with an asymmetric focal loss. For more accurate emotion decoding, we first adopt a label-aware module for emotion-specific neural representation learning and then model the dependency of emotional states by a masked self-attention mechanism. Extensive experiments on two visually evoked emotional datasets show the superiority of our method.},
  keywords={Decoding;Brain modeling;Functional magnetic resonance imaging;Predictive models;Emotion recognition;Dimensionality reduction;Pattern recognition;Fine-grained emotion decoding;multi-label learning;multi-view learning;product of experts (PoEs);variational autoencoder},
  doi={10.1109/TNNLS.2022.3217767},
  ISSN={2162-2388},
  month={July},}@INPROCEEDINGS{8489413,
  author={Li, Yifeng and Zhu, Xiaodan},
  booktitle={2018 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Exponential Family Restricted Boltzmann Machines and Annealed Importance Sampling}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={In this paper, we investigate restricted Boltzmann machines (RBMs) from the exponential family perspective, en-abling the visible units to follow any suitable distributions from the exponential family. We derive a unified view to compute the free energy function for exponential family RBMs (exp-RBMs). Based on that, annealed important sampling (AIS) is generalized to the entire exponential family, allowing for estimating the log-partition function and log-likelihood. Our experiments on a document processing task demonstrate that the generalized free energy functions and AIS estimation perform well in helping capture useful knowledge from the data; the estimated log-partition functions are stable. The appropriate instances of exp-RBMs can generate novel and meaningful samples and can be applied to classification tasks.},
  keywords={Adaptation models;Artificial intelligence;Annealing;Monte Carlo methods;Task analysis;Stochastic processes;Standards;deep learning;generative model;exponential family;restricted Boltzmann machine;annealed importance sampling},
  doi={10.1109/IJCNN.2018.8489413},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10988229,
  author={Ohal, Hemlata Sandip and Balote, Tanushree Shailendra and Joshi, Manasi Hemant and Suryawanshi, Harshada Chandrakant and Suryawanshi, Neel Vinodkumar},
  booktitle={2025 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={Leveraging Convolutional Neural Networks for AI Artwork Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The emergence of AI-made art is pioneering in the creative landscape, yet it challenges quality and authenticity across many image-generation platforms. This paper aims to develop a new AI art detector capable of checking the authenticity of artwork. The developed detector will be designed in such a way that it can evaluate essential factors regarding originality, coherence, and aesthetic appeal. This is achieved by using deep learning techniques through training over a wide variety of datasets, from those generated by AIs and humans within the context of artwork. This process will automatically identify markers that are unique to content generated by AIs. With this, the findings will provide insights into the strengths and limitations of existing AI art models, leading to improvements in the future. Commercial art production, content moderation, and intellectual property management are just some of the possible fields in which our tool can be applied by maintaining quality levels for AI-generated artworks.},
  keywords={Training;Deep learning;Art;Accuracy;Training data;Detectors;Production;Predictive models;Artificial intelligence;Overfitting;Deep learning;Diffusion Model;Generative AI;A.I Art;Detection},
  doi={10.1109/ESCI63694.2025.10988229},
  ISSN={2996-1815},
  month={March},}@INPROCEEDINGS{10825544,
  author={Thompson, Andrew and Sommers, Alexander and Russell-Gilbert, Alicia and Cummins, Logan and Mittal, Sudip and Rahimi, Shahram and Seale, Maria and Jaboure, Joseph and Arnold, Thomas and Church, Joshua},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Multivariate Data Augmentation for Predictive Maintenance using Diffusion}, 
  year={2024},
  volume={},
  number={},
  pages={4240-4247},
  abstract={Predictive maintenance has been used to optimize system repairs in the industrial, medical, and financial domains. This technique relies on the consistent ability to detect and predict anomalies in critical systems. AI models have been trained to detect system faults, improving predictive maintenance efficiency. Typically there is a lack of fault data to train these models, due to organizations working to keep fault occurrences and down time to a minimum. For newly installed systems, no fault data exists since they have yet to fail. By using diffusion models for synthetic data generation, the complex training datasets for these predictive models can be supplemented with high level synthetic fault data to improve their performance in anomaly detection. By learning the relationship between healthy and faulty data in similar systems, a diffusion model can attempt to apply that relationship to healthy data of a newly installed system that has no fault data. The diffusion model would then be able to generate useful fault data for the new system, and enable predictive models to be trained for predictive maintenance. The following paper demonstrates a system for generating useful, multivariate synthetic data for predictive maintenance, and how it can be applied to systems that have yet to fail.},
  keywords={Training;Organizations;Predictive models;Big Data;Diffusion models;Data augmentation;Data models;Artificial intelligence;Predictive maintenance;Synthetic data;diffusion;data augmentation;predictive maintenance;generative AI},
  doi={10.1109/BigData62323.2024.10825544},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{10237327,
  author={Tao, Shuchang and Cao, Qi and Shen, Huawei and Wu, Yunfan and Hou, Liang and Cheng, Xueqi},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Graph Adversarial Immunization for Certifiable Robustness}, 
  year={2024},
  volume={36},
  number={4},
  pages={1597-1610},
  abstract={Despite achieving great success, graph neural networks (GNNs) are vulnerable to adversarial attacks. Existing defenses focus on developing adversarial training or model modification. In this paper, we propose and formulate graph adversarial immunization, i.e., vaccinating part of graph structure to improve certifiable robustness of graph against any admissible adversarial attack. We first propose edge-level immunization to vaccinate node pairs. Unfortunately, such edge-level immunization cannot defend against emerging node injection attacks, since it only immunizes existing node pairs. To this end, we further propose node-level immunization. To avoid computationally intensive combinatorial optimization associated with adversarial immunization, we develop AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune node pairs or nodes. Extensive experiments demonstrate the superiority of AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio of robust nodes by 79$\%$%, 294$\%$%, and 100$\%$%, after immunizing only 5$\%$% of nodes. Furthermore, AdvImmune methods show excellent defensive performance against various attacks, outperforming state-of-the-art defenses. To the best of our knowledge, this is the first attempt to improve certifiable robustness from graph data perspective without losing performance on clean graphs, providing new insights into graph adversarial learning.},
  keywords={Robustness;Graph neural networks;Training;Task analysis;Bars;Optimization;Adversarial machine learning;Adversarial attack;adversarial immunization;certifiable robustness;graph neural networks;node classification},
  doi={10.1109/TKDE.2023.3311105},
  ISSN={1558-2191},
  month={April},}@ARTICLE{10909220,
  author={Zhang, Zhe and Jiang, Yili and Wei, Xin and Chen, Mingkai and Dong, Haiwei and Yu, Shui},
  journal={IEEE Network}, 
  title={Generative-AI for XR Content Transmission in the Metaverse: Potential Approaches, Challenges, and a Generation-Driven Transmission Framework}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={How to efficiently transmit large volumes of Extended Reality (XR) content through current networks has been a major bottleneck in realizing the Metaverse. The recently emerging Generative Artificial Intelligence (GAI) has already revolutionized various technological fields and provides promising solutions to this challenge. In this article, we first demonstrate current networks’ bottlenecks for supporting XR content transmission in the Metaverse. Then, we explore the potential approaches and challenges of utilizing GAI to overcome these bottlenecks. To address these challenges, we propose a GAI-based XR content transmission framework which leverages a cloud-edge collaboration architecture. The cloud servers are responsible for storing and rendering the original XR content, while edge servers utilize GAI models to generate essential parts of XR content (e.g., subsequent frames, selected objects, etc.) when network resources are insufficient to transmit them. A Deep Reinforcement Learning (DRL)-based decision module is proposed to solve the decision-making problems. Our case study demonstrates that the proposed GAI-based transmission framework achieves a 2.8-fold increase in normal frame ratio (percentage of frames that meet the quality and latency requirements for XR content transmission) over baseline approaches, underscoring the potential of GAI models to facilitate XR content transmission in the Metaverse.},
  keywords={Metaverse;Servers;Cloud computing;Haptic interfaces;Rendering (computer graphics);Data models;Training;Real-time systems;Computer architecture;Computational modeling},
  doi={10.1109/MNET.2025.3547385},
  ISSN={1558-156X},
  month={},}@ARTICLE{10229510,
  author={Nie, Lihai and Zhao, Laiping and Li, Keqiu and Shan, Xiaoyang and Qiu, Tie},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={A Game-Based Adversarial DGA Detection Scheme Using Multi-Level Incremental Random Forest}, 
  year={2024},
  volume={11},
  number={1},
  pages={779-792},
  abstract={Security vendors can take down botnets by detecting the malicious domain names crafted by attackers. However, the adversarial Domain Generation Algorithms (DGAs) greatly challenge the existing domain detection schemes, in particular, adversarial DGAs can actively compromise arbitrarily specified domain detection systems by crafting the adversarial domain names. To resist adversarial DGAs, we propose a game theory-based defending strategy, which launches adversarial DGA and trains an incremental domain detector alternately. While we find the game-based strategy cannot achieve the expected detection accuracy due to two problems: the failure of incremental training and the problem of the catastrophic forgetting. To this end, we propose a multi-level incremental random forest model, which settles the above problems by splitting the leaf nodes of the decision trees and increasing the levels of the original random forest. The experimental results on the real-life dataset demonstrate the proposed detection method significantly outperforms the competing schemes when detecting adversarial DGAs (improves the detection AUC by 42%) and presents comparable performance when defending against non-adversarial DGAs.},
  keywords={Game theory;Training;Reliability;Chatbots;Servers;Radio frequency;Incremental learning;Adversarial machine learning;Malware;Malicious domain name detection;incremental learning;adversarial domain generation algorithm},
  doi={10.1109/TNSE.2023.3308126},
  ISSN={2327-4697},
  month={Jan},}@ARTICLE{9252914,
  author={He, Yingzhe and Meng, Guozhu and Chen, Kai and Hu, Xingbo and He, Jinwen},
  journal={IEEE Transactions on Software Engineering}, 
  title={Towards Security Threats of Deep Learning Systems: A Survey}, 
  year={2022},
  volume={48},
  number={5},
  pages={1743-1770},
  abstract={Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning’s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches’ merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.},
  keywords={Deep learning;Security;Data models;Privacy;Predictive models;Training data;Deep learning;poisoning attack;adversarial attack;model extraction attack;model inversion attack},
  doi={10.1109/TSE.2020.3034721},
  ISSN={1939-3520},
  month={May},}@ARTICLE{10433491,
  author={Guo, Lujiale and Chuah, Joon Huang and Raymond, Wong Jee Keen and Gu, Xiaohui and Yao, Jie and Chang, Xiangqian},
  journal={IEEE Access}, 
  title={Unsupervised Feature-Preserving CycleGAN for Fault Diagnosis of Rolling Bearings Using Unbalanced Infrared Thermal Imaging Sample}, 
  year={2024},
  volume={12},
  number={},
  pages={28449-28461},
  abstract={The fault diagnosis of rolling bearing is of great significance in industrial safety. The method of infrared thermal image combined with neural network can diagnose the fault of rolling bearing in a non-contact manner, however its data in different scenes are often unbalanced and difficult to obtain. The generative adversarial networks can solve this problem by generating data with the required features. In this paper, an unsupervised learning framework named Feature-Preserving Cycle-Consistent Generative Adversarial Networks (FP-CycleGAN) is designed for defect detection in unbalanced rolling bearing infrared thermography sample. Since the classical Cycle-Consistent Generative Adversarial Networks (CycleGAN) often must balance the weights between generation, discrimination and consistency loss when doing the feature conversion from source domain to target domain, and the process often results in pattern collapse or feature loss. To avoid this problem, a new discriminator is designed to identify whether the generated image A and B belong to two different classes, and a new class loss are proposed. In order to better extract fault features and perform features migration, the new generator is reconstructed based on the U-Network structure, the convtraspose method of the up-sampling network is replaced by Bicubic Interpolation to effectively avoid the checkerboard effect of the generated images. The defect detection of the expanded dataset was performed using Residual Network and compared with the pre-expansion data to demonstrate the usability of the generated data and the superiority of the proposed FP-CycleGAN method for rolling bearing defect detection in small sample of infrared thermal images.},
  keywords={Generative adversarial networks;Generators;Feature extraction;Rolling bearings;Fault diagnosis;Training;Image reconstruction;Infrared imaging;Thermal analysis;Data models;Fault diagnosis;rolling bearing;infrared thermal imaging;unbalanced data;generative adversarial networks},
  doi={10.1109/ACCESS.2024.3365551},
  ISSN={2169-3536},
  month={},}@ARTICLE{9580859,
  author={Kim, Seong Tae and Küçükaslan, Umut and Navab, Nassir},
  journal={IEEE Access}, 
  title={Longitudinal Brain MR Image Modeling Using Personalized Memory for Alzheimer’s Disease}, 
  year={2021},
  volume={9},
  number={},
  pages={143212-143221},
  abstract={Longitudinal analysis of a disease is an important issue to understand its progression and design prognosis and early diagnostic tools. From the longitudinal images where data is collected from multiple time points, both the spatial structural information and the longitudinal variations are captured. The temporal dynamics are more informative than static observations of the symptoms, particularly for neurodegenerative diseases such as Alzheimer’s disease, whose progression spans over the years with early subtle changes. In this paper, we propose a new generative framework to predict the lesion progression over time. Our method first encodes images into the structural and longitudinal state vectors, where interpolation or extrapolation of feature vectors in the time axis can be performed for the manipulation of these feature vectors. These processed feature vectors can be decoded into image space to predict the image at the time point which we are interested in. During the training, we force the model to encode longitudinal changes into longitudinal state features and capture the structural information in a separate vector. Moreover, we introduce a personalized memory for the online update scheme, which adapts the model to the target subject, which helps the model preserve fine details of brain image structures in each subject. Experimental results on the public longitudinal brain magnetic resonance imaging dataset show the effectiveness of the proposed method.},
  keywords={Diseases;Brain modeling;Alzheimer's disease;Adaptation models;Training;Interpolation;Decoding;Brain MR images;deep learning;generative model;longitudinal analysis;personalized prediction;memory network},
  doi={10.1109/ACCESS.2021.3121609},
  ISSN={2169-3536},
  month={},}@ARTICLE{10741885,
  author={Xu, Yuanbo and Zhuang, Fuzhen and Wang, En and Li, Chaozhuo and Wu, Jie},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Learning Without Missing-At-Random Prior Propensity-A Generative Approach for Recommender Systems}, 
  year={2025},
  volume={37},
  number={2},
  pages={754-765},
  abstract={In recommender systems, it is frequently presumed that missing ratings adhere to a missing at random (MAR) mechanism, implying the absence of ratings is independent of their potential values. However, this assumption fails to hold in real-world scenarios, where users are inclined to rate items they either strongly favor or disfavor, introducing a missing not at random (MNAR) scenario. To tackle this issue, prior researchers have utilized explicit MAR feedbacks to infer the propensities of unobserved, implicit MNAR feedbacks. Nonetheless, acquiring explicit MAR feedbacks is resource-intensive and time-consuming and may not reflect users’ true preferences. Furthermore, most methods have only been tested on synthetic or small-scale datasets, thus their applicability and effectiveness in real-world settings without MAR feedbacks remain unclear. Along these lines, we aim to predict MNAR ratings without MAR prior propensities by exploring the consistency between MAR and MNAR feedbacks and narrowing the gap between them. From the empirical study and preliminary experiment, we hypothesize that user preferences can be treated as the common prior propensity for both MAR and MNAR generative processes. In this way, we extend this hypothesis to a more general MNAR scenario: user preferences learned from MNAR can partially substitute for the prior propensities derived from MAR feedbacks for MNAR recommendation tasks. To validate our hypothesis and approach, we develop a lightweight iterative probabilistic matrix factorization framework (lightIPMF) as a practical method of our methodology, utilizing user preferences extracted from MNAR, not MAR, to estimate MNAR feedbacks. Finally, the experimental results show that modeling user preferences can effectively improve MNAR feedback estimation without MAR feedback, and our proposed lightIPMF outperforms the state-of-the-art MNAR methods in predicting MNAR feedbacks.},
  keywords={Mars;Recommender systems;Probabilistic logic;Predictive models;Estimation;Sparse matrices;Iterative methods;Data models;Testing;Technological innovation;Generative model;missing-not-at-random;recommender systems},
  doi={10.1109/TKDE.2024.3490593},
  ISSN={1558-2191},
  month={Feb},}@ARTICLE{10167850,
  author={Xu, Yuanbo and Wang, En and Yang, Yongjian and Xiong, Hui},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={GS-RS: A Generative Approach for Alleviating Cold Start and Filter Bubbles in Recommender Systems}, 
  year={2024},
  volume={36},
  number={2},
  pages={668-681},
  abstract={Recommender Systems (RSs) typically face the cold-start problem and the filter-bubble problem when users suffer the familiar, repeated, and even predictable recommendations, making them bored and unsatisfied. The key to solving these issues is learning users’ fine-grained preferences and recommending appealing and unexplored items deviating from users’ historical items. However, existing models consider cold-start or filter bubble problems separately and ignore that they can reinforce mutually and damage the models’ performance accuracy. To this end, we devise a novel serendipity-oriented recommender system (Generative Self-constrained Serendipitous Recommender System, GS$^{2}$2-RS) that generates users’ fine-grained preferences to enhance the recommendation performance. Specifically, GS$^{2}$2-RS extracts users’ interest and satisfaction preferences and generates virtual but convincible neighbors’ preferences from themselves with a twin Conditional Generative Adversarial Nets (not from real neighbors). Then we introduce the serendipity item, which is low-interest but high-satisfaction among candidate items. We use the serendipity item to improve the diversity of recommended items, which relieves the filter-bubble problem. Along with this line, a gated mechanism is applied to their fine-grained preferences (interests, satisfactions) to obtain their serendipity items. Finally, these serendipity items are inversely injected into the original user-item rating matrix and build a relatively dense matrix as the input for backbone RS models. Note that GS$^{2}$2-RS tackles cold-start and filter-bubble problems in a unified framework without any additional side information and enriches the interpretability of recommendation models. We comprehensively validate GS$^{2}$2-RS for solving cold-start and filter bubble problems on four real-world benchmark datasets. Extensive experiments illustrate GS$^{2}$2-RS's superiority in accuracy, serendipity, and interpretability over state-of-the-art models. Also, we can plug our model into existing recommender systems as a preprocessing procedure to enhance their performance.},
  keywords={Recommender systems;Information filters;Data models;Task analysis;Logic gates;Computational modeling;Standards;Generative models;cold start;filter bubble;recommendation},
  doi={10.1109/TKDE.2023.3290140},
  ISSN={1558-2191},
  month={Feb},}@ARTICLE{11073566,
  author={Lan, Xuting and Xian, Weizhi and Zhou, Mingliang and Yan, Jielu and Wei, Xuekai and Luo, Jun and Jia, Weijia and Kwong, Sam},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={No-Reference Image Quality Assessment: Exploring Intrinsic Distortion Characteristics via Generative Noise Estimation with Mamba}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={In the field of no-reference image quality assessment (NR-IQA), the visual masking effect has long been a challenging issue. Although existing methods attempt to alleviate the interference caused by masking by generating pseudoreference images, the quality of these images is often constrained by the accuracy and reconstruction capabilities of image restoration algorithms. This can introduce additional biases, thereby affecting the reliability of the evaluation results. To address this problem, we propose a novel generative “noise” estimation framework (GNE-Vim) that eliminates the need for pseudoreference images. Instead, it deeply decouples the distortion components from degraded images and performs quality-aware modelling of these components. During the training phase, the model leverages both reference images and distortion components to guide the learning of the true distortion distribution. In the inference phase, quality prediction is conducted directly on the basis of the decoupled distortion components, making the evaluation results more aligned with human subjective perception. The experimental results demonstrate that the proposed method achieves strong performance across datasets containing various types of distortions. The source code is publicly available at the following website: https://github.com/opencodelxt/GNE-Vim.},
  keywords={Distortion;Image quality;Visualization;Image restoration;Training;Noise;Quality assessment;Phase distortion;Estimation;Accuracy;No-reference image quality assessment;generative noise estimation;vision mamba;fusion network},
  doi={10.1109/TCSVT.2025.3586106},
  ISSN={1558-2205},
  month={},}@ARTICLE{10734385,
  author={Chen, Jienan and Tu, Jun and Wang, Hao and Zheng, Jie and Liu, Hao and Bai, Shenglong and He, Xiantuo and Qian, Weikang},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={GPCB Routing: Generative Pretrained Transformers-Based Printed Circuit Board Routing Method}, 
  year={2025},
  volume={44},
  number={4},
  pages={1420-1433},
  abstract={As electronic devices become increasingly compact, designing printed circuit boards (PCBs) has become more challenging, particularly in the routing step, which is now more complex and time-consuming. In this work, we propose a method that applies generative pretrained transformers (GPTs) for PCB routing, referred to as GPCB routing. Initially, we convert the detailed routing information of the PCB into network flow-based encodings. Consequently, GPCB routing tokenizes routing patterns, effectively transforming the routing task into a form of token encoding prediction. To enhance prediction accuracy, we implement a 2-D sliding window with a local memory scheme, thereby expanding the sensing area of GPCB. Additionally, we propose a multi-information fusion scheme to identify the start and end points of multiple wires to further improve the prediction accuracy. Compared to existing routing methods, GPCB has the distinct advantage of learning routing strategies from human experts, breaking the limitations of traditional model-based routing approaches. Moreover, GPCB operates as a parallel routing method capable of predicting multiple routes simultaneously, resulting in significant enhancements in routing performance. Based on the experimental results, GPCB consistently outperforms in terms of routability, runtime, and wirelength.},
  keywords={Routing;Encoding;Printed circuits;Transformers;Training;Mathematical models;Integrated circuit modeling;Wires;Databases;Data models;Flow encoding;generative pretrained transformers (GPTs);printed circuit boards (PCBs) routing},
  doi={10.1109/TCAD.2024.3486241},
  ISSN={1937-4151},
  month={April},}@INPROCEEDINGS{11102384,
  author={Sukeerthi, Kooragayala and Kesavan, R. and Kalaiselvan, S.A.},
  booktitle={2025 International Conference on Networks and Cryptology (NETCRYPT)}, 
  title={Data Classification Using Sardine Optimized Adversarial Generative Quaternion Network with Fibonacci Q-Matrix Hyperchaotic Encryption Schemes in Cloud}, 
  year={2025},
  volume={},
  number={},
  pages={500-505},
  abstract={In the era of cloud computing sensitive information management requires data classification to function securely across all industries. A protected framework for cloud-based data analytics with privacy preservation emerges through integrating sophisticated encryption techniques with neural networks. The need to protect data grows increasingly critical because organizations must specify precise performance measures as well as classification accuracy metrics. The developing thrilling cybersecurity and cloud data disclosure world continues to inspire many trailblazing privacy protection solutions that leap forward unimpeded. The research proposes and builds a new framework called AS-FiQ-AgQN, which fuses adaptive optimization with advanced encryption and secure classification. The framework secures data confidentiality by employing Fibonacci Q-Matrix Hyperchaotic Encryption. Data classification on the encrypted data is performed using Adversarial generative Quaternion Network which is further optimized using Adaptive Sardine Optimization for better performance of the network. Performances have proven to yield astonishing results, with accuracy above 99.5 %, precision of 99.3 %, and recall above 99.7 %, which affirms the efficiency with which the system operates. Through these evaluations researchers confirm the framework's success in both precise data categorization while ensuring encryption safety at the cloud level. Data classification based on AS-FiQ-AgQN produces a robust secure system to protect privacy that enables accurate predictive cloud analytics with reliable data protection.},
  keywords={Cloud computing;Adaptation models;Accuracy;Adaptive systems;Quaternions;Computational modeling;Neural networks;Data protection;Encryption;Optimization;Cloud Computing;Data Classification;Fibonacci Q-Matrix Hyperchaotic Encryption;Adversarial Generative Quaternion Network;Adaptive Sardine Optimization},
  doi={10.1109/NETCRYPT65877.2025.11102384},
  ISSN={},
  month={May},}@ARTICLE{10268402,
  author={Farooq, Muhammad Ali and Yao, Wang and Costache, Gabriel and Corcoran, Peter},
  journal={IEEE Access}, 
  title={ChildGAN: Large Scale Synthetic Child Facial Data Using Domain Adaptation in StyleGAN}, 
  year={2023},
  volume={11},
  number={},
  pages={108775-108791},
  abstract={In this research work, we proposed a novel ChildGAN, a pair of GAN networks for generating synthetic boys and girls facial data derived from StyleGAN2. ChildGAN is built by performing smooth domain transfer using transfer learning. It provides photo-realistic, high-quality data samples. A large-scale dataset is rendered with a variety of smart facial transformations: facial expressions, age progression, eye blink effects, head pose, skin and hair color variations, and variable lighting conditions. The dataset comprises more than 300k distinct data samples. Further, the uniqueness and characteristics of the rendered facial features are validated by running different computer vision application tests which include CNN-based child gender classifier, face localization and facial landmarks detection test, identity similarity evaluation using ArcFace, and lastly running eye detection and eye aspect ratio tests. The results demonstrate that synthetic child facial data of high quality offers an alternative to the cost and complexity of collecting a large-scale dataset from real children. The complete dataset along with the trained model are open-sourced on our GitHub website and GitHub page: https://github.com/MAli-Farooq/ChildGAN.},
  keywords={Generative adversarial networks;Synthetic data;Data models;Adaptation models;Training;Aerospace electronics;Transfer learning;Dataset;GANs;GDPR;transformations;transfer learning},
  doi={10.1109/ACCESS.2023.3321149},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10378343,
  author={Liu, Xueyi and Wang, Bin and Wang, He and Yi, Li},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation}, 
  year={2023},
  volume={},
  number={},
  pages={854-864},
  abstract={We study the problem of few-shot physically-aware articulated mesh generation. By observing an articulated object dataset containing only a few examples, we wish to learn a model that can generate diverse meshes with high visual fidelity and physical validity. Previous mesh generative models either have difficulties in depicting a diverse data space from only a few examples or fail to ensure physical validity of their samples. Regarding the above challenges, we propose two key innovations, including 1) a hierarchical mesh deformation-based generative model based upon the divide-and-conquer philosophy to alleviate the few-shot challenge by borrowing transferrable deformation patterns from large scale rigid meshes and 2) a physics-aware deformation correction scheme to encourage physically plausible generations. We conduct extensive experiments on 6 articulated categories to demonstrate the superiority of our method in generating articulated meshes with better diversity, higher visual fidelity, and better physical validity over previous methods in the few-shot setting. Further, we validate solid contributions of our two innovations in the ablation study. Project page with code is available at meowuu7.github.io/few-arti-obj-gen.},
  keywords={Deformable models;Visualization;Technological innovation;Computer vision;Philosophical considerations;Codes;Deformation},
  doi={10.1109/ICCV51070.2023.00085},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{10823149,
  author={S, Divya K. and T, Manesh and Prasad, Abhinand K and Venu, Akhila and Devaraj, Akshara and Paul, Albert Mathew},
  booktitle={2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)}, 
  title={Artificial Intelligence for Retina Disease Detection: A Comprehensive Survey}, 
  year={2024},
  volume={},
  number={},
  pages={1310-1316},
  abstract={Efficient and accurate detection of retinal diseases is critical for preventing vision loss and improving patient outcomes. This survey provides an in-depth analysis of state-of-the-art deep-learning techniques for diagnosing retinal diseases using fundus images. It examines various Convolutional Neural Network (CNN) architectures, transfer learning strategies, and attention mechanisms optimized for retinal image analysis. The survey also discusses the integration of Generative Adversarial Networks (GANs), multimodal data fusion, and hybrid models combining traditional and deep learning approaches. Additionally, it emphasizes the role of image preprocessing, dataset balancing, and hyperparameter optimization in enhancing model performance.},
  keywords={Deep learning;Surveys;Glaucoma;Accuracy;Transfer learning;Neural networks;Retina;Internet of Things;Convolutional neural networks;Diseases;AI AI-based retina disease detection;deep learning;Convolutional Neural Networks (CNNs);data augmentation;teleophthalmology;Fundus Images},
  doi={10.1109/ICICNIS64247.2024.10823149},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11085631,
  author={Bharathi, P. Shyamala and Tejaswi, R},
  booktitle={2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)}, 
  title={DL-SecureNet: A Transformer-based Cybersecurity Framework for Threat Detection in 5G and Beyond}, 
  year={2025},
  volume={},
  number={},
  pages={1276-1282},
  abstract={The development of the 5G networks has given a new direction for the faster communication but it has brought new security threats related to its efficacy. In the presented work, DL-SecureNet, a deep learning transformer-based approach is introduced that aim at identifying cybersecurity threats in 5G networks. Based on the CIC-DDoS dataset, the model was developed and tested with different 5G attributes such as packet size, packet delay, protocol, and transfer rate. However, DL-SecureNet uses positional encoding and multi-head attention to model temporal and contextual relationships of packets. Evaluation of the model produced satisfactory results and granted an overall accuracy of 98.4% with precision of 98.5%, recall 98.3%, and F1 score of 98.4% concerning numerous types of attacks such as DDoS, Botnet, SQL Injection, and Malware Communication. The comparison analysis showed that DL-SecureNet was superior to those standard algorithms like CNN, RNN, LSTM, and XGBoost. Also, it is augmented with threat intelligence mapping in real-time using CVE and MITRE ATT&CK frameworks. They demonstrate that DL-SecureNet can be used to implement a proactive network security model for 5G.},
  keywords={Deep learning;Protocols;5G mobile communication;Telecommunication traffic;SQL injection;Transformers;Threat assessment;Real-time systems;Encoding;Standards;5G cybersecurity;Transformer model;DL-SecureNet;threat detection;deep learning;multi-head attention;CIC-DDoS dataset;intrusion detection;network traffic classification;positional encoding;real-time threat intelligence},
  doi={10.1109/ICICV64824.2025.11085631},
  ISSN={},
  month={June},}@ARTICLE{11030589,
  author={Kim, Sojeong and Sohn, Bong-Soo and Lee, Jaesung},
  journal={IEEE Access}, 
  title={End-to-End Design of Webtoon-Style Portrait Stylization System for Real-World Demo Booth}, 
  year={2025},
  volume={13},
  number={},
  pages={102180-102193},
  abstract={Webtoon-style portrait stylization has gained prominence across various fields, including social media, virtual characters, and the Metaverse, with significant applications in demo booths for real-time audience interaction. However, achieving high-quality, real-time stylizations presents challenges, such as preserving facial identity while maintaining the webtoon aesthetics and ensuring robustness to diverse inputs. This study explores diffusion-based generative models for webtoon-style portrait stylization, focusing on real-world scenarios such as demo booths. We address challenges such as artifact reduction and adaptability to varying inputs, showcasing the potential of diffusion models to enhance interactive and artistic applications, particularly in demo booth settings.},
  keywords={Real-time systems;Visualization;Image restoration;Diffusion models;Electronic mail;Adaptation models;Semantics;Robustness;Image synthesis;Image edge detection;Style transfer;image generation;image-to-image translation},
  doi={10.1109/ACCESS.2025.3578868},
  ISSN={2169-3536},
  month={},}@ARTICLE{10817602,
  author={Daffa Izzuddin Wahid, Rahmatulloh and Yudistira, Novanto and Dewi, Candra and Nurmala Sari, Irawati and Pradhikta, Dyanningrum and Fatmawati},
  journal={IEEE Access}, 
  title={Prompt Conditioned Batik Pattern Generation Using LoRA Weighted Diffusion Model With Classifier-Free Guidance}, 
  year={2025},
  volume={13},
  number={},
  pages={2436-2448},
  abstract={Batik, a significant element of Indonesian cultural heritage, is renowned for its intricate patterns and profound philosophical meanings. While preserving traditional batik is crucial, the creation of modern patterns is equally encouraged to keep the art form vibrant and evolving. Current research primarily focuses on batik classification, leaving a gap in the exploration of generative models for batik pattern creation. This paper investigates the application of text-to-image (T2I) generative models to synthesize batik motifs, leveraging latent diffusion models (LDM), Low-Rank Adaptation (LoRA), and classifier-free guidance. Our methodology employed a dataset of 20,000 batik images. Multimodal models such as LLaVA and BLIP were utilized to generate detailed captions for these images. A pretrained LDM was subsequently fine-tuned on its denoising U-Net part, either by naively fine-tuned the entire layer or by employing using LoRA. The fine-tuning process was critical in enhancing the model’s capability to generate high-quality and user-specific batik patterns. The results demonstrated that the LDM fine-tuned on the entire denoising U-Net with LLaVA-captioned images outperformed other models, achieving the lowest Fréchet Inception Distance (FID) and highest Inception Score (IS). The thoroughness of LLaVA captions proved superior to those generated by BLIP, emphasizing the significance of detailed image descriptions in generative tasks. Notably, the model not only replicated existing batik patterns but also innovatively combined multiple motifs and even able to create entirely new designs, as verified by batik expert. This research contributes to the field of computer-assisted batik pattern generation, providing significant advantages for batik artists, manufacturers, and users by accelerating the pattern creation process and expanding the possibilities of batik art.},
  keywords={Diffusion models;Noise reduction;Training;Computational modeling;Decoding;Cultural differences;Adaptation models;Image synthesis;Image color analysis;Text to image;Batik;diffusion model;image generation;caption generation},
  doi={10.1109/ACCESS.2024.3523494},
  ISSN={2169-3536},
  month={},}@ARTICLE{10872904,
  author={Zhou, Wei and Zhang, Donglai and Wang, Hongjie and Li, Jinliang and Jiang, Mingjian},
  journal={IEEE Access}, 
  title={A Meta-Reinforcement Learning-Based Poisoning Attack Framework Against Federated Learning}, 
  year={2025},
  volume={13},
  number={},
  pages={28628-28644},
  abstract={As a distributed machine learning paradigm, federated learning enables clients to collaboratively train a global model without sharing their raw data, thus preserving data privacy while still utilizing the data. However, the distributed nature of federated learning makes it vulnerable to poisoning attacks, which undermine the integrity and availability of the model by injecting carefully crafted perturbations into the data or model. Most existing poisoning attacks rely on heuristic approaches, which are significantly mitigated by robust aggregation strategies during long-term federated learning training. To overcome this limitation, this work proposes a novel poisoning attack framework based on meta-reinforcement learning. The global data distribution of the clients is first inferred from the global gradient using a conditional generative adversarial network. The inferred distribution is then used to simulate the federated learning environment locally for reinforcement learning training. A novel scaling and noise injection attack is introduced by designing unique scaling coefficients and noise values for the gradient of each layer’s parameters using reinforcement learning. Furthermore, meta-reinforcement learning is leveraged to enhance the generalization capability of the attack, ensuring effectiveness across various robust aggregation strategies. Experimental results demonstrate that our approach significantly reduces model accuracy to around 10% across three datasets under various aggregation strategies, outperforming existing methods and exhibiting superior generalization ability and attack performance.},
  keywords={Data models;Servers;Training;Threat modeling;Computational modeling;Predictive models;Federated learning;Training data;Speech recognition;Privacy;Federated learning;poisoning attack;meta-reinforcement learning;generative adversarial network;robust aggregation strategy},
  doi={10.1109/ACCESS.2025.3538891},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10534665,
  author={Meng, S. and Dollahite, R. and Kaur, A. and Schell, P. and Sharma, A. and Ventre, G. and Kranz, I. and Nudelman, G. and Gerling, G. J.},
  booktitle={2024 Systems and Information Engineering Design Symposium (SIEDS)}, 
  title={Developing Design Features to Facilitate AI-Assisted User Interactions}, 
  year={2024},
  volume={},
  number={},
  pages={437-442},
  abstract={Interactive software tools employing generative artificial intelligence (AI) that help users formulate custom system queries are increasingly needed with growth in data quantities, relationships, and complexity. The need to afford such interactions is not new. Indeed, chatbots have long sought to bridge gaps between an individual’s intent and the system’s response. However, generative AI chatbots – in contrast to traditional chatbots that navigate pre-defined, rules-based decision trees – are unique in their promise to accept and respond to highly customized queries. At present though, most still rely upon the precise articulation of a structured prompt. The work herein develops and evaluates design features to facilitate AI-assistive user interactions in query formulation. The design features attempt to balance functional needs of users to make specific, goal-oriented, customized queries, with minimal constraints on exactly articulating pre-defined prompts. In a case study, we wireframe user interface prototypes in the domain of data log management, for evaluation with expert and novice users. Key elements of the design features revolve around the 1) refinement of search categories, 2) context-aware prompt recommendations, and 3) customization of query input per a user’s technical ability.},
  keywords={Bridges;Generative AI;Navigation;Prototypes;User interfaces;Chatbots;User experience;User Experience Design;Interaction Design;Prompt Engineering;AI-Assistive Technologies},
  doi={10.1109/SIEDS61124.2024.10534665},
  ISSN={2994-3531},
  month={May},}@INPROCEEDINGS{10568277,
  author={Chávez, Mauricio Y. and G, Fernanda Ugalde and Aguilar, Mario and Jiménez, Samantha and Mejia-Medina, David A. and Juárez-Ramírez, Reyes},
  booktitle={2023 11th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Exploratory Study in Student's Perception in the Use of ChatGPT}, 
  year={2023},
  volume={},
  number={},
  pages={124-131},
  abstract={This article analyzes the use and perception of ChatGPT, a language model by OpenAI. The article highlights that artificial intelligence is based on a “Generative Pre-Trained Transformer” architecture, which utilizes neuronal networks to learn patterns in textual data. The authors stated that technology can have an impact on education because humans cannot distinguish between essays written by humans and those generated by ChatGPT. However, the negative impact on student negative impact on student performance, with respect to the responsible use of the tool. Regarding the methodology, a survey was conducted with 1100 students from Mexico to analyze their perception and use of ChatG PT. In conclusion, this article highlights the potential of ChatGPT in the educational field, emphasizing the importance of training students in the responsible use of the tool. Furthermore, it emphasizes fostering critical and ethical thinking to make decisions when leveraging generative AI models. The article presents the survey's findings and concludes with a discussion of their implications for education.},
  keywords={Surveys;Training;Technological innovation;Ethics;Data analysis;Statistical analysis;Generative AI;Perception;Students;ChatGPT;AI},
  doi={10.1109/CONISOFT58849.2023.00025},
  ISSN={},
  month={Nov},}@ARTICLE{11016674,
  author={Ahmed Fime, Awal and Mahmud, Saifuddin and Das, Arpita and Islam, Md. Sunzidul and Kim, Jong-Hoon},
  journal={IEEE Access}, 
  title={Automatic Scene Generation: State-of-the-Art Techniques, Models, Datasets, Challenges, and Future Prospects}, 
  year={2025},
  volume={13},
  number={},
  pages={95753-95796},
  abstract={Automatic scene generation is an essential area of research with applications in robotics, recreation, visual representation, training and simulation, education, and more. This survey provides a comprehensive review of the current state-of-the-arts in automatic scene generation, focusing on techniques that leverage machine learning, deep learning, embedded systems, and natural language processing (NLP). We categorize the models into four main types: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Transformers, and Diffusion Models. Each category is explored in detail, discussing various sub-models and their contributions to the field. We also review the most commonly used datasets, such as COCO-Stuff, Visual Genome, and MS-COCO, which are critical for training and evaluating these models. Methodologies for scene generation are examined, including image-to-3D conversion, text-to-3D generation, graph-based methods, and interactive scene generation. Evaluation metrics such as Fréchet Inception Distance (FID), Kullback-Leibler (KL) Divergence, Inception Score (IS), Intersection over Union (IoU), and Mean Average Precision (mAP) are discussed in the context of their use in assessing model performance. The survey identifies key challenges and limitations in the field, such as maintaining realism, handling complex scenes with multiple objects, and ensuring consistency in object relationships and spatial arrangements. By summarizing recent advances and pinpointing areas for improvement, this survey aims to provide a valuable resource for researchers and practitioners working on automatic scene generation.},
  keywords={Autoencoders;Image reconstruction;Transformers;Training;Three-dimensional displays;Diffusion models;Visualization;Surveys;Image synthesis;Decoding;Automatic scene generation;variational autoencoders;generative adversarial networks;transformers;diffusion models;machine learning;deep learning;NLP;3D scene generation;2D image generation;evaluation metrics;COCO-stuff;visual genome;MS-COCO},
  doi={10.1109/ACCESS.2025.3574298},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10521330,
  author={Zhang, Guoxing and Li, Qiuping},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={MarsDrone: A Next-Generation Simulation System for Mars Unmanned Aerial Vehicles}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={In the burgeoning era of interplanetary exploration, the deployment of Unmanned Aerial Vehicles (UAVs) on Mars signifies a transformative approach to extraterrestrial research. To ensure the efficacy of these UAVs in the Martian milieu, there’s an imperative need for sophisticated simulation tools that can adeptly mirror the multifaceted challenges of the Red Planet. This paper presents MarsDrone, a pioneering UAV simulation tool meticulously crafted for Mars. Focusing on simulation realism, MarsDrone combines Generative Model Artificial Intelligence with Discriminative Model Artificial Intelligence to form a closed-loop system, as illustrated in Figure 1, further harnessing Martian data to optimize simulation tools.Central to MarsDrone is its multifaceted simulation framework. While the Terrain Generation Tool stands out, processing genuine Martian data to replicate the planet’s varied topography, the tool’s capabilities extend far beyond. Utilizing the avantgarde Unreal Engine 5, MarsDrone integrates advanced features like Physically-Based Rendering, Nanite, and Lumen. These not only ensure a high-fidelity representation of the Martian environment but also enhance other simulation aspects, from drone dynamics to sensor accuracy. MarsDrone’s sophisticated camera simulations offer a granular recreation of the Martian landscape, facilitating AI training in a controlled yet hyperrealistic milieu. Additionally, MarsDrone’s prowess extends to edge scenario testing, emphasizing the challenges posed by Martian dust. This fine particulate matter, coupled with Mars’s unique atmospheric conditions, can jeopardize the visual clarity essential for UAV operations. MarsDrone’s simulations, replicating these real-world Martian scenarios, ensuring their robust performance amidst Martian adversities.MarsDrone sets a new gold standard in Mars UAV simulations, offering an all-encompassing suite tailored to the genuine needs of Martian UAV exploration. Its commitment to high-fidelity simulations, anchored in authentic Martian data, pioneers a path for UAVs bracing for the rigors of interplanetary missions. As we embark on this audacious journey of space exploration, MarsDrone stands as a testament to human ingenuity, promising to play a pivotal role in safeguarding and advancing our extraterrestrial endeavors. The horizon beckons further refinement of such tools, integrating nascent technologies and galvanizing international collaboration, all in pursuit of unraveling the enigmas of Mars.},
  keywords={Training;Mars;Visualization;Surfaces;Autonomous aerial vehicles;Rendering (computer graphics);Data models},
  doi={10.1109/AERO58975.2024.10521330},
  ISSN={1095-323X},
  month={March},}@ARTICLE{10947303,
  author={Wijesinghe, Achintha and Zhang, Songyang and Wanninayaka, Suchinthaka and Wang, Weiwei and Ding, Zhi},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Diff-GO+: An Efficient Diffusion Goal-Oriented Communication System With Local Feedback}, 
  year={2025},
  volume={24},
  number={8},
  pages={6535-6550},
  abstract={Goal-oriented communication (GO-COM) has recently emerged as an important concept in modern communications, owing partly to the insatiable demand for high bandwidth efficiency in edge networks and Internet-of-Things (IoT) systems. Unlike traditional communication systems that focus on packet transport and accuracy, GO-COM aims to convey information critical to the receiver’s goals. To leverage the strength of emerging generative artificial intelligence (AI) models within GO-COM, this work presents an ultra-efficient GO-COM design built upon the backbone of the diffusion models. This Diff-GO+ model features high spectrum efficiency and flexible feedback control. Specifically, we embed the key information within semantic conditions and incorporate dictionary learning to derive a noise codebook for forward diffusion at the transmitter, with which a corresponding receiver model regenerates messages via denoising. Our proposed compression-friendly semantic conditions and low-dimensional codewords achieve significant reduction in communication overhead and satisfactory message recovery. To control recovery quality, we introduce a “local generative feedback” (LGF) that enables the transmitter to anticipate recovery quality and ensure goal accomplishment at the receiver end. Our experimental results demonstrate that the proposed Diff-GO+ can achieve a better computation-bandwidth tradeoff with ultra-high spectrum efficiency and superior data recovery. Specifically, our Diff-GO+ can achieve 98% compression for image transmission of the Cityscape dataset.},
  keywords={Diffusion models;Semantic communication;Noise;Receivers;Spectral efficiency;Generative AI;Deep learning;Communication systems;Media;Robustness;Goal-oriented communications;semantic communications;diffusion model;bandwidth efficiency},
  doi={10.1109/TWC.2025.3554442},
  ISSN={1558-2248},
  month={Aug},}@INPROCEEDINGS{11078357,
  author={Allam, Abdulrahman and Ahmed, Seif and Hamdi, Ali and Mohammed, Ammar},
  booktitle={2025 4th International Conference on Computer Technologies (ICCTech)}, 
  title={Arabic Large Language Models for Medical Text Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Efficient hospital management systems (HMS) are critical worldwide to address challenges such as overcrowding, limited resources, and poor availability of urgent health care. Existing methods often lack the ability to provide accurate, real-time medical advice, particularly for irregular inputs and underrepresented languages. To overcome these limitations, this study proposes an approach that fine-tunes large language models (LLMs) for Arabic medical text generation. The system is designed to assist patients by providing accurate medical advice, diagnoses, drug recommendations, and treatment plans based on user input. The research methodology required the collection of a unique dataset from social media platforms, capturing real-world medical conversations between patients and doctors. The dataset, which includes patient complaints together with medical advice, was properly cleaned and preprocessed to account for multiple Arabic dialects. Fine-tuning state-of-the-art generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2 Medium, optimized the system’s ability to generate reliable medical text. Results from evaluations indicate that the fine-tuned Mistral-7B model outperformed the other models, achieving average BERT (Bidirectional Encoder Representations from Transformers) Score values in precision, recall, and F1-scores of 68.5%, 69.08%, and 68.5%, respectively. Comparative benchmarking and qualitative assessments validate the system’s ability to produce coherent and relevant medical replies to informal input. This study highlights the potential of generative artificial intelligence (AI) in advancing HMS, offering a scalable and adaptable solution for global healthcare challenges, especially in linguistically and culturally diverse environments.},
  keywords={Training;Adaptation models;Accuracy;Social networking (online);Generative AI;Large language models;Medical services;Transformers;Reliability;Medical diagnostic imaging;nlp;llm;roa;hms;chat-bot;arabic;medical},
  doi={10.1109/ICCTech66294.2025.00010},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10823004,
  author={Leong, Hui Yi and Gao, Yifan and Ji, Shuai},
  booktitle={2024 6th International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={A GEN AI Framework for Medical Note Generation}, 
  year={2024},
  volume={},
  number={},
  pages={423-429},
  abstract={The growing administrative demands of medical documentation, particularly through Electronic Health Records (EHR), have substantially reduced the time available for direct patient care and exacerbated physician burnout. To mitigate this challenge, we introduce MediNotes, an advanced generative AI framework designed to automate the creation of SOAP (Subjective, Objective, Assessment, Plan) notes from medical conversations. MediNotes leverages Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech Recognition (ASR) to process both textual and voice inputs in real time or from recorded audio, generating structured, contextually accurate medical notes. The framework employs cutting-edge techniques such as Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning (PEFT) to optimize model performance in resource-limited settings. Furthermore, MediNotes features a query-based retrieval system, enabling healthcare providers and patients to efficiently access relevant medical information. Evaluation on the ACI-BENCH dataset demonstrates that MediNotes enhances the accuracy, efficiency, and usability of automated medical documentation, presenting a robust solution to alleviate administrative burdens on healthcare professionals while improving the quality of clinical workflows.},
  keywords={Training;Adaptation models;Accuracy;Large language models;Retrieval augmented generation;Medical services;Documentation;Real-time systems;Usability;Automatic speech recognition;LLM;NLP;GenAI;Ambient Listening;Fine-tuning;Medical Report;Retrieval-Augmented Generation},
  doi={10.1109/ICAICA63239.2024.10823004},
  ISSN={2833-8413},
  month={Nov},}@INPROCEEDINGS{10887712,
  author={Méreur, Arthur and Mallet, Antoine and Cogranne, Rémi and Kuribayashi, Minoru},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Forensics Analysis of Residual Noise Texture in digital Images for Detection of Deepfake}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper proposes an original approach for the automatic detection of AI-generated images, using features derived from noise residuals artefacts. Contrary to most current research that leverages sophisticated deep learning models to further improve performance, this study highlights the distinct noise residual characteristics in deepfakes, facilitating the identification of AI-generative images. Our findings highlight some limitations of image models, which can be used for forensic analysis and for future AI-based text-to-image generative models. Broad numerical results on a large and diverse dataset show the interest of the identified features as well as the relevance of the present method.},
  keywords={Deep learning;Deepfakes;Analytical models;Forensics;Digital images;Noise;Text to image;Feature extraction;Numerical models;Speech processing;DeepFakes;Noise residual;Explainable method;Machine learning;Statistical detection},
  doi={10.1109/ICASSP49660.2025.10887712},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{11166399,
  author={Pallewela, Chamudika and Senanayake, Anutthara and Wijayagunawardene, Valuka},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={MediAI: Intelligent AI System for Medicine Identification, Symptom Analysis Chatbot, and QR Code Tracking for Central Province, Sri Lanka}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Medication errors and poor compliance remain major global healthcare challenges, particularly in Sri Lanka, where nearly 40% of patients struggle to identify their medications. These issues contribute to hospital readmissions, financial burdens, and preventable complications. To address this, we present MediAI, an AI-powered medication management system designed to improve adherence and reduce errors. MediAI combines three key components: a deep learning-based medicine identification model using EfficientNet, a generative AI chatbot for symptom analysis, and a QR code-based prescription tracking system. The model was trained on a custom dataset of over 2,700 high-resolution drug images and achieved a classification accuracy of 96.87%. A mixed-methods evaluation—including surveys, interviews, and pilot testing in hospitals and pharmacies—revealed significant improvements in prescription verification, patient compliance, and medication tracking. While MediAI has demonstrated strong potential, current limitations include dependency on internet access and limited handling of complex medical queries. Future development will focus on expanding the image dataset, introducing multi-language support, and enabling offline access to better serve rural and underserved communities. MediAI offers a scalable, impactful solution that bridges traditional and intelligent medication management, aiming to enhance patient safety and healthcare efficiency in low-resource settings.},
  keywords={Drugs;Bridges;Analytical models;Accuracy;Hospitals;QR codes;Chatbots;Safety;Biomedical imaging;Testing;medicine identification;symptom analysis;AI chatbot;QR code tracking;healthcare technology},
  doi={10.1109/ACDSA65407.2025.11166399},
  ISSN={},
  month={Aug},}@ARTICLE{11045331,
  author={Zhang, Jianfei and Li, Bei and Chen, Zhuofan and Liu, Chang and Li, Chen and Lin, Chenghua and Rong, Wenge},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Latent Variable Modelling for Controllable and Diverse Generation from Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Conditional Variational Auto-Encoders (CVAEs) represent a powerful deep generative framework, utilizing latent variables (explicitly modelled hidden states) to capture underlying factors and govern the generation process accordingly. However, this idea is less explored in the era of Large Language Models (LLMs), facing challenges in structural differences between LLMs and traditional CVAEs as well as challenges in posterior collapse (homogeneous latent variables). In this work, we present the first attempt to extend decoder-only LLMs into encoder-decoder CVAEs, aimming at enhancing existing LLMs with flexible control via low-dimensional latent vectors. To achieve this, we introduce a novel optimization objective for effective latent variable modeling and propose a Gradient-only Skip (G-Skip) Connection, which jointly enhances generation controllability while preserving generation quality. Through experiments on AGNews, Yelp and DailyDialog, we validate the effectiveness of our method in achieving latent modelling and latent-guided language generation on the basis of Llama3-8B. Specifically, we establish new state-of-the-art performance in dialogue generation on the DailyDialog dataset, achieving a BERTScore of 88.30 and an FED score of 5.49.},
  keywords={Decoding;Transformers;Training;Large language models;Optimization;Feature extraction;Vectors;Lower bound;Electronic mail;Controllability;Conditional Variational Auto-Encoders;Large Language Models;Controllable Language Generation},
  doi={10.1109/TAI.2025.3581507},
  ISSN={2691-4581},
  month={},}@ARTICLE{8926460,
  author={Lin, Guangfeng and Chen, Wanjun and Liao, Kaiyang and Kang, Xiaobing and Fan, Caixia},
  journal={IEEE Access}, 
  title={Transfer Feature Generating Networks With Semantic Classes Structure for Zero-Shot Learning}, 
  year={2019},
  volume={7},
  number={},
  pages={176470-176483},
  abstract={Feature generating networks face a very important issue, which is the fitting difference (inconsistency) of the distribution between the generated feature and the real data. This inconsistency further influences the performance of the network model because training samples from seen classes are disjointed with testing samples from unseen classes in zero-shot learning (ZSL). In generalized zero-shot learning (GZSL), testing samples are from not only seen classes but also unseen classes to be closer to the practical situation. Therefore, most feature generating networks have difficulty achieving satisfactory performance for challenging GZSLs by adversarial learning the distribution of semantic classes. To alleviate the negative influence of this inconsistency for ZSL and GZSL, transfer feature generating networks with semantic classes structure (TFGNSCS) are proposed for constructing a network model to improve the performance of ZSL and GZSL. TFGNSCS not only can consider the semantic structure relationship between seen and unseen classes, but also can learn the difference of generating features by transferring classification model information from seen to unseen classes in networks. The proposed method can integrate the transfer loss, the classification loss and the Wasserstein distance loss to generate enough CNN features, on which softmax classifiers are trained for ZSL and GZSL. Experiments demonstrate that TFGNSCS outperforms state-of-the-art models on four challenging datasets: CUB, FLO, SUN, and AwA in GZSL.},
  keywords={Semantics;Generative adversarial networks;Gallium nitride;Training;Data models;Propagation losses;Visualization;Feature generating networks;semantic classes structure;transfer loss;zero-shot learning;generalized zero-shot learning},
  doi={10.1109/ACCESS.2019.2958052},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10644635,
  author={Le Hoang, Nguyen and Matsui, Yuta and Hagiwara, Yoshinobu and Taniguchi, Akira and Taniguchi, Tadahiro},
  booktitle={2024 IEEE International Conference on Development and Learning (ICDL)}, 
  title={Compositionality and Generalization in Emergent Communication Using Metropolis-Hastings Naming Game}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This study investigates the emergence of compositionality and generalization within Emergent Communication (EmCom) systems, focusing on emergent language using the Metropolis-Hastings naming game (MHNG). Although the MHNG has been used in previous EmCom research, this is the first study to explore compositionality, the ability to construct complex expressions by combining simpler elements, and generalization, the ability to apply learned patterns to new situations, in emergent language within this language game. We introduce the novel Inter-VAE+VAE model, which equips each agent with dual variational autoencoders (VAEs), specifically designed for cognitive tasks that mirror human perception and language processing. This model, facilitating predictive coding, allows agents to refine their world models through collective experiences, a process rooted in the collective predictive coding hypothesis and differing from isolated learning approaches. Our model was evaluated against baseline models, including the $\beta- \text{VAE}$ and $\beta-\text{TCVAE}$, and was further compared with implementations of the Lewis signaling game using the dSprites and 3Dshapes datasets. The results from these evaluations indicate an improvement in agent communication within the MH naming game and underscore the model's potential in replicating key aspects of human language, particularly compositionality and generalization, in artificial systems.},
  keywords={Measurement;Adaptation models;Symbols;Games;Predictive models;Predictive coding;Linguistics;emergent communication;symbol emergence;metropolis-hastings;naming game;compositionality;generalization;variational autoencoder},
  doi={10.1109/ICDL61372.2024.10644635},
  ISSN={},
  month={May},}@INPROCEEDINGS{10726125,
  author={Kumar, Amidela Anil and Dheepthi Priyangha, S J and Meghana, P and Dheeraj, Muppalla and Aarthi, R},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={XAI – Empowered Ensemble Deep Learning for Deepfake Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Recent technologies have evolved rapidly, allowing the creation of fake videos/images that are realistic. These artificial media pieces have provoked grave worries about the possibilities of their misuse in different areas like politics, journalism, and communication. Consequently, the demand for deepfake detection methods is increasing. In response to this growing threat, there is an increasing demand for robust deepfake detection methods. This paper proposes an ensemble-based approach for deepfake detection, leveraging deep learning techniques. Specifically, we employ a combination of CNN models, including VGG, Xception, RegularizedConvNet, and RegularizedConvDenseNet, designed for image classification. Our approach integrates predictions from multiple models to enhance detection accuracy and reliability. Specifically, the base is formed by the ensemble of RegularizedConvNet and RegularizedConvDenseNet, which cooperate to prevent the spread of deepfakes and protect digital integrity. Experimental results show that our Ensemble method achieve better detection values with accuracy of 94.5% and AUC of 0.98.},
  keywords={Deep learning;Deepfakes;Accuracy;Computational modeling;Neural networks;Training data;Media;Predictive models;Ensemble learning;Reliability;Deepfake detection;Ensemble modeling;Convolutional Neural Network (CNN);classification;multimedia content;Explainable Artifical Intelligence;LIME},
  doi={10.1109/ICCCNT61001.2024.10726125},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10935353,
  author={Deepthi, K. and Shastry, Aditya K.},
  journal={IEEE Access}, 
  title={Retinal-ESRGAN: A Hybrid GAN Model Approach for Retinal Image Super-Resolution Coupled With Reduced Training Time and Computational Resources for Improved Diagnostic Accuracy}, 
  year={2025},
  volume={13},
  number={},
  pages={53366-53376},
  abstract={Medical Image Super-Resolution has always been a subject of interest in medical image processing. However, super-resolved retinal images are a requisite tool for doctors to properly diagnose and treat ophthalmic diseases. The acquisition of high-quality images is challenging owing to several factors including technical hardware limitations, high cost, operator skills, data compatibility, and maintenance issues. This paper proposes Retinal-ESRGAN, a novel hybrid unsupervised GAN model particularly designed for retinal image super-resolution. The model incorporates architectural modifications with respect to generator and discriminator network using Google Colaboratory and TensorFlow 2.0 facilitating limited resource usage. To address resource constraints, a training strategy involving pausing and resuming in batches is implemented. The experiments conducted have demonstrated Retinal-ESRGAN’s potential that achieved an average PSNR of 35.22 dB and SSIM of 0.916, outperforming both SRGAN and ESRGAN showing PSNR metric improvement of 4.8% over SRGAN and 10.5% improvement over ESRGAN. Also, a 5.7% improvement over SRGAN and 22.4% improvement over ESRGAN in SSIM metric, Inception score of 6.02, Fréchet Inception Distance of 25.31 and accuracy of 94.98% utilizing significantly less training time and computational resources.},
  keywords={Retina;Feature extraction;Training;Generators;Medical diagnostic imaging;Generative adversarial networks;Superresolution;Convolution;Computational modeling;Accuracy;ESRGAN;Fréchet inception distance;high resolution (HR) image;inception score;low resolution (LR) image;PSNR;retinal image;SRGAN;SSIM},
  doi={10.1109/ACCESS.2025.3553457},
  ISSN={2169-3536},
  month={},}@ARTICLE{10106503,
  author={Han, Ruidong and Wang, Xiaofeng and Bai, Ningning and Wang, Qin and Liu, Zinian and Xue, Jianru},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={FCD-Net: Learning to Detect Multiple Types of Homologous Deepfake Face Images}, 
  year={2023},
  volume={18},
  number={},
  pages={2653-2666},
  abstract={With the rapid development of artificial intelligence technology, a variety of GAN generated deepfake face images/videos have emerged endlessly. The abuse of deepfake has brought serious negative effects to many industries. Therefore, there is an urgent need to develop advanced methods to combat the abuse of deepfake. As far as we know, there are almost no techniques that can distinguish multiple types of homologous deepfake face images. In this study, we propose a method based on the multi-classification task to address this issue. The proposed method relies on a novel network framework named FCD-Net that consists of the facial synaptic saliency module (FSS), the contour detail feature extraction module (CDFE), and the distinguishing feature fusion module (DFF). Utilizing this method, the imperceptible features introduced by deepfake can be exposed, and the differences caused by different types of deepfake can be distinguished, even if deepfake images are homologous. To test the proposed method and compare it with other SOTA methods, we establish a new homologous dataset named HDFD that contains real face images, entire face synthesis images, face swap images, and facial attribute manipulation images. Among them, the three types of deepfake images are all generated from the same real face images through different deepfake techniques. Abundant experiment results demonstrate that the proposed method has a high-level detection accuracy and relatively strong robustness against content-preserving manipulations. Moreover, the generalization of our method is superior to other SOTA methods.},
  keywords={Deepfakes;Faces;Feature extraction;Face recognition;Generative adversarial networks;Image color analysis;Frequency-domain analysis;Deepfake detection;homologous face images;facial synaptic saliency (FSS);contour detail feature extraction;distinguishable feature fusion (DFF)},
  doi={10.1109/TIFS.2023.3269152},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{11146236,
  author={Tian, Zhaofeng},
  booktitle={2024 International Conference on Internet of Things, Robotics and Distributed Computing (ICIRDC)}, 
  title={Adaptive Generation and Analysis of Classical Music Style Based on Big Data}, 
  year={2024},
  volume={},
  number={},
  pages={392-395},
  abstract={With the in-depth research on artificial intelligence, it has undergone profound industrial changes in various fields. The research aims to explore the use of generative artificial intelligence in music creation by combining big data. This study uses big data analysis and machine learning algorithms to extract features and model styles from a large number of classical music works, achieving adaptive generation of classical music styles. Afterwards, this study conducts performance tests on the model, and the results showed that the maximum error rate of our method was 10 %, and the highest accuracy was 100 %. In the evaluation of generated music segments, the average accuracy of the music generated by our model in terms of style can reach 81 %, and the average evaluation on other indicators is above 7 points. The experimental results demonstrate that this study provides a new perspective and tool for the creation and analysis of classical music, effectively promoting the adaptive generation and innovative development of music styles, and demonstrating the enormous potential of big data and machine learning technology in the field of music creation.},
  keywords={Adaptation models;Machine learning algorithms;Accuracy;Soft sensors;Machine learning;Big Data;Feature extraction;Rhythm;Convolutional neural networks;Robots;Classical music;machine learning algorithms;convolutional neural network;feature extraction;generate analysis},
  doi={10.1109/ICIRDC65564.2024.00075},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11070527,
  author={Dixit, Mrudul and Kalyanshetti, Shreeni},
  booktitle={2025 4th OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 5.0}, 
  title={Carbon Footprint Analysis and Emission Reduction for Electric Vehicles}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The increasing global carbon footprint has made it clear that we need better solutions for sustainable transportation. Electric Vehicles (EVs) are key to reducing Greenhouse Gas Emission (GHG), but their production—especially the manufacturing of lithium-ion batteries—poses significant environmental and ethical issues. This paper investigates how Generative Artificial Intelligence (Gen AI) can help address these issues and improve the sustainability of EVs.The study uses a comprehensive synthetic datasets that includes details on manufacturing processes, battery life cycle, and performance metrics to develop AI-driven methods for enhancing EV sustainability. Machine learning algorithms such as Linear Regression, Neural Networks, and XGBoost are applied to predict environmental impacts and optimize resource efficiency.Root Mean Square Error (RMSE) is a metric used to assess the efficacy of prediction models; the smaller the RMSE, the more efficient the model is. Along with the technical analysis, choice of biodegradable, plant-based batteries as a sustainable alternative to traditional lithium-ion batteries is explored. The new direction in EV technology intends to reduce environmental effects through sustainable methods of making and disposing of batteries. The research along with its paper establishes that artificial intelligence tools advance electric vehicle sustainability at a substantial level. The goal pursues predictive enhancement and new battery research that leads to improved sustainability across transportation domains.},
  keywords={Technological innovation;Biological system modeling;Transportation;Production;Predictive models;Electric vehicles;Batteries;Sustainable development;Carbon footprint;Synthetic data;Electric Vehicles;Sustainability;Battery life Environmental Impact;Predictive Models},
  doi={10.1109/OTCON65728.2025.11070527},
  ISSN={},
  month={April},}@ARTICLE{10466733,
  author={Truong, Vu Tuan and Le, Hung Duy and Le, Long Bao},
  journal={IEEE Access}, 
  title={Trust-Free Blockchain Framework for AI-Generated Content Trading and Management in Metaverse}, 
  year={2024},
  volume={12},
  number={},
  pages={41815-41828},
  abstract={The rapid development of the metaverse and generative Artificial Intelligence (GAI) has led to the emergence of AI-Generated Content (AIGC). Unlike real-world products, AIGCs are represented as digital files, thus vulnerable to plagiarism and leakage on the Internet. In addition, the trading of AIGCs in the virtual world is prone to various trust issues between the involved participants. For example, some customers may try to avoid the payment after receiving the desired AIGC products, or the content sellers refuse to grant the products after obtaining the license fee. Existing digital asset management (DAM) systems often rely on a trusted third-party authority to mitigate these issues. However, this might lead to centralization problems such as the single-point-of-failure (SPoF) when the third parties are under attacks or being malicious. In this paper, we propose MetaTrade, a blockchain-empowered DAM framework that is designed to tackle these urgent trust issues, offering secured AIGC trading and management in the trustless metaverse environment. MetaTrade eliminates the role of the trusted third party, without requiring trust assumptions among participants. Numerical results show that MetaTrade offers higher performance and lower trading cost compared to existing platforms, while security analysis reveals that the framework is resilient against plagiarism, SPoF, and trust-related attacks. To showcase the feasibility of the design, a decentralized application (DApp) has been built on top of MetaTrade as a marketplace for metaverse AIGCs.},
  keywords={Metaverse;Artificial intelligence;Content distribution networks;Asset management;Digital systems;Generative AI;Plagiarism;Trustless services;Security;Decentralized applications;Cyberattack;Blockchains;Metaverse;blockchain;AI-generated content (AIGC);digital asset management},
  doi={10.1109/ACCESS.2024.3376509},
  ISSN={2169-3536},
  month={},}@ARTICLE{10251523,
  author={Seo, Eunil and Elmroth, Erik},
  journal={IEEE Access}, 
  title={MadFed: Enhancing Federated Learning With Marginal-Data Model Fusion}, 
  year={2023},
  volume={11},
  number={},
  pages={102669-102680},
  abstract={As the demand for intelligent applications at the network edge grows, so does the need for effective federated learning (FL) techniques. However, FL often relies on non-identically and non-independently distributed local datasets across end devices, which could result in considerable performance degradation. Prior solutions, such as model-driven approaches based on knowledge distillation, meta-learning, and transfer learning, have provided some reprieve. However, their performance suffers under heterogeneous local datasets and highly skewed data distributions. To address these challenges, this study introduces the MArginal Data fusion FEDerated Learning (MadFed) approach, a groundbreaking fusion of model- and data-driven methodologies. By utilizing marginal data, MadFed mitigates data distribution skewness, improves the maximum achievable accuracy, and reduces communication costs. Furthermore, the study demonstrates that the fusion of marginal data can significantly improve performance even with minimal data entries, such as a single entry. For instance, it provides up to a 15.4% accuracy increase and 70.4% communication cost savings when combined with established model-driven methodologies. Conversely, relying solely on these model-driven methodologies can result in poor performance, especially with highly skewed datasets. Significantly, MadFed extends its effectiveness across various FL algorithms and offers a unique method to augment label sets of end devices, thereby enhancing the utility and applicability of federated learning in real-world scenarios. The proposed approach is not only efficient but also adaptable and versatile, promising broader application and potential for widespread adoption in the field.},
  keywords={Data models;Federated learning;Data integration;Training;Performance evaluation;Costs;Computational modeling;Edge computing;Federated learning;edge computing;data heterogeneity;data-driven approaches;label set augmentation;data skewness},
  doi={10.1109/ACCESS.2023.3315654},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11063842,
  author={Reddy, T Sudeep and K, Aiswarya Milan and Palaniswamy, Suja},
  booktitle={2025 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)}, 
  title={Comparative Study of Adversarial Image Attacks}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Adversarial attacks exploit subtle input modifications to mislead deep neural networks (DNNs), posing significant risks to critical applications like autonomous vehicles and medical imaging. This study examines adversarial image generation using methods such as Fast Gradient Sign Method (FGSM), DeepFool, and Carlini & Wagner (C&W) attacks to assess DNN robustness. This approach involves generating adversarial samples with controlled perturbations and evaluating their impact on model performance. The findings contribute to a deeper understanding of adversarial risks and pave the way for more resilient AI systems capable of withstanding deceptive inputs in real-world applications. The results highlight FGSM's efficiency in rapid perturbation generation, DeepFool's minimal iterative modifications, and C& W's optimized, imperceptible attacks, all of which significantly reduce model accuracy},
  keywords={Accuracy;Image synthesis;Perturbation methods;Computational modeling;Artificial neural networks;Robustness;Iterative methods;Artificial intelligence;Autonomous vehicles;Biomedical imaging;Adversarial Images;Adversarial Attacks;Image Per-turbation;Model Robustness;Miss-classification},
  doi={10.1109/ICECCC65144.2025.11063842},
  ISSN={},
  month={May},}@ARTICLE{9738474,
  author={Das, Swagatam and Mullick, Sankha Subhra and Zelinka, Ivan},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={On Supervised Class-Imbalanced Learning: An Updated Perspective and Some Key Challenges}, 
  year={2022},
  volume={3},
  number={6},
  pages={973-993},
  abstract={The problem of class imbalance has always been considered as a significant challenge to traditional machine learning and the emerging deep learning research communities. A classification problem can be considered as class imbalanced if the training set does not contain an equal number of labeled examples from all the classes. A classifier trained on such an imbalanced training set is likely to favor those classes containing a larger number of training examples than the others. Unfortunately, the classes that contain a small number of labelled instances usually correspond to rare and significant events. Thus, poor classification accuracy on these classes may lead to severe consequences. In this article, we aim to provide a comprehensive summary of the rich pool of research works attempting to combat the adversarial effects of class imbalance efficiently. Specifically, following a formal definition of the problem of class imbalance, we explore the plethora of traditional machine learning approaches aiming to mitigate its adversarial effects. We further discuss the state-of-the-art deep-learning-based approaches for improving a classifier’s resilience against class imbalance and highlight the need for techniques tailored for such a paradigm. Moreover, we look at the emerging applications where class imbalance can be a major concern. Finally, we outline a few open problems along with the various challenges emerging with the advent of modern applications, deep learning paradigm, and new sources of data.},
  keywords={Training;Deep learning;Performance evaluation;Support vector machines;Object detection;Machine learning;Supervised learning;Class-imbalanced classification;deep learning;future challenges;machine learning;open problems;performance evaluation indices},
  doi={10.1109/TAI.2022.3160658},
  ISSN={2691-4581},
  month={Dec},}@ARTICLE{9257451,
  author={Xu, Zhi-Jing and Wang, Rong-Fei and Wang, Juan and Yu, Da-Hai},
  journal={IEEE Access}, 
  title={Parkinson’s Disease Detection Based on Spectrogram-Deep Convolutional Generative Adversarial Network Sample Augmentation}, 
  year={2020},
  volume={8},
  number={},
  pages={206888-206900},
  abstract={As an essential biological feature of human beings, voiceprint is increasingly used in medical research and diagnosis, especially in identifying Parkinson's Disease (PD). This paper proposes a Spectrogram Deep Convolutional Generative Adversarial Network (S-DCGAN) for sample augmentation to overcome the limited amount of existing patient voiceprint datasets and samples. S-DCGAN generates a high-resolution spectrogram by increasing network layers, adding the Spectral Normalization (SN) method, and combining feature matching strategy. The high-similarity and low-distortion spectrogram are selected in light of Structural Similarity Index (SSIM) values and Peak Signal to Noise Ratio (PSNR) to augment the samples. Fréchet Inception Distance (FID) and GAN-train result show the generalization ability of the generated data. We construct the ResNet50 model with a Global Average Pooling(GAP) layer to extract the voiceprint features and classify them effectively to improve recognition accuracy. The GAP suppresses the over-fitting problem and optimizes quickly. Finally, on the Sakar dataset, comparative experiments were conducted on different models and classification methods. Results show that the S-DCGAN-ResNet50 hybrid model can achieve the highest voiceprint recognition accuracy of 91.25% and specificity of 92.5%, which can distinguish between PD patients and healthy people more precisely compared with DCGAN-ResNet50. It augments the application environment of voiceprint recognition in the medical field and makes it universal in different datasets.},
  keywords={Spectrogram;Speech recognition;Brain modeling;Training;Feature extraction;Gallium nitride;Parkinson's disease;Parkinson’s disease;ResNet50;S-DCGAN;sample augumentation;spectrogram},
  doi={10.1109/ACCESS.2020.3037775},
  ISSN={2169-3536},
  month={},}@ARTICLE{10414986,
  author={Hassan, Syed Zain Ul and Rafi, Muhammad and Frnda, Jaroslav},
  journal={IEEE Access}, 
  title={GCZRec: Generative Collaborative Zero-Shot Framework for Cold Start News Recommendation}, 
  year={2024},
  volume={12},
  number={},
  pages={16610-16620},
  abstract={The aim of personalized news recommendation is to suggest news stories to the users that are most interesting for them. To improve the user experience, it is important that these news items are not only relevant to the user but also get recommended to them as soon as they are available. The inability of traditional collaborative filtering approach to recommend such cold start items has led to techniques that incorporate latent features of items in order to make cold start recommendations such as content based filtering and deep neural network-based approaches. However, these existing techniques do not make use of any collaborative information between users and items as well as latent features at the same time and thus fail to provide any serendipity which is an important aspect of any recommender system. Moreover, these underlying collaborative signals between users and items are crucial to improving the overall quality of recommender systems and can also be utilized to make cold start recommendations. In this paper, we propose the Generative Collaborative Zero-Shot Recommender System framework (GCZRec) which makes use of both the latent user and item features as well as the underlying collaborative information to generate both warm start and cold start recommendations. We evaluate our framework for news recommendation task given cold start and warm start cases for both users and news items. We also discuss that our model can be plugged in and used as preprocessing to improve the performance of an existing recommender system.},
  keywords={Recommender systems;Semantics;Zero-shot learning;Task analysis;Filtering;User experience;Media;Collaborative filtering;Information filtering;News recommendation;cold start problem;zero-shot learning;recommender system},
  doi={10.1109/ACCESS.2024.3359053},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10934368,
  author={Zhang, Qi},
  booktitle={2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)}, 
  title={Research on Intelligent Layout of Indoor Floor Plans based on Gated Recurrent Units}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={With the swift progression of intelligent automation, its applications in architecture and interior design have significantly evolved, leading to more efficient and intelligent solutions for floor plan layout generation. Traditional methods for indoor floor planning rely on heuristic or rule-based approaches, which often fail to optimize space utilization, accessibility, and functional requirements effectively. Existing AI-powered approaches, including convolutional neural networks (CNNs) and deep reinforcement learning, have shown significant potential but face challenges in capturing long-term dependencies and dynamic adjustments in complex layouts. To overcome these challenges, we propose a novel approach that leverages Gated Recurrent Units (GRU) to model spatial relationships and sequential dependencies in floor plan generation. Additionally, we introduce the Whale Optimization Algorithm (WOA) to enhance the efficiency of layout optimization by improving space utilization, room positioning, and traffic flow. The proposed method is evaluated using a benchmark dataset of architectural floor plans, achieving a layout coherence score of 94.3%, space optimization efficiency of 92.7%, and computational speed improvement of 28.5% compared to existing deep learning-based approaches. The deployment of this system can significantly enhance the automation and intelligence of indoor architectural design, offering a practical and efficient solution for architects and urban planners.},
  keywords={Intelligent automation;Layout;Refining;Manuals;Whale optimization algorithms;Real-time systems;Computational efficiency;Planning;Floors;Optimization;indoor floor plan layout;artificial intelligence;gated recurrent unit (GRU);whale optimization algorithm (WOA);architectural design;space optimization},
  doi={10.1109/ICISCN64258.2025.10934368},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9457953,
  author={Attia, Amr and Faezipour, Miad and Abuzneid, Abdelshakour},
  booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Network Intrusion Detection with XGBoost and Deep Learning Algorithms: An Evaluation Study}, 
  year={2020},
  volume={},
  number={},
  pages={138-143},
  abstract={This paper introduces an effective Network Intrusion Detection Systems (NIDS) framework that deploys incremental statistical damping features of the packets along with state-of- the-art machine/deep learning algorithms to detect malicious patterns. A comprehensive evaluation study is conducted between eXtreme Gradient Boosting (XGBoost) and Artificial Neural Networks (ANN) where feature selection and/or feature dimensionality reduction techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are also integrated into the models to decrease the system complexity for achieving fast responses. Several experimental runs confirm how powerful machine/deep learning algorithms are for intrusion detection on known attacks when combined with the appropriate features extracted. To investigate unknown attacks, the models were trained on a subset of the attack datasets, while a different set (with a different attack type) was kept aside for testing. The decent results achieved further support the belief that through supervised learning, the model could additionally detect unknown attacks.},
  keywords={Dimensionality reduction;Deep learning;Damping;Scientific computing;Network intrusion detection;Artificial neural networks;Feature extraction;NIDS;Machine Learning;ANN;XGBoost;LDA;PCA},
  doi={10.1109/CSCI51800.2020.00031},
  ISSN={},
  month={Dec},}@ARTICLE{11025476,
  author={Chang, Ching-Chun and Gao, Kai and Xu, Shuying and Kordoni, Anastasia and Leckie, Christopher and Echizen, Isao},
  journal={IEEE Access}, 
  title={Hypnopaedia-Aware Machine Unlearning via Psychometrics of Artificial Mental Imagery}, 
  year={2025},
  volume={13},
  number={},
  pages={103880-103897},
  abstract={Neural backdoors represent insidious cybersecurity loopholes that render learning machinery vulnerable to unauthorised manipulations, potentially enabling the weaponisation of artificial intelligence with catastrophic consequences. A backdoor attack involves the clandestine infiltration of a trigger during the learning process, metaphorically analogous to hypnopaedia, where ideas are implanted into a subject’s subconscious mind under the state of hypnosis or unconsciousness. When activated by a sensory stimulus, the trigger evokes a conditioned reflex that directs a machine to mount a predetermined response. In this study, we propose a cybernetic framework for constant surveillance of backdoor threats, driven by the dynamic nature of untrustworthy data sources. We develop a self-aware unlearning mechanism to autonomously detach a machine’s behaviour from the backdoor trigger. Through reverse engineering and statistical inference, we detect deceptive patterns and estimate the likelihood of backdoor infection. We employ model inversion to elicit artificial mental imagery, using stochastic processes to disrupt optimisation pathways and avoid convergent but potentially flawed patterns. This is followed by hypothesis analysis, which estimates the likelihood of each potentially malicious pattern as the true trigger and infers the probability of infection. The primary objective of this study is to maintain a stable state of equilibrium between knowledge fidelity and backdoor vulnerability.},
  keywords={Data models;Soft sensors;Vehicle dynamics;Reverse engineering;Predictive models;Optimization;Cyberspace;Cybernetics;Weapons;Trojan horses;Cybersecurity;machine unlearning;neural backdoors;psychometrics;reverse engineering},
  doi={10.1109/ACCESS.2025.3576800},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8461174,
  author={Grigorescu, Sorin M.},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Generative One-Shot Learning (GOL): A Semi-Parametric Approach to One-Shot Learning in Autonomous Vision}, 
  year={2018},
  volume={},
  number={},
  pages={7127-7134},
  abstract={Highly Autonomous Driving (HAD) systems rely on deep neural networks for the visual perception of the driving environment. Such networks are train on large manually annotated databases. In this work, a semi-parametric approach to one-shot learning is proposed, with the aim of bypassing the manual annotation step required for training perceptions systems used in autonomous driving. The proposed generative framework, coined Generative One-Shot Learning (GOL), takes as input single one-shot objects, or generic patterns, and a small set of so-called regularization samples used to drive the generative process. New synthetic data is generated as Pareto optimal solutions from one-shot objects using a set of generalization functions built into a generalization generator. GOL has been evaluated on environment perception challenges encountered in autonomous vision.},
  keywords={Pareto optimization;Training;Autonomous vehicles;Generators;Linear programming;Probability density function},
  doi={10.1109/ICRA.2018.8461174},
  ISSN={2577-087X},
  month={May},}@INPROCEEDINGS{10773923,
  author={Loevenich, Johannes F. and Adler, Erik and Bécue, Adrien and Velazquez, Alexander and Wrona, Konrad and Boshnakov, Vasil and Falkcrona, Jerry and Nordbotten, Nils and Worthington, Olwen L. and Röning, Juha and Rigolin, Roberto and Lopes, F.},
  booktitle={MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM)}, 
  title={Training Autonomous Cyber Defense Agents: Challenges & Opportunities in Military Networks}, 
  year={2024},
  volume={},
  number={},
  pages={158-163},
  abstract={This paper addresses the development and training of robust autonomous cyber defense (ACD) agents within military networks. We propose an architecture that integrates a hybrid AI model comprising Multi-Agent Reinforcement Learning (MARL), Large Language Models (LLMs), and a rule-based system into blue and red agent teams distributed across network devices. The primary goal is to automate key cybersecurity tasks such as monitoring, detection, and mitigation, thereby augmenting the capabilities of cybersecurity professionals in protecting critical military infrastructure. This architecture is designed to operate in modern network environments characterized by segmented clouds and software-defined controllers, which facilitate the deployment of ACD agents and other cybersecurity tools. The agent teams were evaluated in an Automated Cyber Operation (ACO) gym, which simulates NATO protected core networks and enables reproducible training and testing of autonomous agents. The paper concludes with an examination of the main challenges encountered in the training of ACD agents, with a particular focus on the security of the data and the robustness of the AI models during the training/testing phase.},
  keywords={Training;Measurement;Military communication;Privacy;Prevention and mitigation;Reinforcement learning;Robustness;Autonomous agents;Monitoring;Testing;Autonomous Cyber Security;Multi-Agent Reinforcement Learning;Large Language Models},
  doi={10.1109/MILCOM61039.2024.10773923},
  ISSN={2155-7586},
  month={Oct},}@INPROCEEDINGS{10139981,
  author={Chaudhuri, Arjun and Chen, Ching-Yuan and Talukdar, Jonti and Chakrabarty, Krishnendu},
  booktitle={2023 IEEE 41st VLSI Test Symposium (VTS)}, 
  title={Functional Test Generation for AI Accelerators using Bayesian Optimization∗}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={We propose a black-box optimization method to generate functional test patterns for AI inferencing accelerators. Functional testing is faster than structural testing as scan chains are not used for shifting in patterns and shifting out test responses. Moreover, functional testing reduces "over-testing" by targeting the detection of functionally critical faults for a given application workload. We use Bayesian Optimization for targeted test-image generation for stuck-at faults in a systolic array-based accelerator. Our framework supports test-pattern compaction and leverages various types of error regularization for enforcing functional-likeness of the generated test images. We achieve high fault coverage using a small set of test images for pin-level faults in 16-bit and 32-bit floating-point processing elements of the systolic array achieves high fault coverage with a small set of test images.},
  keywords={Optimization methods;Closed box;AI accelerators;Very large scale integration;Systolic arrays;Bayes methods;Compaction},
  doi={10.1109/VTS56346.2023.10139981},
  ISSN={2375-1053},
  month={April},}@ARTICLE{10848555,
  author={Wen, Minghao and Liang, Dong and Ye, Haibo and Tu, Huawei},
  journal={Journal of Intelligent Construction}, 
  title={Architectural Facade Design with Style and Structural Features using Stable Diffusion Model}, 
  year={2024},
  volume={2},
  number={4},
  pages={1-12},
  abstract={With advancements in digital technology, the field of architectural design has increasingly embraced data and algorithms to enhance design efficiency and quality. Recent advancements in text-to-image (T2I) generation models have enabled the creation of images that correspond to textual descriptions. Howev-er, textual descriptions struggle to capture essential style characteristics in style images. In this study, we proposed a method for architectural facade design based on the stable diffusion model (SDM) that combined stylistic images or keywords as input with the structural conditions of content images to generate images with both stylistic and architectural features. By employing the con- strastive language-image pre-training (CLIP) image encoder to convert the style image into its initial image embedding and feature extraction from multilayer cross-attention and training optimization to obtain a pretrained image embed-ding, the proposed method extracts stylistic features from style images and converts them into corresponding embeddings. This process enables the generated images to embody stylistic features and artistic semantic information. Further-more, the T2I adapter model is employed to use the architectural structure of content images as conditional guidance, thereby ensuring that the generated images exhibit the corresponding structural features. By leveraging these two aspects, the proposed method can decorate architecture with stylistic features from stylistic images while preserving the architectural structure features of content images, resulting in images that reflect the content images after style transformation. Our method is mainly used in architectural design applications. It was capable of generating facade images from flat design drawings, three-di-mensional (3D) architectural models, and hand-drawn sketches and has achieved commendable results.},
  keywords={Training;Adaptation models;Visualization;Translation;Three-dimensional displays;Diversity reception;Text to image;Feature extraction;Nonhomogeneous media;Diffusion models;architectural design;digital facade generation;style transfer;stable diffusion;T2I-adapter},
  doi={10.26599/JIC.2024.9180034},
  ISSN={2958-2652},
  month={December},}@INPROCEEDINGS{9070448,
  author={Lee, Dongkun and Nam, Jehyun and Choi, Ho-Jin},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Deep Electric Pole Anomaly Detection and Unsupervised Description Generation}, 
  year={2020},
  volume={},
  number={},
  pages={535-537},
  abstract={In the past few years, image recognition and computer vision with deep learning have evolved rapidly to become a commercial application that assists traditional human decision making with the engineering of data professionals. However, there are a number of problems with existing deep learning. Deep learning is a black box method that suffers from over-fitting problems more seriously than traditional learning methods and cannot be used to understand the decision-making process in the middle layer. It is used skeptically for applications that require reliability and transparency such as safety. The recent generation model gives a user understanding of the data manifold and is emerging as a new unsupervised learning method. In particular, we focus on the unsupervised learning of these generation models and propose experiments that can be used to detect abnormalities in power distribution facilities. We propose techniques to mitigate dataset shortages by learning and analyzing dataset in power environments with extreme shortages of abnormal dataset, and providing an assessment of the abnormal dataset. The module is still experimental and has no results. But in the future, it is expected to be used as a convergence technology of the power industry and artificial intelligence.},
  keywords={Gallium nitride;Generative adversarial networks;Machine learning;Computer architecture;Data models;Computational modeling;Conferences;Machine learning, Electric pole, Generative modeling},
  doi={10.1109/BigComp48618.2020.00-10},
  ISSN={2375-9356},
  month={Feb},}@ARTICLE{11125898,
  author={Guo, Ruoyu and Pagnucco, Maurice and Song, Yang},
  journal={IEEE Transactions on Multimedia}, 
  title={Exploring Multi-feature Relationship in Retinex Decomposition for Low-light Image Enhancement}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Despite the recent advancements in deep learning techniques, existing unsupervised low-light image enhancement methods fail to improve global brightness and restore colour due to the lack of high-quality training targets. Moreover, real-world low-light images inevitably contain noise, which significantly reduces image visibility and quality, further complicating the enhancement process. However, current unsupervised approaches tend to oversimplify or ignore the noise in low-light images. To address these issues, we first revise the traditional Retinex decomposition to better integrate with unsupervised deep learning frameworks. Then, we design a Local and Global Illumination-Guided Network for removing corruption from the reflectance component, which improves enhancement quality by not only investigating multi-feature similarity and attention mechanism based on the Retinex theory but also leveraging local details and long-range dependencies. Furthermore, by analysing the attributes of corruption within the reflectance component, we introduce a novel reflectance enhancement loss to effectively remove noise without using ground truth. The code is available at: https://github.com/RuoyuGuo/ErcRetinex.},
  keywords={Reflectivity;Lighting;Image color analysis;Feature extraction;Image enhancement;Training;Brightness;Unsupervised learning;Noise reduction;Image restoration;Low-light image enhancement;unsupervised learning;Retinex theory},
  doi={10.1109/TMM.2025.3599099},
  ISSN={1941-0077},
  month={},}@ARTICLE{10842354,
  author={Wang, Chunxin and Xie, Qing and Zhang, Yutong and Zhang, QianQian and Zhang, Ruoquan and Xie, Jun},
  journal={IEEE Transactions on Power Delivery}, 
  title={Transformer Incremental Fault Diagnosis Method Using Lossless Estimation and Balanced Training}, 
  year={2025},
  volume={40},
  number={2},
  pages={889-899},
  abstract={Incremental fault diagnosis is an effective way to address the discrepancies between the actual diagnosis data distribution and the trained model. However, the traditional incremental learning method has the problem of forgetting the feature of historical faults and reducing the diagnostic accuracy when applied. A transformer incremental fault diagnosis method using lossless estimation and balanced training is proposed in this study. Firstly, this method achieves lossless estimation of historical task gradients in incremental updates based on Gaussian–Hermite transformation, thereby preserving historical task information as much as possible. Then, to enhance the “anti-forgetting” capability of historical features while ensuring the ability to learn new features, the method balances the weights between the loss functions of new and historical tasks to optimize the update direction of the network training. Finally, the verification results using real data collected from multiple locations indicate that the proposed method has both “anti-forgetting” ability for historical tasks and the ability to learn new tasks effectively. Moreover, the lowest accuracy rate is 97.04% in the test, and the incremental training efficiency is relatively high.},
  keywords={Training;History;Estimation;Power transformer insulation;Oil insulation;Incremental learning;Generators;Vectors;Transformers;Fitting;Balanced training;dissolved gas-in-oil analysis;incremental learning;lossless estimation},
  doi={10.1109/TPWRD.2025.3528121},
  ISSN={1937-4208},
  month={April},}@ARTICLE{10989750,
  author={Tian, Ye and Zheng, Wentao and Shao, Yinghao and Zhang, He and Sun, Jian},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={MJTG: A Multi-vehicle Joint Trajectory Generator for Complex and Rare Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={The advancement of Highly Automated Vehicles (HAVs) safety testing plays a crucial role in the large-scale deployment of Automated Driving Systems (ADS). Scenario-based testing has become increasingly popular due to its customizability and testing efficiency. Unfortunately, the scarcity of collected realworld scenario data constrains the construction of a high-coverage scenario library. It is of great value to make small data into big data by generating fresh new scenarios out of existing ones. A common approach is to use Data-driven Scenario Generation (DSG) methods. However, these methods suffer from some drawbacks, including the low quality of the generated scenarios (i.e., the scenario fidelity) and the lack of diversity and criticality compared to the original scenarios (i.e., the scenario directivity). These problems are mainly caused by the complex and dynamic nature of scenarios and the rarity of critical situations in naturalistic driving environment. To address those challenges, we propose a data-driven method named the Multi-vehicle Joint Trajectory Generator (MJTG), devised to selectively generate potentially feasible scenarios based on limited collected scenarios. The MJTG framework has demonstrated significant effectiveness in terms of fidelity and directivity. Generated scenarios exhibit over 85% compliance with functional check standards. Additionally, the framework excels in addressing directivity, resulting in a reduction of 0.66 seconds in the modified time-tocollision. Overall, the MJTG framework holds promising prospects in constructing high-coverage test-worthy scenario libraries and advancing the safety testing of HAVs.},
  keywords={Testing;Trajectory;Scenario generation;Safety;Ontologies;Libraries;Generators;Training;Data mining;Roads;Data-driven;highly automated vehicles;safety test;scenario generation},
  doi={10.1109/TVT.2025.3567636},
  ISSN={1939-9359},
  month={},}@ARTICLE{11154837,
  author={Ni, He and Jiang, Yicheng and Chen, Ruida and Zhang, Yun},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={STRE-Diffusion: Few-Shot Spaceborne ISAR Target Recognition via Spatial-Time Relationship Estimation and Improved Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-19},
  abstract={In spaceborne inverse synthetic aperture radar (ISAR) space target recognition, the high-speed relative motion restricts the observation time, leading to insufficient and low diversity ISAR datasets and reduced recognition accuracy. In existing methods, the dataset augmentation can be achieved by interpolating the azimuth angle of ISAR images obtained in the observation region. However, they lack the capability to extrapolate images in non-observation region, limiting the effectiveness in enhancing the angular diversity of the dataset. In this article, a few-shot spaceborne ISAR target recognition method via spatial-time relationship estimation and improved diffusion network (STRE-Diffusion) is proposed. The proposed method incorporates the spatial-time relationship (STR) into ISAR images prediction, which not only provides additional information for target motion modeling but also enhances predicted image details. First, visual-domain motion cues are extracted via optical flow estimation. Next, STR is estimated using the optical flow of strong scatterers. Subsequently, a motion cues extrapolation denoising (MCED) module is employed to extrapolate the cross-domain motion cues to the future through inverse diffusion process. In the MCED, a lightweight spatial-time bi-level routing transformer (ST-BiFormer) is proposed for spatial-time feature fusion and alignment. Then the extrapolated motion cues are mapped back to the image domain to obtain the predicted images. Finally, the predicted images are used to construct the extended dataset, which is applied to few-shot recognition. Experimental results show that the predicted ISAR images generated by proposed method have higher authenticity. The few-shot recognition performance based on the extended dataset has been significantly improved.},
  keywords={Spaceborne radar;Target recognition;Radar imaging;Training;Estimation;Imaging;Aerospace and electronic systems;Data augmentation;Azimuth;Orbits},
  doi={10.1109/TAES.2025.3608122},
  ISSN={1557-9603},
  month={},}@ARTICLE{11136125,
  author={Zhao, Dongxu and Yang, Yang and Yang, Jingwen and Li, Beichen and He, Yuan and Lang, Yue},
  journal={IEEE Internet of Things Journal}, 
  title={Ra-SPD: Radar Signal Interference Mitigation Using Spectral-Spatial Decomposition}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the continuous evolution of radar RF sensing technology in the Internet of Things (IoT) field, the deployment of co-frequency communication devices within smart home and healthcare environments is becoming increasingly prevalent. Concurrently, the proliferation of RF signals has rendered the challenge of spectrum resource allocation increasingly prominent, exacerbating the issue of severe mutual interference among these co-frequency devices. This severe interference exhibits intricate attributes, characterized by prolonged duration, a broad frequency range, and high level power intensity. When experiencing interference, the target’s echo is notably veiled, rendering recovery through standard technologies exceedingly challenging. From our perspective, in the presence of severe interference, the emphasis ought to be on elegantly reconstructing the target’s echo located in the period of interference, rather than merely eliminating the negative impact on interferences from the raw radar signals. Based on this thought, we have explored the feasibility of a deep network model for interference mitigation of radar signals in this paper, and introduced an interference mitigation method, namely Ra-SPD. The Ra-SPD is crafted through a dual design methodology, integrating the mask-guided spectral decomposition mechanism alongside the region-aware spatial decomposition scheme. The primary objective of the initial design is to enhance the capabilities of the standard deep model, enabling it to distinguish the duration of interference present in radar signals, particularly when the semantic alignment between the restored radar signal and the corresponding ground truth is at risk of being disrupted. Meanwhile, the improvement of local information related to the restored radar signal is accomplished using the spatial decomposition scheme, aimed at increasing the robustness of low-intensity signal segments against severe interference. Experimental results demonstrate superior performance of Ra-SPD at 15%, 40%, and 80% three different signal-to-interference ratio conditions, achieving PSNR: 35.73/32.99/30.28 dB; SSIM: 0.96/0.95/0.92; FID: 23.41/30.97/43.92; and MAE: 0.014/0.024/0.036 across conditions. Our method consistently outperforms six benchmark algorithms in all objective metrics and subjective evaluations, exhibiting 7.6%, 1.5%, 5.5%, and 17.2% average improvements in four metrics relative to the suboptimal method, highlighting significant advantages in interference mitigation and signal quality preservation.},
  keywords={Interference;Radar;Prevention and mitigation;Image restoration;Internet of Things;Training;Spectrogram;Standards;Semantics;Radio frequency;Interference mitigation;radar signals;deep learning;image restoration},
  doi={10.1109/JIOT.2025.3602201},
  ISSN={2327-4662},
  month={},}@INPROCEEDINGS{11142388,
  author={Tkachenko, Dmytro and Rumiński, Jacek},
  booktitle={2025 17th International Conference on Human System Interaction (HSI)}, 
  title={Evaluating the Sensitivity of Baseline Classifiers and Detection Model to GAN-Based Inpainting in Lung CT Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={In this study, we investigate the robustness of various deep learning models to inpainted lung CT scans. We train several image classification models (EfficientNet, MobileNet, DenseNet, and a custom CNN) as well as one object detection model (YOLO) to determine whether a scan contains a tumor or not. We then apply a pretrained GAN to inpaint tumor regions in test images, effectively hiding cancer presence. The resulting set of modified images is then evaluated by the previously trained models. We observe that the classifiers test accuracy remains almost unchanged, whereas the YOLO model adjusts its predictions, failing to detect tumors that were previously visible. This suggests that classification models may have learned non-obvious patterns (or biases) associated with cancer presence, possibly integrating features from larger image regions rather than focusing solely on localized tumor areas. In contrast, the YOLO model depends more on local visual features and is more vulnerable to inpainting-based adversarial attacks. These findings highlight the need to consider model architecture and susceptibility to image manipulation when deploying AI in human-system interaction tasks.},
  keywords={YOLO;Visualization;Analytical models;Lungs;Computed tomography;Predictive models;Feature extraction;Tumors;Immune system;Cancer;classification;inpainting;YOLO;convolutional neural networks},
  doi={10.1109/HSI66212.2025.11142388},
  ISSN={2158-2254},
  month={July},}@ARTICLE{9889702,
  author={Rafi, Taki Hasan and Woong Ko, Young},
  journal={IEEE Access}, 
  title={HeartNet: Self Multihead Attention Mechanism via Convolutional Network With Adversarial Data Synthesis for ECG-Based Arrhythmia Classification}, 
  year={2022},
  volume={10},
  number={},
  pages={100501-100512},
  abstract={Cardiovascular disease is now one of the leading causes of morbidity and mortality. Electrocardiogram (ECG) is a reliable tool for monitoring the health of the cardiovascular system. Currently, there has been a lot of focus on accurately categorizing heartbeats. There is a high demand for automatic ECG classification systems to assist medical professionals. But there is a big issue in obtaining original data extensively in medical domains in rare diseases, so it is essential to have a robust solution adopting this challenge. So, we need a solution that can address the problem of tackling data insufficiency, which is a major concern nowadays for medical applications. Without having, significant training samples the overall output can be demised. But the recent works on ECG classification did not address the challenge of solving the data insufficiency label problem. To overcome this issue, we developed a new generative adversarial network-based deep learning method called HeartNet for tackling the data insufficiency problem. The proposed deep learning method is compressed by a multi-head attention mechanism on CNN architecture. The main challenge of insufficient data labels is solved by adversarial data synthesis by adopting a generative adversarial network (GAN) with generating additional training samples. It drastically improves the overall performance of the proposed method by 5-10% on each insufficient data label category. Since the training samples are increased. We evaluated our proposed method utilizing the MIT-BIH dataset. Our proposed method has shown  $99.67\pm {0.11}$  accuracy and 89.24± 1.71 MCC trained with adversarial data synthesized dataset. However, we have also utilized two individual datasets as Atrial Fibrillation Detection Database and PTB Diagnostic Database to see the performance and generalization of our proposed model on ECG classification. The effectiveness and robustness of the proposed method are validated by extensive experiments, comparison, and analysis. Later on, we also highlighted some limitations of this work.},
  keywords={Electrocardiography;Deep learning;Convolutional neural networks;Generative adversarial networks;Feature extraction;Training data;Adaptation models;Arrhythmia;CNN;multi-head attention;GAN;electrocardiography (ECG)},
  doi={10.1109/ACCESS.2022.3206431},
  ISSN={2169-3536},
  month={},}@ARTICLE{9662304,
  author={Guo, Liang and Renze, Luo and Xingyu, Li and Juanjuan, Tuo and Lei, Canru and Yang, Zhou},
  journal={IEEE Access}, 
  title={Logging Data Completion Based on an MC-GAN-BiLSTM Model}, 
  year={2022},
  volume={10},
  number={},
  pages={1810-1822},
  abstract={Due to environmental interference and operational errors, problems such as incomplete and random missing logging data have occurred during the geophysical logging data collection process. Since it is difficult to establish a geophysical model based on logging data and geological information, the data complementation effect of conventional methods is not very satisfactory. In this paper, we propose an MC-GAN-BiLSTM model based on spatiotemporal sequence prediction. In the model, we adopt a generative adversarial network (GAN) as a network framework, and a long short-term memory (LSTM) neural network and a bi-directional long short-term memory (Bi-LSTM) as the basic modules. We use the LSTM instead of a fully-connected layer in the GAN to extract the potential information in the logging data depth domain. We complete the logging data missing values through an encoding-decoding structure that includes the Bi-LSTM. In addition, the generator module also uses multiscale convolution to fully extract the logging data features. We use logging data random missing values and consecutive missing values to simulate a field data acquisition environment and threshold control to simulate a laboratory processing environment for experiments. The experimental results show that the coefficient of determination (R2) of the GAN-LSTM model reaches 0.906 when 30% of random logging data are missing and 0.851 when 30% of consecutive logging data are missing. The effect of the model proposed in this paper is significantly higher than the commonly used random forest (RF), sequence to sequence (seq2seq) and generative adversarial interpolation network (GAIN) models.},
  keywords={Data models;Neural networks;Logic gates;Generative adversarial networks;Support vector machines;Recurrent neural networks;Predictive models;Logging data;missing value;GAN;LSTM;GAIN;GAN-LSTM},
  doi={10.1109/ACCESS.2021.3138194},
  ISSN={2169-3536},
  month={},}
