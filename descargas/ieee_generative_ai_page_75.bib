@INPROCEEDINGS{10452489,
  author={Ghadekar, Premanand and Patil, Srushtiraj and Sant, Hiranmayee},
  booktitle={2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Video Regeneration Using Image Diffusion Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={In this paper, we propose a novel approach for video editing using a combination of image diffusion models and frame interpolation. Our method utilizes the custom image diffusion models to enhance or edit the video frames and generate high-quality outputs. We also incorporate frame interpolation techniques to improve the temporal resolution of the edited videos. Our approach involves applying the diffusion models to each video frame separately after sorting according to quality and speed we need and then using interpolation to create a smooth transition between the edited frames. Our experiments demonstrate that our method achieves faster with relatively less loss of quality results compared to existing video editing methods. We also evaluate the impact of different image diffusion models on the quality of the edited videos. Our work provides a new direction for video editing using image diffusion models and frame interpolation and can be applied to various video editing applications.},
  keywords={Interpolation;Image resolution;Image edge detection;Estimation;Artificial intelligence;Sorting;video editing;diffusion models;image interpolation;machine learning;video synthesis;deep learning;image processing;computer vision;image diffusion;high-resolution video},
  doi={10.1109/ICDSAAI59313.2023.10452489},
  ISSN={},
  month={Dec},}@ARTICLE{10570347,
  author={Guo, Zhongyuan and Li, Jiawei and Lei, Jia and Liu, Jinyuan and Zhou, Shihua and Wang, Bin and Kasabov, Nikola K.},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Multiscale Bilateral Attention Fusion Network for Pansharpening}, 
  year={2024},
  volume={5},
  number={11},
  pages={5828-5843},
  abstract={High-resolution multispectral (HRMS) images combine spatial and spectral information originating from panchromatic (PAN) and reduced-resolution multispectral (LRMS) images. Pansharpening performs well and is widely used to obtain HRMS images. However, most pansharpening approaches determine the ratio of PAN and LRMS images through direct interpolation, which may introduce artifacts and distort the color of the fused results. To address this issue, an unsupervised progressive pansharpening framework, MSBANet, is proposed, which adopts a multistage fusion strategy. Each stage contains an attention interactive extraction module (AIEM) and a multiscale bilateral fusion module (MBFM). The AIEM extracts spatial and spectral features from input images and captures the correlations between features. The MBFM can efficiently integrate information from the AIEM and improve MSBANet context awareness. We design a hybrid loss function that enhances the ability of the fusion network to store spectral and texture details. In qualitative and quantitative experimental studies on four datasets, MSBANet outperformed state-of-the-art pansharpening techniques. The code will be released.},
  keywords={Pansharpening;Feature extraction;Task analysis;Artificial intelligence;Remote sensing;Spatial resolution;Data mining;Attention network;bilateral fusion;hybrid loss;progressive network;pansharpening},
  doi={10.1109/TAI.2024.3418378},
  ISSN={2691-4581},
  month={Nov},}@INPROCEEDINGS{10849481,
  author={Jia, Mengjie and Li, Yanyan and Yuan, Jiawei},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={A Robust UAV Tracking Solution in the Adversarial Environment}, 
  year={2024},
  volume={},
  number={},
  pages={945-952},
  abstract={Unmanned aerial vehicles (UAVs) are ideal platforms for object tracking due to their high mobility and advanced sensing capabilities. Recent advancements in AI have enhanced UAV tracking by integrating deep learning models with UAV systems, but they also introduce security concerns due to the vulnerabilities of deep learning models to adversarial attacks. To address this challenge, we propose a new UAV tracking solution integrating a reconstruction module with an anomaly detection module to enhance the robustness of UAV tracking systems against attacks. Our reconstruction module processes video frames to mitigate adversarial impacts without compromising tracking performance on clean frames. The anomaly detection module employs a reference generator to dynamically construct adversarial reference samples for feature map comparisons to effectively detect attacks. We evaluated our solution against state-of-the-art attacks on three benchmarks. The results show that our solution improves the tracking performance under attack conditions, achieving an average precision of 97.4% and a success rate of 96.3% of the original tracking. Additionally, our method achieves a 98.9% attack detection rate with a 4.23% false positive rate in anomaly detection. The evaluation results demonstrate the effectiveness of our approach in enhancing the robustness and reliability of UAV tracking systems in the adversarial environment.},
  keywords={Deep learning;Autonomous aerial vehicles;Feature extraction;Robustness;Generators;Sensors;Security;Object tracking;Artificial intelligence;Anomaly detection;uav object tracking;robustness enhancement;anomaly detection;input reconstruction},
  doi={10.1109/ICTAI62512.2024.00136},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{11050551,
  author={Kulkarni, Prashant and Namer, Assaf},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Temporal Context Awareness: A Defense Framework Against Multi-Turn Manipulation Attacks on Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={930-935},
  abstract={Many Large Language Models (LLMs) today are vulnerable to multi-turn manipulation attacks,where adversaries gradually build context through seemingly benign conversational turns to elicit harmful or unauthorized responses. These attacks exploit the temporal nature of dialogue to evade single-turn detection methods, posing a significant risk to the safe deployment of LLMs. This paper introduces the Temporal Context Awareness (TCA)framework, a novel defense mechanism designed to address this challenge by continuously analyzing semantic drift, cross-turn intention consistency, and evolving conversational patterns. The TCA framework integrates dynamic context embedding analysis, cross-turn consistency verification, and progressive risk scoring to detect and mitigate manipulation attempts effectively. Preliminary evaluations on simulated adversarial scenarios demonstrate the framework's potential to identify subtle manipulation patterns often missed by traditional detection techniques, offering a much-needed layer of security for conversational AI systems. In addition to outlining the design of TCA, we analyze diverse attack vectors and their progression across multi-turn conversations, providing valuable insights into adversarial tactics and their impact on LLM vulnerabilities. Our findings underscore the pressing need for robust, context-aware defenses in conversational AI systems and highlight the TCA framework as a promising direction for securing LLMs while preserving their utility in legitimate applications},
  keywords={Conversational artificial intelligence;Large language models;Semantics;Context awareness;Pressing;Oral communication;Vectors;Security;Manipulator dynamics;LLM Security;Multi-turn attacks;prompt security;obfuscation;prompt injection;security;trustworthy AI;jailbreak},
  doi={10.1109/CAI64502.2025.00164},
  ISSN={},
  month={May},}@INPROCEEDINGS{10528484,
  author={Zhou, Hongwei and Gao, Min and Chao, Meiling and Song, Yuqi and Xiong, Qingyu and Wen, Junhao and Zhang, Yi},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Data Hybrid Attacks Based on Sensitive Users in Recommender Systems}, 
  year={2023},
  volume={},
  number={},
  pages={445-452},
  abstract={Recommender systems, which make precise recommendations through historical interaction data of users and items, have been widely used in real life. But at the same time, because of the open nature of the systems, they are vulnerable to malicious attacks. To explore the robustness of recommender systems, researchers have proposed various attack methods and analyzed their adverse effects. Although the current attack methods can make good results on rating-based recommender systems, they lack thinking about an attack for a social recommendation. Social relationships have a non-negligible role in recommender systems, but there is still little related research, which only constructs fake rating profiles and fake relationship profiles with simple rules, without considering the mutual influence of rating and social information. To address the above problem, we study the attack method with deeply hidden features and propose a hybrid attack method based on sensitive users. It can further enhance the hidden features of fake users, and then use social attacks to assist in attacking social recommendation methods. Empirical results on three public datasets show that our method outperforms traditional methods in terms of attack effectiveness and has excellent anti-detection capability. Ultimately, we hope that our method could enable more analysis for such a hybrid attack and guides for investigating effective prevention measures.},
  keywords={Deep learning;Robustness;Artificial intelligence;Recommender systems;Recommender Systems;Data Hybrid Attack;Deep Hidden Features;Sensitive Users},
  doi={10.1109/ACAIT60137.2023.10528484},
  ISSN={},
  month={Nov},}@ARTICLE{10521888,
  author={Wu, Keyu and Chen, Shengkai and Wu, Min and Xiang, Shili and Jin, Ruibing and Xu, Yuecong and Li, Xiaoli and Chen, Zhenghua},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Reinforced Reweighting for Self-Supervised Partial Domain Adaptation}, 
  year={2024},
  volume={5},
  number={9},
  pages={4813-4822},
  abstract={Domain adaptation enables the reduction of distribution differences across domains, allowing for effective knowledge transfer from one domain to a different domain. In recent years, partial domain adaptation (PDA) has attracted growing interest due to its focus on a more realistic scenario, where the target label space is a subset of the source label space. As the source and target domains do not possess the same label space in the PDA setting, it is challenging but crucial to mitigate the domain gap without incurring negative transfer. In this article, we propose a reinforced reweighting united with self-supervised adaptation (R2SA) method to address the challenges in PDA by leveraging the merits of deep reinforcement learning (DRL) and self-supervised learning (SSL) simultaneously in a cooperative way. Reinforced reweighting aims to learn a source reweighting policy automatically based on information provided by the PDA model, while self-supervised adaptation aims to boost the adaptability of the PDA model through an additional self-supervised objective on the target domain. Extensive experiments on several cross-domain benchmarks demonstrate that our method achieves state-of-the-art results, with larger performance gains on more challenging tasks.},
  keywords={Adaptation models;Data models;Task analysis;Artificial intelligence;Self-supervised learning;Feature extraction;Predictive models;Partial domain adaptation (PDA);reinforcement learning (RL);self-supervised learning (SSL)},
  doi={10.1109/TAI.2024.3397288},
  ISSN={2691-4581},
  month={Sep.},}@INPROCEEDINGS{10528416,
  author={Wu, Zhenqian and Li, Xiaoyuan and Ren, Yazhou and Pu, Xiaorong and Zhu, Xiaofeng and He, Lifang},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Self-Paced Neutral Expression-Disentangled Learning for Facial Expression Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1316-1325},
  abstract={The accuracy of facial expression recognition is typically influenced by the following factors: high similarities across different expressions, disturbing factors, and the subtle and rapid micro-movements of the face. One potential solution to overcome these obstacles is to leverage the hidden neutral information present in neutral expression images. In this paper, we propose a Self-Paced Neutral Expression-Disentangled Learning (SPNDL) model, which aims to disentangle the neutral information from facial expressions, facilitating the extraction of crucial features and variations. This approach enables the capture of discriminative details within similar expressions and the detection of micro-facial movements. To enhance the learning of these neutral expression-disentangled features (NDFs) and mitigate the challenges posed by non-convex optimization, we propose a self-paced learning (SPL) strategy based on NDFs during the training phase. SPL learns samples from easy to complex by increasing the number of samples selected into the training process, which enables to effectively suppress the negative impacts caused by low-quality samples and inconsistently distributed NDFs. Experiments on three popular databases (i.e., CK+, Oulu-CASIA, and RAF-DB) show the effectiveness of our proposed method.},
  keywords={Training;Databases;Face recognition;Learning (artificial intelligence);Feature extraction;Data mining;Optimization;Facial expression recognition;Disturbance-disentangling;Self-paced learning},
  doi={10.1109/ACAIT60137.2023.10528416},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10730539,
  author={Zhang, Yapeng and Wu, Gaochang and Chai, Tianyou},
  booktitle={2024 6th International Conference on Industrial Artificial Intelligence (IAI)}, 
  title={Large-Scale Pre-Training Improves Anomaly Detection in Fused Magnesium Smelting Process}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Reliable anomaly detection is essential for the fused magnesium smelting process. Existing researches typically pursuing network structure to improve model performance, while overlooking the influence of data scale for training. To tackle the problem of limited labeled data in real industrial scenarios, this paper utilizes network pre-trained on large-scale public dataset, ImageNet1K, alleviating the issue of insufficient samples for fused magnesium furnace anomaly detection. This paper also uses a large-scale fused magnesium furnace anomaly dataset FMF-Big for a baseline comparison. The empirically study shows that, even though the data in ImageNet1K differs greatly from the fused magnesium furnace scenario, the pre-trained model brought a 1.81% increase in accuracy (97.59% detection accuracy). To our best knowledge, this is the first exploration to investigate the impact of large-scale pre-training on the anomaly detection in fused magnesium smelting process. We believe the study will have inspiring implications for subsequent researches.},
  keywords={Training;Accuracy;Furnaces;Smelting;Transformers;Data models;Magnesium;Reliability;Artificial intelligence;Anomaly detection;anomaly detection;fused magnesium furnace;pre-training;vision transformer;large model},
  doi={10.1109/IAI63275.2024.10730539},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10849417,
  author={Sutariya, Jayam and Flourens, Cooper and Thom, Nathan and Hand, Emily},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Face Verification with Veridical and Caricatured Images using Prominent Attributes}, 
  year={2024},
  volume={},
  number={},
  pages={1038-1047},
  abstract={Caricatures, with their exaggerated features, offer an efficient means for individuals to recognize each other compared to veridical (real) images. However, matching veridical images to caricatures remains a challenging task in machine learning. This difficulty stems from poor quality caricature datasets lacking clear labels, inadequate labeling in widely used veridical image datasets like CelebA, and a shift away from attribute-based representations due to the rise of neural networks. These issues significantly impact the accuracy of face verification tasks between caricatures and veridical images. To address these challenges, this paper introduces a classification protocol for prominent facial feature recognition and a verification protocol for matching celebrity veridical images to their caricatures. We utilize CarVer, a recently curated dataset comprising both veridical and caricature images accompanied by detailed prominent attribute labels. Our approach aims to develop a set of prominent facial attributes that can effectively represent both real and caricatured images, enabling improved face verification across these modalities. This research has potential applications across various industries where robust cross-modal face recognition is crucial.},
  keywords={Industries;Protocols;Image recognition;Accuracy;Face recognition;Neural networks;Machine learning;Learning (artificial intelligence);Labeling;Facial features;face verification;facial features;caricatures;AI explainability;multi-label learning},
  doi={10.1109/ICTAI62512.2024.00148},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{11081343,
  author={Kavitha, M. and Raushan, Ritik and Priya and Kumar, Rahul},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={DEFENDAI: Enhancing Deepfake Detection with Hybrid Architectures using EfficientNet, XceptionNet, Vision Transform and LSTM}, 
  year={2025},
  volume={},
  number={},
  pages={952-958},
  abstract={The rapid evolution of deepfake technologies presents a significant threat to digital authenticity and societal trust. This research introduces DEFENDAI, a hybrid deepfake detection system integrating EfficientNet, XceptionNet, Vision Transformers (ViT), and Long Short-Term Memory (LSTM) networks to harness both spatial and temporal features. Trained on a curated subset of the DFDC dataset, the system uses a multi-stage pipeline involving frame extraction, resizing, normalization, and augmentation for preprocessing. Feature extraction is performed using CNNs and ViTs, while LSTM captures sequential dependencies across frames. A web-based implementation using FastAPI and Streamlit enables real-time user interaction and deployment. Comparative results show the hybrid model significantly outperforms standalone models in precision, recall, and generalization. Future work aims at extending datasets, integrating adversarial robustness, and refining real-time detection pipelines for broader applicability.},
  keywords={Training;Deepfakes;Computer vision;Pipelines;Transforms;Feature extraction;Transformers;Real-time systems;Artificial intelligence;Long short term memory;Deepfake Detection;Vision Transformers;EfficientNet;XceptionNet;LSTM;DFDC;FastAPI;Streamlit;AI Security},
  doi={10.1109/ICSSAS66150.2025.11081343},
  ISSN={},
  month={June},}@ARTICLE{10753449,
  author={Liu, Hongmin and Zhang, Canbin and Fan, Bin and Xu, Jinglin},
  journal={IEEE Transactions on Image Processing}, 
  title={Pro2Diff: Proposal Propagation for Multi-Object Tracking via the Diffusion Model}, 
  year={2024},
  volume={33},
  number={},
  pages={6508-6520},
  abstract={Multi-object tracking (MOT) aims to estimate the bounding boxes and ID labels of objects in videos. The challenging issue in this task is to alleviate competitive learning between the detection and tracking subtasks, for which, two-stage Tracking-By-Detection (TBD) optimizes the two subtasks individually, and the single-stage Joint Detection and Tracking (JDT) adjusts the complex network architectures finely in an end-to-end pipeline. In this paper, we propose a new MOT method, i.e., Proposal Propagation via Diffusion Models, called Pro2Diff, which integrates a diffusion model into the proposal propagation in multi-object tracking, focusing on the model training process rather than complex network design. Specifically, using a generative approach, Pro2Diff generates a considerable number of noisy proposals for the tracking image sequence in the forward process, and subsequently, Pro2Diff learns the discrepancies between these noisy proposals and the actual bounding boxes of the tracked objects, gradually optimizing these noisy proposals to obtain the initial sequence of real tracked objects. By introducing the denoising diffusion process into multi-object tracking, we have made three further important findings: 1) Generative methods can effectively handle multi-object tracking tasks; 2) Without the need to modify the model structure, we propose self-conditional proposal propagation to enhance model performance effectively during inference; 3) By adjusting the numbers of proposals and iterations appropriately for different tracking sequences, the optimal performance of the model can be achieved. Extensive experimental results on MOT17 and DanceTrack datasets demonstrate that Pro2Diff outperforms current end-to-end multi-object tracking methods. We achieve 61.9 HOTA on DanceTrack and 57.6 HOTA on MOT17, reaching the competitive result of the JDT approach.},
  keywords={Training;Noise reduction;Noise;Diffusion processes;Complex networks;Benchmark testing;Diffusion models;Proposals;Noise measurement;Videos;Multi-object tracking;proposal propagation;diffusion models},
  doi={10.1109/TIP.2024.3494600},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10959194,
  author={Hadzhikoleva, Stanka and Borisova, Maria and Hadzhikolev, Emil and Raykova, Zhelyazka},
  booktitle={2025 24th International Symposium INFOTEH-JAHORINA (INFOTEH)}, 
  title={Building and Utilizing a Specialized Chatbot in Physics Education – Experiment and Results}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This article presents an experiment involving the development and using of a specialized chatbot, named PhysicsGPT, in physics education. The experiment was conducted with 7th-grade students during their study of the topic "Light and Sound". PhysicsGPT was created using the Zapier platform, which allows for the creation of chatbots without programming. The chatbot provides students with personalized guidance, examples, and facts, assisting them in understanding the material. The study involved 22 students who used PhysicsGPT to develop coursework on a topic related to the total internal reflection of light. A survey of students' opinions revealed that they highly valued the chatbot's usefulness and ease of use. A significant portion of the students found the information provided to be relevant and easy to understand. The analysis uncovered a moderate correlation between the chatbot's ease of use and its perceived usefulness, as well as between the understanding of the lesson and the overall impression of working with the chatbot.},
  keywords={Surveys;Correlation;Generative AI;Education;Buildings;Chatbots;Reflection;Physics education;Programming profession;Astronomy;physics education;specialized chatbot;AI tools},
  doi={10.1109/INFOTEH64129.2025.10959194},
  ISSN={2767-9470},
  month={March},}@INPROCEEDINGS{7354009,
  author={Luperto, Matteo and D'Emilio, Leone and Amigoni, Francesco},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={A generative spectral model for semantic mapping of buildings}, 
  year={2015},
  volume={},
  number={},
  pages={4451-4458},
  abstract={Consider a mobile robot exploring an initially unknown school building and assume that it has already discovered some classrooms, offices, and bathrooms. What can the robot infer about the presence and the locations of other classrooms and offices in the school building? This paper makes a step toward providing an answer to the above question by proposing a system based on a generative model that is able to represent the topological structures and the semantic labeling schemas of buildings and to predict the structure and the schema for unexplored portions of these environments. We represent the buildings as undirected graphs, whose nodes are rooms and edges are physical connections between them. Given an initial knowledge base of graphs, our approach, relying on a spectral analysis of these graphs, segments each graph for finding significant subgraphs and clusters them according to their similarity. A graph representing a new building or an unvisited part of a building is eventually generated by sampling subgraphs from clusters and connecting them.},
  keywords={Buildings;Semantics;Robot sensing systems;Labeling;Feature extraction;Knowledge based systems},
  doi={10.1109/IROS.2015.7354009},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9788144,
  author={Kantheti, Bhargav and Javvaji, Mukesh Kumar},
  booktitle={2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={Medical Image Classification for Disease Prediction with the aid of Deep Learning approaches}, 
  year={2022},
  volume={},
  number={},
  pages={1442-1445},
  abstract={In the medical and health care system, the demand for medical imaging has risen way above any chart; e very field of medical science needs an image to understand the problems and diseases better, for example, Computed Tomography Scan (better known as CT scan), Radiography, Magnetic Resonance Imaging (MRI), X-Rays, etc. These are now done by medical images using artificial intelligence and machine learning which also includes Convolutional Neural Network (CNN), which plays a significant role in producing these images to predict diseases. Without being precisely programmed, machine learning takes input data, learns from it, and then predicts based on it. The prominent role of the images is to detect the disease in a visualized form, and after that, it is beneficial for the doctor to detect any irregularities and the machine learning and deep learning models to detect the difference from normal. The paper’s primary purpose is to find an improved accuracy for the deep learning models and compare the artificial neural network with convolutional neural network and deep neural network and prove which is better for the following problem statement.},
  keywords={Deep learning;Convolution;Computed tomography;Magnetic resonance imaging;X-rays;Feature extraction;Convolutional neural networks;Medical Image Classification;Deep Learning;Neural Networks;Artificial Neural Network;Deep Neural Network;Convolution Neural Network;Data Science},
  doi={10.1109/ICICCS53718.2022.9788144},
  ISSN={2768-5330},
  month={May},}@INPROCEEDINGS{11094871,
  author={Chen, Tianxing and Mu, Yao and Liang, Zhixuan and Chen, Zanxin and Peng, Shijia and Chen, Qiangyu and Xu, Mingkun and Hu, Ruizhen and Zhang, Hongyuan and Li, Xuelong and Luo, Ping},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation}, 
  year={2025},
  volume={},
  number={},
  pages={1735-1744},
  abstract={Recent advances in imitation learning for 3D robotic manipulation have shown promising results with diffusion-based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and semantic understanding. We present G3Flow, a novel framework that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation requirements. By incorporating semantic flow into diffusion policies, extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% success rates on terminal-constrained manipulation and cross-object generalization respectively. Our results demonstrate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic policies.},
  keywords={Solid modeling;Three-dimensional displays;Foundation models;Annotations;Semantics;Manuals;Feature extraction;Real-time systems;Digital twins;Pattern recognition;robotic manipulation;semantic flow;diffusion policy;3d manipulation;semantic representation},
  doi={10.1109/CVPR52734.2025.00169},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10541961,
  author={Wu, Zi-Wei and Qu, Huamin and Zhang, Kang},
  journal={Artificial Life}, 
  title={A Survey of Recent Practice of Artificial Life in Visual Art}, 
  year={2024},
  volume={30},
  number={1},
  pages={106-135},
  abstract={Nowadays, interdisciplinary fields between Artificial Life, artificial intelligence, computational biology, and synthetic biology are increasingly emerging into public view. It is necessary to reconsider the relations between the material body, identity, the natural world, and the concept of life. Art is known to pave the way to exploring and conveying new possibilities. This survey provides a literature review on recent works of Artificial Life in visual art during the past 40 years, specifically in the computational and software domain. Having proposed a set of criteria and a taxonomy, we briefly analyze representative artworks of different categories. We aim to provide a systematic overview of how artists are understanding nature and creating new life with modern technology.},
  keywords={ALife art;generative art;AI art;nature},
  doi={10.1162/artl_a_00433},
  ISSN={1064-5462},
  month={Feb},}@INPROCEEDINGS{10411531,
  author={Li, Linfeng and Liu, Hai and Zhang, Zhaoli},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Anisotropic Knowledge Empowers Efficient Recommender System through Generative-Mapping Representation Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1352-1359},
  abstract={This work tackles two crucial challenges in representation learning-based rating prediction for engineering recommender systems. Firstly, it addresses the disparity between the ideal rating distribution and the actual scattered-grain rating distribution by incorporating a Poisson prior generative paradigm that assigns higher a priori representations to higher rating values, rather than treating all rating forms equally. Secondly, it explores the interactivity between the rating matrix and contextual information by mapping them through a low-dimensional real-valued vector, enabling a deeper understanding of users’ interests and hidden interest relationships. Graph convolutional networks are employed to mitigate the impact of user preferences and enhance the estimation of interaction factors. Experimental results on six benchmark datasets demonstrate the proposed method’s effectiveness in improving recommendation accuracy and novelty when replacing user decisions.},
  keywords={Representation learning;Training;Filtering;Computational modeling;Task analysis;Recommender systems;Tuning;poisson distribution;recommender system;frame-level features;anisotropic feature interaction;collaborative filtering},
  doi={10.1109/ICDMW60847.2023.00173},
  ISSN={2375-9259},
  month={Dec},}@ARTICLE{9993791,
  author={Huang, Jiaqi and Huang, Qiushi and Mou, Gaoyang and Wu, Chenye},
  journal={IEEE Transactions on Smart Grid}, 
  title={DPWGAN: High-Quality Load Profiles Synthesis With Differential Privacy Guarantees}, 
  year={2023},
  volume={14},
  number={4},
  pages={3283-3295},
  abstract={Smart meters have collected massive amounts of fine-grained load data from users, enabling various load profile analyses that can help improve the efficiency of smart grids. However, the smart meter data may leak private information, raising public concerns. To address this issue, current approaches typically employ data perturbation mechanisms or data generation mechanisms to ensure privacy when analyzing load profiles, but these approaches are either inflexible or not guaranteed to mitigate the leakage issue. To this end, we propose a differentially private Wasserstein Generative Adversarial Networks (DPWGAN) approach in this study. This approach can privately convert a real-world dataset into a high-quality synthetic load dataset so that studies and analyses conducted on the synthetic dataset can automatically satisfy user-level differential privacy guarantees. The extensive numerical studies highlight that our approach acts as an excellent substitute for the original dataset in real-world load profiling tasks.},
  keywords={Differential privacy;Privacy;Generative adversarial networks;Training;Smart meters;Data analysis;Synthetic data;Data synthesis;GAN;differential privacy;load profiling},
  doi={10.1109/TSG.2022.3230671},
  ISSN={1949-3061},
  month={July},}@INPROCEEDINGS{10569110,
  author={Iglesias, José Antonio and Monterrubio, José María and Sesmero, María Paz and Sanchis, Araceli},
  booktitle={2024 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS)}, 
  title={Generation and Evaluation of Medical Images Based on Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Within the framework of evolving intelligent systems, the work on generating and evaluating synthetic medical images using diffusion models represents a significant step towards overcoming the challenge of data scarcity, a critical obstacle in current medical research. In this paper, a system for generating and evaluating synthetic medical images using diffusion models is presented. It addresses the challenge of insufficient public medical datasets, proposing artificial intelligence models to generate synthetic medical data. This aims to overcome current research limitations due to data scarcity. The research of this paper includes the development of a system architecture and a proof of concept, assessing the feasibility of using synthetic images in healthcare.},
  keywords={Training;Ethics;Adaptation models;Adaptive systems;Systems architecture;Medical services;Data models},
  doi={10.1109/EAIS58494.2024.10569110},
  ISSN={2473-4691},
  month={May},}@INPROCEEDINGS{9002791,
  author={Gu, Ziqing and An, Yunlong and Tan, Fangyuan and Li, Yang and Zheng, Sifa},
  booktitle={2019 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={A Game Theory Approach to Attack-Defense Strategy for Perception of Connected Vehicles}, 
  year={2019},
  volume={},
  number={},
  pages={2587-2594},
  abstract={The malicious attacks to vehicle communication facilities can expose connected vehicles to security threats, making them lose control and causing traffic accidents. It is usually hard to predict the attacked targets, which makes it difficult to design a proactive defensive strategy. This study tackles the issue of connected vehicles under bounded data injection attacks. We propose an active attack-defense model based on game theory and solve the model using a reinforcement learning-based method. The vehicle-attacker system is modeled as a game-theoretic framework based on the generalized weakened fictitious play, in which each player uses the best actions to react to the opponents’ empirical actions. A novel mixed adversarial reinforcement learning is employed to learn the attack-defense model which makes the model self-evolutionary. Moreover, our model is validated in a car-following scenario through numerical simulations and comparative studies under different forms of attacks. Results show that our model can recognize the attacked sensors and the defensive effectiveness of connected vehicles increases by nearly 30% under bounded attacks.},
  keywords={Handheld computers;Computational intelligence;Game theory;Learning (artificial intelligence);Markov processes;connected vehicles;attack and defense;game theory;generalized weakened fictitious play;adversarial reinforcement learning.},
  doi={10.1109/SSCI44817.2019.9002791},
  ISSN={},
  month={Dec},}@ARTICLE{10445254,
  author={Ye, Tian},
  journal={IEEE Access}, 
  title={The Analysis of Optimization Strategy of Industrial Design in Automatic Sketch Generation Based on Deep Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={32361-32368},
  abstract={This study is devoted to exploring the strategy of automatic sketch generation and optimization of industrial design based on deep learning. By combining the Generative Adversarial Network (GAN) with the optimization algorithm, this paper proposes an innovative method to realize the automatic generation of high-quality and diverse industrial design sketches. In the experiment, this paper selects SketchyCAD and other public data sets, trains them through deep learning model, and introduces genetic algorithm(GA) and differential evolution algorithm to optimize the parameters. In terms of experimental results, we observed that the quality of generated sketches was significantly improved, and the design sketches generated by the mode (GAN+GA) were more realistic and innovative. The introduction of optimization strategy further improves the generation effect and intelligently adjusts the model parameters to adapt to different design styles. In this paper, the influence of hyperparameter tuning is analyzed in detail, and it is found that the adjustment of learning rate plays a key role in generating quality and diversity. However, the experiment also revealed some challenges and room for improvement. We noticed that the generated results may have the risk of over-fitting in the training process, and with the increase of training times, the diversity gradually decreased. This suggests that more complex model structure and richer data sets are needed to improve the generalization performance. Generally speaking, this study provides new ideas and methods for the integration of deep learning and industrial design. By innovatively combining generation model and optimization algorithm, this research has contributed beneficial research results to the development of industrial design automation. This research is of great significance to promote the intelligence and innovation in the field of industrial design.},
  keywords={Deep learning;Optimization;Data models;Convolutional neural networks;Generative adversarial networks;Shape measurement;Deep learning;Genetic algorithms;Optimization methods;Industrial engineering;Design engineering;Automatic programming;Graphics;Deep learning;sketch generation;optimization strategy;industrial design;generative adversarial network;genetic algorithm},
  doi={10.1109/ACCESS.2024.3370438},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8461918,
  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Chen, Yun-Nung and Wong, Kam-Fai},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning}, 
  year={2018},
  volume={},
  number={},
  pages={6149-6153},
  abstract={This paper presents a new method - adversarial advantage actor-critic (Adversarial A2C), which significantly improves the efficiency of dialogue policy learning in task-completion dialogue systems. Inspired by generative adversarial networks (GAN), we train a discriminator to differentiate responses/actions generated by dialogue agents from responses/actions by experts. Then, we incorporate the discriminator as another critic into the advantage actor-critic (A2C) framework, to encourage the dialogue agent to explore state-action within the regions where the agent takes actions similar to those of the experts. Experimental results in a movie-ticket booking domain show that the proposed Adversarial A2C can accelerate policy exploration efficiently.},
  keywords={Task analysis;Training;Gallium nitride;Learning (artificial intelligence);Natural languages;Motion pictures;Trajectory;task-completion dialogue;reward function;adversarial learning;policy learning;reinforcement learning},
  doi={10.1109/ICASSP.2018.8461918},
  ISSN={2379-190X},
  month={April},}@ARTICLE{9770451,
  author={Saif, Shahela and Tehseen, Samabia and Ali, Syed Sohaib and Kausar, Sumaira and Jameel, Amina},
  journal={IT Professional}, 
  title={Generalized Deepfake Video Detection Through Time-Distribution and Metric Learning}, 
  year={2022},
  volume={24},
  number={2},
  pages={38-44},
  abstract={Rapid advancements in the field of computer vision and AI have enabled the creation of synthesized images and videos known as deepfakes. Deepfakes are used as a source of spreading false news and misinformation. The constant evolution of generative models, used for creating deepfakes, makes it difficult and yet very important to find effective generalized solutions for such deepfake videos. We have designed a generalized deepfake detector by creating a two-stream network that uses CNN-LSTM as its backbone. Our contributions in this article are twofold: 1) using a time-distributed network to create representations using spatial and temporal information of a video, and 2) improving the discriminative ability of the extracted feature embeddings by using metric learning during training. Results gathered through extensive experiments show the effectiveness of our solution even on a cross-modal FaceForensic++ dataset proving the generalization ability of the solution.},
  keywords={Computer vision;Training data;Artificial intelligence;Computational modeling;Detectors;Feature extraction;Data mining;Deepfakes;Fake news},
  doi={10.1109/MITP.2022.3168351},
  ISSN={1941-045X},
  month={March},}@INPROCEEDINGS{9859350,
  author={Upadhyay, Abhinav and Dubey, Alpana and Arora, Veenu and Kuriakose, Suma Mani and Agarawal, Shaurya},
  booktitle={2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={FLNet: Graph Constrained Floor Layout Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={In this work, we propose a generative-based approach, FLNet, to synthesize floor layout plans guided by user constraints. Our approach considers user inputs in the form of boundary, room types, and spatial relationships and generates the layout design satisfying these requirements. We evaluated our approach on floor plans data, RPLAN, consisting of 80,000 vector-graphics floor plans of residential buildings designed by professional architects. We perform both qualitative and quantitative analysis along three metrics - Layout generation accuracy, Realism, and Quality to evaluate the generated layout designs. We compare our approach with the existing baselines and outperform on all these metrics. The layout designs generated by our approach are more realistic and of better quality.},
  keywords={Measurement;Statistical analysis;Conferences;Layout;Buildings;Artificial intelligence;Floors;Floor Plan;AI Design;GCN},
  doi={10.1109/ICMEW56448.2022.9859350},
  ISSN={},
  month={July},}@ARTICLE{9516689,
  author={Zhang, Lefei and Lan, Meng and Zhang, Jing and Tao, Dacheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Stagewise Unsupervised Domain Adaptation With Adversarial Self-Training for Road Segmentation of Remote-Sensing Images}, 
  year={2022},
  volume={60},
  number={},
  pages={1-13},
  abstract={Road segmentation from remote-sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this article, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GANs)-based interdomain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intradomain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of the target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intradomain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods. The code is available at https://github.com/LANMNG/RoadDA.},
  keywords={Roads;Image segmentation;Adaptation models;Task analysis;Data models;Feature extraction;Predictive models;Remote sensing (RS);road segmentation;self-training;unsupervised domain adaptation (UDA)},
  doi={10.1109/TGRS.2021.3104032},
  ISSN={1558-0644},
  month={},}@ARTICLE{9583266,
  author={Tian, Xiaoyang and Shao, Jie and Ouyang, Deqiang and Shen, Heng Tao},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={UAV-Satellite View Synthesis for Cross-View Geo-Localization}, 
  year={2022},
  volume={32},
  number={7},
  pages={4804-4815},
  abstract={The goal of cross-view image matching based on geo-localization is to determine the location of a given ground-view image (front view) by matching it with a group of satellite-view images (vertical view) with geographic tags. Due to the rapid development of unmanned aerial vehicle (UAV) technology in recent years, it has provided a real viewpoint close to 45 degrees (oblique view) to bridge the visual gap between views. However, existing methods ignore the direct geometric space correspondence of UAV-satellite views, and only use brute force for feature matching, leading to inferior performance. In this context, we propose an end-to-end cross-view matching method that integrates cross-view synthesis module and geo-localization module, which fully considers the spatial correspondence of UAV-satellite views and the surrounding area information. To be specific, the cross-view synthesis module includes two parts: the oblique view of UAV is first converted to the vertical view by perspective projection transformation (PPT), which makes the UAV image closer to the satellite image; then we use conditional generative adversarial nets (CGAN) to synthesize the UAV image with vertical view style, which is close to the real satellite image by learning the converted UAV as the input image and the real satellite image as the label. Geo-localization module refers to existing local pattern network (LPN), which explicitly considers the surrounding environment of the target building. These modules are integrated in a single architecture called PCL, which mutually reinforce each other. Our method is superior to the existing UAV-satellite cross-view methods, which improves by about 5%.},
  keywords={Satellites;Unmanned aerial vehicles;Feature extraction;Task analysis;Location awareness;Image matching;Visualization;Cross-view image matching;geo-localization;image synthesis},
  doi={10.1109/TCSVT.2021.3121987},
  ISSN={1558-2205},
  month={July},}@ARTICLE{9084254,
  author={Du, Yingjun and Xu, Jun and Zhen, Xiantong and Cheng, Ming-Ming and Shao, Ling},
  journal={IEEE Transactions on Image Processing}, 
  title={Conditional Variational Image Deraining}, 
  year={2020},
  volume={29},
  number={},
  pages={6288-6301},
  abstract={Image deraining is an important yet challenging image processing task. Though deterministic image deraining methods are developed with encouraging performance, they are infeasible to learn flexible representations for probabilistic inference and diverse predictions. Besides, rain intensity varies both in spatial locations and across color channels, making this task more difficult. In this paper, we propose a Conditional Variational Image Deraining (CVID) network for better deraining performance, leveraging the exclusive generative ability of Conditional Variational Auto-Encoder (CVAE) on providing diverse predictions for the rainy image. To perform spatially adaptive deraining, we propose a spatial density estimation (SDE) module to estimate a rain density map for each image. Since rain density varies across different color channels, we also propose a channel-wise (CW) deraining scheme. Experiments on synthesized and real-world datasets show that the proposed CVID network achieves much better performance than previous deterministic methods on image deraining. Extensive ablation studies validate the effectiveness of the proposed SDE module and CW scheme in our CVID network. The code is available at https://github.com/Yingjun-Du/VID.},
  keywords={Rain;Estimation;Optimization;Learning systems;Image color analysis;Decoding;Channel estimation;Conditional variational auto-encoder;single image deraining;spatial attention map;channel-wise deraining},
  doi={10.1109/TIP.2020.2990606},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10309548,
  author={Shangguan, Zhegong and Ding, Mengyuan and Yu, Chuang and Chen, Chaona and Tapus, Adriana},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Robot self-recognition via facial expression sensorimotor learning}, 
  year={2023},
  volume={},
  number={},
  pages={2591-2597},
  abstract={To develop robots that can show cognitive functions, we must learn from the knowledge of human cognition. Existing biological and psychological evidence suggests that self-face perception and sensorimotor learning mechanisms play a crucial role in self-recognition. However, one of the most important self-identity cues – facial information – has not been extensively studied in the robot self-recognition task. Current research on robot self-recognition primarily relies on the recognition of high-precision targets and tracking of manipulator motions, where the self-perception of facial information is not well studied. In this work, we propose a novel approach to achieve self-recognition via self-perception of facial expressions. Specifically, we developed a Conditional Generative Adversarial Network (CGAN) model using the knowledge on human cognitive and sensorimotor functions. It allows the robot to be aware of self-face (i.e., off-line model). Passing the observed visual variations in a mirror and comparing them to self-perceptive information, the robot can recognize the self through an online Bayesian learning regression. The results of our first experiment show that the robot can recognize itself in a mirror. The results from the second experiment show that our algorithm could be tricked by a similar robot with the same facial expressions, which is similar to the rubber hand illusion (RHI).},
  keywords={Visualization;Target tracking;Face recognition;Robot sensing systems;Prediction algorithms;Inference algorithms;Bayes methods},
  doi={10.1109/RO-MAN57019.2023.10309548},
  ISSN={1944-9437},
  month={Aug},}@ARTICLE{10955274,
  author={Zhao, Xiaoyan and Mansor, Zulkefli and Razali, Rozilawati and Zakree Ahmad Nazri, Mohd and Xiong, Xin},
  journal={IEEE Access}, 
  title={Advancing Agile Software Cost Estimation Through Data Synthesis: A Comparative Analysis of Five Generation Techniques}, 
  year={2025},
  volume={13},
  number={},
  pages={63219-63236},
  abstract={Agile has been used in software development for over 20 years and is the preferred development method for more than 85% of software companies. However, cost estimation in agile development remains a significant challenge. This is reflected in the fact that the accuracy of estimation still needs improvement, and most cost estimation techniques still rely on the team’s experience and knowledge. While machine learning algorithms have performed better in this area, the lack of sufficient agile cost data hinders large-scale training and in-depth research. To address this issue, this study selected five data generation techniques—Variational Autoencoder (VAE), Wasserstein Generative Adversarial Network (WGAN), Synthetic Minority Over-sampling Technique for Nominal and Continuous Features (SMOTE-NC), Data Augmentation for Tabular Data (Augmentation), and Tabular Data Diffusion Probabilistic Models (TabDDPM)—based on the characteristics of agile cost data. Using cost data from 75 agile projects, these techniques were employed to generate three sets of data with sizes of 200, 500, and 1000. A performance evaluation model was created based on consistency, authenticity, diversity, and effectiveness to verify the performance of these generated data. The experimental results show that WGAN consistently scored 16 out of 20 points across all three data sets, excelling in data consistency and authenticity. SMOTE-NC and Augmentation Were followed. SMOTE-NC scored 15 out of 20 points for all data sizes and performed best in terms of effectiveness, with an MMRE of 88.16% and a PRED (0.2) of 84.5%. Augmentation performed the best when generating 1000 data points. These findings highlight the potential of data generation technologies, particularly WGAN, in enhancing agile cost estimation and providing guidance on selecting the appropriate amount of data. This lays a foundation for further development of machine learning algorithms in this field and offers valuable insights for other researchers.},
  keywords={Costs;Estimation;Generative adversarial networks;Training;Agile software development;Data models;Hidden Markov models;Data collection;Data augmentation;Autoencoders;Data generation technique;software cost estimation;agile software development;variational autoencoder;Wasserstein generative adversarial network;synthetic minority over-sampling technique for nominal and continuous features;data augmentation for tabular data;tabular data diffusion probabilistic model},
  doi={10.1109/ACCESS.2025.3558715},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10937453,
  author={Chen, Xiaoming and Han, Dehao and Qu, Qiang and Shen, Yiran},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VF-Lens: Enhancing Visual Perception of Visually Impaired Users in VR via Adversarial Learning with Visual Field Attention}, 
  year={2025},
  volume={},
  number={},
  pages={420-430},
  abstract={This research aims to enhance the image perception of visually impaired users in VR environments. We propose VF-Lens, a model that adaptively compensates for light sensitivity based on the user’s visual field impairment, acting as a virtual lens between the visually impaired users and the VR world. VF-Lens is designed as a tailored generative adversarial learning model with a generator and discriminator, offering applicability to various types of visual impairments while bypassing engineering complexities. The generator creates a "hyperimage" tailored to the user’s visual field impairment, which then undergoes a particular regression process to predict and replicate the real perception of the visually impaired user. The discriminator then evaluates the similarity between the replicated perception and the original image. Through adversarial training, the generator can produce hyperimages that adapt to the user’s visual field parameters, enabling them to perceive the image more similarly to normal-vision users. We further improve VF-Lens by proposing new "visual field attention" mechanisms that prioritize and refine visual information in the user’s visual field. Extensive evaluation, encompassing both visually impaired participants and simulations, has been conducted to demonstrate the effectiveness of VF-Lens in improving visual perception for visually impaired users. Moreover, we establish a standardized evaluation process involving tailored metrics as well as objective and subjective evaluations to promote reusability and comparability for future research in this field.},
  keywords={Human computer interaction;Training;Visualization;Adaptation models;Three-dimensional displays;Visual impairment;Virtual reality;Generators;Adversarial machine learning;Visual perception;Human-centered computing—Human computer interaction (HCI);Human-centered computing—Virtual Reality},
  doi={10.1109/VR59515.2025.00066},
  ISSN={2642-5254},
  month={March},}@ARTICLE{11048489,
  author={Luo, Jingfeng and You, Chengwan and Zheng, Bochuan},
  journal={IEEE Access}, 
  title={Adversarial Dual-Distortion Image Rectification: Integrating Multi-Scale Discriminators and Central Self-Attention Modules}, 
  year={2025},
  volume={13},
  number={},
  pages={109172-109184},
  abstract={Fisheye cameras, leveraging their wide-angle imaging advantages, have been widely adopted in industrial pipeline inspection, particularly for monitoring the inner wall conditions of cylindrical pipelines such as those in oil and gas wells. To achieve geometric reconstruction of pipeline wall images captured by fisheye lenses, this study proposes a method aimed at simultaneously correcting pipeline wall deformations (caused by cylindrical geometry) and fisheye lens distortions. Literature research reveals that fisheye distortion correction for pipeline wall images remains in its infancy, lacking real-scene datasets and data synthesis methods to support deep learning model training. Addressing this bottleneck, we constructed a dual-distortion mathematical model based on geometric optics principles and proposed a parametric simulation-based synthesis method to generate a specialized synthetic dataset tailored for training fisheye distortion correction networks. Existing fisheye correction methods exhibit performance degradation when processing dual-distortion images. To overcome this, we innovatively designed a central self-attention module adapted to fisheye imaging characteristics. This module dynamically enhances geometric feature extraction in the central region of distorted images, effectively suppressing distortion diffusion effects in peripheral areas. Additionally, we introduced a multi-scale discriminator with a pyramidal feature fusion architecture to strengthen the synergistic optimization of local textures and global structures in generative adversarial networks. Experimental results demonstrate that, compared to state-of-the-art correction methods, our approach achieves significant improvements in both objective quantitative metrics (e.g., SSIM, PSNR) and visual consistency. These findings validate the method’s robust capability to effectively correct fisheye-distorted pipeline wall images for industrial inspection applications, providing a reliable solution for related fields.},
  keywords={Distortion;Pipelines;Lenses;Training;Convolutional neural networks;Cameras;Feature extraction;Calibration;Oils;Generative adversarial networks;Fisheye distortion correction;generative adversarial networks;pipeline wall images;multi-scale discriminator;central self-attention},
  doi={10.1109/ACCESS.2025.3582046},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9609002,
  author={Liao, Jiaxuan and Wang, Xiaoping},
  booktitle={2021 International Conference on Neuromorphic Computing (ICNC)}, 
  title={Self-Supervised GAN For Occluded Facial Expression Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={386-393},
  abstract={Facial occlusion is an important reason for the accuracy decline of facial emotion recognition in the wild. Nowadays almost all existing occluded facial emotion recognition methods are based on supervised learning. To solve this problem, we propose a generative adversarial network based framework in this paper to perform occluded facial expression recognition in a self-supervised way. The framework is composed of one generator G and three discriminators Do, D1, D2, the generator G is to complete occluded facial part and has an encoder-decoder architecture. Three discriminators have same network structure, the real-fake discriminator D0 is to identify whether input image is generated, the mask discriminator D1 is to identify whether occlusion exists in image, the emotion discriminator D2 is a classifier which used to output emotion label. Four loss functions are used in training process, which are adversarial loss Ladv; pixel loss Lpixel; mask loss Lmaskand classification loss Lc. To verify the effectiveness of this proposed method, experiments are designed on two datasets: CK+ and AffectNet, and we analyze the results of experiments, which proves that the framework this paper proposed can effectively reduce the influence of facial occlusion on emotion recognition.},
  keywords={Training;Emotion recognition;Image recognition;Costs;Neuromorphic engineering;Face recognition;Supervised learning;expression recognition;occluded;self-supervised;GAN},
  doi={10.1109/ICNC52316.2021.9609002},
  ISSN={},
  month={Oct},}@ARTICLE{10991966,
  author={Sharma, Yogesh Kumar and Tomar, Deepak Singh and Pateriya, R. K. and Solanki, Surendra},
  journal={IEEE Access}, 
  title={GNSTAM: Integrating Graph Networks With Spatial and Temporal Signature Analysis for Enhanced Android Malware Detection}, 
  year={2025},
  volume={13},
  number={},
  pages={81326-81346},
  abstract={The sophistication of Android malware poses significant threats to user security and privacy. Traditional detection methods struggle with rapid malware evolution and benign application diversity, leading to high false positive rates and limited adaptability. This paper introduces a hybrid methodology leveraging advanced machine learning techniques to enhance accuracy and adaptability in Android malware detection. It begins with collecting and preprocessing a comprehensive dataset of benign and malicious applications. An efficient Generative Adversarial Network (GAN) is employed to generate synthetic malware samples, effectively augmenting the dataset and enhancing the diversity of the malware samples under study process. To model the intricate relationships between applications, an efficient Graph Neural Network (GNN) process is utilized. Incorporating transformers, sequences of system and API calls are analyzed, harnessing this ability to discern patterns indicative of malicious activities. Additionally, a one-shot learning model tailored for the detection of new malware variants with minimal examples is introduced, enabling rapid adaptation to emerging threats. Federated learning preserves user privacy by training the model across a distributed network. A reinforcement learning model initiates proactive defenses, identifying optimal actions against malware threats. This methodology advances Android malware detection, showing over 5.9% improvement in detection accuracy, 4.5% reduction in false positives, and enhanced adaptability to new malware variants. It ensures enhanced security for Android users while preserving privacy. Evaluation results highlight its practical applicability in real-time scenarios.},
  keywords={Malware;Machine learning;Feature extraction;Adaptation models;Accuracy;Privacy;Federated learning;Security;Graph neural networks;Generative adversarial networks;Android malware detection;generative adversarial network;graph neural network;federated learning;one-shot learning},
  doi={10.1109/ACCESS.2025.3567338},
  ISSN={2169-3536},
  month={},}@ARTICLE{10923691,
  author={Jeong, Seongyeop and Sung, June Sig and Hwang, Inchul and Choi, Jaesik},
  journal={IEEE Transactions on Audio, Speech and Language Processing}, 
  title={An Automated Method to Correct Artifacts in Neural Text-to-Speech Models}, 
  year={2025},
  volume={33},
  number={},
  pages={1375-1388},
  abstract={Recent advances in deep learning technology have enabled high-quality speech synthesis, and text-to-speech models are widely used in a variety of applications. However, even state-of-the-art models still produce artificial speech, highlighting the need to correct errors in synthesized speech. Traditional speech correction methodologies face a number of challenges, such as the inefficiency of manually specifying errors, the need to retrain models, and the need for additional data to correct synthesized speech errors. In this paper, we present a novel approach that detects and corrects contextual errors within a model to improve synthetic speech without requiring additional resources or model retraining. Specifically, we analyze the inherent limitations of neural network encoders responsible for contextualizing input sentences and propose a method that automatically identifies abnormal encoder context vectors. We also introduce a correction algorithm that enhances the quality of speech by correcting the incorrect relationships between phonemes that cause abnormal encoder context vectors. Our algorithm demonstrated a 25.86% and 4.69% reduction in alignment errors and a 2.25% and 0.63% improvement in objective metrics such as Fréchet Wav2Vec distance for the Tacotron2 and VITS models. In addition, our algorithm improved the comparative mean opinion scores, a subjective evaluation, by 1.34 and 0.52. The results support the feasibility of a novel approach to identify, correct, and improve defects automatically in neural speech synthesis models. The results support the feasibility of a novel approach to identify, correct, and improve defects automatically in neural speech synthesis models.},
  keywords={Vectors;Context modeling;Data models;Training;Analytical models;Neurons;Speech enhancement;Inference algorithms;Biological neural networks;Text to speech;Contextualization error;interpreting neural networks;speech artifacts;speech correction;text-to-speech},
  doi={10.1109/TASLPRO.2025.3550718},
  ISSN={2998-4173},
  month={},}@INPROCEEDINGS{11093000,
  author={Jia, Zexi and Huang, Chuanwei and Zhu, Yeshuang and Fei, Hongyan and Duan, Xiaoyue and Yuan, Zhiqiang and Deng, Ying and Zhang, Jiapei and Zhang, Jinchao and Zhou, Jie},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Secret Lies in Color: Enhancing AI-Generated Images Detection with Color Distribution Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={13445-13454},
  abstract={The advancement of Generative Adversarial Networks (GANs) and diffusion models significantly enhances the realism of synthetic images, driving progress in image processing and creative design. However, this progress also necessitates the development of effective detection methods, as synthetic images become increasingly difficult to distinguish from real ones. This difficulty leads to societal issues, such as the spread of misinformation, identity theft, and online fraud. While previous detection methods perform well on public benchmarks, they struggle with our benchmark, FakeART, particularly when dealing with the latest models and cross-domain tasks (e.g., photo-to-painting). To address this challenge, we develop a new synthetic image detection technique based on color distribution. Unlike real images, synthetic images often exhibit uneven color distribution. By employing color quantization and restoration techniques, we analyze the color differences before and after image restoration. We discover and prove that these differences closely relate to the uniformity of color distribution. Based on this finding, we extract effective color features and combine them with image features to create a detection model with only 1.4 million parameters. This model achieves state-of-the-art results across various evaluation benchmarks, including the challenging FakeART dataset.},
  keywords={Training;Technological innovation;Quantization (signal);Image color analysis;Identity theft;Semantics;Benchmark testing;Feature extraction;Image restoration;Pattern recognition;ai-generated images;synthetic images detecting;ai safety;color distribution},
  doi={10.1109/CVPR52734.2025.01255},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10653943,
  author={Hellinger, Rolf and Bischoff, Martin},
  booktitle={PCIM Europe 2024; International Exhibition and Conference for Power Electronics, Intelligent Motion, Renewable Energy and Energy Management}, 
  title={The Impact of AI on the Entire Power Electronics Lifecycle}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificial Intelligence (AI) is experiencing a surge in recent breakthroughs. Advances in generative AI enable machines to create texts, pictures, videos, and other digital assets that were unimaginable just few years ago. Its potential to transform the global economy is already evident. For industry, in particular for the power electronics sector, these innovations offer huge potential. We give an overview on the state of power electronics and where AI can help us to overcome the present main challenges. Considering the power electronics lifecycle, we highlight selected references where artificial intelligence is already generating added value. We discuss present trends showing how AI will help us to innovate, design, develop, manufacture, and operate our systems.},
  keywords={},
  doi={10.30420/566262480},
  ISSN={},
  month={June},}@ARTICLE{11071263,
  author={Elsharif, Wala and Alzubaidi, Mahmood and Agus, Marco},
  journal={IEEE Access}, 
  title={Cultural Bias in Text-to-Image Models: A Systematic Review of Bias Identification, Evaluation, and Mitigation Strategies}, 
  year={2025},
  volume={13},
  number={},
  pages={122636-122659},
  abstract={Despite their continuous advancements, text-to-image (TTI) models often reflect and reinforce cultural biases, perpetuating stereotypes often inherent in their training data. This systematic review critically examines cultural bias in text-to-image (TTI) models, addressing gaps in existing research by analyzing its manifestations, evaluation methods, and mitigation strategies—both directly and through the lens of intersectionality with other bias dimensions. A comprehensive literature review was conducted across multiple major databases, following a rigorously structured search strategy, resulting in the selection of 58 studies spanning bias analysis, evaluation frameworks, and mitigation techniques. Thematic findings highlight that gender bias was the most extensively studied, appearing in 53 studies (91%), followed by racial/ethnic bias (42 studies) and other social biases (41 studies). Furthermore, the review explores how these biases intersect and compound in AI-generated imagery, shaping and reinforcing cultural bias. Our findings reveal the following key aspects: 1) the lack of standardization and scalability in bias evaluation, 2) the lack of a fully effective mitigation strategy, 3) contributed TTI benchmarks favoring Western-centric perspectives. We finally propose future directions to improve fairness and representation in TTI models.},
  keywords={Cultural differences;Prevention and mitigation;Text to image;Systematic literature review;Solid modeling;Artificial intelligence;Computational modeling;Visualization;Natural language processing;Medical services;AI ethics;AI fairness;bias evaluation;bias mitigation;CLIP;cultural bias;generative AI;gender bias;prompt engineering;racial bias;responsible AI;text-to-image models},
  doi={10.1109/ACCESS.2025.3585745},
  ISSN={2169-3536},
  month={},}@ARTICLE{11104254,
  author={Agarwal, Ishita and Sakthivel, V. and Prakash, P.},
  journal={IEEE Access}, 
  title={Toward Inclusive Healthcare: An LLM-Based Multimodal Chatbot for Preliminary Diagnosis}, 
  year={2025},
  volume={13},
  number={},
  pages={136420-136432},
  abstract={This paper presents the design and development of a multimodal medical chatbot that leverages Gemini-2.0-Flash Model alongside a novel Retrieval-Augmented Generation (RAG) architecture to support preliminary medical diagnosis and recommendations. The system integrates textual prompt analysis and medical image interpretation, aiming to improve healthcare accessibility, particularly for underserved populations. Focused on data-rich medical conditions, the chatbot generates reliable diagnostic insights based on natural language inputs and/or medical images, requiring minimal user expertise. The proposed RAG-based architecture incorporates a curated medical knowledge base and structured retrieval mechanisms, significantly reducing hallucinations and enhancing response credibility compared to direct Large Language Model (LLM) querying. By demonstrating the efficacy of multimodal reasoning in conjunction with structured retrieval, this work paves the way for more accessible, accurate, and scalable AI-driven health support systems.},
  keywords={Medical diagnostic imaging;Chatbots;Medical services;Retrieval augmented generation;Accuracy;Oral communication;Artificial intelligence;Vectors;Training;Random forests;Gemini;generative AI;large language models;medical chatbot;medical diagnosis;retrieval augmented generation},
  doi={10.1109/ACCESS.2025.3594218},
  ISSN={2169-3536},
  month={},}@ARTICLE{11036671,
  author={Sayeedi, Md. Faiyaz Abdullah and Bin Hossain, Maaz and Hassan, Md. Kamrul and Afrin, Sabrina and Hossain, Molla Md. Sabit and Hossain, Md. Shohrab},
  journal={IEEE Access}, 
  title={JailbreakTracer: Explainable Detection of Jailbreaking Prompts in LLMs Using Synthetic Data Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={123708-123723},
  abstract={The emergence of Large Language Models (LLMs) has revolutionized natural language processing (NLP), enabling remarkable advancements across various applications. However, these models remain susceptible to adversarial prompts, commonly referred to as jailbreaks, which exploit their vulnerabilities to bypass ethical and safety constraints. These prompts manipulate LLMs to produce harmful or forbidden outputs, posing serious ethical and security challenges. In this study, we propose JailbreakTracer, a novel framework leveraging synthetic data generation and Explainable AI (XAI) to detect and classify jailbreaking prompts. We first construct two comprehensive datasets: a Toxic Prompt Classification Dataset, combining real-world and synthetic jailbreak prompts, and a Forbidden Question Reasoning Dataset, categorizing forbidden queries into 13 distinct scenarios with clear reasoning labels. Synthetic toxic prompts are generated using a fine-tuned GPT model, achieving an attack success rate of 95.1%, effectively addressing the class imbalance. Using transformer-based architectures, we train classifiers that achieved 97.25% accuracy in detecting jailbreak prompts and 100% accuracy in categorizing forbidden questions. Our approach integrates XAI techniques, such as LIME, to ensure interpretability and transparency in the model’s predictions. Extensive evaluations demonstrate the efficacy of JailbreakTracer in detecting and reasoning about jailbreak prompts, providing a critical step toward enhancing the safety and accountability of LLMs. The dataset and code are available on GitHub: https://github.com/faiyazabdullah/JailbreakTracer},
  keywords={Ethics;Cognition;Synthetic data;Natural language processing;Artificial intelligence;Adaptation models;Security;Robustness;Prevention and mitigation;Passwords;Natural language processing;large language models;jailbreaking;text classification;synthetic data;generative AI;explainable AI},
  doi={10.1109/ACCESS.2025.3579996},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8950863,
  author={Li, Tianwei and Zhang, Kun and Li, Wei and Huang, Qian},
  booktitle={2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM)}, 
  title={Research on ROI Algorithm of Ship Image Based on Improved YOLO}, 
  year={2019},
  volume={},
  number={},
  pages={130-133},
  abstract={In this paper, a region of interest (ROI) extraction algorithm based on YOLO algorithm is proposed. The algorithm optimizes the output tensor dimension of YOLO model, generates different image quality images of naval vessels by using image degradation function, and retrains the network by means of migration learning, which enhances the accuracy and detection rate of the algorithm. Compared with the original algorithm, the detection rate of the algorithm is improved by 4.25% on average, which proves the effectiveness of the algorithm.},
  keywords={YOLO;Image quality;Tensors;Atmospheric modeling;Transfer learning;Linear programming;Generative adversarial networks;Hardware;Manufacturing;Marine vehicles;YOLO, Transfer learning, ROI, patchGAN},
  doi={10.1109/AIAM48774.2019.00033},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10590124,
  author={Rais, Khadija and Amroune, Mohamed and Haouam, Mohamed Yassine and Bendib, Issam},
  booktitle={2023 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Comparative Study of Data Augmentation Approaches for Improving Medical Image Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1226-1234},
  abstract={In recent years, data augmentation has advanced to the point where it no longer relies on traditional photometric or geometric image processing techniques, such as rotation, scale, and filtering. Instead, it has turned to deep learning-based approaches like variational autoencoders (VAEs) and generative adversarial networks (GANs). This shift has made data augmentation a crucial discipline for improving artificial intelligence models, especially in fields like medicine where collecting labeled data is difficult and expensive. This study focuses on using data augmentation approaches to enhance convolutional neural networks (CNNs) classification accuracy. These approaches include geometric modifications, color space transformations, and generative techniques, and they are employed to augment the BraTS20 (Brain Tumor Segmentation 2020) dataset. After comparing the accuracy of CNNs with and without the augmented dataset, we found that DCGANs and AttnGAN achieved the highest accuracy among all techniques. They generated images that closely resembled those in BraTS20 compared to other methods.},
  keywords={Deep learning;Image segmentation;Accuracy;Training data;Data augmentation;Data models;Convolutional neural networks;Data augmentation;classic techniques;generative techniques;Medical image classification;Brain tumor},
  doi={10.1109/CSCI62032.2023.00200},
  ISSN={2769-5654},
  month={Dec},}@INPROCEEDINGS{10769814,
  author={Ding, Bosong and Kirtay, Murat and Spigler, Giacomo},
  booktitle={2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)}, 
  title={Imitation of Human Motion Achieves Natural Head Movements for Humanoid Robots in an Active-Speaker Detection Task}, 
  year={2024},
  volume={},
  number={},
  pages={645-652},
  abstract={Head movements are crucial for social human-human interaction. They can transmit important cues (e.g., joint attention, speaker detection) that cannot be achieved with verbal interaction alone. This advantage also holds for human-robot interaction. Even though modeling human motions through generative AI models has become an active research area within robotics in recent years, the use of these methods for producing head movements in human-robot interaction remains underexplored. In this work, we employed a generative AI pipeline to produce human-like head movements for a Nao humanoid robot. In addition, we tested the system on a real-time active-speaker tracking task in a group conversation setting. Overall, the results show that the Nao robot successfully imitates human head movements in a natural manner while actively tracking the speakers during the conversation. Code and data from this study are available at https://bosongding.github.io/Humanoids2024Air-Lab/.},
  keywords={Codes;Tracking;Generative AI;Pipelines;Humanoid robots;Human-robot interaction;Oral communication;Real-time systems},
  doi={10.1109/Humanoids58906.2024.10769814},
  ISSN={2164-0580},
  month={Nov},}@ARTICLE{9069260,
  author={Chen, Songkui and Shi, Daming and Sadiq, Muhammad and Cheng, Xiaochun},
  journal={IEEE Access}, 
  title={Image Denoising With Generative Adversarial Networks and its Application to Cell Image Enhancement}, 
  year={2020},
  volume={8},
  number={},
  pages={82819-82831},
  abstract={This paper proposes an image denoising training framework based on Wasserstein Generative Adversarial Networks (WGAN) and applies it to cell image denoising. Cell image denoising is a challenging task which has high requirement on the recovery of feature details. Current popular convolutional neural network (CNN) based denoising methods encounter a blurriness issue that denoised images are blurry on texture details, which is fatal for the cell image denoising. In this paper, to solve the blurriness issue, we first theoretically analyze the cause of the blurriness issue. Subsequently, an image denoising training framework with WGAN based adversarial learning is proposed. This training framework solves the blurriness issue by guiding the denoising network to find the distribution space of real clean images rather than the distribution space of blurry images and introducing feature information. Experimental results show that this training framework can effectively solve the blurriness issue and achieve better denoising performance than the state-of-the-art denoising methods. Meanwhile, the application of this training framework on cell image denoising also achieves satisfactory performance. Recovered cell images of this training framework are clear on feature details.},
  keywords={Noise reduction;Training;Image denoising;Noise measurement;Task analysis;Feature extraction;Gallium nitride;Image denoising;cell image denoising;blurriness issue;adversarial learning;Wasserstein generative adversarial networks},
  doi={10.1109/ACCESS.2020.2988284},
  ISSN={2169-3536},
  month={},}@ARTICLE{9088273,
  author={Regazzoni, Carlo S. and Marcenaro, Lucio and Campo, Damian and Rinner, Bernhard},
  journal={Proceedings of the IEEE}, 
  title={Multisensorial Generative and Descriptive Self-Awareness Models for Autonomous Systems}, 
  year={2020},
  volume={108},
  number={7},
  pages={987-1010},
  abstract={In a computational context, self-awareness (SA) is a capability of an autonomous system to describe the acquired experience about itself and its surrounding environment with appropriate models and correlate them incrementally with the currently perceived situation to expand its knowledge continuously. This article introduces a bio-inspired framework for generative and descriptive dynamic models that support SA computationally and efficiently. Generative models facilitate predicting future states, while descriptive models enable the selection of the representation that best fits the current observation. Our framework is founded on the analysis and extension of three bio-inspired theories that have studied SA from different viewpoints, and we demonstrate how probabilistic techniques, such as cognitive dynamic Bayesian networks and generalized filtering paradigms, can learn appropriate models from multidimensional proprioceptive and exteroceptive signals acquired by the autonomous system. We discuss essential capabilities for SA and show how our modeling framework supports these capabilities in theory and through a case study where a mobile robot uses multisensorial data to determine its internal and environmental state as well as distinguishing among normal and abnormal behaviors.},
  keywords={Biological system modeling;Sensors;Autonomous systems;Computational modeling;Brain modeling;Context modeling;Anomaly detection;autobiographical memory (AM);autobiographical self (AS);Bayesian inference;cognitive dynamic systems (CDSs);model creation;self-awareness (SA)},
  doi={10.1109/JPROC.2020.2986602},
  ISSN={1558-2256},
  month={July},}@ARTICLE{9944861,
  author={Ye, Fei and Bors, Adrian G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dynamic Self-Supervised Teacher-Student Network Learning}, 
  year={2023},
  volume={45},
  number={5},
  pages={5731-5748},
  abstract={Lifelong learning (LLL) represents the ability of an artificial intelligence system to learn successively a sequence of different databases. In this paper we introduce the Dynamic Self-Supervised Teacher-Student Network (D-TS), representing a more general LLL framework, where the Teacher is implemented as a dynamically expanding mixture model which automatically increases its capacity to deal with a growing number of tasks. We propose the Knowledge Discrepancy Score (KDS) criterion for measuring the relevance of the incoming information characterizing a new task when compared to the existing knowledge accumulated by the Teacher module from its previous training. The KDS ensures a light Teacher architecture while also enabling to reuse the learned knowledge whenever appropriate, accelerating the learning of given tasks. The Student module is implemented as a lightweight probabilistic generative model. We introduce a novel self-supervised learning procedure for the Student that allows to capture cross-domain latent representations from the entire knowledge accumulated by the Teacher as well as from novel data. We perform several experiments which show that D-TS can achieve the state of the art results in LLL while requiring fewer parameters than other methods.},
  keywords={Task analysis;Mixture models;Training;Generative adversarial networks;Data models;Computational modeling;Self-supervised learning;Lifelong learning;representation learning;self-supervised learning;teacher-student framework},
  doi={10.1109/TPAMI.2022.3220928},
  ISSN={1939-3539},
  month={May},}@INBOOK{10237045,
  author={Vijay Mishra, Kumar and Elbir, Ahmet M. and Zaghloul, Amir I.},
  booktitle={Advances in Electromagnetics Empowered by Artificial Intelligence and Deep Learning}, 
  title={Machine Learning for Metasurfaces Design and Their Applications}, 
  year={2023},
  volume={},
  number={},
  pages={281-317},
  abstract={Metasurfaces are increasingly emerging as enabling technologies to meet the demands for multi&#x2010;functional, small form&#x2010;factor, efficient, reconfigurable, tunable, and low&#x2010;cost radio&#x2010;frequency (RF) components because of their ability to manipulate waves in a sub&#x2010;wavelength thickness through modified boundary conditions. They enable design of reconfigurable intelligent surfaces (RISs) for adaptable wireless channels and smart radio environments, wherein the inherently stochastic nature of the wireless environment is transformed into a programmable propagation channel. In particular, space&#x2010;limited RF applications, such as communications and radar, that have strict radiation requirements are currently being investigated for potential RIS deployment. The RIS comprises sub&#x2010;wavelength units or meta&#x2010;atoms, which are independently controlled and whose geometry and material determine the spectral response of the RIS. Conventionally, designing RIS to yield the desired electromagnetic (EM) response requires trial&#x2010;and&#x2010;error by iteratively investigating a large possibility of various geometries and materials through thousands of full&#x2010;wave EM simulations. In this context, machine learning/deep learning (ML/DL) techniques are proving critical in reducing the computational cost and time of RIS inverse design. Instead of explicitly solving Maxwell's equations, DL models learn physics&#x2010;based relationships through supervised training data. The ML/DL techniques also aid in RIS deployment for numerous wireless applications, which requires dealing with multiple channel links between the base station (BS) and the users. As a result, the BS and RIS beamformers require a joint design, wherein the RIS elements must be rapidly reconfigured. The lower computation time and model&#x2010;free nature of DL makes it robust against the data imperfections and environmental changes in RIS&#x2010;aided communications. At the physical layer, DL has been shown to be effective for RIS signal detection, channel estimation, and active/passive beamforming using architectures such as supervised, unsupervised, and reinforcement learning. This chapter provides a synopsis of DL techniques for both inverse RIS design and RIS&#x2010;assisted wireless systems.},
  keywords={Array signal processing;Scattering;Radio frequency;Transmitting antennas;Surface waves;Surface impedance;Reflector antennas},
  doi={10.1002/9781119853923.ch9},
  ISSN={},
  publisher={IEEE},
  isbn={9781119853909},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10237045},}@ARTICLE{9623476,
  author={Zhou, Huabing and Wu, Wei and Zhang, Yanduo and Ma, Jiayi and Ling, Haibin},
  journal={IEEE Transactions on Multimedia}, 
  title={Semantic-Supervised Infrared and Visible Image Fusion Via a Dual-Discriminator Generative Adversarial Network}, 
  year={2023},
  volume={25},
  number={},
  pages={635-648},
  abstract={Image fusion synthesizes a new image from multiple images of the same scene. The synthesized image should be suitable for human visual perception and follow-up high-level image-processing tasks. However, existing methods focus on fusing low-level features, ignoring high-level semantic perception information. We propose a new end-to-end model to obtain a more semantically consistent image in infrared and visible image fusion, termed semantic-supervised dual-discriminator generative adversarial network (SDDGAN). In particular, we design an information quantity discrimination (IQD) block to guide fusion progress. For each source image, the block determines the weight for preserving each semantic object’s feature. By this way, the generator learns to fuse various semantic objects via different weights to preserve their characteristics. Moreover, the dual discriminator is employed to identify the distribution of infrared and visible information in the fused image. Each discriminator acts on a certain modality (infrared/visible) of different semantic objects in the fused image to preserve and enhance their modality features. Thus, our fused image is more informative. Both the thermal radiation in the infrared image and the visible image texture details can be well preserved. Qualitative and quantitative experiments demonstrate the superiority of our SDDGAN over state-of-the-art methods in terms of visual effects, efficiency, and quantitative metrics.},
  keywords={Image fusion;Semantics;Feature extraction;Generators;Transforms;Generative adversarial networks;Games;Image fusion;infrared image;visible image;semantic supervised;dual-discriminator},
  doi={10.1109/TMM.2021.3129609},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10730717,
  author={Corpus, Sally G. and Villanueva, Alonica R.},
  booktitle={2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={Speech Emotion Recognition in Filipino Spoken Language Using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={430-435},
  abstract={SER, or Speech Emotion Recognition, is an essential research area with implications in numerous sectors. However, implementing SER for languages with limited resources presents substantial obstacles, primarily due to data scarcity, which often results in overfitting. Overfitting is a phenomenon where a model excessively learns from training data, which subsequently hampers its effectiveness on new data. To mitigate these issues, several strategies have been suggested by researchers, including transfer learning method, synthetic data generation using GAN or generative adversarial network and hyperparameter tuning. These methods aim to augment the efficiency of SER systems in low-resource languages. The study focuses on speech emotion recognition in the Filipino language. Audio data are extracted from YouTube, which comprises Filipino movies, TV series, interviews, and other content available. The Deep Convolutional Generative Adversarial Network, often abbreviated as DCGAN, is a mechanism employed for the creation of artificial or ‘synthetic’ data. The five (5) classes of emotions include Sadness, Anger, Fear, Sarcasm, and Neutral. Transfer learning with hyper tuning of parameters of VGG19 architecture was used to build the SER classification model. The training accuracy is 97%, and 95% for testing accuracy. The study concludes that the model enhances the baseline accuracy for the Filipino SER.},
  keywords={Training;Emotion recognition;Accuracy;Video on demand;Transfer learning;Training data;Speech recognition;Generative adversarial networks;Web sites;Tuning;VGG19;DCGAN;transfer learning;synthetic data;Filipino language;SER;low resource language;hyperparameter tuning;overfitting},
  doi={10.1109/IICAIET62352.2024.10730717},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9670977,
  author={Phute, Mansi and Sahastrabudhe, Aditi and Pimparkhede, Sameer and Potphode, Shubham and Rengade, Kshitij and Shilaskar, Swati},
  booktitle={2021 International Conference on Artificial Intelligence and Machine Vision (AIMV)}, 
  title={A Survey on Machine Learning in Lithography}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Lithography is the process of transferring the geometric patterns from the masks to the resist material on the semiconductor. It is a very important part of VLSI fabrication that is critical when it comes to the efficient functioning of circuits. Many state-of-the-art methods use Machine Learning (ML) to identify lithography patterns that can cause issues in the future as these algorithms can predict defects in patterns which the machine has not encountered before. This paper focuses on the need for Machine Learning in the lithography process, and the various algorithms used like Support Vector Machines (SVM), Artificial Neural Networks (ANN) and Convolutional Neural Networks (CNN). There are multiple applications including Hotspot detection, Optical Proximity Correction (OPC), Sub Resolution Assist Feature (SRAF), Phase Shift Masks (PSM), and Resist Modelling. The major issue faced by Machine Learning algorithms is that of false positives. It can be reduced by utilizing the Gaussian process after initial detection.},
  keywords={Support vector machines;Machine learning algorithms;Lithography;Transfer learning;Resists;Gaussian processes;Very large scale integration;Machine Learning;Lithography;Feature Extraction;Artificial Neural Networks},
  doi={10.1109/AIMV53313.2021.9670977},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10632687,
  author={Blancaflor, Eric and Calpo, Amiel Heart and Cebrian, Spencer Jireh and Siquioco, Frederick},
  booktitle={2024 5th International Conference on Industrial Engineering and Artificial Intelligence (IEAI)}, 
  title={A Comprehensive Review of Neural Network-Based Approaches for Predicting Phishing Websites and URLs}, 
  year={2024},
  volume={},
  number={},
  pages={96-101},
  abstract={This comprehensive review discusses recent advancements in the modern technology of phishing detection and prevention, focusing on Neural Network-based machine learning techniques. The surge in cyber threats, particularly phishing attacks, needs a continuous innovation to combat fast advancing tactics by malicious people. The synthesis of various studies highlights the effectiveness of Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Feature Engineering/Extraction techniques, and Ensemble Methods in multiple phishing detection scenarios. ANNs demonstrate reliability in countering phishing attacks across domains, including email security, URL-based detection, and protection against pharming. CNNs exhibit remarkable performance in recognizing intricate patterns within data, particularly in URL and email analysis. RNNs showcase efficacy in real-time frameworks and optimal feature selection algorithms, capturing dependencies and patterns over time. Ensemble Methods emerge as a paradigm shift, offering heightened accuracy, efficiency, and adaptability. Feature engineering techniques, emphasizing URL-centric approaches and multi-modal features, play a crucial role in enhancing Neural Network performance. Comparative assessments highlight the nuanced landscape of phishing detection, with Neural Networks showcasing strengths alongside traditional machine learning algorithms. The dynamic nature of phishing attacks underscores the importance of ongoing research and innovation in machine learning techniques to ensure robust cybersecurity measures against evolving cyber threats.},
  keywords={Uniform resource locators;Technological innovation;Recurrent neural networks;Reviews;Phishing;Feature extraction;Electronic mail;Phishing;Cyber-security;Machine Learning;Artificial Neural Networks;Convolutional Neural Networks;Recurrent Neural Networks;Ensemble Methods;Feature Engineering},
  doi={10.1109/IEAI62569.2024.00025},
  ISSN={},
  month={April},}@INPROCEEDINGS{10873771,
  author={Tian, Ying and Zhou, Wang and Haq, Amin Ul},
  booktitle={2024 21st International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)}, 
  title={Detection of Deepfakes: Protecting Images and Vedios Against Deepfake}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent years, with the rapid development of deep learning and computer vision technology, the forgery technology of images and videos has become increasingly mature, posing new challenges to information security and social stability. Behind the re-evolution of deepfake lies the rampant proliferation of fake content, which is used for election tampering, identity fraud, fraud, spreading fake news, and so on. To address these challenges, researchers are constantly exploring and developing image-based deepfake detection techniques, which aim to effectively identify and prevent deepfake content in images and videos. This article will introduce the current development status of deepfake detection technology, present its principles and methods in detail, and look ahead to its future development directions.},
  keywords={Deep learning;Deepfakes;Computer vision;Voting;Neural networks;Information security;Information processing;Forgery;Fraud;Computer security;Deepfake Generation;Deepfake Detection;Convolutional Neural Networks},
  doi={10.1109/ICCWAMTIP64812.2024.10873771},
  ISSN={2576-8964},
  month={Dec},}@INPROCEEDINGS{10048907,
  author={Park, JinSeon and Tae Kim, Ki and Park, Seong-Bae and Seon Hong, Choong},
  booktitle={2023 International Conference on Information Networking (ICOIN)}, 
  title={Abnormal Client Detection Federated Learning Using Image Vectors}, 
  year={2023},
  volume={},
  number={},
  pages={742-745},
  abstract={Federated learning is a distributed machine learning system that can learn AI models in cooperation with each other without directly sharing data stored in multiple locations. Since federated learning requires training the model without direct access to the client data, AI models can be trained while protecting the client’s data. In the presence of clients with relatively different data distributions from other clients, this can lead to poor model learning performance in federated learning. In this paper, we propose a method to obtain cosine similarity by computing the vector inner product based on the vector for the client’s image data, and to improve the performance of federated learning by eliminating clients with low similarity. Compared to the case of conducting federated learning without detecting abnormal clients, the performance improvement of 6% was confirmed when the proposed method was applied.},
  keywords={Training;Federated learning;Computational modeling;Distributed databases;Information sharing;Data models;Convergence;Federated Learning;Vector;Cosine Similarity},
  doi={10.1109/ICOIN56518.2023.10048907},
  ISSN={1976-7684},
  month={Jan},}@INPROCEEDINGS{10778362,
  author={Liu, Huaizhe and Wu, Jiaqi and Zhuang, Xinyi and Wu, Hongjia and Gao, Lin},
  booktitle={2024 22nd International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)}, 
  title={Joint Communication and Computation Scheduling for MEC-Enabled AIGC Services Based on Generative Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={345-352},
  abstract={Artificial Intelligence-Generated Content (AIGC) based on Generative Diffusion Model (GDM) has emerged as a promising paradigm of content generation, revolutionizing the creation of diverse contents and driving significant technological advancements. Due to the low latency requirements of AIGC services, mobile edge computing (MEC) has become a crucial enabling technology for these services. In this work, we consider an MEC-enabled GDM-based AIGC network, which consists of multiple GDMs with varying sizes and capabilities deployed on edge computing servers (ES), and multiple mobile users (UEs) with diverse latency and accuracy requirements requesting AIGC services from ES through wireless access points (APs). In such a scenario, we are interested in the joint communication and computation scheduling problem for UEs, which involves selecting the appropriate APs (along with the communication bandwidth allocation) and the appropriate ES (together with the computation resource allocation and model inference optimization) for UEs, considering both the UEs' heterogeneous requirements and the GDMs' heterogeneous capabilities. To address the problem in a practical scenario with decentralized, autonomous, and self-interested UEs, we formulate a non-cooperative game, called the Joint User Association and Computation Offloading (JUACO) game, where each UE acts as a game player, selecting the best AP (for communication) as well as the best ES and the best GDM model inference step (for computation), aiming to minimize the inference time while meeting the specified inference accuracy requirement. We prove that the proposed JUACO game is a potential game, thus guaranteeing the existence of Nash equilibrium (NE) and the convergence of simple best response-based distributed algorithms to NE. Simulation results demonstrate that the proposed JDACO game approach significantly reduces the inference time and meets the required accuracy compared to traditional methods, validating the effectiveness and practicality of the game-theoretic approach in real-world scenarios.},
  keywords={Accuracy;Processor scheduling;Computational modeling;Wireless networks;Games;Wireless access points;Nash equilibrium;Diffusion models;Distributed algorithms;Optimization;Artificial Intelligence-Generated Content;Generative Diffusion Model;Mobile Edge Computing},
  doi={},
  ISSN={2690-3342},
  month={Oct},}@ARTICLE{11077819,
  author={Almadhor, Ahmad and Sampedro, Gabriel Avelino and Zaidi, Monji Mohamed and Juanatas, Roben A. and Hejaili, Abdullah Al},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Generative AI-Driven Context-Aware BDI-Based Smart Routing Protocol for Intelligent Transportation Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={This research introduces a newly proposed Belief-Desire-Intention (BDI) agent-based Smart Routing Protocol for Intelligent Transportation Systems (ITS) that leverages context-aware decision-making inspired by Generative Artificial Intelligence (GAI) reasoning models. The protocol enhances routing decisions by considering real-time traffic conditions, weather, road blockages, peak-hour dynamics, and AI-generated predictions of future congestion. By incorporating BDI reasoning, the system dynamically adjusts routes based on both current and forecasted conditions, thereby improving the overall performance of vehicular networks. Our proposed system aims to optimise key performance indicators such as Packet Delivery Ratio (PDR), End-to-End Delay, Throughput, Control Overhead, Traffic Flow, Routing Decision Accuracy, and Energy Consumption. Simulation results demonstrate that the BDI-based system outperforms traditional routing protocols like Ad hoc On-Demand Distance Vector (AODV) and Dynamic Source Routing (DSR), showing significant improvements in PDR, Throughput, and Energy Efficiency. For example, the BDI-based system achieved a PDR of 98%, compared to 85% for AODV and 80% for DSR. Additionally, the BDI protocol reduced end-to-end delay by 25% and energy consumption by 30% compared to the baseline protocols. These findings underscore the BDI-based system’s capability for real-time, adaptive routing in ITS, enabling optimal network performance and resource efficiency through context-aware and predictive routing decisions.},
  keywords={Routing;Routing protocols;Real-time systems;Vehicle dynamics;Intelligent transportation systems;Vehicle-to-everything;Transportation;Meteorology;Delays;Context modeling;Generative artificial intelligence (GAI);BDI-based routing;intelligent transportation systems (ITS);context-aware routing},
  doi={10.1109/TITS.2025.3570237},
  ISSN={1558-0016},
  month={},}@INPROCEEDINGS{10537371,
  author={R, Siddharth and Kumar, S. Ganesh},
  booktitle={2024 2nd International Conference on Networking and Communications (ICNWC)}, 
  title={Personalized Music Improvisation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This research paper focuses on developing a model that learns and emulates the personalized musical style of composers. Pitch estimation of music signals is a fundamental problem in automatic transcription systems, with applications in music information retrieval and automated musicological analysis. While pitch estimation for monophonic music signals is considered solved, estimating the pitch of multiple concurrent sources remains challenging. This paper explores the challenges and techniques involved in monophonic pitch detection, highlighting the differences from polyphonic detection and emphasizing the need for improved accuracy. Additionally, the paper discusses the potential of using machine learning algorithms to synchronize music in real-time with a composer’s input, enabling interactive improvisation. The project’s scope includes the development of algorithms for pitch detection, synchronization, and AI-based melody generation, with applications ranging from music composition assistance to interactive performance systems.},
  keywords={Machine learning algorithms;Music;Estimation;Real-time systems;Distance measurement;Multiple signal classification;Synchronization},
  doi={10.1109/ICNWC60771.2024.10537371},
  ISSN={},
  month={April},}@ARTICLE{10902142,
  author={He, Chunming and Shen, Yuqi and Fang, Chengyu and Xiao, Fengyang and Tang, Longxiang and Zhang, Yulun and Zuo, Wangmeng and Guo, Zhenhua and Li, Xiu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Diffusion Models in Low-Level Vision: A Survey}, 
  year={2025},
  volume={47},
  number={6},
  pages={4630-4651},
  abstract={Deep generative models have gained considerable attention in low-level vision tasks due to their powerful generative capabilities. Among these, diffusion model-based approaches, which employ a forward diffusion process to degrade an image and a reverse denoising process for image generation, have become particularly prominent for producing high-quality, diverse samples with intricate texture details. Despite their widespread success in low-level vision, there remains a lack of a comprehensive, insightful survey that synthesizes and organizes the advances in diffusion model-based techniques. To address this gap, this paper presents the first comprehensive review focused on denoising diffusion models applied to low-level vision tasks, covering both theoretical and practical contributions. We outline three general diffusion modeling frameworks and explore their connections with other popular deep generative models, establishing a solid theoretical foundation for subsequent analysis. We then categorize diffusion models used in low-level vision tasks from multiple perspectives, considering both the underlying framework and the target application. Beyond natural image processing, we also summarize diffusion models applied to other low-level vision domains, including medical imaging, remote sensing, and video processing. Additionally, we provide an overview of widely used benchmarks and evaluation metrics in low-level vision tasks. Our review includes an extensive evaluation of diffusion model-based techniques across six representative tasks, with both quantitative and qualitative analysis. Finally, we highlight the limitations of current diffusion models and propose four promising directions for future research. This comprehensive review aims to foster a deeper understanding of the role of denoising diffusion models in low-level vision.},
  keywords={Diffusion models;Noise;Surveys;Training;Remote sensing;Superresolution;Reviews;Noise reduction;Optimization;Mathematical models;Diffusion models;score-based stochastic differential equations;low-level vision tasks;medical image processing;remote sensing data processing;video processing},
  doi={10.1109/TPAMI.2025.3545047},
  ISSN={1939-3539},
  month={June},}@ARTICLE{10479476,
  author={Zhang, Sicheng and Fu, Jiangzhi and Yu, Jiarun and Xu, Huaitao and Zha, Haoran and Mao, Shiwen and Lin, Yun},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Channel-Robust Class-Universal Spectrum-Focused Frequency Adversarial Attacks on Modulated Classification Models}, 
  year={2024},
  volume={10},
  number={4},
  pages={1280-1293},
  abstract={With the improvement of basic designs and the evolution of key algorithms, artificial intelligence (AI) has been considered by both industry and academia as the most promising solution for many electromagnetic space problems, such as automatic modulation classification (AMC). However, the fact that AI-based AMC models are vulnerable to adversarial examples mystifies the optimism. Adversarial attacks help researchers to reexamine AI-based AMC models and promote safe applications. In this paper, we study the frequency leakage and glitch problems caused by high frequency components in the adversarial perturbations of existing attack algorithms. We propose a Spectrum-focused Frequency Adversarial Attack (SFAA) algorithm to suppress the high frequency components to alleviate such problems. Next, we leverage meta-learning to improve the transferability of the proposed algorithm for black-box attacks. We also train a Channel-robust Class-universal Spectrum-focused Frequency Adversarial Attack (CrCu-SFAA) generative model using the generative adversarial network framework. Finally, extensive experiments using qualitative and quantitative indicators demonstrate that the proposed algorithm achieves an improved attack performance, and our proposed approach of reducing out-of-band high frequency components of the adversarial perturbations improves the concealment and adversarial signal quality.},
  keywords={Perturbation methods;Electromagnetics;High frequency;Frequency-domain analysis;Data models;Computational modeling;Closed box;Automatic modulation classification (AMC);frequency adversarial attack;spectrum focus;channel-robustness;class universal},
  doi={10.1109/TCCN.2024.3382126},
  ISSN={2332-7731},
  month={Aug},}@ARTICLE{9047869,
  author={Bi, Xiaojun and Xing, Junyao},
  journal={IEEE Access}, 
  title={Multi-Scale Weighted Fusion Attentive Generative Adversarial Network for Single Image De-Raining}, 
  year={2020},
  volume={8},
  number={},
  pages={69838-69848},
  abstract={With the rapid development of outdoor vision system, removing rain streaks from a single image has attracted considerable attention as rain streaks can affect the quality of the image taken in rainy days, and interrupt the key information, which will greatly reduce the use value of the image, thus affecting the performance of traffic, safety monitoring and other facilities. Although the deep learning methods have achieved satisfying performance in single image de-raining, there are still two problems: First, the rain streaks contained in one dataset we can use are limited, and in the case of real rainy days, the rain streak density is diverse, it is impossible to accurately classify them. Therefore, the existing rain removal models cannot remove rain streaks properly for images with different rain streak density which attend to over or under rain removal. Secondly, the results of single image after rain removal model often appear the phenomenon of variegated spots, image contrast saturation change and even unsmooth rain streak after rain removal. We use a three-way multi-scale weighted fusion module to enhance the feature extraction, and then generate an attention map through the improved spatial attentive module to accurately locate the location of the rain streaks. After the combination of the two, we will obtain the foreground information of the rain streaks. Through the characteristic of mutual game in the training mechanism of GAN, we can enhance the rain streak location recognition and effectively remove the rain at the same time. Through the training mechanism of the GAN network game, we can enhance the rain line location recognition and effectively remove the rain at the same time. Experiments show that our network achieves superior performance, it has high generalization for different rain streak density, and ensures that the contrast and saturation of the image are not changed.},
  keywords={Rain;Generative adversarial networks;Training;Deep learning;Task analysis;Feature extraction;Gallium nitride;Improved spatial attentive mechanism;single image rain removal;condition generative adversarial networks;multi-scale weighted fusion},
  doi={10.1109/ACCESS.2020.2983436},
  ISSN={2169-3536},
  month={},}@ARTICLE{10155132,
  author={Phienphanich, Phongphan and Lerthirunvibul, Nichapa and Charnnarong, Ekabhat and Munthuli, Adirek and Tantibundhit, Charturong and Suwanwela, Nijasri C.},
  journal={IEEE Access}, 
  title={Generalizing a Small Facial Image Dataset Using Facial Generative Adversarial Networks for Stroke’s Facial Weakness Screening}, 
  year={2023},
  volume={11},
  number={},
  pages={64886-64896},
  abstract={Stroke is a medical emergency resulting from disruption of blood supply to different parts of the brain which leads to facial weakness and paralysis as the brain is the control center. Stroke is the leading cause of long-term disability which significantly changes the patient’s life. This paper introduces the use of facial image dataset containing neutral and smiling expressions to classify facial weakness which is a common sign of stroke. Our “real facial image dataset” comprises of face images of normal subjects and stroke patients. However, to increase the dataset, we added another dataset known as “FaceGAN dataset”. This additional dataset contains a pair of neutral and smiling facial image synthesized from public datasets which were augmented to generate two additional smiling images at eight different age groups. The faces were divided into left and right side using facial landmark detection technique and corrected for geometric distortions through affine transformation matrix from Delaunay triangulation. An autoencoder model composed of ConvNeXt encoder and ConvNet decoder was trained and used to fine-tune a facial weakness classification model from our proposed architecture. Results from four-fold cross validation showed that the model validation was less prone to overfitting when used with the FaceGAN dataset, with an average AUC of 0.76 and F1-score of 71.19%, compared to without FaceGAN data which only achieved an F1-score of 61.54%. This study shows that the FaceGAN can efficiently generalize models for programs with a small dataset for use with stroke detection. This work can be further improved and optimized for clinical application in the future.},
  keywords={Face recognition;Stroke (medical condition);Videos;Generative adversarial networks;Data models;Neurons;Facial generative adversarial networks;facial weakness;FAST;small dataset;stroke-screening},
  doi={10.1109/ACCESS.2023.3287389},
  ISSN={2169-3536},
  month={},}@ARTICLE{9272961,
  author={Du, Kangning and Zhou, Huaqiang and Cao, Lin and Guo, Yanan and Wang, Tao},
  journal={IEEE Access}, 
  title={MHGAN: Multi-Hierarchies Generative Adversarial Network for High-Quality Face Sketch Synthesis}, 
  year={2020},
  volume={8},
  number={},
  pages={212995-213011},
  abstract={Face sketch synthesis has made significant progress in the past few years. Recently, GAN-based methods have shown promising results on image-to-image translation problems, especially photo-to-sketch synthesis. Because the facial sketch has a hyper-abstract style and continuous graphic elements, compared with other image styles, its local details are easier to expose small artifacts and blur. The existing face sketch synthesis methods lack models for specific facial regions and usually generate face sketches with coarse structures. To synthesis high-quality sketches and overcome the blurs and deformations, this paper proposes a novel Multi-Hierarchies GAN, which divides the face image into multiple hierarchical structures to learn different regions’ features of the face. It includes three modules: a local region module, mask module, and fusion module. The local region module can learn the detailed features of different local regions of the face by GAN. The mask module can generate a coarse facial structure of a sketch and uses the facial feature extractor to enhance the high-level image and learn the latent spaces’ feature. The fusion module can generate the final sketch by combining fine local regions and coarse facial structure. Extensive qualitative and quantitative experiments illustrate that the proposed method outperforms the state-of-the-art methods on the CUFS and CUFSF standard datasets and photos on the internet.},
  keywords={Faces;Gallium nitride;Training;Generative adversarial networks;Feature extraction;Face recognition;Facial features;Face sketch synthesis;generative adversarial network;facial feature extractor;multi-hierarchies GAN},
  doi={10.1109/ACCESS.2020.3041284},
  ISSN={2169-3536},
  month={},}@ARTICLE{10703051,
  author={Yang, Zhiguang and Qin, Hanqin and Zeng, Shan and Li, Bing and Tang, Yuanyan},
  journal={IEEE Access}, 
  title={DGFusion: A Novel Infrared and Visible Image Fusion Method Based on Diffusion and Generative Adversarial Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={147051-147064},
  abstract={In current deep learning-based infrared and visible image fusion algorithms, the image processing step involves converting the RGB channels of visible image into luminance channels. These methods usually pay more attention to the texture details in the image and neglect the equally important color information, which contradicts human vision. Color information, a crucial role in human visual perception, is one of the most intuitive evaluation metrics for image fusion. In order to restore the color of fused images, researchers have made many attempts, such as enhancing brightness or contrast. but the fusion results are not satisfied. Dif-Fusion compensates for the lack of color information by creating a multi-channel data distribution. However, the balance of the multi-channel data distribution still poses a problem. Based on Dif-Fusion, we propose an enhanced algorithm named DGFusion. Firstly, we change the Information input mechanism to balance the weights of infrared image features and visible image, which can enhance the expression of infrared information. Meanwhile, for obtain deep-level features, UNet++ replaces the original U-Net structure of the diffusion model. Furthermore, we introduce a discriminator in the fusion network for superior texture detail preservation. We conducted comparative experiments and ablation studies, which shows that the DGFusion yields superior fusion results. Ablation experiments show that DGFusion improves on most metrics compared to the unmodified method, validating the effectiveness of our approach. Comparison experiments show that our method outperforms several state-of-the-art fusion methods in terms of metrics and visual effects.},
  keywords={Feature extraction;Image fusion;Image color analysis;Generative adversarial networks;Diffusion models;Deep learning;Transforms;Measurement;Visualization;Infrared and visible image fusion;diffusion network;UNet++;generative adversarial network},
  doi={10.1109/ACCESS.2024.3472479},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10467535,
  author={Sharma, Richa and Mangla, Monika and Patil, Sharvari and Gonsalves, Priyanca and Agarwal, Neha},
  booktitle={2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Lung Disease Detection from Chest X-Ray Using GANs}, 
  year={2024},
  volume={},
  number={},
  pages={565-572},
  abstract={Lung diseases such as pneumonia, tuberculosis (TB), chronic obstructive pulmonary disease (COPD), and COVID-19 cause serious lung damage. According to the World Health Organization, long-term illnesses account for upto 10.38% of the total mortality rate in India. The early and accurate diagnosis of these diseases can have a significant impact on the lives of patients, as a delayed diagnosis prevents counselling. As a result, early discovery is critical for human survival, necessitating the use of innovative techniques and cutting-edge technologies to accelerate recovery and enhance long-term survival rates. To solve this issue, Generative Adversarial Networks (GAN) and Convolutional Neural Networks (CNN) are used to create an automated method for detecting and classifying various lung diseases. This study intends to construct a comprehensive framework to effectively recognize and differentiate various lung diseases by leveraging the capabilities of GANs and CNNs. The proposed system will deliver categorization results that will be supplemented by explanations based on Explainable AI techniques. The proposed strategy ensures transparency and provides insights into the system's decision-making process. The proposed system enables the early detection of lung diseases with minimal effort and time. The power of GANs, CNNs, and XAI can be used to improve the efficiency and accuracy of identifying lung diseases, thereby improving outcomes and long-term survival rates.},
  keywords={Pneumonia;Explainable AI;Tuberculosis;Pulmonary diseases;Lung;Organizations;Medical services;Lung Disease Detection;COVID-19;Convolutional Neural Networks (CNNs);Explainable Artificial Intelligence (XAI)},
  doi={10.1109/IDCIoT59759.2024.10467535},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11050619,
  author={Glaser, Charles and Guven, Erhan},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Generative Data Visualization with JSON Representations}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Data visualization can provide a wealth of insights for any organization. However, creating data visualizations can take significant time and domain-specific knowledge. Generative data visualization has the potential to streamline this process by creating complex and insightful visualizations with minimal user intervention. Large Language Models (LLMs) can respond to natural language queries with various media. However, they have yet to be widely applied to data visualization tasks in an enterprise setting due to the complexities of model training, resource consumption, and upkeep. This study tests an approach to generative data visualization through in-context learning, using an LLM to output user data visualization requests in the JSON format. Overall, the LLM performs poorly, even with multiple in-context examples. However, the study also yields insights into targeted adjustments that can potentially boost in-context learning for this use-case.},
  keywords={Measurement;Vocabulary;Accuracy;Data visualization;Data models;Grammar;Reliability;Prompt engineering;Tuning;Testing;data visualization;generative models;JSON representations;explainability},
  doi={10.1109/CAI64502.2025.00022},
  ISSN={},
  month={May},}@INPROCEEDINGS{11073726,
  author={Ficzere, Dániel and Varga, Pál},
  booktitle={NOMS 2025-2025 IEEE Network Operations and Management Symposium}, 
  title={AI-Driven Network Configuration and Operation}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={The recent advancements in network automation, powered by AI and Generative AI, have created new opportunities to automate both network configuration and operations. This paper outlines the main focus areas of my proposed PhD dissertation, which centers on using AI to streamline network configuration and improve operational efficiency. My research aims to develop solutions for automating network configurations through Generative AI, as well as leveraging AI technologies to enhance Intent-Based Networking functionalities. By integrating AI-driven approaches, my work seeks to address key challenges, including automating network configurations with Large Language Models and developing effective anomaly detection techniques in cellular network traffic.},
  keywords={Cellular networks;Automation;Generative AI;Large language models;Anomaly detection;Network Automation;Intent-Based Networking;Anomaly Detection;LLM},
  doi={10.1109/NOMS57970.2025.11073726},
  ISSN={2374-9709},
  month={May},}@ARTICLE{9881493,
  author={Rang, Xiaodi and Ma, Chaoqing and Zheng, Qiang and Zhao, Guangzhi},
  journal={IEEE Access}, 
  title={A Generative Adversarial Network for Multistyle Unsupervised Image Transfer Based on Dynamic Position Normalization}, 
  year={2022},
  volume={10},
  number={},
  pages={96284-96295},
  abstract={Image transfer is a technology that changes the image effect by processing the image’s color, contour, line, and other information. To stylize the visual appearance of the output will be adapted to the subject of the original image, this paper proposes an image feature transfer method called Cycle-DPN-GAN, based on the Cycle-Consistent Adversarial Networks. Firstly, Positional Normalization-Dynamic Moment Shortcut (PONO-DMS) module is introduced to learn more structural information from the input image, and the edge blurring and object losing are efficiently alleviated. In addition, the Multi-Scale-Structural Similarity Index (MS-SSIM) loss is added to the reconstruction loss, which improves visual perceptions and enhances the constraints on the reconstructed image in terms of image brightness, color contrast and structure. In this model, to verify the feasibility and superiority of the proposed method, the data sets of monet2photo, vangogh2photo, ukiyoe2photo and cezanne2photo are performed in the experiments, and the Inception Score and Fréchet Inception Distance evaluation index are improved. In addition, ablation studies are performed to demonstrate the validity of each proposed component. In this paper, the results of the quantitative evaluation are consistent with the qualitative evaluation. It can be demonstrated that the images generated by Cycle-DPN-GAN have higher visual quality.},
  keywords={Generators;Deep learning;Image reconstruction;Generative adversarial networks;Decoding;Painting;Visualization;CycleGAN;deep learning;generative adversarial network;image transfer},
  doi={10.1109/ACCESS.2022.3205013},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8859439,
  author={Li, Meng and Wang, Jian and Yang, Yi and Huang, Weixing and Du, Wenjuan},
  booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Improving GAN-Based Calligraphy Character Generation using Graph Matching}, 
  year={2019},
  volume={},
  number={},
  pages={291-295},
  abstract={The Chinese character generation with specific style is the key of personal calligraphy font generation. In recent years, with the development of artificial intelligence, researchers have used deep learning to solve the calligraphy generation problem, which can automatically generate personal calligraphy fonts. However, the end-to-end approaches possibly generate wrong results in terms of glyph structure of Chinese character because of lack of constraints on glyph structures. This paper proposes a method that adds constraints of glyph structure to deep neural network in order to improve the correctness of generation. This paper represents the glyph structure of Chinese character with glyph nodes extracted by object detection method. We compute the glyph structure loss between inputs and the generated results by the deep learning-based graph matching method. Experiments show that our method significantly increases the accuracy of Chinese character generation. For showing our method more clearly, we use the print as the original style samples in this paper.},
  keywords={Image edge detection;Deep learning;Feature extraction;Libraries;Character generation;Automation;Object detection;font style transfer;generative models;graph matching;deep learning;personalized Chinese character},
  doi={10.1109/QRS-C.2019.00062},
  ISSN={},
  month={July},}@INPROCEEDINGS{10550809,
  author={Nirusanan, Thavachelvam and Prasanth, Senthan and Banujan, Kuhaneswaran and Kumara, Samantha},
  booktitle={2024 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={Refining Large Language Models for Tabular Data Analysis in Business Domain by Laymen Text}, 
  year={2024},
  volume={7},
  number={},
  pages={1-5},
  abstract={Organizations face various challenges when analyzing tabular data. One of the key challenges is the complexity of business data analytics tasks. These tasks involve handling large volumes of data, organizing and structuring it, and extracting valuable insights. Additionally, employees who work in financial analysis often encounter a significant workload on tools such as Excel, SAP, PowerBI, and Tabula. This workload can result in increased effort and time required to analyze and make sense of the data. Organizations must address these challenges to ensure efficient and effective analysis of tabular data. Organizations spend more money to perform business tasks. Nowadays, there are many efficient models in artificial intelligence to perform text, audio, video, and image-based tasks, but no efficient models are available to perform tabular-based tasks specifically. Pandas Python library provides various functionalities and APIs that are useful for business data analysis. This research solved the problem with tabular data using Python Pandas code generation. Here, the researchers used two datasets, each with 50 records. The Large Language Model (LLM) is a supervised Learning Pre-Trained Foundation Model (SSL PFM) category based on text generation. The SSL PFM helps the language models learn the context of language and world knowledge. During this research, models such as LLaMA-2, Falcon, CodeLlama, and Mistral were considered for analysis. Each of these models consists of a Billion parameters. Moreover, Quantization techniques were incorporated to reduce model size, enabling models to load with minimal hardware. After quantizing the model, Parameter Efficient Fine Tuning (PEFT) trains the dataset using only a few model layers; other layers are frozen. Well-known experts with Pandas evaluate the fine-tuned models of chosen language models. Finally, mistral-7B produced prominent results in analyzing business tasks and producing summarized results.},
  keywords={Data analysis;Codes;Computational modeling;Organizations;Predictive models;Hardware;Libraries;DPO;Generative AI;Large Language Model;Pretrained Foundation Model;Self-supervised learning},
  doi={10.1109/SCSE61872.2024.10550809},
  ISSN={2613-8662},
  month={April},}@INPROCEEDINGS{11015117,
  author={Ramakrishnan, Arun and Kiruthika, S. and Athanesious, J. Joshan},
  booktitle={2024 International Conference on Communication, Computing, Smart Materials and Devices (ICCCSMD)}, 
  title={HEALTH-OPS: An AI Driven Healthcare Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In the realm of advancing healthcare and the ever-growing use of Artificial Intelligence, the medical chatbots are no less than a companion for medical professionals as well as common people. These chatbots are advanced enough to provide relevant and accurate medical information which helps people be more aware and health conscious and also aid in early detection and diagnosis of their ailments. In this work beyond the traditional question answering chatbots and we presented a smart healthbot capable of understanding context and generating text. With more features like supporting images and pdf file formats, the solution also incorporates a multi class chest X-Ray classification model which classifies an inputted chest X-Ray image into one of 14 thoracic conditions with an accuracy of 91% and a text summarization model specifically trained on biomedical text which can outline the input long format text file.},
  keywords={Accuracy;Biological system modeling;Text summarization;Medical services;Machine learning;Chatbots;User experience;Usability;X-ray imaging;Medical diagnostic imaging;Medical Chatbot;X-Ray Classification;Text Summarization;Efficient Net;Large Language Model (LLM);Generative Pretrained Transformer (GPT);Flask},
  doi={10.1109/ICCCSMD63546.2024.11015117},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11142107,
  author={Trang Pham, Thi Thu and Pham, Phuc Hoang and Pham, Van Hoang and Tran, Hieu Minh and Duong, Tan Nghia},
  booktitle={2025 10th International Conference on Applying New Technology in Green Buildings (ATiGB)}, 
  title={ViGPT Researcher: An AI-Powered Online Research Assistant for Comprehensive and In-Depth Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={199-204},
  abstract={The rapid advancement of artificial intelligence (AI) has significantly transformed the landscape of online research. Large language models (LLMs) have shown great potential in assisting researchers; however, they also present challenges such as hallucinations, bias, and limited contextual depth. This paper introduces ViGPT Researcher, an AI-powered research assistant designed to enhance online research by integrating Retrieval Augmented Generation (RAG) and Plan-and-Solve Prompting (PS Prompting). By leveraging these two techniques, ViGPT Researcher improves information retrieval, minimizes hallucination, and provides users with comprehensive, well-structured research reports. The system is implemented as a web-based application, offering an intuitive user interface and supporting real-time research workflows. This study evaluates the effectiveness of ViGPT Researcher in comparison with existing AI research assistants such as ChatGPT and Gemini. The findings suggest that our approach significantly enhances accuracy, relevance, and user experience in AI-assisted research.},
  keywords={Accuracy;Law;Large language models;Retrieval augmented generation;User interfaces;Transformers;Information retrieval;User experience;Robustness;Real-time systems;Retrieval-augmented generation;natural languge processing;generative pretrained transformer},
  doi={10.1109/ATiGB66719.2025.11142107},
  ISSN={},
  month={July},}@INPROCEEDINGS{9825415,
  author={Sonkiya, Priyank and Bajpai, Vikas and Bansal, Anukriti},
  booktitle={2022 IEEE 7th International conference for Convergence in Technology (I2CT)}, 
  title={Stock Price Prediction Using Artificial Intelligence: A Survey}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={The stock market and the investments into it are two aspects which have gained quite some popularity in the previous decade. Although a lot of people are involved in trading and analysis of the stock market yet stock price prediction remains an exigent task. Scientists and researchers have tried their hands on different methodologies performing the same. A variety of techniques ranging from basic statistical models to stat of the art Deep Learning models have been used in performing stock price prediction. The beginning of the previous decade began with people using trivial machine learning regression models to determine stock prices, then some tried incurring sentiment analysis along with the stock regression. And finally toward the end of the decade a bunch of authors have shown very promising results using advanced Neural Networks like Generative Adversarial Networks. The aim and the purpose of this paper is to summarise the recent progress in the stock price prediction domain, comparing the recent works in the same domain. Comparison has been done on parameters like data set used, sentiments, models / algorithms, evaluation metrics and the conclusion laid down by different authors.},
  keywords={Deep learning;Sentiment analysis;Social networking (online);Neural networks;Predictive models;Prediction algorithms;Data models;Stock Market;Stock Price Prediction;Machine Learning;Deep Learning and Survey},
  doi={10.1109/I2CT54291.2022.9825415},
  ISSN={},
  month={April},}@ARTICLE{10201469,
  author={Shamsolmoali, Pourya and Zareapoor, Masoumeh and Zhou, Huiyu and Tao, Dacheng and Li, Xuelong},
  journal={IEEE Transactions on Image Processing}, 
  title={VTAE: Variational Transformer Autoencoder With Manifolds Learning}, 
  year={2023},
  volume={32},
  number={},
  pages={4486-4500},
  abstract={Deep generative models have demonstrated successful applications in learning non-linear data distributions through a number of latent variables and these models use a non-linear function (generator) to map latent samples into the data space. On the other hand, the non-linearity of the generator implies that the latent space shows an unsatisfactory projection of the data space, which results in poor representation learning. This weak projection, however, can be addressed by a Riemannian metric, and we show that geodesics computation and accurate interpolations between data samples on the Riemannian manifold can substantially improve the performance of deep generative models. In this paper, a Variational spatial-Transformer AutoEncoder (VTAE) is proposed to minimize geodesics on a Riemannian manifold and improve representation learning. In particular, we carefully design the variational autoencoder with an encoded spatial-Transformer to explicitly expand the latent variable model to data on a Riemannian manifold, and obtain global context modelling. Moreover, to have smooth and plausible interpolations while traversing between two different objects’ latent representations, we propose a geodesic interpolation network different from the existing models that use linear interpolation with inferior performance. Experiments on benchmarks show that our proposed model can improve predictive accuracy and versatility over a range of computer vision tasks, including image interpolations, and reconstructions.},
  keywords={Data models;Interpolation;Manifolds;Computational modeling;Feature extraction;Representation learning;Manifold learning;Deep generative models;autoencoders;spatial-transformer},
  doi={10.1109/TIP.2023.3299495},
  ISSN={1941-0042},
  month={},}@ARTICLE{10105864,
  author={Tan, Yong Xuan and Lee, Chin Poo and Neo, Mai and Lim, Kian Ming and Lim, Jit Yan},
  journal={IEEE Access}, 
  title={Enhanced Text-to-Image Synthesis With Self-Supervision}, 
  year={2023},
  volume={11},
  number={},
  pages={39508-39519},
  abstract={The task of Text-to-Image synthesis is a difficult challenge, especially when dealing with low-data regimes, where the number of training samples is limited. In order to address this challenge, the Self-Supervision Text-to-Image Generative Adversarial Networks (SS-TiGAN) has been proposed. The method employs a bi-level architecture, which allows for the use of self-supervision to increase the number of training samples by generating rotation variants. This, in turn, maximizes the diversity of the model representation and enables the exploration of high-level object information for more detailed image construction. In addition to the use of self-supervision, SS-TiGAN also investigates various techniques to address the stability issues that arise in Generative Adversarial Networks. By implementing these techniques, the proposed SS-TiGAN has achieved a new state-of-the-art performance on two benchmark datasets, Oxford-102 and CUB. These results demonstrate the effectiveness of the SS-TiGAN method in synthesizing high-quality, realistic images from text descriptions under low-data regimes.},
  keywords={Semantics;Generators;Image synthesis;Generative adversarial networks;Computer architecture;Image synthesis;Visualization;Text mining;Text-to-image synthesis;generative model;GAN;self-supervised learning;generative adversarial networks},
  doi={10.1109/ACCESS.2023.3268869},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8953692,
  author={Qu, Yanyun and Chen, Yizi and Huang, Jingying and Xie, Yuan},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Enhanced Pix2pix Dehazing Network}, 
  year={2019},
  volume={},
  number={},
  pages={8152-8160},
  abstract={In this paper, we reduce the image dehazing problem to an image-to-image translation problem, and propose Enhanced Pix2pix Dehazing Network (EPDN), which generates a haze-free image without relying on the physical scattering model. EPDN is embedded by a generative adversarial network, which is followed by a well-designed enhancer. Inspired by visual perception global-first theory, the discriminator guides the generator to create a pseudo realistic image on a coarse scale, while the enhancer following the generator is required to produce a realistic dehazing image on the fine scale. The enhancer contains two enhancing blocks based on the receptive field model, which reinforces the dehazing effect in both color and details. The embedded GAN is jointly trained with the enhancer. Extensive experiment results on synthetic datasets and real-world datasets show that the proposed EPDN is superior to the state-of-the-art methods in terms of PSNR, SSIM, PI, and subjective visual effect.},
  keywords={Image dehazing;Translation;Image color analysis;Scattering;Transforms;Generative adversarial networks;Visual effects;Generators;Visual perception;Synthetic data;Low-level Vision},
  doi={10.1109/CVPR.2019.00835},
  ISSN={2575-7075},
  month={June},}@ARTICLE{1262308,
  author={Bahlmann, C. and Burkhardt, H.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The writer independent online handwriting recognition system frog on hand and cluster generative statistical dynamic time warping}, 
  year={2004},
  volume={26},
  number={3},
  pages={299-310},
  abstract={In this paper, we give a comprehensive description of our writer-independent online handwriting recognition system frog on hand. The focus of this work concerns the presentation of the classification/training approach, which we call cluster generative statistical dynamic time warping (CSDTW). CSDTW is a general, scalable, HMM-based method for variable-sized, sequential data that holistically combines cluster analysis and statistical sequence modeling. It can handle general classification problems that rely on this sequential type of data, e.g., speech recognition, genome processing, robotics, etc. Contrary to previous attempts, clustering and statistical sequence modeling are embedded in a single feature space and use a closely related distance measure. We show character recognition experiments of frog on hand using CSDTW on the UNIPEN online handwriting database. The recognition accuracy is significantly higher than reported results of other handwriting recognition systems. Finally, we describe the real-time implementation of frog on hand on a Linux Compaq iPAQ embedded device.},
  keywords={Handwriting recognition;Hidden Markov models;Speech recognition;Genomics;Bioinformatics;Orbital robotics;Extraterrestrial measurements;Character recognition;Spatial databases;Linux},
  doi={10.1109/TPAMI.2004.1262308},
  ISSN={1939-3539},
  month={March},}@INPROCEEDINGS{9207637,
  author={Fidel, Gil and Bitton, Ron and Shabtai, Asaf},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={State-of-the-art deep neural networks (DNNs) are highly effective in solving many complex real-world problems. However, these models are vulnerable to adversarial perturbation attacks, and despite the plethora of research in this domain, to this day, adversaries still have the upper hand in the cat and mouse game of adversarial example generation methods vs. detection and prevention methods. In this research, we present a novel detection method that uses Shapley Additive Explanations (SHAP) values computed for the internal layers of a DNN classifier to discriminate between normal and adversarial inputs. We evaluate our method by building an extensive dataset of adversarial examples over the popular CIFAR-10 and MNIST datasets, and training a neural network-based detector to distinguish between normal and adversarial inputs. We evaluate our detector against adversarial examples generated by diverse state-of-the-art attacks and demonstrate its high detection accuracy and strong generalization ability to adversarial inputs generated with different attack methods.},
  keywords={Perturbation methods;Automobiles;Detectors;Cats;Training;Artificial intelligence;Neurons;Adversarial Learning;Explainable AI;SHAP;Deep Learning},
  doi={10.1109/IJCNN48605.2020.9207637},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10079087,
  author={Lin, Yijing and Du, Hongyang and Niyato, Dusit and Nie, Jiangtian and Zhang, Jiayi and Cheng, Yanyu and Yang, Zhaohui},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Blockchain-Aided Secure Semantic Communication for AI-Generated Content in Metaverse}, 
  year={2023},
  volume={4},
  number={},
  pages={72-83},
  abstract={The construction of virtual transportation networks requires massive data to be transmitted from edge devices to Virtual Service Providers (VSP) to facilitate circulations between the physical and virtual domains in Metaverse. Leveraging semantic communication for reducing information redundancy, VSPs can receive semantic data from edge devices to provide varied services through advanced techniques, e.g., AI-Generated Content (AIGC), for users to explore digital worlds. But the use of semantic communication raises a security issue because attackers could send malicious semantic data with similar semantic information but different desired content to break Metaverse services and cause wrong output of AIGC. Therefore, in this paper, we first propose a blockchain-aided semantic communication framework for AIGC services in virtual transportation networks to facilitate interactions of the physical and virtual domains among VSPs and edge devices. We illustrate a training-based targeted semantic attack scheme to generate adversarial semantic data by various loss functions. We also design a semantic defense scheme that uses the blockchain and zero-knowledge proofs to tell the difference between the semantic similarities of adversarial and authentic semantic data and to check the authenticity of semantic data transformations. Simulation results show that the proposed defense method can reduce the semantic similarity of the adversarial semantic data and the authentic ones by up to 30% compared with the attack scheme.},
  keywords={Semantics;Blockchains;Metaverse;Transportation;Artificial intelligence;Image edge detection;Security;Metaverse;blockchain;semantic communication;semantic attacks;semantic defenses},
  doi={10.1109/OJCS.2023.3260732},
  ISSN={2644-1268},
  month={},}@ARTICLE{4447672,
  author={Kokkinos, Iasonas and Evangelopoulos, Georgios and Maragos, Petros},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Texture Analysis and Segmentation Using Modulation Features, Generative Models, and Weighted Curve Evolution}, 
  year={2009},
  volume={31},
  number={1},
  pages={142-157},
  abstract={In this work we approach the analysis and segmentation of natural textured images by combining ideas from image analysis and probabilistic modeling. We rely on AM-FM texture models and specifically on the Dominant Component Analysis (DCA) paradigm for feature extraction. This method provides a low-dimensional, dense and smooth descriptor, capturing essential aspects of texture, namely scale, orientation, and contrast. Our contributions are at three levels of the texture analysis and segmentation problems: First, at the feature extraction stage we propose a regularized demodulation algorithm that provides more robust texture features and explore the merits of modifying the channel selection criterion of DCA. Second, we propose a probabilistic interpretation of DCA and Gabor filtering in general, in terms of Local Generative Models. Extending this point of view to edge detection facilitates the estimation of posterior probabilities for the edge and texture classes. Third, we propose the weighted curve evolution scheme that enhances the Region Competition/ Geodesic Active Regions methods by allowing for the locally adaptive fusion of heterogeneous cues. Our segmentation results are evaluated on the Berkeley Segmentation Benchmark, and compare favorably to current state-of-the-art methods.},
  keywords={Image texture analysis;Image segmentation;Image edge detection;Feature extraction;Image analysis;Algorithm design and analysis;Demodulation;Robustness;Gabor filters;Filtering;Image Processing and Computer Vision;Texture;Edge and feature detection;Segmentation;Image models;Statistical;Image Processing and Computer Vision;Texture;Edge and feature detection;Segmentation;Image models;Statistical},
  doi={10.1109/TPAMI.2008.33},
  ISSN={1939-3539},
  month={Jan},}@ARTICLE{9839630,
  author={Zhang, Mingqiang and Zhang, Haixia and Fang, Yuguang and Yuan, Dongfeng},
  journal={IEEE Network}, 
  title={Learning-Based Data Transmissions for Future 6G Enabled Industrial IoT: A Data Compression Perspective}, 
  year={2022},
  volume={36},
  number={5},
  pages={180-187},
  abstract={The sixth-generation (6G) wireless system has been perceived to be the technology to connect everything. This will generate a huge volume of data traffic, resulting in severe spectrum shortage and system latency. To address this issue, data compression is considered to be indispensable for 6G to achieve efficient data transmissions, increase spectrum efficiency, and reduce system latency. Consequently, data compression technologies based on machine learning and deep learning, commonly known as learning-based data compressions, have received intensive attention lately. Taking the industrial IoT (IIoT) as a use case, this article attempts to explore the latest research progress on learning-based data compressions toward efficient data transmissions. Specifically, we first propose a novel learning-based data compression framework for edge-cloud collaborative IIoT. Then, we summarize the learning-based data transmission methods which are involved with various layers of the proposed edge-cloud collaborative framework. Moreover, we conduct a case study to show that our learning-based data transmission methods can effectively reduce the volume of the transmitted data. Finally, we highlight several promising future research directions on the learning-based data compression, such as robustness and instability of deep models, deep model optimization, and future deployment strategies for 6G enabled IIoT.},
  keywords={Industrial Internet of Things;Data compression;6G mobile communication;Artificial intelligence;Data models;Collaboration;Data aggregation},
  doi={10.1109/MNET.109.2100384},
  ISSN={1558-156X},
  month={Sep.},}@ARTICLE{10742564,
  author={Wang, Heng and Zhang, Jianhua and Nie, Gaofeng and Yu, Li and Yuan, Zhiqiang and Li, Tongjie and Wang, Jialin and Liu, Guangyi},
  journal={IEEE Communications Magazine}, 
  title={Digital Twin Channel for 6G: Concepts, Architectures and Potential Applications}, 
  year={2025},
  volume={63},
  number={3},
  pages={24-30},
  abstract={Digital twin channel (DTC) is the real-time mapping of radio channels and associated communication operations from the physical world to the digital world, which is expected to provide significant performance enhancements for the sixth-generation (6G) communication system. This article aims to bridge the gap between conventional channel twin research and emerging DTC by defining five evolution levels of channel twins from aspects including methodology, data category, and application. Up to now, the industry and academia have made significant progress in the fourth-level twin, and have begun the research on the fifth-level twin, that is, autonomous DTC, which offers the opportunity for a new 6G communication paradigm. This article subsequently provides detailed insights into the requirements and possible architecture of a complete DTC for 6G. Then, the feasibility of real-time DTC is experimentally validated. Finally, drawing from the 6G typical usages, we explore the potential applications and the open issues in future DTC research.},
  keywords={Real-time systems;Decision making;6G mobile communication;Artificial intelligence;Sensors;Wireless communication;Manuals;Digital twins;Data models;Wireless sensor networks},
  doi={10.1109/MCOM.001.2400213},
  ISSN={1558-1896},
  month={March},}@ARTICLE{10285419,
  author={Wang, Shulan and Liu, Qin and Xu, Yang and Jiang, Hongbo and Wu, Jie and Wang, Tian and Peng, Tao and Wang, Guojun},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Protecting Inference Privacy With Accuracy Improvement in Mobile-Cloud Deep Learning}, 
  year={2024},
  volume={23},
  number={6},
  pages={6522-6537},
  abstract={With the wide spread of data-driven deep learning applications, a growing number of users outsource compute-intensive inference processes to the cloud. To protect inference privacy, Liu et al. (INFOCOM 2022) proposed two steganography-based solutions, named $\mathsf{GHOST}$GHOST and $\mathsf{GHOST}^+$GHOST+, relying on the mobile-cloud collaborative framework, where the mobile device hides sensitive images into public cover images before feature extraction, while launching adversarial attacks on the cloud-side deep neural network (DNN) to obtain desired results. Although both solutions demonstrate significant advantages in private deep learning, they suffer from limited practicality; since the inference accuracy decreases sharply as the hiding ratio increases. To address this, we propose two improved solutions, $\mathsf{IGHO}$IGHO and $\mathsf{IGHO}^+$IGHO+, which ensure high inference accuracy even when abundant sensitive images need to be hidden. Specifically, $\mathsf{IGHO}$IGHO as the improved version of $\mathsf{GHOST}$GHOST proposes two feature fusion methods, feature synthesis and pixel synthesis, to preprocess cover images, making the poisoned DNN learn hidden sensitive features better, while $\mathsf{IGHO}^+$IGHO+ as the improved version of $\mathsf{GHOST}^+$GHOST+ designs a novel feature mining generative adversarial network (FMGAN) to craft adversarial perturbations highly robust against variable sensitive types. Experimental results show that the proposed solutions highly improve the practicality of $\mathsf{GHOST}$GHOST and $\mathsf{GHOST}^+$GHOST+.},
  keywords={Privacy;Deep learning;Cryptography;Steganography;Feature extraction;Training;Generative adversarial networks;Mobile cloud;deep learning;inference privacy;steganography;adversarial attacks},
  doi={10.1109/TMC.2023.3323450},
  ISSN={1558-0660},
  month={June},}@ARTICLE{10278413,
  author={Afzal, Muhammad Usman and Abdellatif, Alaa Awad and Zubair, Muhammad and Mehmood, Muhammad Qasim and Massoud, Yehia},
  journal={IEEE Access}, 
  title={Privacy and Security in Distributed Learning: A Review of Challenges, Solutions, and Open Research Issues}, 
  year={2023},
  volume={11},
  number={},
  pages={114562-114581},
  abstract={In recent years, the way that machine learning is used has undergone a paradigm shift driven by distributed and collaborative learning. Several approaches have emerged to enable pervasive computing and distributed learning in ubiquitous Internet of Things (IoT) systems. Numerous decentralized strategies have been proposed to deal with the limitations of centralized learning, including privacy and latency due to sharing local data, while utilizing distributed computations as a promising substitute to centralized learning. However, such distributed learning schemes come with new security and privacy concerns that should be addressed. Thus, in this paper, we first provide an overview for the emerging paradigms developed for distributed learning. Then, we performed a comprehensive survey for the privacy and security challenges associated with distributed learning along with the presented solutions to overcome them. Furthermore, we highlight key challenges and open future research directions toward implementing more robust distributed systems.},
  keywords={Distance learning;Computer aided instruction;Security;Privacy;Artificial intelligence;Data privacy;Servers;Internet of Things;Adversarial machine learning;Data privacy and security;Internet of Things (IoT);deep learning;adversarial attacks},
  doi={10.1109/ACCESS.2023.3323932},
  ISSN={2169-3536},
  month={},}@ARTICLE{10504424,
  author={Pahk, Jinu and Park, Seongjeong and Shim, Jungseok and Son, Sungho and Lee, Jungki and An, Jinung and Lim, Yongseob and Choi, Gyeungho},
  journal={IEEE Robotics and Automation Letters}, 
  title={Lane Segmentation Data Augmentation for Heavy Rain Sensor Blockage Using Realistically Translated Raindrop Images and CARLA Simulator}, 
  year={2024},
  volume={9},
  number={6},
  pages={5488-5495},
  abstract={Lane segmentation and Lane Keeping Assist System (LKAS) play a vital role in autonomous driving. While deep learning technology has significantly improved the accuracy of lane segmentation, real-world driving scenarios present various challenges. In particular, heavy rainfall not only obscures the road with sheets of rain and fog but also creates water droplets on the windshield or lens of the camera that affects the lane segmentation performance. There may even be a false positive problem in which the algorithm incorrectly recognizes a raindrop as a road lane. Collecting heavy rain data is challenging in real-world settings, and manual annotation of such data is expensive. In this research, we propose a realistic raindrop conversion process that employs a contrastive learning-based Generative Adversarial Network (GAN) model to transform raindrops randomly generated using Python libraries. In addition, we utilize the attention mask of the lane segmentation model to guide the placement of raindrops in training images from the translation target domain (real Rainy-Images). By training the ENet-SAD model using the realistically Translated-Raindrop images and lane ground truth automatically extracted from the CARLA Simulator, we observe an improvement in lane segmentation accuracy in Rainy-Images. This method enables training and testing of the perception model while adjusting the number, size, shape, and direction of raindrops, thereby contributing to future research on autonomous driving in adverse weather conditions.},
  keywords={Rain;Image segmentation;Training;Mathematical models;Generative adversarial networks;Data augmentation;Computer vision;Robot vision systems;Computer vision for automation;data sets for robotic vision;simulation and animation},
  doi={10.1109/LRA.2024.3390587},
  ISSN={2377-3766},
  month={June},}@INPROCEEDINGS{10134594,
  author={Wani, Shaiq and Ahuja, Sachin and Kumar, Abhishek},
  booktitle={2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={A review on Brain Tumor Detection using Deep Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={363-370},
  abstract={Artificial Intelligence has in the last 10 years drastically transformed the globe. Owing to its ability to manage vast volumes of data, deep Learning (DL) a subsection of Artificial Intelligence and machine learning, especially in the healthcare area, has proven exceptional accomplishments. New developments in DL based techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), Generative adversarial neural networks, and many more that have shown exceptional potential for inaccurate categorization of brain cancers. The major goal of this paper is mainly to critically examine the previous identification and classification efforts of brain tumors using MRI (Magnetic Resonance Imaging) data. In this work, we review the best current algorithms for deep learning applications in Brain tumor classification problem contexts and utilization. The most significant findings that have been documented thus far are addressed, and finally, the research’s present hurdles are also highlighted. The conclusion discusses the pros and limitations of deep neural networks. In the time ahead using the results reported in this paper, researchers will be able to evaluate recent studies extensively and gain a knowledge of how different DL algorithms function.},
  keywords={Deep learning;Recurrent neural networks;Communication systems;Magnetic resonance imaging;Medical services;Feature extraction;Classification algorithms;Artificial Intelligence;Deep Learning;Convolutional Neural Networks;Generative Adversarial Networks;Recurrent neural networks;Classification;Magnetic Resonance Imaging},
  doi={10.1109/CSNT57126.2023.10134594},
  ISSN={2473-5655},
  month={April},}@ARTICLE{10379089,
  author={Park, Hanhoon},
  journal={IEEE Access}, 
  title={Semantic Super-Resolution via Self-Distillation and Adversarial Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={2361-2370},
  abstract={Semantic super-resolution (SR) is an approach that improves the SR performance by leveraging semantic information about the scene. This study develops a novel semantic SR method that is based on the generative adversarial network (GAN) framework and self-distillation. A discriminator is adversarially trained along with a generator to extract semantic features from images and distinguish semantic differences between images. To train the generator, an additional adversarial loss is computed from the discriminator’s outputs of SR images belonging to the same category and minimized via self-distillation. This guides the generator to learn implicit category-specific semantic priors. We conducted experiments for SR of text and face images using the Enhanced Deep Super-Resolution (EDSR) generator and the SRGAN discriminator. Experimental results showed that our method can contribute to improving both the quantitative and qualitative quality of SR images. Although the improvement varied depending on image category and dataset, the peak signal-to-noise ratio (PSNR) value increased by up to 0.87 dB and the perceptual index (PI) decreased by up to 0.17 by using our method. Our method outperformed existing semantic SR methods.},
  keywords={Semantics;Feature extraction;Superresolution;Generative adversarial networks;Training;Face recognition;Image resolution;Text categorization;Adversarial machine learning;Image super-resolution;semantic super-resolution;self-distillation;adversarial learning;text images;face images;EDSR;SRGAN},
  doi={10.1109/ACCESS.2023.3349023},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9998162,
  author={Shaw, Priti and Sankaranarayanan, Suresh and Lorenz, Pascal},
  booktitle={2022 6th International Conference on Communication and Information Systems (ICCIS)}, 
  title={Early Esophageal Malignancy Detection Using Deep Transfer Learning and Explainable AI}, 
  year={2022},
  volume={},
  number={},
  pages={129-135},
  abstract={Esophageal malignancy is a rare form of cancer that starts in the esophagus and spreads to the other parts of the body, impacting a severe risk on the liver, lungs, lymph nodes, and stomach. Studies have shown that esophageal cancer is one of the most prevalent causes of cancer mortality. In 2020, 604100 individuals have been diagnosed with this deadly disease. There are a good number of medical studies, carried out on this topic, every year. A similar focus is also imparted on the AI-based deep learning models for the classification of malignancy. But the challenge is that the AI models are all complex and lack transparency. There is no available information to explain the opacity of such models. And as AI-based medical research seeks reliability, it becomes very important to bring in explainability. So we, through this research, have used Explainable AI(XAI) entitled LIME for creating trust-based models for the early detection of esophageal malignancy. We have used a simple CNN model and several transfer learning-based models, for this study. We have taken the actual endoscopic images from the Kvasir-v2 dataset resulting in an accuracy of 88.75%. with the DenseNet-201 model followed by the usage of an Explainable AI model, Lime, for giving an explanation for the images classified. The deep learning model, combined with explainable AI, helps in getting a clear picture of the regions contributing toward the malignancy prediction and promotes confidence in the model, without the intervention of a domain expert.},
  keywords={Deep learning;Stomach;Transfer learning;Lung;Predictive models;Reliability;Artificial intelligence;Esophageal malignancy;healthcare;CNN;Deep Learning;transfer learning;Explainable AI;LIME},
  doi={10.1109/ICCIS56375.2022.9998162},
  ISSN={},
  month={Oct},}@ARTICLE{10909613,
  author={Guo, Shuaishuai and Zhang, Anbang and Wang, Yanhu and Feng, Chenyuan and Quek, Tony Q. S.},
  journal={IEEE Network}, 
  title={Semantic-Enabled 6G Communication: A Task-oriented and Privacy-preserving Perspective}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Task-oriented semantic communication (ToSC) emerges as an innovative approach in the 6G landscape, characterized by the transmission of only vital information that is directly pertinent to a specific task. While ToSC offers an efficient mode of communication, it concurrently raises concerns regarding privacy, as sophisticated adversaries might possess the capability to reconstruct the original data from the transmitted features. This paper provides an in-depth analysis of privacy-preserving strategies specifically designed for ToSC relying on deep neural network-based joint source and channel coding (DeepJSCC). Our study encompasses a detailed comparative assessment of trustworthy feature perturbation methods such as differential privacy (DP) and encryption, alongside intrinsic security incorporation approaches like adversarial learning to train the JSCC and learning-based vector quantization (LBVQ). Our comparative analysis underscores the integration of advanced explainable learning algorithms into communication systems, positing a new benchmark for privacy standards in the forthcoming 6G era.},
  keywords={Feature extraction;Training;Privacy;Encryption;Security;Perturbation methods;Noise;Artificial intelligence;Adversarial machine learning;Protection},
  doi={10.1109/MNET.2025.3547760},
  ISSN={1558-156X},
  month={},}@ARTICLE{10684121,
  author={Nguyen, Duy-Thanh and Bhattacharjee, Abhiroop and Moitra, Abhishek and Panda, Priyadarshini},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={MCAIMem: A Mixed SRAM and eDRAM Cell for Area and Energy-Efficient On-Chip AI Memory}, 
  year={2024},
  volume={32},
  number={11},
  pages={2023-2036},
  abstract={AI chips commonly employ SRAM memory as buffers for their reliability and speed, which contribute to high performance. However, SRAM is expensive and demands significant area and energy consumption. Previous studies have explored replacing SRAM with emerging technologies, such as nonvolatile memory, which offers fast read memory access and a small cell area. Despite these advantages, nonvolatile memory’s slow write memory access and high write energy consumption prevent it from surpassing SRAM performance in AI applications with extensive memory access requirements. Some research has also investigated embedded dynamic random access memory (eDRAM) as an area-efficient on-chip memory with similar access times as SRAM. Still, refresh power remains a concern, leaving the trade-off among performance, area, and power consumption unresolved. To address this issue, this article presents a novel mixed CMOS cell memory design that balances performance, area, and energy efficiency for AI memory by combining SRAM and eDRAM cells. We consider the proportion ratio of one SRAM and seven eDRAM cells in the memory to achieve area reduction using mixed CMOS cell memory. In addition, we capitalize on the characteristics of deep neural network (DNN) data representation and integrate asymmetric eDRAM cells to lower energy consumption. To validate our proposed MCAIMem solution, we conduct extensive simulations and benchmarking against traditional SRAM. Our results demonstrate that the MCAIMem significantly outperforms these alternatives in terms of area and energy efficiency. Specifically, our MCAIMem can reduce the area by 48% and energy consumption by  $3.4\times $  compared with SRAM designs, without incurring any accuracy loss.},
  keywords={Random access memory;System-on-chip;Artificial intelligence;Memory management;Transistors;Energy efficiency;Nonvolatile memory;AI;embedded dynamic random access memory (eDRAM);energy efficiency;on-chip memory;SRAM},
  doi={10.1109/TVLSI.2024.3439231},
  ISSN={1557-9999},
  month={Nov},}@ARTICLE{11010915,
  author={Li, Yinghua and Hao, Weiao and Zeng, Hao and Wang, Longguang and Xu, Jian and Routray, Sidheswar and Jhaveri, Rutvij H. and Gadekallu, Thippa Reddy},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Cross-Scale Texture Supplementation for Reference-based Medical Image Super-Resolution}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Magnetic Resonance Imaging (MRI) is a widely used medical imaging technique, but its resolution is often limited by acquisition time constraints, potentially compromising diagnostic accuracy. Reference-based Image Super-Resolution (RefSR) has shown promising performance in addressing such challenges by leveraging external high-resolution (HR) reference images to enhance the quality of low-resolution (LR) images. The core objective of RefSR is to accurately establish correspondences between the reference HR image and the LR images. In pursuit of this objective, this paper develops a Self-rectified Texture Supplementation network for RefSR (STS-SR) to enhance fine details in MRI images and support the expanding role of autonomous AI in healthcare. Our network comprises a texture-specified selfrectified feature transfer module and a cross-scale texture complementary network. The feature transfer module employs highfrequency filtering to facilitate the network concentrating on fine details. To better exploit the information from both the reference and LR images, our cross-scale texture complementary module incorporates the All-ViT and Swin Transformer layers to achieve feature aggregation at multiple scales, which enables high-quality image enhancement that is critical for autonomous AI systems in healthcare to make accurate decisions. Extensive experiments are performed across various benchmark datasets. The results validate the effectiveness of our method and demonstrate that the method produces state-of-the-art performance as compared to existing approaches. This advancement enables autonomous AI systems to utilize high-quality MRI images for more accurate diagnostics and reliable predictions.},
  keywords={Superresolution;Accuracy;Biomedical imaging;Artificial intelligence;Magnetic resonance imaging;Feature extraction;Transformers;Image reconstruction;Image restoration;Electronic mail;reference-based super-resolution;vision transformer;texture transfer;feature aggregation;image enhancement},
  doi={10.1109/JBHI.2025.3572502},
  ISSN={2168-2208},
  month={},}@ARTICLE{10494999,
  author={Yavuz, Can},
  journal={IEEE Security & Privacy}, 
  title={A Multidisciplinary Look at History and Future of Deepfake With Gartner Hype Cycle}, 
  year={2024},
  volume={22},
  number={3},
  pages={50-61},
  abstract={Deepfake has been swiftly advanced, misused, and democratized, which created overhype around it. To separate the overhype from reality, this study presents a multidisciplinary history of deepfake and speculates on the future of deepfake using the Gartner Hype Cycle.},
  keywords={Deepfakes;Media;Artificial intelligence;Productivity;History;Regulation;Social networking (online);Synthetic data;Audio-visual systems},
  doi={10.1109/MSEC.2024.3380324},
  ISSN={1558-4046},
  month={May},}@INPROCEEDINGS{9797955,
  author={Vaccari, Ivan and Carlevaro, Alberto and Narteni, Sara and Cambiaso, Enrico and Mongelli, Maurizio},
  booktitle={IEEE INFOCOM 2022 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={On The Detection Of Adversarial Attacks Through Reliable AI}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Adversarial machine learning manipulates datasets to mislead machine learning algorithm decisions. We propose a new approach able to detect adversarial attacks, based on eXplainable and Reliable AI. The results obtained show how canonical algorithms may have difficulty in identifying attacks, while the proposed approach is able to correctly identify different adversarial settings.},
  keywords={Machine learning algorithms;Conferences;Adversarial machine learning;Reliability;Artificial intelligence;machine learning;detection algorithms;adversarial machine learning;reliable},
  doi={10.1109/INFOCOMWKSHPS54753.2022.9797955},
  ISSN={},
  month={May},}@INPROCEEDINGS{10962577,
  author={Shan, Md. Abdul Kahhar Siddiki and Mahmud, Tasfin and Rahman, Tahmid Noor and Islam, Md. Sazzadul},
  booktitle={2024 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)}, 
  title={VGG19 and Inception V3 Performance Evaluation for Early Leukemia Detection from Blood Smears}, 
  year={2024},
  volume={},
  number={},
  pages={200-205},
  abstract={Leukemia is a type of cancer that affects the blood and bone marrow, where the body produces an abnormal amount of white blood cells, leading to impaired immune function. We evaluated two deep learning models, VGG19 and Inception V3, for their performance in detecting leukemia from blood smear images. These models exhibited strong capabilities in distinguishing between benign, early, pre-leukemia, and progressive stages of the disease. The results indicate that both models performed well, with Inception V3 achieving an accuracy of 97.5% and VGG19 achieving 95.5%. Inception V3 showed a slight advantage in early detection sensitivity due to its complex architecture. These findings suggest that these AI-driven approaches have significant potential to enhance the speed, accuracy, and efficiency of leukemia diagnosis. However, further validation with larger datasets is needed to ensure generalizability, contributing to the integration of AI in medical diagnostics, particularly in hematological malignancies.},
  keywords={Deep learning;White blood cells;Performance evaluation;Accuracy;Sensitivity;Microscopy;Leukemia;Computer architecture;Medical diagnosis;Artificial intelligence;VGG19;Inception V3;AI;Dataset;Leukemia},
  doi={10.1109/BECITHCON64160.2024.10962577},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10984420,
  author={Pilaniwala, Pankaj},
  booktitle={2024 Eighth International Conference on Parallel, Distributed and Grid Computing (PDGC)}, 
  title={Integrating GenAI in Advancing Game Product Management and Development}, 
  year={2024},
  volume={},
  number={},
  pages={558-563},
  abstract={The paper focuses on using GenAI for a sustainable long-term gaming development process. Games have been around for a long time and the process of building them has not changed much. The paper presents a futuristic study of integrating GenAI into the current game development lifecycle and enhancing the overall video game development process. The paper showcases different phases of a game development lifecycle and highlights some of the issues gaming companies face. The paper presents a compelling argument for using GenAI to improve the game development process and showcases how GenAI can be leveraged in each step of gaming development. The paper highlights the long-term sustainability of using GenAI in gaming development workflows.},
  keywords={Industries;Human computer interaction;Video games;Navigation;Games;Companies;Artificial intelligence;Sustainable development;Faces;Investment;genAI;human-computer interaction;game development lifecycle;AI;product management},
  doi={10.1109/PDGC64653.2024.10984420},
  ISSN={2573-3079},
  month={Dec},}@INPROCEEDINGS{11113893,
  author={Pasupuleti, Nagalakshmi and Lakshmi, D. Venkata},
  booktitle={2025 5th International Conference on Expert Clouds and Applications (ICOECA)}, 
  title={Deepfake Methods and Statistics: A Comprehensive Review}, 
  year={2025},
  volume={},
  number={},
  pages={964-971},
  abstract={AI-generated images, videos, and audio, or deepfakes, have become increasingly problematic since they can easily be used to spread false information, harass people, and commit fraud. Although deepfakes have valid applications, their ability to deceive people into believing misleading information raises significant concerns. This paper discusses difficulty of identifying deepfakes, along with techniques researchers have developed. It also discusses evolution of deepfake technology and what it means for future. In order to keep up with more realistic deepfakes, this research aims to provide an overview of current deepfake detection approaches and emphasize the need for new, more reliable ones. A comprehensive analysis of deepfake approaches is provided in this paper, which promotes the adoption of new and more reliable techniques for coping with the incredibly complicated deepfakes by examining the history of deepfake and most recent detection techniques.},
  keywords={Deepfakes;Ethics;Reviews;Forgery;Fraud;Reliability;History;Artificial intelligence;Deepfake;GANs;Generated Images Introduction;Image Forgery;AI Ethics;Multimodal Analysis},
  doi={10.1109/ICOECA66273.2025.00168},
  ISSN={},
  month={March},}@INBOOK{9822790,
  author={Mushtaq, Snowber and Singh, Omkar},
  booktitle={Big Data Analytics for Internet of Things}, 
  title={Deep Learning Architectures for IoT Data Analytics}, 
  year={2021},
  volume={},
  number={},
  pages={143-166},
  abstract={Internet of Things (IoT) is internet over a network covering different devices or things and facilitating communication among them, thus generating a massive amount of fast, real‐time data. Applications have become intelligent with the rise of the IoT and the connection of different types of equipment has explored all aspects of modern needs. The IoT has gained massive popularity and used in a large variety of applications, due to active developments in hardware‐ and software‐based connected devices with communication between them. IoT has created a new dimension in the internet world, by new forms of communication such as between humans and things, and between two things. A large volume of data is generated from different types of equipment. Deep Learning (DL) techniques are applied to boost the intellect and the capabilities of an application. DL, a branch of Machine Learning (ML), is a novel technology and has attracted researchers due to its magical results. DL can handle a tremendous amount of complex, multidimensional, unstructured data, and can better recognize and extract features. Thus, it helps to deploy the IoT in a complex environment. Applying it to the IoT discovers valuable information worthy of decision‐making and quality control of crucial devices. DL technology has recently been deployed in the IoT to facilitate real‐time data from the complex environment and to build real‐time applications with accuracy and time like real‐time health monitoring, recognition of activities of students in a class, etc. They require a high level of quality and accuracy to monitor critical conditions of the environment and evaluate performance with less response time and accuracy. This chapter provides you with a brief study of multiple DL architectures and their applications in IoT.},
  keywords={Artificial intelligence;Training;Deep learning;Internet of Things;Feature extraction;Biological neural networks;Prediction algorithms},
  doi={10.1002/9781119740780.ch5},
  ISSN={},
  publisher={Wiley},
  isbn={9781119740766},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9822790},}@ARTICLE{11096072,
  author={Wang, Haotian and Tao, Jun and Gao, Yu and Chi, Dingwen and Zhu, Yuehao},
  journal={IEEE Transactions on Network and Service Management}, 
  title={A Two-Way Auction Approach Toward Data Quality Incentive Mechanisms for Mobile Crowdsensing}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the rapid growth of smart devices, mobile crowdsensing is becoming one of the most important and attractive paradigms to acquire information from physical environments. Low-quality data, a notorious but widely found issue, degrades the availability and preciseness of sensing services, especially for these complex sensing task scenarios. However, few existing incentive mechanisms frequently ignore the issue of data quality. In this paper, we define user reputation and user task preferences in a new perspective, while predicting the number of users likely to upload high-quality data by combining Poisson distribution. Then, the maximum expectation algorithm is employed to evaluate the parameter values of the Poisson distribution. Subsequently, a two-way auction mechanism is proposed, which encourages users to participate in the sensing task and improves the match between tasks and users. We adopt the number of high-quality data that the user may upload as a factor in the user’s offer to maximize the quality of data received by the platform. The analysis based on the model lays a theoretical foundation on the incentive process of mobile crowdsensing considering data quality. The evaluation results show that our mechanism outperforms other existing techniques, in terms of robustness and efficiency.},
  keywords={Sensors;Mobile computing;Data integrity;Crowdsensing;Data models;Recruitment;Mobile handsets;Hands;Games;Artificial intelligence;Mobile crowdsensing;Data quality;Incentive mechanism},
  doi={10.1109/TNSM.2025.3592489},
  ISSN={1932-4537},
  month={},}@ARTICLE{10778281,
  author={Li, Mengsi and Zhang, Jie and Hong, Yan and Xie, Xiangpeng and Zhang, Meng and Guo, Song},
  journal={IEEE Internet of Things Journal}, 
  title={Advanced Product Personalization in Blockchain-Enabled Metaverse: A Diffusion Model for Automatic Style Generation}, 
  year={2025},
  volume={12},
  number={8},
  pages={10304-10315},
  abstract={The Metaverse is a user-generated virtual world, aiming to provide highly personalized experiences for users. A product personalization design platform is a critical direction for the Metaverse’s future development, enhancing user experience by offering personalized services. Blockchain technology ensures the security and privacy of user data, and enables personalized services through smart contracts, offering opportunities for personalization platforms. However, blockchain’s decentralization can lead to excessive product data, resulting in ineffective data management and optimization, subsequently confusing personalized product design styles, which diminishes user experience. To address these issues, this study proposes the product style automatic generation system (PSAGS), centered on an image generation unit. The system outputs images with style information based on the input product text, achieving precise quantization and visualization of product styles, thereby enhancing user engagement and loyalty to the Metaverse. The image generation unit, with a standardization module can standardize the product style, namely, the relationship between product design elements and user emotions, addressing the problem of managing vast style data due to blockchain’s decentralization. The generation module utilizes a diffusion model enhanced with contrastive language-image pretraining (CLIP) to generate style images, deepening the Metaverse experience. Optimizations include dilation convolution in the UNet architecture to enhance image quality and fine-grained CLIP transformations for improved image and text alignment. Results demonstrate the system’s effectiveness in streamlining design processes and improving image quality in personalized product design, with wide applications in the Metaverse.},
  keywords={Metaverse;Blockchains;Artificial intelligence;Text to image;Diffusion models;Product design;Image synthesis;Electronic mail;Standardization;Optimization;Automatic generation;blockchain;diffusion model;Metaverse;product style},
  doi={10.1109/JIOT.2024.3511667},
  ISSN={2327-4662},
  month={April},}@INPROCEEDINGS{10895837,
  author={Sharma, Vaishali and Sharma, Deepank and Kumar Punia, Sanjeev},
  booktitle={2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)}, 
  title={Algorithmic Approaches, Practical Implementations and Future Research Directions in Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={121-126},
  abstract={Machine learning has become a disruptive force that is advancing technology and changing industries. With an emphasis on algorithmic techniques, real-world applications, and important future research avenues, this study examines the rapidly changing field of machine learning. It explores the fundamentals of supervised, unsupervised, and semi-supervised learning algorithms and highlights how they are used in a variety of fields, including autonomous systems, healthcare, and finance. The report highlights the potential of several important future research directions, such as explainable AI, robust and privacy-preserving learning, quantum machine learning, and multimodal AI, to overcome present constraints and open up hitherto unheard-of possibilities. These multidisciplinary research avenues emphasize the value of interdisciplinary cooperation in addressing difficult problems and guaranteeing the creation of morally sound and significant AI systems. In order to help academics and practitioners who want to progress the subject, this review attempts to give a thorough grasp of the present situation and potential future direction of machine learning.},
  keywords={Industries;Machine learning algorithms;Explainable AI;Reviews;Force;Transportation;Finance;Medical services;Learning (artificial intelligence);Semisupervised learning;Machine Learning;Algorithmic Approaches;Practical Implementations;Semi-Supervised Learning;Explainable AI;Robustness;Privacy-Preserving Learning;Quantum Machine Learning;Multimodal AI;Ethical AI;Cross-Domain Collaboration;Future Research Directions},
  doi={10.1109/ICAC2N63387.2024.10895837},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10611605,
  author={Zheng, Chuanxiong and Zhang, Lei and Wang, Hui and Gomez, Randy and Nichols, Eric and Li, Guangliang},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Shaping Social Robot to Play Games with Human Demonstrations and Evaluative Feedback}, 
  year={2024},
  volume={},
  number={},
  pages={9517-9523},
  abstract={In this paper, building on recent advances in the fields of gaming AI and social robotics, we present a new approach to facilitate the social robot Haru to imitate game strategies from human players’ demonstrated trajectories and evaluative feedback in a real-time two-player game. Our research shows that Haru is able to learn and imitate human different game strategies from human players in a human time scale. In addition, our results show that human evaluative feedback plays an important role in allowing Haru to obtain a better performance via our method than human player’s demonstrations. Finally, results of our user study indicate that Haru imitating human player’s game strategies via our method is perceived to be more human-like and have better game performance and experience than self-learning from pre-defined reward functions via traditional deep reinforcement learning.},
  keywords={Social robots;Buildings;Games;Deep reinforcement learning;Real-time systems;Trajectory;Artificial intelligence},
  doi={10.1109/ICRA57147.2024.10611605},
  ISSN={},
  month={May},}@INPROCEEDINGS{11011286,
  author={Sharma, Palvi and Gupta, Shubham and Sharma, Monu and Gupta, Neha},
  booktitle={2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)}, 
  title={Deepfake Detection and Mitigation Using Advanced CNN: Ensuring Digital Content Integrity}, 
  year={2025},
  volume={},
  number={},
  pages={661-665},
  abstract={Deepfake technology, with its hyper-realistic fake content and backed by AI technologies, presents a serious threat to the authenticity and security of digital content. The study deployed three different machine learning-based models: Support Vector Machine (SVM), Convolutional Neural Network (CNN), and Vision Transformers (ViT) for discrimination of real and fake faces. The model leverages the capabilities of CNN for automatic spatial feature learning from raw data, giving it exceptional performance in the discrimination between real and fake content. Performance measures were made in terms of accuracy, precision, recall, and F1 score and compared against traditional methods; the model remains a trustworthy contribution towards deepfake detection. Findings indicate that the CNN exceeded others with the highest precisions of up to 92.21%, recall of 90%, F1 score of 93%, and perfect accuracy of 97.25%. The paper further discusses the future developments, for instance, architectural improvements and real-time applications and expanding detection capacity outside visual data for a more formidable approach towards new deepfake formats, such as audio and text. This endeavor lays the groundwork for viable and scalable roads against the rising onslaught on digital media manipulation, ensuring content integrity in an AI-driven world.},
  keywords={Support vector machines;Representation learning;Deepfakes;Visualization;Accuracy;Roads;Transformers;Convolutional neural networks;Security;Artificial intelligence;Deepfake detection;digital media;ML;CNN;image and video analysis;AI},
  doi={10.1109/InCACCT65424.2025.11011286},
  ISSN={},
  month={April},}@INPROCEEDINGS{11124134,
  author={Blasch, Erik},
  booktitle={2025 28th International Conference on Information Fusion (FUSION)}, 
  title={Situation Awareness Using Fuzzy Analytical Hierarchy Processing}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Information Fusion seeks to combine observations from sensor data and/or human opinions to make a decision with reduced uncertainty. One common method for physics-based and human-derived information fusion (PHIF) that is well established is that of the analytical hierarchy processing (AHP). When decisions are not crisp, the Fuzzy AHP is a method towards establishing the boundary between decisions and the consistency among the decisions. In this paper, the Fuzzy AHP is utilized to assess information fusion designs developed from the Multisource AI Scorecard Table (MAST). MAST has devised a set of scores that can be ordinal (crisp), discrete (probabilities), and semantic (labels); and this paper explores a measure of uncertainty as from static fuzzy logic. The main finding is that assessment from different users with different ratings (i.e., Level 5 Fusion) can be enhanced by fuzzy AHP to check for consistency. If the combined PHIF results are not consistent, users would then need to subjectively identify weaknesses and limitations to continue the information fusion systems engineering design process.},
  keywords={Uncertainty;Image processing;Semantics;Measurement uncertainty;Data integration;Signal processing;Systems engineering and theory;Software;Information management;Artificial intelligence;Data Fusion;Multisource AI Scorecard Table;Situation Analysis;Information Management;Analytical Hierarchy Processing},
  doi={10.23919/FUSION65864.2025.11124134},
  ISSN={},
  month={July},}
