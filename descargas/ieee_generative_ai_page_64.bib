@INPROCEEDINGS{8756559,
  author={Xu, Lu and Hu, Chen and Li, Yinqi and Tao, Ji’an and Xue, Jianru and Mei, Kuizhi},
  booktitle={2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)}, 
  title={Deep Conditional Variational Estimation for Depth-Based Hand Poses}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  abstract={We propose a novel and effective approach for 3D hand pose estimation on single depth image. Instead of doing deterministic regression from depth images, our model focuses on learning a latent distribution to model the high dimensional space of pose joints, which can also be interpreted as a kinematics model for human hands. Specifically, the proposed network combines the framework of conditional variational autoencoder which learns an encoder and a decoder with standard convolutional network. The encoder models the latent variable as a prior or a regularization for the pose joints. Then probabilistic inference is performed by the decoder to generate the output prediction conditioned on input depth images. In addition, we introduce a pool-convolution module to improve the localization regression of the network. The architecture can be trained end-to-end. In experiments, we demonstrate the effectiveness of our proposed approach in comparison to various state-of-art holistic regression approaches.},
  keywords={Hands;Location awareness;Three-dimensional displays;Pose estimation;Kinematics;Gesture recognition;Probabilistic logic;Decoding;Convolutional neural networks;Standards},
  doi={10.1109/FG.2019.8756559},
  ISSN={},
  month={May},}@ARTICLE{10794620,
  author={Yang, Wen and Wu, Jinjian and Ma, Jupo and Li, Leida and Dong, Weisheng and Shi, Guangming},
  journal={IEEE Transactions on Image Processing}, 
  title={Learning Frame-Event Fusion for Motion Deblurring}, 
  year={2024},
  volume={33},
  number={},
  pages={6836-6849},
  abstract={Motion deblurring is a highly ill-posed problem due to the significant loss of motion information in the blurring process. Complementary informative features from auxiliary sensors such as event cameras can be explored for guiding motion deblurring. The event camera can capture rich motion information asynchronously with microsecond accuracy. In this paper, a novel frame-event fusion framework is proposed for event-driven motion deblurring (FEF-Deblur), which can sufficiently explore long-range cross-modal information interactions. Firstly, different modalities are usually complementary and also redundant. Cross-modal fusion is modeled as complementary-unique features separation-and-aggregation, avoiding the modality redundancy. Unique features and complementary features are first inferred with parallel intra-modal self-attention and inter-modal cross-attention respectively. After that, a correlation-based constraint is designed to act between unique and complementary features to facilitate their differentiation, which assists in cross-modal redundancy suppression. Additionally, spatio-temporal dependencies among neighboring inputs are crucial for motion deblurring. A recurrent cross attention is introduced to preserve inter-input attention information, in which the current spatial features and aggregated temporal features are attending to each other by establishing the long-range interaction between them. Extensive experiments on both synthetic and real-world motion deblurring datasets demonstrate our method outperforms state-of-the-art event-based and image/video-based methods.},
  keywords={Transformers;Event detection;Cameras;Videos;Image restoration;Convolutional neural networks;Redundancy;Correlation;Computer vision;Visualization;Event-based motion deblurring;cross-modality feature fusion;vision transformer},
  doi={10.1109/TIP.2024.3512362},
  ISSN={1941-0042},
  month={},}@ARTICLE{10584320,
  author={Zhang, Zikai and Ding, Chuntao and Li, Yidong and Yu, Jinhui and Li, Jingyi},
  journal={IEEE Transactions on Services Computing}, 
  title={SECaaS-Based Partially Observable Defense Model for IIoT Against Advanced Persistent Threats}, 
  year={2024},
  volume={17},
  number={6},
  pages={4267-4280},
  abstract={With the advancement of intelligent and networked technology, the Industrial Internet of Things (IIoT) faces an escalating threat from cyberattacks, especially by Advanced Persistent Threat (APT) attacks. These novel and complex attacks, characterized by their dynamic nature and life-long duration, pose significant challenges to existing security protection methods. The challenges are twofold, i.e., sparse reward problem in the long-lasting attack, and partial observation of attack actions. To this end, we propose a Security-as-a-Service based reinforcement learning method, namely Attention Augmented Dueling Deep Q-learning Network (AD2QN), to make real-time defense strategies for the hot standby IIoT. First, we build the attack-defend confrontation model as black boxes interact with the IIoT environment to play a long-lasting partially observable zero-sum stochastic game on the server. Then, to dynamically generate optimal defense strategies as the service, AD2QN is proposed employing information completion and prediction to more informed action selection. Furthermore, AD2QN utilizes an iteratively updated reward network to deal with the sparse reward problem. Extensive simulation results shown that the defense strategies generated by our method have a higher defense success rate and a stable defense performance with the average success rate of 0.7384, while the average success rate of baseline methods was 0.7375, in the best case.},
  keywords={Industrial Internet of Things;Security;Games;Stochastic processes;Q-learning;Computer crime;Uncertainty;Advanced persistent threat;attack-defend confrontation;decision-making;Industrial Internet of Things;partially observable;reinforcement learning;security-as-a-service;sparse reward},
  doi={10.1109/TSC.2024.3422870},
  ISSN={1939-1374},
  month={Nov},}@ARTICLE{10234446,
  author={Tian, Yu and Huang, Yalin and Zhang, Kunbo and Liu, Yue and Sun, Zhenan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Polarized Image Translation From Nonpolarized Cameras for Multimodal Face Anti-Spoofing}, 
  year={2023},
  volume={18},
  number={},
  pages={5651-5664},
  abstract={In face antispoofing, it is desirable to have multimodal images to demonstrate liveness cues from various perspectives. However, in most face recognition scenarios, only a single modality, namely visible lighting (VIS) facial images is available. This paper first investigates the possibility of generating polarized (Polar) images from VIS cameras without changing the existing recognition devices to improve the accuracy and robustness of Presentation Attack Detection (PAD) in face biometrics. A novel multimodal face antispoofing framework is proposed based on the machine-learning relationship between VIS and Polar images of genuine faces. Specifically, a dual-modal central differential convolutional network (CDCN) is developed to capture the inherent spoofing features between the VIS and the generated Polar modalities. Quantitative and qualitative experimental results show that our proposed framework not only generates realistic Polar face images but also improves the state-of-the-art face anti-spoofing results on the VIS modal database (i.e. CASIA-SURF). Moreover, a polar face database, CASIA-Polar, has been constructed and will be shared with the public at https://biometrics.idealtest.org to inspire future applications within the biometric anti-spoofing field.},
  keywords={Face recognition;Faces;Feature extraction;Imaging;Three-dimensional displays;Robustness;Costs;Face antispoofing;image translation;polarization;multimodal},
  doi={10.1109/TIFS.2023.3310348},
  ISSN={1556-6021},
  month={},}@ARTICLE{10975134,
  author={Lou, Zhengzheng and Xue, Hang and Wang, Yanzheng and Zhang, Chaoyang and Yang, Xin and Hu, Shizhe},
  journal={IEEE Transactions on Image Processing}, 
  title={Parameter-Free Deep Multi-Modal Clustering With Reliable Contrastive Learning}, 
  year={2025},
  volume={34},
  number={},
  pages={2628-2640},
  abstract={Deep multi-modal clustering (DMC) expects to improve clustering performance by exploiting abundant information available from multiple modalities. However, different modalities usually have heterogeneous distribution with uneven quality. This may lead to limited performance, especially for contrastive multi-modal clustering, which inevitably performs contrastive learning between high-quality and low-quality modalities. To tackle this challenge, we propose a novel framework named parameter-free deep multi-modal clustering with reliable contrastive learning (PDMC-RCL). Specifically, the reliable contrastive learning quantifies the relationship between contrastive modality pairs with weight values that will promote the discriminative features learning from useful modality pairs and slow down or even prevent the learning from unreliable modality pairs. Moreover, the reliable contrastive learning is imposed simultaneously at both the feature-level and cluster-level in this framework so that the feature representation learning can benefit from multi-level contrastive learning. It is worth noting that our PDMC-RCL method is parameter-free, which can achieve promising performance without additional hyperparameter tuning. Experimental results on various datasets show the effectiveness of our method over typical state-of-the-art compared DMCs. The source code is available on https://github.com/ShizheHu},
  keywords={Contrastive learning;Reliability;Feature extraction;Noise;Representation learning;Training;Data mining;Semantics;Data augmentation;Clustering methods;Deep multi-modal clustering;contrastive learning},
  doi={10.1109/TIP.2025.3562083},
  ISSN={1941-0042},
  month={},}@ARTICLE{10706819,
  author={Lei, Tao and Song, Wenbiao and Zhang, Weichuan and Du, Xiaogang and Li, Chenxia and He, Lifeng and Nandi, Asoke K.},
  journal={IEEE Transactions on Radiation and Plasma Medical Sciences}, 
  title={Semi-Supervised 3-D Medical Image Segmentation Using Multiconsistency Learning With Fuzzy Perception-Guided Target Selection}, 
  year={2025},
  volume={9},
  number={4},
  pages={421-432},
  abstract={Semi-supervised learning methods based on the mean teacher model have achieved great success in the field of 3-D medical image segmentation. However, most of the existing methods provide auxiliary supervised signals only for reliable regions, but ignore the effect of fuzzy regions from unlabeled data during the process of consistency learning, which results in the loss of more valuable information. Besides, some of these methods only employ multitask learning to improve models’ performance, but ignore the role of consistency learning between tasks and models, thereby weakening geometric shape constraints. To address the above issues, in this article, we propose a semi-supervised 3-D medical image segmentation framework with multiconsistency learning for fuzzy perception-guided target selection. First, we design a fuzzy perception-guided target selection strategy from multiple perspectives and adopt the fusion method of fuzziness minimization and the fuzzy map momentum update to obtain a fuzzy region. By incorporating the fuzzy region into consistency learning, our model can effectively exploit more useful information from the fuzzy region of unlabeled data. Second, we design a multiconsistency learning strategy that employs intratask and intermodal mutual consistency learning as well as cross-model cross-task consistency learning to effectively learn the shape representation of fuzzy regions. The strategy can encourage the model to agree on predictions for different tasks in fuzzy regions. Experiments demonstrate that the proposed framework outperforms the current mainstream methods on two popular 3-D medical datasets, the left atrium segmentation dataset, and the brain tumor segmentation dataset. The code will be released at: https://github.com/SUST-reynole.},
  keywords={Data models;Image segmentation;Predictive models;Computational modeling;Three-dimensional displays;Perturbation methods;Medical diagnostic imaging;Solid modeling;Uncertainty;Plasmas;Consistency learning;fuzzy estimation;medical image segmentation;semi-supervised learning},
  doi={10.1109/TRPMS.2024.3473929},
  ISSN={2469-7303},
  month={April},}@INPROCEEDINGS{10969364,
  author={K, Logeswaran and P, Suresh and S, Savitha and Sah, Swati and Kr, Prasanna Kumar and R, Rajadevi and M, Vasugi and B, Sujit and T, Akilesh},
  booktitle={2025 3rd International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)}, 
  title={Empowering Leather Quality Assurance: Leveraging Convolutional Neural Networks for Precise Defect Detection and Classification}, 
  year={2025},
  volume={},
  number={},
  pages={629-634},
  abstract={Ensuring the development of high-quality leather products in the industrial sector requires a strong focus on leather quality assurance. In this study, a unique method for enhancing leather quality assurance using Convolutional Neural Networks (CNNs) is presented. Our suggested solution is able to detect and classify leather defects with amazing accuracy and efficiency by utilizing deep learning. CNN integration for leather fault categorization and detection offers a more reliable and affordable solution, ensuring better product quality and satisfied customers. Existing methods for inspecting leather products often suffer from inconsistent quality control, subjective manual inspection processes, and the complexity of accurately identifying and classifying various types of leather defects. By addressing these challenges, the proposed system aims to empower leather manufacturers with a reliable and efficient solution for ensuring consistent product quality and reducing production costs. Convolutional Neural Networks (CNNs) are integrated to provide a highly automated and effective solution that overcomes these scaling challenges. The proposed CNN-based method uses deep learning algorithms to decode images, which allows the system to swiftly and precisely analyse enormous volumes of visual data. To make sure the CNN can differentiate between various defect classes and keep a low rate of false positives and false negatives, its accuracy, precision, recall, and F1 score should be carefully assessed.},
  keywords={Deep learning;Visualization;Quality assurance;Accuracy;Quality control;Product design;Quality assessment;Convolutional neural networks;Reliability;Defect detection;Leather quality assurance;defect detection;defect classification;Convolutional Neural Networks (CNNs);deep learning;image analysis;manufacturing industry;product quality control;automation;precision},
  doi={10.1109/ISACC65211.2025.10969364},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10752209,
  author={Vashistha, Mudit and Jain, Sarthak and Pandey, Shubham and Pradhan, Aryan and Tarwani, Sandhya},
  booktitle={2024 IEEE Region 10 Symposium (TENSYMP)}, 
  title={A Comparative Analysis of Machine Learning and Deep Learning Approaches in Deepfake Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Deepfakes refer to the visual media where the faces, bodily movements have been digitally altered using some software or program, this has proven to be more of a double edged sword as it also contributes towards content creation and media creation that may be used for positive purposes. To combat this situation, measures to detect deep fake in the media is a credible approach. This work showcases a comparative analysis among 3 Deep Learning as well as 3 Machine Learning algorithms in order to reach a conclusive state of determining the best algorithms that can be implemented for Deepfake detection. For the machine learning algorithms, KNN, SVM and Logistic Regression have been used whereas CNN, TCN and CNN + LSTM have been used for the Deep Learning Algorithm. Detection of deepfakes through these algorithms works by sequentially processing, analyzing and classifying the features on the basis of the dataset fed for the algorithms. The chosen metrics for performing a comparison between each of the algorithms are Accuracy and F1 Score. The development, implementation and comparison of the algorithms was carried out on Google Collab and Jupyter Notebook. Upon comparative analysis of the algorithms between each other, it was found that CNN had the highest accuracy and Fl-score of 0.9409 and 0.7225 respectively with KNN being the worst-performing algorithm with an accuracy 0.5770 and F1 score of 0.4088 respectively.},
  keywords={Deep learning;Deepfakes;Visualization;Machine learning algorithms;Accuracy;Nearest neighbor methods;Media;Prediction algorithms;Feature extraction;Classification algorithms;Deepfake Detection;Face Morphing;Convolutional Neural Networks;Machine Learning;Deep Learning;Support Vector Machine;K-Nearest Neighbour;Face Recognition},
  doi={10.1109/TENSYMP61132.2024.10752209},
  ISSN={2642-6102},
  month={Sep.},}@INPROCEEDINGS{10493627,
  author={Reedy, Medarametla Varshitha and B, Abdul Naeem and Reddy, Tunga Kartikeya and V, Benedict Vinusha and Pramila, R. Priyanka},
  booktitle={2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)}, 
  title={Natural Language Translation Engine For Announcements and Information Dissemination At Stations}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={In today's fast-paced global travel landscape, effective communication is crucial, especially in dynamic environments like railway stations. This research paper addresses linguistic diversity challenges by developing a cutting-edge natural language translation engine tailored for railway communication. It integrates advanced technologies such as Transformer-based Automatic Speech Recognition (ASR) with noise reduction algorithms like Wav2Vec 2.0 and an Attention-based Convolutional Recurrent Neural Network (CRNN) to overcome background noise and diverse speech patterns. The system also includes multilingual Text-to-Speech (TTS) and Machine Translation (MT) capabilities for clear communication in real-time across multiple languages. Interactive features like a responsive Chatbot and Interactive Voice Response System (IVRS) enhance user engagement. This approach meets passengers' linguistic needs while promoting inclusivity. Technically, the project explores algorithms like the Transformer model, positional encoding, self-attention mechanisms, and feed-forward neural networks. The research aims to revolutionize railway communication and establish a precedent for technology-driven public transportation, creating universally accessible and user-friendly travel experiences.},
  keywords={Railway communication;Recurrent neural networks;Noise reduction;Linguistics;Transformers;Chatbots;Real-time systems;Automatic Speech Recognition (ASR);Text-to-Speech model (TTS);Wave2Vec;Convolutional Recurrent Neural Network (CRNN);Natural Translation Engine;Transformation- based model;Chatbot;IVRS},
  doi={10.1109/ic-ETITE58242.2024.10493627},
  ISSN={},
  month={Feb},}@ARTICLE{10851802,
  author={Sun, Yulin and Shi, Guangming and Dong, Weisheng and Li, Xin and Dong, Le and Xie, Xuemei},
  journal={IEEE Transactions on Image Processing}, 
  title={Local Uncertainty Energy Transfer for Active Domain Adaptation}, 
  year={2025},
  volume={34},
  number={},
  pages={816-827},
  abstract={Active Domain Adaptation (ADA) improves knowledge transfer efficiency from the labeled source domain to the unlabeled target domain by selecting a few target sample labels. However, most existing active sampling methods ignore the local uncertainty of neighbors in the target domain, making it easier to pick out anomalous samples that are detrimental to the model. To address this problem, we present a new approach to active domain adaptation called Local Uncertainty Energy Transfer (LUET), which integrates active learning of local uncertainty confusion and energy transfer alignment constraints into a unified framework. First, in the active learning module, the uncertainty difficult and representative samples from the target domain are selected through local uncertainty energy selection and entropy-weighted class confusion selection. And the active learning strategy based on local uncertainty energy will avoid selecting anomalous samples in the target domain. Second, for the discrimination issue caused by domain shift, we use a global and local energy-transfer alignment constraint module to eliminate the domain gap and improve accuracy. Finally, we used negative log-likelihood loss for supervised learning of source domains and query samples. With the introduction of sample-based energy metrics, the active learning strategy is more closely with the domain alignment. Experiments on multiple domain-adaptive datasets have demonstrated that our LUET can achieve outstanding results and outperform existing state-of-the-art approaches.},
  keywords={Uncertainty;Active learning;Energy exchange;Feature extraction;Adaptation models;Data models;Entropy;Adversarial machine learning;Transformers;Sun;Local uncertainty energy;entropy-weighted local class confusion;negative log-likelihood loss;global and local energy alignment;active domain adaptation},
  doi={10.1109/TIP.2025.3530788},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10849419,
  author={Zhang, Weitao and Xie, Shaorong and Luo, Xiangfeng and Xiao, Wenwen and Wang, Tao},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Efficient Compensation of Action for Reinforcement Learning Policies in Sim2Real}, 
  year={2024},
  volume={},
  number={},
  pages={889-896},
  abstract={Simulation to reality (sim-to-real) transfer is a promising alternative for training behavioral policies in reinforcement learning (RL). However, many significant differences between the simulator and the real-world environment cause policies to make inconsistent actions in simulation and reality. So policies trained in simulation often perform poorly in real-world. Researchers have explored various methods to bridge this gap, including building highly realistic simulators, implementing automated domain randomization, and employing domain adaptation method. However, highly realistic simulators and automated domain randomization methods rely heavily on extensive real-world data and complex manual processes. Domain adaptation methods require the collection and annotation of high-precision real-world data, leading to low learning efficiency and restricting these methods to specific domains. To address these challenges, this paper proposes transforming the discrepancies in the transfer process into a problem of action sequence similarity. By enhancing the similarity between policy action sequences, we aim to reinforce the consistency of policy actions made in both simulation and reality. For the challenging issue of annotating real-world data, we employ a Generative Adversarial Network (GAN) framework to construct a sim-to-real consistency loss function, thus avoiding reliance on precise real-world data sampling and calibration. To avoid a large amount of real-world data sampling, we introduce Bayesian optimization to accurately and efficiently search for the optimal parameters of the compensation module. Through extensive experiments in multiple sim-to-sim scenarios as well as sim-to-real scenarios, we demonstrate that our method significantly reduces the precision and quantity requirements for real-world data sampling while maintaining high transfer performance.},
  keywords={Training;Visualization;Buildings;Reinforcement learning;Manuals;Generative adversarial networks;Bayes methods;Computational efficiency;Calibration;Optimization;Sim-to-Real Transfer;Reinforcement Learning;Acton Consistency;Domain Adaptation},
  doi={10.1109/ICTAI62512.2024.00129},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10956538,
  author={S, Markkandan and Reji, Sharon Deena and B, Sairam and L, Shivani Narayan and S, Mridhula},
  booktitle={2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)}, 
  title={Comparative Study of Hybrid Machine Learning Models for Indoor Localization Using SVM, GPR, and GCN with Augmented Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Indoor positioning technology is vital for numerous applications, from intelligent building systems to emergency services. This research presents an innovative hybrid approach to indoor localization, combining Support Vector Machines (SVM), Gaussian Process Regression (GPR), and Graph Convolutional Networks (GCNs). We enhance these models' effectiveness through advanced data augmentation methods, including the use of Generative Adversarial Networks (GANs). Our methodology is tested using the UJI Indoor Localization dataset, demonstrating notable enhancements in positioning precision. The GCN-augmented technique, in particular, yields superior performance metrics compared to other approaches. This study provides insights into the practical applicability and generalization capabilities of our hybrid model in real-world indoor environments.},
  keywords={Support vector machines;Location awareness;Graph convolutional networks;Computational modeling;Gaussian processes;Machine learning;Data augmentation;Generative adversarial networks;Data models;Indoor environment;Indoor Positioning;Support Vector Machines;Gaussian Process Regression;Graph Convolutional Networks;Data Augmentation},
  doi={10.1109/AICECS63354.2024.10956538},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10925045,
  author={Wang, Yuanzhe and Jin, Yi and Wang, Xu and Zeng, Yuqiao and Li, Yidong},
  booktitle={2024 IEEE Smart World Congress (SWC)}, 
  title={VIDF-SR: A Multispectral Super-resolution Method Based on Visible-infrared Difference Feature Maps}, 
  year={2024},
  volume={},
  number={},
  pages={1794-1799},
  abstract={Multispectral super-resolution aims to recover a high-resolution image from low-resolution images of two modalities. The primary challenges are the poor quality of visible light images under low-light conditions and the inherently low resolution and insufficient structural information in infrared images. There are relatively few approaches dedicated to multispectral super-resolution so far as most methods focus on single-image super-resolution. The existing methods on multispectral super-resolution typically rely on simple convolution and upsampling techniques, resulting in suboptimal performance. To address these issues, we propose a multispectral super-resolution method based on visible-infrared difference feature maps (VIDF-SR) in this study. Specifically, to effectively utilize the superior structural information from the visible light modality and the clearer texture and edge information from the infrared modality during the reconstruction process, we introduce the Modal Information Complementary Module (MICM), which generates fused features based on the difference feature maps between the two modalities, retaining key information from both. Furthermore, to produce clearer multispectral super-resolution images, we design the Feature Map Super-Resolution Module (FMSR), incorporating a diffusion model into our framework to achieve superior performance. Experiments on the M3FD dataset show that our method surpasses current approaches, with ablation studies providing additional confirmation of the effectiveness of each component.},
  keywords={Convolution;Image edge detection;Superresolution;Diffusion models;Image reconstruction;super-resolution;multispectral images;modality complementarity},
  doi={10.1109/SWC62898.2024.00276},
  ISSN={2993-396X},
  month={Dec},}@INBOOK{10951625,
  author={Imtiyaz, Isra and Anuranjana and Kaur, Sanmukh and Gautam, Anubhav},
  booktitle={Explainable Machine Learning Models and Architectures}, 
  title={Applications of Artificial Neural Networks in Optical Performance Monitoring}, 
  year={2023},
  volume={},
  number={},
  pages={123-140},
  abstract={Summary <p>The calculation and collection of numerous physical properties of transmitted signals and different optical network components is known as optical performance monitoring (OPM). Interest in OPM has increased as a result of advancements in optical networking, particularly with regard to signal quality metrics including optical signal&#x2010;to&#x2010;noise ratio (SNR), Q&#x2010;factor, and dispersion. This paper reviews OPM algorithms and their advantages and disadvantages. It also reviews artificial intelligence (AI) methods in optical networks, and applications of AI in optical networking, optical impairments, and component faults.</p>},
  keywords={Optical fibers;Adaptive optics;Monitoring;Optical fiber networks;High-speed optical techniques;Optical noise;Optical filters;Optical diffraction;Signal to noise ratio;Optical receivers},
  doi={10.1002/9781394186570.ch8},
  ISSN={},
  publisher={Wiley},
  isbn={9781394186563},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951625},}@INPROCEEDINGS{10579255,
  author={Cheng, Yunlong and Huang, Xiuqi and Liu, Zifeng and Chen, Jiadong and Gao, Xiaofeng and Fang, Zhen and Yang, Yongqiang},
  booktitle={2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={FEDGE: An Interference-Aware QoS Prediction Framework for Black-Box Scenario in IaaS Clouds with Domain Generalization}, 
  year={2024},
  volume={},
  number={},
  pages={128-138},
  abstract={Public cloud providers embrace multi-tenancy as a strategy to enhance the utilization and efficiency of resources. However, co-located virtual machines (VMs) suffer from qualityof-service (QoS) degradation caused by shared resource interference. Existing solutions for predicting QoS degradation often rely on the assumption of online access to application-level information. However, in a production environment, this assumption proves invalid as the VMs are black boxes to the providers. This intrinsic characteristic of the IaaS cloud necessitates the prediction model to generalize to unfamiliar applications and imposes specific criteria on the monitorable metrics.To meet the black-box scenario under Infrastructure as a Service (IaaS) cloud computing, we present a novel framework, FEDGE, that can predict interference-aware QoS (IA-QoS) of co-located VMs using only low-level monitorable metrics before migration. Specifically, FEDGE utilizes a stochastic gates layer to select the most informative features from the high-dimensional resource and hardware metrics, which helps to reduce the monitoring overhead. Furthermore, we design a multi-domain MMD-based adversarial denoising autoencoder to regularize the learned hidden representations and prevent over-fitting on the source domains. Next, we employ a multi-layer perceptron (MLP) to accurately predict complex QoS degradation using the learned representations with domain generalization. Experimental results demonstrate that FEDGE outperforms other state-of-the-art methods in terms of both generalizability and effectiveness.},
  keywords={Measurement;Degradation;Cloud computing;Closed box;Stochastic processes;Quality of service;Production},
  doi={10.1109/IPDPS57955.2024.00020},
  ISSN={1530-2075},
  month={May},}@INPROCEEDINGS{11087069,
  author={Zheng, Yi and Liu, Hao and He, Yiting and Peng, Guanle and Diao, Limin},
  booktitle={2025 IEEE 2nd International Conference on Electronics, Communications and Intelligent Science (ECIS)}, 
  title={An Incremental Learning-Based Mechanism to Deploying Radio Map Estimation Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Radio map estimation is a key technology in wireless communications and is critical for network planning, location-based services, and resource management. However, the deployment of deep learning based radio map estimation models in new data-sparse scenarios faces challenges of data scarcity and poor continuous model adaptation. To address these issues, this paper proposes a three-step deployment framework based on Incremental Learning. The framework first provides a priori knowledge for the deep learning model through offline training using data generated by the experience propagation model. Then, the pre-trained model is deployed online in a new scenario for initial prediction; and, the model is efficiently and continuously optimized and adaptively tuned in the feedback updating phase using a small amount of real-world measurement data continuously arriving in the new scenario through our proposed incremental learning mechanism. Our mechanism is designed to efficiently incorporate new knowledge and retain old knowledge, thus mitigating the catastrophic forgetting problem. We evaluate the performance of the framework on the public dataset. The experimental results show that, the incremental learning method proposed in this paper can reduce the RMSE from 6.2 dB to 5.1 dB.},
  keywords={Deep learning;Wireless communication;Training;Adaptation models;Incremental learning;Estimation;Predictive models;Data models;Planning;Resource management;radio map estimation;deep learning;model deployment;incremental learning;wireless propagation},
  doi={10.1109/ECIS65594.2025.11087069},
  ISSN={},
  month={May},}@INPROCEEDINGS{10588464,
  author={Xiang, Tong and Zhao, Hongxia and Zhu, Fenghua and Chen, Yuanyuan and Lv, Yisheng},
  booktitle={2024 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Self-Aware Adaptive Alignment: Enabling Accurate Perception for Intelligent Transportation Systems}, 
  year={2024},
  volume={},
  number={},
  pages={595-601},
  abstract={Achieving top-notch performance in Intelligent Transportation detection is a critical research area. However, many challenges still need to be addressed when it comes to detecting in a cross-domain scenario. In this paper, we propose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient alignment mechanism and recognition strategy. Our proposed method employs a specified attention-based alignment module trained on source and target domain datasets to guide the image-level features alignment process, enabling the local-global adaptive alignment between the source domain and target domain. Features from both domains, whose channel importance is re-weighted, are fed into the region proposal network, which facilitates the acquisition of salient region features. Also, we introduce an instance-to-image level alignment module specific to the target domain to adaptively mitigate the domain gap. To evaluate the proposed method, extensive experiments have been conducted on popular cross-domain object detection benchmarks. Experimental results show that SA3 achieves superior results to the previous state-of-the-art methods.},
  keywords={Adaptive systems;Accuracy;Intelligent vehicles;Transportation;Object detection;Virtual reality;Benchmark testing;Domain adaptation;virtual reality;object detection},
  doi={10.1109/IV55156.2024.10588464},
  ISSN={2642-7214},
  month={June},}@ARTICLE{10814653,
  author={Gao, Zihan and Li, Lingling and Liu, Xu and Jiao, Licheng and Liu, Fang and Yang, Shuyuan},
  journal={IEEE Transactions on Multimedia}, 
  title={Uncertainty Guided Progressive Few-Shot Learning Perception for Aerial View Synthesis}, 
  year={2025},
  volume={27},
  number={},
  pages={1177-1192},
  abstract={View synthesis of aerial scenes has gained attention in the recent development of applications such as urban planning, navigation, and disaster assessment. This development is closely connected to the recent advancement of the Neural Radiance Field (NeRF). However, when autonomousaerial vehicles(AAVs) encounter constraints such as limited perspectives or energy limitations, NeRF degrades with sparsely sampled views in complex aerial scenes. On this basis, we aim to solve this problem in a few-shot manner. In this paper, we propose Uncertainty Guided Perception NeRF (UPNeRF), an uncertainty-guided perceptual learning framework that focuses on applying and improving NeRF in few-shot aerial view synthesis (FSAVS). First, simply optimizing NeRF in complex aerial scenes with sparse input can lead to overfitting in training views, resulting in a collapsed model. To address this, we propose a progressive learning strategy that utilizes the uncertainty present in sparsely sampled views, enabling a gradual transition from easy to hard learning. Second, to take advantage of the inherent inductive bias in the data, we introduce an uncertainty-aware discriminator. This discriminator leverages convolutional capabilities to capture intricate patterns in the rendered patches associated with uncertainty. Third, direct optimization of NeRF lacks prior knowledge of the scene. This, coupled with a reduction in training views, can result in unrealistic rendering. To overcome this, we present a perceptual regularizer that incorporates prior knowledge through prompt tuning of a self-supervised pre-trained vision transformer. In addition, we adopt a sampled scene annealing strategy to enhance training stability. Finally, we conducted experiments with two public datasets, and the positive results indicate our method is effective.},
  keywords={Neural radiance field;Three-dimensional displays;Uncertainty;Training;Rendering (computer graphics);Overfitting;Solid modeling;Data models;Cameras;Geometry;Few-shot aerial view synthesis;neural radiance field},
  doi={10.1109/TMM.2024.3521727},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{9942130,
  author={Xia, Pengcheng and Huang, Yixiang and Wang, Yuxiang and Zhong, Zhiwei and Liu, Chengliang},
  booktitle={2022 Global Reliability and Prognostics and Health Management (PHM-Yantai)}, 
  title={Selective Kernel Prototypical Network for Few-shot Motor Fault Diagnosis with Unseen Faults}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Motor fault diagnosis has been paid increasing attention in the academia and industry. Data-driven methods have achieved great success with massive experimental data to train the model. In industrial scenes, data sparsity is a nonnegligible problem restricting the application of these methods. Not enough data can be accessible in faulty conditions and very limited data may be obtained for some less frequently occurring faults. To tackle this few-shot learning problem, a prototypical network based on selective kernel network is proposed in this paper. One-dimensional selective kernel network is used to enhance the embedding learning ability of prototypical network based on its dynamic convolutional kernel selection mechanism. The effectiveness and superiority of the proposed method are verified on motor experimental data. Results show that the proposed method can achieve high diagnosis accuracy on unseen faults which have very few samples.},
  keywords={Fault diagnosis;Measurement;Industries;Data models;Kernel;Task analysis;Prognostics and health management;motor;few-shot learning;fault diagnosis;prototypical network;selective kernel},
  doi={10.1109/PHM-Yantai55411.2022.9942130},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10827575,
  author={Chen, Zhengdong and Pang, Xiaodong and Chen, Binbin},
  booktitle={2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={CSA-LSTF: An Improved Method for Time Series Data Anomaly Detection}, 
  year={2024},
  volume={},
  number={},
  pages={17-22},
  abstract={With the development of industrial Internet of Things (IoT), a large number of sensors have been deployed in practical environments to collect data. These data possess characteristics such as large volume, high dimensionality, strong time series nature, and high real-time requirements, making anomaly detection difficult. Traditional anomaly detection mainly focuses on single data streams, which leads to high model maintenance costs and an inability to determine dependencies be-tween multidimensional time series data, thus failing to meet the practical requirements of current industrial IoT scenarios. This paper proposes an improved algorithm CSA-LSTF(Convolutional Sparse Self Attention-Long Sequence Time-Series Forecasting) based on LSTF, which utilizes convolutional sparse self-attention mechanism to enhance LSTF and applies this algorithm to the framework of Generative Adversarial Network (GAN) for training. Anomalies are detected through discrimination results and reconstruction errors. Experimental results demonstrate that the CSA-LSTF method can timely detect anomalies and take preventive measures, enhancing the risk resistance capability of industrial IoT systems. Compared with existing anomaly detection models, it enhances model stability, improves anomaly detection accuracy, and surpasses existing multidimensional un-supervised time series data anomaly detection models.},
  keywords={Adaptation models;Costs;Time series analysis;Generative adversarial networks;Prediction algorithms;Data models;Maintenance;Anomaly detection;Streams;Industrial Internet of Things;Time Series Data;Data Anomaly Detection;LSTF;GAN},
  doi={10.1109/PRAI62207.2024.10827575},
  ISSN={},
  month={Aug},}@ARTICLE{11108243,
  author={Qiu, Lina and Ying, Zuorui and Song, Xianyue and Feng, Weisen and Zhou, Chengju and Pan, Jiahui},
  journal={IEEE Transactions on Affective Computing}, 
  title={MTADA: A Multi-task Adversarial Domain Adaptation Network for EEG-based Cross-subject Emotion Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={In electroencephalogram (EEG)-based emotion recognition, the applicability of most current models is limited by inter-subject variability and emotion complexity. This study proposes a multi-task adversarial domain adaptation (MTADA) network to enhance cross-subject emotion recognition performance. The model first employs a domain matching strategy to select the source domain that best matches the target domain. Then, adversarial domain adaptation is used to learn the difference between source and target domains, and a fine-grained joint domain discriminator is constructed to align them by incorporating category information. At the same time, a multi-task learning mechanism is utilized to learn the intrinsic relationships between different emotions and predict multiple emotions simultaneously. We conducted comprehensive experiments on two public datasets, DEAP and FACED. On DEAP, the average accuracies for valence, arousal and dominance are 76.39%, 69.74% and 68.26%, respectively. On FACED, the average accuracies for valence and arousal are 78.90% and 77.95%. When using the subject from DEAP as the source domain to predict the subjects in FACED, the accuracies for valence and arousal are 61.07% and 60.82%. These results show that our MTADA model improves cross-subject emotion recognition and outperforms most state-of-the-art methods, which may provide new approach for EEG-based emotion brain-computer interface systems.},
  keywords={Emotion recognition;Brain modeling;Electroencephalography;Feature extraction;Multitasking;Adaptation models;Accuracy;Training;Affective computing;Computational modeling;Emotion recognition;Electroencephalogram(EEG);Cross-subject;Domain adaptation;Multi-task learning},
  doi={10.1109/TAFFC.2025.3595137},
  ISSN={1949-3045},
  month={},}@INPROCEEDINGS{9718870,
  author={Zhuan, Zilong and Huang, Yingjie and Wang, Zheng and Gao, Yining},
  booktitle={2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)}, 
  title={Latent Space Conditional Adversarial Learning for Intelligent Dialogue Generation}, 
  year={2021},
  volume={},
  number={},
  pages={592-596},
  abstract={Generating reasonable and relevant responses remains a challenge for dialogue generation tasks. Currently, latent variable models in deep learning have been applied in text generation, but these latent variables are often full of randomness, leading to uncontrollable generated responses. In this paper, we propose a two-stage conditional-response generation framework based on latent space adversarial learning. Our model first performs representation learning of latent sentence encoding through an autoencoder, and then adds the latent variables of the context to the latent space adversarial learning so that the latent representations of replies generated by our encoder can be highly correlated with our context and further, we decode the latent encoding representations of replies into sentences of replies. The related experimental results show that our model can generate more contextually relevant, fluent, and rich replies than the baseline model.},
  keywords={Representation learning;Deep learning;Information science;Computational modeling;Adversarial machine learning;Encoding;Task analysis;dialog generation;deeplearning;adversarial learning;variational autoencoder;variational autoencoder},
  doi={10.1109/CISAI54367.2021.00120},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9699971,
  author={Shen, Xinqi and Zhang, Mengzhou and Li, Mei and Lu, Quanbo},
  booktitle={2021 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI)}, 
  title={A New Rolling Bearing Fault Diagnosis Method Based on Few Samples}, 
  year={2021},
  volume={},
  number={},
  pages={500-506},
  abstract={It is difficult to apply deep learning theory to realize intelligent fault monitoring and diagnosis of rolling bearings. Therefore, a new method which is to combine a generative adversarial network (GAN) and a stacked sparse autoencoders (SSAE) is proposed in this paper. Specifically, the vibration signals of rolling bearings are through the fast Fourier transform (FFT) preprocessing so that those time domain data are transformed to frequency domain, then the GAN model is used for sample generation and expansion and a SSAE network completes automatic extraction of features and finally a softmax classification layer is added. The experimental results show that this proposed method can achieve excellent diagnostic accuracy.},
  keywords={Fault diagnosis;Vibrations;Fast Fourier transforms;Frequency-domain analysis;Rolling bearings;Generative adversarial networks;Feature extraction;Rolling Bearing;Fault Diagnosis;Deep Learning;Few Samples},
  doi={10.1109/IAAI54625.2021.9699971},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10647480,
  author={Huang, Junhao and Zhang, Fang and Liu, Meiliang and Si, ZhengYe and Zhao, Zhiwen},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)}, 
  title={A Novel Architecture for Image Vectorization with Increasing Granularity}, 
  year={2024},
  volume={},
  number={},
  pages={1560-1566},
  abstract={In vector graphics, images are described by mathematical formulas with full image details even after scaling. Most research on generating vector graphics from raster images adopt the approach of splicing graphic fragments, which cannot perfectly retain the original topological structure and details of images. In this paper, we propose TSVec, a novel model for generating high-quality vector graphics by raster images, which takes advantages of the vision transformer and image super-resolution to enhance the granularity of vectorization, especially suitable for dealing with low-resolution raster images. The experimental results show that the vector graphics generated by TSVec outperform the current unsupervised vector generation models.},
  keywords={Graphics;Adaptation models;Computer vision;Splicing;Image processing;Superresolution;Process control;Image Vectorization;Bezier Curve Fitting;Attention Mechanism;Deep Learning;Super-Resolution},
  doi={10.1109/ICIP51287.2024.10647480},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{11127460,
  author={Distelzweig, Aron and Kosman, Eitan and Look, Andreas and Janjoš, Faris and Manivannan, Denesh K. and Valada, Abhinav},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Motion Forecasting via Model-Based Risk Minimization}, 
  year={2025},
  volume={},
  number={},
  pages={7011-7018},
  abstract={Forecasting the future trajectories of surrounding agents is crucial for autonomous vehicles to ensure safe, efficient, and comfortable route planning. While model ensembling has improved prediction accuracy in various fields, its application in trajectory prediction is limited due to the multi-modal nature of predictions. In this paper, we propose a novel sampling method applicable to trajectory prediction based on the predictions of multiple models. We first show that conventional sampling based on predicted probabilities can degrade performance due to missing alignment between models. To address this problem, we introduce a new method that generates optimal trajectories from a set of neural networks, framing it as a risk minimization problem with a variable loss function. By using state-of-the-art models as base learners, our approach constructs diverse and effective ensembles for optimal trajectory sampling. Extensive experiments on the nuScenes prediction dataset demonstrate that our method surpasses current state-of-the-art techniques, achieving top ranks on the leaderboard. We also provide a comprehensive empirical study on ensembling strategies, offering insights into their effectiveness. Our findings highlight the potential of advanced ensembling techniques in trajectory prediction, significantly improving predictive performance and paving the way for more reliable predicted trajectories.},
  keywords={Risk minimization;Accuracy;Predictive models;Sampling methods;Trajectory;Safety;Planning;Reliability;Forecasting;Robotics and automation},
  doi={10.1109/ICRA55743.2025.11127460},
  ISSN={},
  month={May},}@INPROCEEDINGS{10958473,
  author={Muralidharan, Adarsh and Aji, Minsa and Jacob, Rohan Jacob and Cherukat, Shasna and V, Priya C},
  booktitle={2024 IEEE 21st India Council International Conference (INDICON)}, 
  title={Integrated ML Approach for Fake News Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The unchecked spreading of news supported by social media and other forms of mass communication has led to a spike in the spread of Fake News and False Information. The effects of fake news can be far-reaching and impactful. Sometimes we can easily identify fake news, but the majority of the time it is hard to distinguish from valid news. This emphasizes the need for accurate detection methods for fake news. To aid with fake news detection, this paper proposes a novel system for detecting fake News with the help of a machine learning model. This method extracts textual features and utilizes advanced machine learning algorithms for fake news detection. We used term frequency-inverse document frequency (TF-IDF) vectorizer and implemented the Passive Aggressive Classifier alongside it. Experimental results of proposed model shows a better rate for precision & recall with an accuracy of 99 %. F1 scores for fake and true news obtained are 99% and 98% respectively. The results demonstrate the effectiveness of this approach in accurately detecting fake news and distinguishing it from true news.},
  keywords={Uniform resource locators;Accuracy;Machine learning algorithms;Social networking (online);Predictive models;Feature extraction;Real-time systems;Reliability;Fake news;Monitoring;Fake News;Machine Learning;Ensemble Method;Passive Aggressive Classifier;TF-IDF},
  doi={10.1109/INDICON63790.2024.10958473},
  ISSN={2325-9418},
  month={Dec},}@INPROCEEDINGS{10275461,
  author={Lin, Jianjie and Rickert, Markus and Wen, Long and Pan, Fengjunjie and Knoll, Alois},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Knowledge-Augmented Anomaly Detection in Small Lot Production for Semantic Temporal Process Data}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={To mitigate unforeseen operational interruptions caused by potential malfunctions in robotic systems employed in industrial automation, we propose an innovative strategy for anomaly detection that incorporates a Transformer-based reconstruction network for identifying irregularities in skill-oriented manufacturing. Leveraging a semantic representation of processes, products, and resources, a semantic manufacturing execution system synthesizes an appropriate robot program and carries out the process. Our technique utilizes these descriptions to partition and automatically assign pertinent process data, facilitating the automated configuration of the anomaly detection pipeline. To overcome limited data availability, we employ a sliding window technique for data augmentation and capitalize on the attention mechanism of the Transformer to effectively extract semantic interdependencies from the time series data. By examining the discrepancies between the reconstructed time series data and the original, we can detect anomalies related to the manufacturing process. Through experiments conducted on an actual robot workcell, we demonstrate that our approach surpasses alternative competitive concepts.},
  keywords={Training;Manufacturing processes;Service robots;Semantics;Time series analysis;Transformers;Manufacturing},
  doi={10.1109/ETFA54631.2023.10275461},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10436943,
  author={Chen, Zheyuan and Ren, Xiaoxu and Qiu, Chao and Wang, Xiaofei and Luo, Tao and Niyato, Dusit},
  booktitle={GLOBECOM 2023 - 2023 IEEE Global Communications Conference}, 
  title={Bi-Meta: Bi-Alternating Resource Provisioning and Heterogeneous Auction for Mobile Metaverse}, 
  year={2023},
  volume={},
  number={},
  pages={5481-5486},
  abstract={The presence of Metaverse has elicited escalating attention in the next-generation Internet, followed by a large number of computationally intensive tasks, such as augmented reality, virtual reality, artificial intelligence-generated content (AIGC) applications, etc. With the popularity of mobile communication technology, mobile metaverse is becoming increasingly widespread. The resources required for these applications are rapidly growing in parallel with increasing demands from mobile Metaverse users (MUs), putting pressure on Metaverse service providers (MSPs) with limited resources, especially in mobile computing scenarios. Inspired by the burgeoning communication and computing technologies, the mobile Metaverse market between mobile MUs and MSPs is developing vigorously. However, there still remain numerous challenges in this market, including hierarchical mobile Metaverse structure, temporal dependencies, as well as heterogeneous incentive. In this paper, we propose a bi-alternating resource provisioning and heterogeneous auction approach for mobile Metaverse, named Bi-Meta. At the high level, resource provisioning is formulated as a Lyapunov problem minimizing average delay, solved by a novel bi-level based generative adversarial network, i.e., BiGAN. At the low level, a price- guided double dutch auction (PG-DDA) mechanism is presented for heterogeneous resource matching, with the designed PG- DDA smart contract. Based on the realistic edge-cloud company's traces, the experimental results verify that our proposed scheme achieves optimal latency and social welfare.},
  keywords={Metaverse;Smart contracts;Mobile communication;Internet;Task analysis;Next generation networking;Mobile computing},
  doi={10.1109/GLOBECOM54140.2023.10436943},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{9956698,
  author={Li, Shuo and Liu, Xiyan and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
  booktitle={2022 26th International Conference on Pattern Recognition (ICPR)}, 
  title={Components Regulated Generation of Handwritten Chinese Text-lines in Arbitrary Length}, 
  year={2022},
  volume={},
  number={},
  pages={1378-1385},
  abstract={Generating readable images of handwritten Chinese text-lines is very challenging due to complicated topological structures in Chinese. To address this problem, we propose a components regulated model named HCT-GAN to generate the entire lines of Chinese handwriting from text-line labels. Specifically, HCT-GAN is designed as a CGAN-based architecture that additionally integrates a Chinese text encoder (CTE), a sequence recognition module(SRM), and a spatial perception module (SPM). Compared with the one-hot embedding, CTE learns the latent content representation by reusing the structure and component embedding shared among the Chinese characters. SRM provides sequence-level constraints to the generated images. SPM can adaptively constrain the spatial correlation between the generated components, which facilitates the modeling of characters with complicated topological structures. Benefiting from such artful modeling, our model suffices to generate images of handwritten Chinese text-lines in arbitrary length. Extensive experimental results demonstrate that our model achieves state-of-the-art performance in handwritten Chinese lines generation.},
  keywords={Couplings;Adaptation models;Correlation;Text recognition;Image synthesis},
  doi={10.1109/ICPR56361.2022.9956698},
  ISSN={2831-7475},
  month={Aug},}@INPROCEEDINGS{11139511,
  author={Wei, Zeyu and Wan, Hanwen and Huang, Yuxuan and Wu, Zuxiang and Zhu, Bin and Sun, Zhenglong and Chen, Jun and Ji, Xiaoqiang},
  booktitle={2025 IEEE International Conference on Real-time Computing and Robotics (RCAR)}, 
  title={Fast Aesthetic Image Generation for Paper-Cutting Style}, 
  year={2025},
  volume={},
  number={},
  pages={1078-1083},
  abstract={This paper proposes a efficient novel workflow based on stable diffusion named SAP, in which ControlNet is incorporated to regulate the structural consistency of images throughout the transformation process. Furthermore, a LoRA model specifically trained for Chinese paper cutting is applied to achieve stable and effective style transfer. Extensive image and numerical data demonstrate that the SCP method effectively enhances the aesthetic appeal of images in the Chinese paper-cutting style while preserving the original image structure and leveraging the powerful text-to-image capabilities of diffusion models. Specifically, with this method, the machine-evaluated aesthetic score improves by approximately 10.84%.},
  keywords={Image synthesis;Pipelines;Text to image;Process control;Diffusion models;Real-time systems;Numerical models;Robots},
  doi={10.1109/RCAR65431.2025.11139511},
  ISSN={},
  month={June},}@INPROCEEDINGS{11081215,
  author={U, Farjana and K, Senthilnaathan and M, Rohit},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Comparative Analysis of Model Performance on CLAHE Enhanced Synthetic GAN Images vs Real Fundus Images}, 
  year={2025},
  volume={},
  number={},
  pages={36-42},
  abstract={Deep learning-based automated models have been of much interest for detecting many eye disorders from retinal fundus images. But the performance of their models is normally constrained by access to big, well-labeled, and diversified datasets. Data augmentation processes, including rotation, flipping, and contrast, can only create restricted variations and can fail to completely represent real-world pathological examples in their diversity. This project overcomes the data shortage issue by utilizing Generative Adversarial Networks (GANs) to generate realistic eye fundus images, which increases dataset diversity and enhances model generalization. The dataset is made up of 4,000 grayscale fundus images from the APTOS dataset, preprocessed with Contrast Limited Adaptive Histogram Equalization (CLAHE) for improved contrast. A GAN model is trained to produce synthetic images of various stages of eye diseases, which are classified into "No Disease" and "Affected Stages." CycleGAN is also used to produce additional variations with medical relevance. These synthetic images are incorporated into the training process of two deep learning models: (1) a five-layer convolutional custom Convolutional Neural Network (CNN) and (2) a transfer learning-based model based on EfficientNet-B3. To evaluate the efficacy of synthetic data, both models are trained on and without GAN-generated images. Performance is measured in terms of accuracy, F1-score, and other appropriate metrics. The results show that the addition of synthetic images enhances classification accuracy, showing the utility of GAN-generated data in medical imaging tasks. This work demonstrates the promise of GAN-based data augmentation for improving deep learning models in ophthalmology. Future research will aim to optimize GAN architectures, extend synthesis to other retinal diseases, and incorporate multimodal clinical data for more complete disease diagnosis.},
  keywords={Deep learning;Training;Adaptation models;Accuracy;Generative adversarial networks;Retina;Data augmentation;Data models;Convolutional neural networks;Synthetic data;GAN;CycleGAN;Diabetic retinopathy;CLAHE;EfficientNet-B3;Retinal fundus image},
  doi={10.1109/ICSSAS66150.2025.11081215},
  ISSN={},
  month={June},}@INPROCEEDINGS{9412234,
  author={Du, Shuaiyuan and Hong, Chaoyi and Pan, Zhiyu and Feng, Chen and Cao, Zhiguo},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Parallel Network to Learn Novelty from the Known}, 
  year={2021},
  volume={},
  number={},
  pages={2172-2179},
  abstract={Towards multi-class novelty detection, we propose an end-to-end trainable Parallel Network (PN) using no additional data but only the training set. Our key idea is to first divide the training set into successive subtasks of pseudo-novelty detection to simulate real scenarios. We then design PN which consists of multiple branches to address the pseudo-tasks. This yields a more compact and discrminative classification space and forms a natural ensemble. In practice, we sample a random subset of the training classes as pseudo-novel and regard the extra as known. The sampling result forms a sub-task fed to one branch of PN. All training classes are equally divided as pseudo-novel to PN's branches for better data balance and model diversity. By distinguishing between the known and the diverse pseudo-novel, PN extracts the concept of novelty in a compact classification space. This provides PN with generalization ability to real novel classes which are absent during training. During online inference, this ability is further strengthened with the ensemble of PN's multiple branches. Experiments on three public datasets show our method's superiority to the mainstream methods.},
  keywords={Training;Aggregates;Training data;Benchmark testing;Data models;Pattern recognition;Anomaly detection},
  doi={10.1109/ICPR48806.2021.9412234},
  ISSN={1051-4651},
  month={Jan},}@INPROCEEDINGS{10462792,
  author={Wang, Qixin and Fan, Chaoqiong and Jia, Tianyuan and Han, Yuyang and Wu, Xia},
  booktitle={2023 International Conference on Neuromorphic Computing (ICNC)}, 
  title={MI-MAMI: Multisensory Integration Model Inspired by the Macro and Micro Mechanisms of the Human Brain}, 
  year={2023},
  volume={},
  number={},
  pages={388-397},
  abstract={The interaction between information from different senses is crucial for multisensory integration. Without the interaction, traditional deep learning methods exhibit inferior performance in the integration. On the contrary, the human brain has an inherently remarkable ability in this area, which stems from the macro and micro brain networks. Based on this superiority, we propose a brain-inspired model called MI-MAMI (Multisensory Integration model inspired by the MAcro and MIcro mechanisms). Aligning with the macro brain network that consists of unisensory pathways and integration regions, a similar framework of the MI-MAMI is established. The MI-MAMI incorporates unisensory processing streams and an integration block, enabling the comprehensive integration of multisensory information. In addition, inspired by the diverse neurons and the synaptic connections found in integration regions, we innovatively design weight constraints to regulate information transmission among neurons. Leveraging these mechanisms at both the macro and micro levels, our model is biologically plausible and enables more effective interaction and integration. To validate the performance of the MI-MAMI model, we conducted a case study focusing on multisensory emotion recognition. The results show that our model surpasses state-of-the-art brain-inspired baselines on RAVDESS and eNTERFACE’05 datasets, showcasing the potential of brain-inspired methods for advancing multisensory integration.},
  keywords={Emotion recognition;Adaptation models;Neuromorphic engineering;Biological system modeling;Neurons;Information processing;Brain modeling;brain-inspired;multisensory integration;interaction;multisensory emotion recognition},
  doi={10.1109/ICNC59488.2023.10462792},
  ISSN={},
  month={Dec},}@ARTICLE{10841446,
  author={Ma, Wenping and Chen, Chuang and Ma, Mengru and Zhang, Hekai and Zhu, Hao and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={An Adaptive Dual-Supervised Cross-Deep Dependency Network for Pixel-Wise Classification}, 
  year={2025},
  volume={63},
  number={},
  pages={1-13},
  abstract={With the advancement of remote sensing (RS) technology and satellite observation, the task of fusing multisource data, such as multispectral (MS) and panchromatic (PAN) images, has become increasingly important. However, image fusion involving certain semantic differences can hinder the model’s ability to learn effective feature mappings. To reconstruct richer and more consistent features during fusion, we propose an adaptive dual-supervised cross-deep dependency network (ADCD-Net), which consists of two training stages. Stage I uses a semantic perceptual self-supervision strategy (SPS) to learn deep features across different modalities, thereby reducing semantic differences while mining its own non-singular features. Stage II uses the deep temporal Mamba module (DTM-Module) to interactively learn the output of each network layers, which are able to take part in the deep feature reinforcement and improve the classification performance of semantic information. Finally, to eliminate channel redundancy during the two-stage network training process while enhancing spatial location memory and feature discrimination in the 2-D features, we propose a deformable interactive attention module (DIA-Module) to further bolster feature representation capabilities. Additionally, we conduct comparative and transfer experiments on multiple RS datasets, achieving outstanding classification results. Our code is available at https://github.com/ChenC1027/ADCD-Net.},
  keywords={Semantics;Remote sensing;Training;Adaptive systems;Adaptation models;Visualization;Image fusion;Feature extraction;Transformers;Image reconstruction;Deformable attention;fusion classification;Mamba;remote sensing (RS);self-supervision},
  doi={10.1109/TGRS.2025.3529749},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10235114,
  author={Gagan, N and Sai Kumar, S and Keerthana, M and Vaidya, Samarth S and Rastogi, Vanshika},
  booktitle={2023 World Conference on Communication & Computing (WCONF)}, 
  title={Android Malware Detection}, 
  year={2023},
  volume={},
  number={},
  pages={1-14},
  abstract={Since android smartphones are so popular, malware writers have found them to be lucrative targets, which poses a serious risk to user security and privacy. To address this problem, several malware detection strategies have been put out recently. In this review paper, we examine an efficient method for detecting malware on Android that makes use of machine learning techniques. Our method divides Android applications into benign and harmful categories using a combination of feature extraction methods and machine learning algorithms. Overall, our suggested method offers a dependable and efficient alternative for detecting Android malware, giving consumers a more dependable and safe environment for mobile computing.},
  keywords={Privacy;Machine learning algorithms;Machine learning;Feature extraction;Malware;Security;Smart phones;Android;Smartphone;Malware;Static;Dynamic;Hybrid;Machine learning},
  doi={10.1109/WCONF58270.2023.10235114},
  ISSN={},
  month={July},}@INPROCEEDINGS{10228101,
  author={Gamage, Gihan and Kahawala, Sachin and Mills, Nishan and De Silva, Daswin and Manic, Milos and Alahakoon, Damminda and Jennings, Andrew},
  booktitle={2023 IEEE 32nd International Symposium on Industrial Electronics (ISIE)}, 
  title={Augmenting Industrial Chatbots in Energy Systems using ChatGPT Generative AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Chatbots, the automation of communicative labor, have been widely deployed in industrial applications and systems. Built upon the Generative Pre-trained Transformer 3 (GPT-3), ChatGPT is a Generative Artificial Intelligence (AI) primed to transform all pre-existing chatbot capabilities with human-like conversation skills. It has already disrupted many disciplines including tertiary education and academic research methods, with increasing adoption in simple to complex tasks. However, the augmentation of pre-existing industrial chatbots with generative AI capabilities has not been fully investigated and demonstrated in recent literature. In this paper, we address this gap by presenting the augmentation of a pre-existing chatbot using ChatGPT generative AI capabilities. Our contribution encompasses the ten primary human-like conversation capabilities of ChatGPT, its augmentation of the pre-existing functionalities and the adopted prompt engineering strategies. Each capability is empirically demonstrated on Cooee, a functionally deployed chatbot in the microgrid energy systems of the La Trobe Energy Analytics Platform (LEAP).},
  keywords={Industrial electronics;Education;Oral communication;Microgrids;Transforms;Carbon dioxide;Chatbots;Generative AI;ChatGPT;Chatbots;Energy AI;conversational experience;prompt engineering;Net zero carbon emissions;microgrid optimization;machine learning},
  doi={10.1109/ISIE51358.2023.10228101},
  ISSN={2163-5145},
  month={June},}@INPROCEEDINGS{9913526,
  author={Saravanan, Shruti and Sudha, K.},
  booktitle={2022 Fifth International Conference on Computational Intelligence and Communication Technologies (CCICT)}, 
  title={GPT-3 Powered System for Content Generation and Transformation}, 
  year={2022},
  volume={},
  number={},
  pages={514-519},
  abstract={Enabling computer systems to understand and generate natural language has been an up-and-coming field of research. Latest advancements in Natural Language Processing (NLP) have made headway progress in facilitating this, like the GPT-3 language prediction model created by OpenAI. Given the capacity of the GPT-3 model, this study capitalizes on how the model can be used to generate and transform content without manual help from humans – how well plausibly GPT3-authored text most nearly passes as human-like prompts for content generation and manipulation purposes. This attempt is presented in the context of automated story writing. It also sheds light on the potential abuses of the tool and its raw capabilities as its limitations.},
  keywords={Computational modeling;Transforms;Manuals;Writing;Predictive models;Natural language processing;Communications technology;Artificial Intelligence;Natural Language Processing;OpenAI;GPT-3;GPT-2;Language Models;Content Generation;Story Generation},
  doi={10.1109/CCiCT56684.2022.00096},
  ISSN={},
  month={July},}@INPROCEEDINGS{11050703,
  author={AlMakinah, Rawan and Norcini-Pala, Andrea and Disney, Lindsey and Canbaz, M. Abdullah},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Enhancing Mental Health Support Through Human-AI Collaboration: Toward Secure and Empathetic AI-Enabled Chatbots}, 
  year={2025},
  volume={},
  number={},
  pages={196-202},
  abstract={Access to mental health support remains limited, particularly in marginalized communities facing structural and cultural barriers. This paper examines the potential of AI-enabled chatbots as scalable solutions by evaluating advanced large language models (LLMs) such as GPT-4, Mistral, and Llama v3.1 for their ability to provide empathetic responses in mental health contexts. While these models show promise in generating coherent and structured responses, they struggle to replicate the emotional depth and adaptability of human therapists. Additionally, challenges related to trustworthiness, bias, and privacy persist due to unreliable datasets and limited collaboration with mental health professionals. To address these limitations, we propose a federated learning framework that ensures data privacy, reduces bias, and incorporates continuous clinician validation to enhance response quality. This approach aims to develop a secure, evidence-based AI chatbot capable of offering trustworthy and empathetic mental health support, thereby advancing AI's role in digital mental health care.},
  keywords={Measurement;Data privacy;Privacy;Large language models;Prevention and mitigation;Collaboration;Mental health;Medical services;Chatbots;Reliability;Chatbot;Digital Mental Health;Collaborative Intelligence;Generative Artificial Intelligence;Large Language Models;Federated Learning},
  doi={10.1109/CAI64502.2025.00038},
  ISSN={},
  month={May},}@INPROCEEDINGS{10528647,
  author={Cheng, Pengfei and Huang, Peiliang and Xu, Chenchu and Han, Longfei},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Region Guided Transformer for Single Image Raindrop Removal}, 
  year={2023},
  volume={},
  number={},
  pages={964-972},
  abstract={Raindrops can significantly degrade image quality by introducing unwanted reflections and occlusions. Although deep learning-based methods have shown promise in removing these artifacts, they often struggle to completely eliminate raindrop traces and restore the original scene. To address this challenge, we propose a novel Region Guided Transformer Network (RGTN) for single-image raindrop removal. Our RGTN incorporates a unique attention mechanism called Mask-Window Multi-head Self-Attention (MW-MSA), which utilizes a degraded region mask to selectively process the degradations and focus on clean background information. Extensive experiments conducted on benchmark raindrop removal datasets demonstrate the superiority of our RGTN network over existing methods. Our approach achieves more detailed and realistic results. To further validate the generalization ability and robustness of our network, we also evaluate its performance on other related tasks, such as snow removal, using datasets like Snow 100K. The results indicate that our network outperforms the latest methods in this domain as well.},
  keywords={Learning systems;Image quality;Degradation;Snow;Benchmark testing;Transformers;Robustness;Image Restoration;Raindrop Removal;Transformer;Multi-head Self-Attention},
  doi={10.1109/ACAIT60137.2023.10528647},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11081267,
  author={G, Govinda Rajulu. and Sharmila, L. and Venkatesan, D. and Appa M.A.Y, Peer Mohamed and Chinnadurai, Jayapraksah and S, Kalvikkarasi.},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Decentralization and Security Challenges in Blockchain-Enabled IoT}, 
  year={2025},
  volume={},
  number={},
  pages={394-399},
  abstract={The blockchain on which the Bitcoin is based upon is very attractive because it has the potential to enhance safety and confidentiality within the IoT network. In this particular topic, there is similarly extensive amount of research has carried out in the academic as well as the various corporate research fields. Proof of Work (POW) is one of the fundamental types of cryptographic puzzles that implements a pivotal function in ensuring the integrity of the blockchain To do this, it maintains and records transactions history that cannot be tampered with. Further, in BC, the variable Public Key (PK) is used in the storage of user identities thus making the system more secure. Blockchain has been successfully incorporated into various other non-rewarding applications and use cases including distributed storage systems, proof of location, healthcare, etc. To ensure the identification of the utilization of the blockchain technology for enhancement of security in IoT, the study considered publications of current research articles and activities. It was to discover the challenges that relate to using blockchain in securing IoT and proposed ways of dealing with them. The interactions and possible solutions related to decentralization and security issues of blockchain-based Internet Of Things (IoT) systems With the rise of IoT systems, it is apparent that blockchain-based systems for the handling of large volumes of IoTs data are required for improved decentralization and security. Blockchain has appeared as a possible solution to those problems since it offers the transparent and distributed architecture for data and transactions storage. However, there are other factors that should be consider in such a way to integrate blockchain in IoT system in a proper way. . As a result, these abstract aims to define the major challenges related to decentralization and security in systems based on ‘blockchain’ with reference to IoT, including factors of scalability, privacy, consensus, and trust. It also looks at potential solutions such as sharding, privacy-preserving measures, improvement of consensus algorithms, and approaches to identity management. Addressing these challenges, the IoT systems with the help of blockchain technology can gain higher decentralization and security levels to enable the safe and efficient IoT devices operation in various spheres including smart home, health-care systems, automotive industry, and logistic systems.},
  keywords={Sharding;Scalability;Public key;Smart homes;Blockchains;Safety;Security;Resource management;Reliability;Industrial Internet of Things;Blockchain;IoT;Proof of domain;public key},
  doi={10.1109/ICSSAS66150.2025.11081267},
  ISSN={},
  month={June},}@ARTICLE{10493074,
  author={Dorjsembe, Zolnamar and Pao, Hsing-Kuo and Odonchimed, Sodtavilan and Xiao, Furen},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis}, 
  year={2024},
  volume={28},
  number={7},
  pages={4084-4093},
  abstract={Artificial intelligence (AI) in healthcare, especially in medical imaging, faces challenges due to data scarcity and privacy concerns. Addressing these, we introduce Med-DDPM, a diffusion model designed for 3D semantic brain MRI synthesis. This model effectively tackles data scarcity and privacy issues by integrating semantic conditioning. This involves the channel-wise concatenation of a conditioning image to the model input, enabling control in image generation. Med-DDPM demonstrates superior stability and performance compared to existing 3D brain imaging synthesis methods. It generates diverse, anatomically coherent images with high visual fidelity. In terms of dice score in the tumor segmentation task, Med-DDPM achieves 0.6207, close to the 0.6531 dice score of real images, and outperforms baseline models. Combined with real images, it further increases segmentation accuracy to 0.6675, showing the potential of the proposed method for data augmentation. This model represents the first use of a diffusion model in 3D semantic brain MRI synthesis, producing high-quality images. Its semantic conditioning feature also shows potential for image anonymization in biomedical imaging, addressing data and privacy issues.},
  keywords={Three-dimensional displays;Brain modeling;Biomedical imaging;Image segmentation;Magnetic resonance imaging;Artificial intelligence;Diffusion models;Image synthesis;Data augmentation;Conditional diffusion models;semantic image synthesis;generative models;anonymization;data augmentation},
  doi={10.1109/JBHI.2024.3385504},
  ISSN={2168-2208},
  month={July},}@ARTICLE{10500520,
  author={Xue, Xiao and Yu, Xiangning and Zhou, Deyu and Wang, Xiao and Bi, Chongke and Wang, Shufang and Wang, Fei-Yue},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Computational Experiments for Complex Social Systems: Integrated Design of Experiment System}, 
  year={2024},
  volume={11},
  number={5},
  pages={1175-1189},
  abstract={Powered by advanced information industry and intelligent technology, more and more complex systems are exhibiting characteristics of the cyber-physical-social systems (CPSS). And human factors have become crucial in the operations of complex social systems. Traditional mechanical analysis and social simulations alone are powerless for analyzing complex social systems. Against this backdrop, computational experiments have emerged as a new method for quantitative analysis of complex social systems by combining social simulation (e.g., ABM), complexity science, and domain knowledge. However, in the process of applying computational experiments, the construction of experiment system not only considers a large number of artificial society models, but also involves a large amount of data and knowledge. As a result, how to integrate various data, model and knowledge to achieve a running experiment system has become a key challenge. This paper proposes an integrated design framework of computational experiment system, which is composed of four parts: generation of digital subject, generation of digital object, design of operation engine, and construction of experiment system. Finally, this paper outlines a typical case study of coal mine emergency management to verify the validity of the proposed framework.},
  keywords={Analytical models;Systematics;Computational modeling;Integrated design;Emergency services;Data models;Complexity theory;Artificial society;computational experiments;model integration;operation engine;technology integration},
  doi={10.1109/JAS.2023.123639},
  ISSN={2329-9274},
  month={May},}@INPROCEEDINGS{10463413,
  author={Mishra, Amit Kumar and Sharma, Rahul and Singh, Jagendra and Singh, Prabhishek and Diwakar, Manoj and Tiwari, Mohit},
  booktitle={2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)}, 
  title={A Novel Deep Learning Based Approach for Detecting and Localization of Pulmonary Cancer}, 
  year={2024},
  volume={},
  number={},
  pages={555-560},
  abstract={In the ever-evolving field of scientific diagnostics, the early diagnosis of pulmonary most cancers continues a key undertaking. This observation proposed a unique deep learning-primarily based approach, in particular using Generative Adversarial Networks (GANs), intending to modernise the identification and localization of pulmonary malignancies through scientific imaging. Our models, trained using a varied dataset, demonstrated a promising accuracy fee of 70% within the sample set, suggesting its ability to adeptly differentiate between malignant and non-malignant instances in scientific images. While the conclusions suggest a significant growth in lung cancer detection, also they highlight locations demanding in addition refinement. The balance of technological prowess and scientific significance, as reflected through criteria like sensitivity and specificity, remains a focus topic for future projects. The outcomes of this research are substantial. Beyond the on the spot discoveries, the take a look at emphasizes the transformational possibility of incorporating sophisticated AI approaches into healthcare. As the scientific network grapples with the difficulties of early cancer identification, gear like the one displayed in this study ought to usher in a new age in diagnostics-marked by accuracy, efficiency, and patient-centricity. In conclusion, this have a look at now not simplest adds a fresh diagnostic tool to the sector but moreover sets the way for future innovations within the confluence of AI and healthcare.},
  keywords={Location awareness;Technological innovation;Lung cancer;Lung;Medical services;Sensitivity and specificity;Generative adversarial networks;Artificial intelligence;Medical diagnostic imaging;Tumors;Pulmonary cancer detection;Generative Adversarial Networks;medical imaging diagnostics;deep learning;early-stage diagnostics},
  doi={10.1109/Confluence60223.2024.10463413},
  ISSN={2766-421X},
  month={Jan},}@INPROCEEDINGS{11011355,
  author={K, Sathyabama and V, Yuvaraj and A, Aslam Khan},
  booktitle={2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)}, 
  title={Intelligent Alzheimer’s Care: Predictive and Supportive Systems with GNNs and GANs}, 
  year={2025},
  volume={},
  number={},
  pages={183-188},
  abstract={Alzheimer is a very hard type of disorder due to the progressive impact on memory functions together with other cognitive and daily performances. Traditional approaches were failing at predicting advance and providing individual treatment. In this, have developed an AI technology project based on GNNs for disease prediction and care management process, for which could not find a precedent; coupled with that, use Generative Adversarial Network for stimulating memory and creating patient-friendly environment. Through training of a single drop of brain imaging, cognitive, and environment data, the created system estimates regions which have been thought to contribute to regional development toward Alzheimer’s disease with an accuracy of 97%. It oversees control of recurrent appointments, increases memory throughput via a user-specific media message and aids in post-discharge appointments, which is useful to patients and caretakers in the goal of bettering their experience and quality of life.},
  keywords={Training;Accuracy;Medical services;Predictive models;Generative adversarial networks;Throughput;Graph neural networks;Alzheimer's disease;Artificial intelligence;Testing;Alzheimer's disease;Graph Neural Networks (GNNs);Generative Adversarial Networks (GANs);disease prediction;personalized care;memory recall;health support;AI in healthcare;cognitive function;neurodegenerative disorders},
  doi={10.1109/InCACCT65424.2025.11011355},
  ISSN={},
  month={April},}@INPROCEEDINGS{4530008,
  author={Gresser, J.-Yves},
  booktitle={2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications}, 
  title={Terminology & Information Science(s)}, 
  year={2008},
  volume={},
  number={},
  pages={1-10},
  abstract={Information systems now cover the "key know-how" of companies or institutions and their global environments. This implies a close cooperation between persons in the global "playing field", experts and linguists aiming at: - spotting in due time the emergence of new concepts or artefacts which may represent as much threats to as opportunities for businesses or public administrations as for the IT department itself; - translating correctly from one linguistic universe to the other. This is often the first step. Terminology is closely linked to innovation. New concepts are needed to comprehend the world and its transformations. New words are needed to designate or to go along with these concepts. Words belong to natural languages. Concepts belong to technical domains. The concept moved gradually to the center of terminology. But while conceptualisation works fairly well in natural language, it is not sufficient to drive computers. We need a terminology applying the principles and formal rules of formal ontology while not forgetting its deep linguistical roots: an "ontoterminology"?},
  keywords={Terminology;Biomedical informatics;Natural languages;Information systems;Computer science;Engines;Programming profession;Ontologies;Artificial intelligence;Companies;terminology;linguistics;knowledge;knowledge management;information;information sciences;information systems;business intelligence;ontology;taxonomy},
  doi={10.1109/ICTTA.2008.4530008},
  ISSN={},
  month={April},}@ARTICLE{8640025,
  author={Arslan, Abdullah Taha and Seke, Erol},
  journal={IEEE Access}, 
  title={Face Depth Estimation With Conditional Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={23222-23231},
  abstract={Depth map estimation and 3-D reconstruction from a single or a few face images is an important research field in computer vision. Many approaches have been proposed and developed over the last decade. However, issues like robustness are still to be resolved through additional research. With the advent of the GPU computational methods, convolutional neural networks are being applied to many computer vision problems. Later, conditional generative adversarial networks (CGAN) have attracted attention for its easy adaptation for many picture-to-picture problems. CGANs have been applied for a wide variety of tasks, such as background masking, segmentation, medical image processing, and superresolution. In this work, we developed a GAN-based method for depth map estimation from any given single face image. Many variants of GANs have been tested for the depth estimation task for this work. We conclude that conditional Wasserstein GAN structure offers the most robust approach. We have also compared the method with other two state-of-the-art methods based on deep learning and traditional approaches and experimentally shown that the proposed method offers great opportunities for estimation of face depth maps from face images.},
  keywords={Gallium nitride;Generators;Training;Face;Estimation;Three-dimensional displays;3D face reconstruction;generative adversarial networks;deep learning},
  doi={10.1109/ACCESS.2019.2898705},
  ISSN={2169-3536},
  month={},}@ARTICLE{9585128,
  author={Kim, Kuekyeng and Park, Chanjun and Seo, Jaehyung and Lim, Heuiseok},
  journal={IEEE Access}, 
  title={Grounded Vocabulary for Image Retrieval Using a Modified Multi-Generator Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={144614-144623},
  abstract={With the recent increase in requirement of both natural-language and visual information, the demand for research on seamless multi-modal processing for effective retrieval of these types of information has increased. However, because of the unstructured nature of images, it is difficult to retrieve images that accurately represent the input text. In this study, we utilized an augmented version of a multi-generator generative adversarial network that uses BERT embeddings and attention maps as input to enable grounded vocabulary for visual representations. We compared the performance of our proposed model with those of other state-of-the-art text input-based image retrieval methods on the MSCOCO and Flikr30K datasets, and the results showed the potential of our proposed method. Even with limited vocabulary, our proposed model was comparable to other state-of-the-art performances on R@10 or even exceed them in R@1. Moreover, we revealed the unique properties of our method by demonstrating how it could perform successfully even when using more descriptive text or short sentences as input.},
  keywords={Vocabulary;Generators;Image retrieval;Visualization;Bit error rate;Task analysis;Training;Artificial intelligence;artificial neural network;computer vision;image processing;search methods},
  doi={10.1109/ACCESS.2021.3122547},
  ISSN={2169-3536},
  month={},}@INBOOK{11062543,
  author={Kumar, Rishi},
  booktitle={Winning the AI Arms Race: Defeating China and Russia, Re-establishing American Superpower for Global Prosperity and the Greater Good with Artificial Intelligence}, 
  title={18 A Trust Crisis: Deepfakes and Scamsters}, 
  year={2025},
  volume={},
  number={},
  pages={159-162},
  abstract={Rishi Kumar offers an insightful and compelling exploration of how artificial intelligence is set to shape America&#x2019;s future and its standing on the global stage with "Winning the AI Arms Race &#x2013; Defeating China and Russia, Re-establishing American Superpower for Global Prosperity and the Greater Good with Artificial Intelligence." With his extensive experience as an award-winning Silicon Valley C-suite executive, a former congressional candidate, an executive board member of the state party, and an elected leader in his city, Kumar brings a visionary yet grounded perspective on leveraging AI&#x2019;s transformative potential. His unique expertise in technology, public policy, and public service allows him to present strategies that could significantly influence national and global advancements in AI. The book is structured around three pivotal themes: strengthening and safeguarding America&#x2019;s superpower status, countering the threats posed by malicious actors, and harnessing AI for the greater global good. This book is essential reading for policy makers navigating the complexities of AI&#x2019;s future and business leaders aiming to position themselves for success in the AI-driven world. It&#x2019;s an indispensable resource for anyone looking to understand and influence the future of AI.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788743800880},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11062543},}@INPROCEEDINGS{10957442,
  author={Garg, Ankit and Verma, Deepti and Pandey, Laxmi and Kumar, K.Suresh and Singh, Rashmi and Sharma, Anubhav},
  booktitle={2025 International Conference on Intelligent Control, Computing and Communications (IC3)}, 
  title={Enhancing Tailored Travel by Integrating Generative AI with Insights Driven by Personality}, 
  year={2025},
  volume={},
  number={},
  pages={404-409},
  abstract={A key component of personalized travel recommendation systems, the Retrieval-Augmented Generator (RAG) architecture has been revolutionized by recent developments in generative Artificial Intelligence (AI). The RAG framework uses natural language inputs and combines retrieval-based techniques with large-scale language models to produce user-specific, contextually appropriate recommendations. By using iterative learning to adjust dynamically, these systems make sure that recommendations change in tandem with changing user preferences, new travel trends, and contextual factors. The Big Five (BF) features and the Myers-Briggs Type Indicator (MBTI), two scientifically based personality models, are used in this study to expand these capabilities. More personalization results from this integration, which improves the system's capacity to match recommendations with unique user preferences and behaviors. In order to show how personality-driven insights enhance the potential of generative AI, the study investigates the fundamental techniques and real-world implementations of this approach. Our system's key performance measures include an 82% accuracy rate, a 78% user satisfaction rate, and performance breakdowns of 75% for introversion and 85% for extraversion. These outcomes provide a fair assessment of the system's contributions to customized travel planning by being compared to those of current systems to contextualize enhancements.},
  keywords={Performance evaluation;Correlation;Generative AI;Statistical analysis;Large language models;Natural languages;Psychology;Market research;Planning;Recommender systems;trip suggestion;large language models (LLM);retrieval augmentation;personality model},
  doi={10.1109/IC363308.2025.10957442},
  ISSN={},
  month={Feb},}@ARTICLE{10680046,
  author={Lee, Hyeon-Ju and Buu, Seok-Jun},
  journal={IEEE Access}, 
  title={Deep Generative Replay With Denoising Diffusion Probabilistic Models for Continual Learning in Audio Classification}, 
  year={2024},
  volume={12},
  number={},
  pages={134714-134727},
  abstract={Accurate classification of audio data is essential in various fields such as speech recognition, safety management, healthcare, security, and surveillance. However, existing deep learning classifiers typically require extensive pre-collected data and struggle to adapt to the emergence of new audio classes over time. To address these challenges, this paper proposes a continual learning method utilizing Diffusion-driven Generative Replay (DDGR). The proposed DDGR method continuously updates the model at each training stage with high-quality generated data from Denoising Diffusion Probabilistic Models (DDPM), preserving existing knowledge. Furthermore, by embedding disentangled representations through a triplet network, the model can effectively recognize new classes as they emerge. This approach overcomes the problem of catastrophic forgetting and effectively resolves the issue of data scalability in a continual learning setup. The proposed method achieved the highest AIA values of 95.45% and 72.99% on the Audio MNIST and ESC-50 datasets, respectively, compared to existing continual learning methods. Additionally, for Audio MNIST, it showed IM −0.01, FWT 0.27, FM 0.06, and BWT −0.06, indicating that it best preserves prior knowledge while learning new data most effectively. For ESC-50, it demonstrated IM of −0.12, FWT of 0.09, FM of 0.17, and BWT of −0.17. These results validate the efficacy of the DDGR method in maintaining prior knowledge while integrating new information and highlight the complementary role of the triplet network in enhancing feature representation.},
  keywords={Continuing education;Data models;Generators;Adaptation models;Speech recognition;Spectrogram;Scalability;Audio systems;Classification algorithms;Noise reduction;Audio classification;continual learning;generative replay;triplet network;denoising diffusion probabilistic model},
  doi={10.1109/ACCESS.2024.3459954},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11015134,
  author={S.Sahebzathi and Kumar, K.R. Vishnu and Fiaz, S. Muhammad and Pramodh, K.},
  booktitle={2024 International Conference on Communication, Computing, Smart Materials and Devices (ICCCSMD)}, 
  title={Emergency Management System using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The past century has transformed our way of living as well as our relationship with the collective. A large portion of the population now resides in cities and, therefore, cities are facing tremendous pressure to provide them public services more efficiently with sustainability for enhancing their quality of living-to be richer against which the impacts of changes in climate would become more challenging in the near future. In this complicated situation, overcrowded urban centers become vulnerable to numerous emergencies that may spring up at any moment with devastating human and economic losses. Smart city initiatives are actively being deployed in various locations, focusing on detection, alerting, and managing emergencies. This article examines existing smart city solutions for crisis management and categorizes emergency-oriented systems according to the technologies and services used. The text also gives a related overview of recent breakthroughs in Artificial Intelligence and Big Data concerning their application within the context of urban emergency management, which could imply directions for innovative development and structures the given solutions by relevant criteria. Finally, the paper lists open research challenges and promising trends and directions for further investigation in this critical area.},
  keywords={Smart cities;Prevention and mitigation;Big Data;Emergency services;Market research;Telecommunication computing;Sustainable development;Springs;Smart materials;Meteorology;Smart Cities;Urbanization;Emergency Management;Crisis Detection;Alerting Systems;Mitigation Strategies;Artificial Intelligence (AI);Big Data;Public Services;Urban Sustainability;Human Safety},
  doi={10.1109/ICCCSMD63546.2024.11015134},
  ISSN={},
  month={Dec},}@ARTICLE{9763831,
  author={Noakoasteen, Oameed and Vijayamohanan, Jayakrishnan and Gupta, Arjun and Christodoulou, Christos},
  journal={IEEE Open Journal of Antennas and Propagation}, 
  title={Antenna Design Using a GAN-Based Synthetic Data Generation Approach}, 
  year={2022},
  volume={3},
  number={},
  pages={488-494},
  abstract={In this paper, we propose the use of GANs as learned, data-driven knowledge database that can be queried for rapid synthesis of suitable antenna designs given a desired response. As an example, we consider the problem of designing the Log-Periodic Folded Dipole Array (LPFDA) antenna for two non-overlapping ranges of Q-factor values. By representing the antenna with the vector of its structural parameters and considering each desirable range of the Q-factor as a class, we transform our problem to that of generating new samples from a given class. We develop two alternative models, a Conditional Wasserstein GAN and a label-switched library of vanilla Wasserstein GANs and train them with a dataset of features and their associated labels (parameter vectors and Q-factor range). The main component of these models is a generator network that learns to map a normally distributed noise vector along with a binary label to the vector of parameters of candidate structures. We demonstrate that in inference mode, these models can be relied upon for fast generation of suitable designs.},
  keywords={Generators;Q-factor;Training;Antennas;Structural engineering;Generative adversarial networks;Libraries;Artificial neural networks;electromagnetics;machine learning},
  doi={10.1109/OJAP.2022.3170798},
  ISSN={2637-6431},
  month={},}@ARTICLE{9420104,
  author={Xie, Chen and Yang, Kecheng and Wang, Anni and Chen, Chunxu and Li, Wei},
  journal={IEEE Access}, 
  title={A Mura Detection Method Based on an Improved Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={68826-68836},
  abstract={Mura is defined as visual unevenness on the display panel. It can cause unpleasant feelings, so it is necessary to perform Mura inspection during the display quality test. However, Mura is quite difficult to be detected because of its irregular shape and size as well as its low contrast. To solve this practical problem, we proposed a GAN-based model named UADD-GAN to detect Mura in this work. Consisting of a proposed UADD generator and a discriminator, the model is trained using only normal samples, after which the generator is able to simulate the distribution of normal samples. During training, the generator takes normal images as inputs and outputs their reconstructions, while the discriminator receives images and determines whether they are original or reconstructed ones, defiantly helping the generator to perform reconstructions better. The symmetric structure and the skip-adding operation make it easy for the UADD generator to reconstruct the normal samples well. In the detecting procedure, the generator performs worse in the reconstruction of samples with Mura so that we can distinguish them from the normal ones. To make full use of discriminator, we use multiple feature layers of the discriminator for supervision instead of using only the classification layer, helping the generator to reconstruct normal samples better. Meanwhile, a two-side detecting method was used to detect Mura since all the samples are not in a square shape and it greatly improved the detecting accuracy. We have conducted experiments of Mura data sets with different proportion and our research indicates that our proposed method surpasses other state of the art methods.},
  keywords={Generators;Image reconstruction;Generative adversarial networks;Training;Anomaly detection;Feature extraction;Testing;Mura detection;multiple supervision;UADD generator},
  doi={10.1109/ACCESS.2021.3076792},
  ISSN={2169-3536},
  month={},}@ARTICLE{9474500,
  author={Lazcano, Diego and Franco, Nicolás Fredes and Creixell, Werner},
  journal={IEEE Access}, 
  title={HGAN: Hyperbolic Generative Adversarial Network}, 
  year={2021},
  volume={9},
  number={},
  pages={96309-96320},
  abstract={Recently, Hyperbolic Spaces in the context of Non-Euclidean Deep Learning have gained popularity because of their ability to represent hierarchical data. We propose that it is possible to take advantage of the hierarchical characteristic present in the images by using hyperbolic neural networks in a GAN architecture. In this study, different configurations using fully connected hyperbolic layers in the GAN, WGAN, CGAN, and the mapping network of the StyleGAN2 are tested in what we call the HGAN, HWGAN, HCGAN, and HStyleGAN, respectively. Furthermore, we test multiple values of curvature and introduce an exponential way to train it. The results are measured using the Inception Score (IS) and the Fréchet Inception Distance (FID) over the MNIST dataset and with FID over CIFAR-10. Depending on the configuration and space curvature, better results are achieved for each proposed hyperbolic version than their euclidean counterpart.},
  keywords={Generative adversarial networks;Neural networks;Manifolds;Generators;Deep learning;Task analysis;GAN;WGAN;CGAN;StyleGAN2;hyperbolic spaces;Poincaré ball;hyperbolic neural networks;HGAN},
  doi={10.1109/ACCESS.2021.3094723},
  ISSN={2169-3536},
  month={},}@ARTICLE{9864324,
  author={He, Daojing and Dai, Jiayu and Liu, Xiaoxia and Zhu, Shanshan and Chan, Sammy and Guizani, Mohsen},
  journal={IEEE Network}, 
  title={Adversarial Attacks for Intrusion Detection Based on Bus Traffic}, 
  year={2022},
  volume={36},
  number={4},
  pages={203-209},
  abstract={A communication bus is used to transmit electronic signals between components, realize functional integration through information sharing, and improve system efficiency. The current research on intrusion detection based on bus traffic is mainly pertaining to machine learning or time logic detection. However, recent studies have shown that machine learning models perform poorly in defense of various adversarial attacks. In this article, we propose a method based on generative adversarial networks to transform normal traffic into adversarial and malicious ones. To be closer to reality, adversarial example generation models on two threat scenarios are proposed. At the same time, the distance metric L2 is introduced in the loss function to ensure the authenticity of the generated adversarial examples. To evaluate our method, we use the traffic generated by the model to various intrusion detection systems based on bus. Experimental results show that the model is effective because the detection rate of different intrusion detection models decreases after the traffic is processed. Thus, the traffic generated by our models can be used as training data to enhance the accuracy of intrusion detection systems.},
  keywords={Intrusion detection;Protocols;Generative adversarial networks;Security;Telecommunication traffic;Training data;Data models;Adversarial machine learning},
  doi={10.1109/MNET.105.2100353},
  ISSN={1558-156X},
  month={July},}@INPROCEEDINGS{10941283,
  author={Santhakumar, D. and Soundararajan, Gopalakrishnan and Chintala, Suman and Bharath, E. and Murthy Inumula, Krishna and Chouhan, Kuldeep},
  booktitle={2025 International Conference on Pervasive Computational Technologies (ICPCT)}, 
  title={Optimizing Supply Chain Operations through Machine Learning: Opportunities and Challenges}, 
  year={2025},
  volume={},
  number={},
  pages={406-410},
  abstract={In order to create enduring partnerships that are essential to attaining long-term economic success, supply chain management, or SCM, is essential. Strict standards and decision-making procedures are necessary for efficient SCM, and they have a big influence on the final results. In order to efficiently manage inventory levels and logistical procedures, this study offers a complex ML system that combines demand forecasts and optimisation methods. For predicting demand, the approach uses a Generative Adversarial Networks (GAN) model, taking advantage of its capacity to process time-series data and identify intricate trends. The algorithm achieves an impressive 96.4% forecasting of demand accuracy by examining extensive historical sales data and a variety of supply chain variables. In order to optimise inventory rules, the investigation also uses optimisation techniques, particularly Genetic Algorithms (GA) in conjunction with Simulated Annealing (SA). The model demonstrated significant improvements when implemented to an actual commercial logistics system. Notably, it reduced storage expenses by 19% and increased order fulfillment rates by 24%, proving its capacity for combining service standards with financial efficiency. The findings highlight the significant impact of ML on improving supply chain efficiency and show how it can result in significant cost savings and improve service quality.},
  keywords={Supply chain management;Costs;Accuracy;Supply chains;Demand forecasting;Machine learning;Predictive models;Generative adversarial networks;Standards;Genetic algorithms;Machine learning;Supply chain management;Cost;Logistic;KNN},
  doi={10.1109/ICPCT64145.2025.10941283},
  ISSN={},
  month={Feb},}@ARTICLE{9781402,
  author={Luo, Mandi and Wu, Haoxue and Huang, Huaibo and He, Weizan and He, Ran},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Memory-Modulated Transformer Network for Heterogeneous Face Recognition}, 
  year={2022},
  volume={17},
  number={},
  pages={2095-2109},
  abstract={Heterogeneous face recognition (HFR) aims at matching face images across different domains. It is challenging due to the severe domain discrepancies and overfitting caused by small training datasets. Some researchers apply a “recognition via generation” strategy and propose to solve the problem by translating images from a given domain into the visual domain. However, in many HFR tasks such as near-infrablack HFR, there is no paiblack data, which makes it an unsupervised generation. Pose variations, background differences, and many other factors present challenges. Moreover, the generated results lack diversity since many previous works regard this image translation as a “one-to-one” generation task. Considering the information deficiency in the input images, we propose to formulate this image translation process as a “one-to-many” generation problem. Specifically, we introduce reference images to guide the generation process. We propose a memory module to explore the prototypical style patterns of the reference domain. After self-supervised updating, the memory items are attentively aggregated to represent the style information. Moreover, to subtly fuse the contents of input images with the style of reference images, we propose a novel style transformer module. Specifically, we crop the encoded input and reference feature maps into patches, and use the style transformer to establish long-range dependencies between the input and reference patches. Thus, the style of every input patch is transferblack based on those of the most relevant reference patches. Extensive experiments on multiple datasets for various HFR tasks, including NIR-VIS, thermal-VIS, sketch-photo, and gray-RGB, are conducted. The robustness and effectiveness of the proposed MMTN are demonstrated both quantitatively and qualitatively.},
  keywords={Face recognition;Task analysis;Transformers;Encoding;Feature extraction;Image recognition;Memory modules;Heterogeneous face recognition;style transformer;memory network},
  doi={10.1109/TIFS.2022.3177960},
  ISSN={1556-6021},
  month={},}@ARTICLE{9622164,
  author={Luo, Xiaoqing and Gao, Yuanhao and Wang, Anqi and Zhang, Zhancheng and Wu, Xiao-Jun},
  journal={IEEE Transactions on Multimedia}, 
  title={IFSepR: A General Framework for Image Fusion Based on Separate Representation Learning}, 
  year={2023},
  volume={25},
  number={},
  pages={608-623},
  abstract={This paper proposes an image fusion framework based on separate representation learning, called IFSepR. We believe that both the co-modal image and the multi-modal image have common and private features based on prior knowledge, exploiting this disentangled representation can help to image fusion, especially to fusion rule design. Inspired by the autoencoder network and contrastive learning, a multi-branch encoder with contrastive constraints is built to learn the common and private features of paired images. In the fusion stage, based on the disentangled features, a general fusion rule is designed to integrate the private features, then combining the fused private features and the common feature are fed into the decoder, reconstructing the fused image. We perform a series of evaluations on three typical image fusion tasks, including multi-focus image fusion, infrared and visible image fusion, medical image fusion. Quantitative and qualitative comparison with five state-of-art image fusion methods demonstrates the advantages of our proposed model.},
  keywords={Image fusion;Feature extraction;Task analysis;Image reconstruction;Decoding;Transforms;Knowledge engineering;Autoencoder;contrastive learning;disentangled feature learning;fusion rule;image fusion},
  doi={10.1109/TMM.2021.3129354},
  ISSN={1941-0077},
  month={},}@ARTICLE{10559607,
  author={Shao, Yang and Zhou, Yueying and Gong, Peiliang and Sun, Qianru and Zhang, Daoqiang},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={A Dual-Adversarial Model for Cross-Time and Cross-Subject Cognitive Workload Decoding}, 
  year={2024},
  volume={32},
  number={},
  pages={2324-2335},
  abstract={Electroencephalogram (EEG) signals are widely utilized in the field of cognitive workload decoding (CWD). However, when the recognition scenario is shifted from subject-dependent to subject-independent or spans a long period, the accuracy of CWD deteriorates significantly. Current solutions are either dependent on extensive training datasets or fail to maintain clear distinctions between categories, additionally lacking a robust feature extraction mechanism. In this paper, we tackle these issues by proposing a Bi-Classifier Joint Domain Adaptation (BCJDA) model for EEG-based cross-time and cross-subject CWD. Specifically, the model consists of a feature extractor, a domain discriminator, and a Bi-Classifier, containing two sets of adversarial processes for domain-wise alignment and class-wise alignment. In the adversarial domain adaptation, the feature extractor is forced to learn the common domain features deliberately. The Bi-Classifier also fosters the feature extractor to retain the category discrepancies of the unlabeled domain, so that its classification boundary is consistent with the labeled domain. Furthermore, different adversarial distance functions of the Bi-Classifier are adopted and evaluated in this model. We conduct classification experiments on a publicly available BCI competition dataset for recognizing low, medium, and high cognitive workload levels. The experimental results demonstrate that our proposed BCJDA model based on cross-gradient difference maximization achieves the best performance.},
  keywords={Feature extraction;Brain modeling;Adaptation models;Electroencephalography;Task analysis;Machine learning;Decoding;Cognitive workload decoding;electroencephalogram (EEG);joint domain adaptation;adversarial learning;cross-time;cross-subject},
  doi={10.1109/TNSRE.2024.3415364},
  ISSN={1558-0210},
  month={},}@INPROCEEDINGS{9995300,
  author={Chen, Yuqi and Liu, Juan and Jiang, Peng and Feng, Jing and Cao, Dehua and Pang, Baochuan},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Context-Guided Attention Method for Integrating Features of Histopathological Patches}, 
  year={2022},
  volume={},
  number={},
  pages={2281-2286},
  abstract={Lots of researchers have studied for classifying histopathological whole slide images (WSIs). Since a WSI is too large to be processed directly, researchers usually cut it into many small-sized patches and then integrate the discriminative features extracted from the patches to obtain a slide-level feature of the WSI. The integration strategy generating the slide-level features is crucial for the WSI classification model. Lots of attention-based methods have been proposed for such purpose. However, most attention-based methods do not take the patches relationship into consideration, which affects the classification performance of the models. In this work, we propose a novel Context-Guided attention (CGattention) method to integrate the patch-level features, which constructs a context vector to simulate the global context information of the whole WSI and implicitly characterizes the relationship between patches in the WSI. When evaluated on two publicly available datasets, the CGattention based model obtained the better performance than other attention-based models.},
  keywords={Representation learning;Image analysis;Histopathology;Computational modeling;Lung cancer;Feature extraction;Computational efficiency;Histopathology;Whole Slide Image (WSI);Context-Guided Attention;Feature Integration;Classification},
  doi={10.1109/BIBM55620.2022.9995300},
  ISSN={},
  month={Dec},}@ARTICLE{11015950,
  author={Chang, Ching-Chun and Chen, Fan-Yun and Gu, Shih-Hong and Gao, Kai and Wang, Hanrui and Echizen, Isao},
  journal={IEEE Access}, 
  title={Imitation Game for Adversarial Disillusion With Chain-of-Thought Reasoning in Generative AI}, 
  year={2025},
  volume={13},
  number={},
  pages={95085-95093},
  abstract={As the cornerstone of artificial intelligence, machine perception confronts a fundamental threat posed by adversarial illusions. These adversarial attacks manifest in two primary forms: deductive illusion, where specific stimuli are crafted based on the victim model’s general decision logic, and inductive illusion, where the victim model’s general decision logic is shaped by specific stimuli. The former exploits the model’s decision boundaries to create a stimulus that, when applied, interferes with its decision-making process. The latter reinforces a conditioned reflex in the model, embedding a backdoor during its learning phase that, when triggered by a stimulus, causes aberrant behaviors. The multifaceted nature of adversarial illusions calls for a unified defence framework, addressing vulnerabilities across various forms of attack. In this study, we propose a disillusion paradigm based on the concept of an imitation game. At the heart of the imitation game lies a multimodal generative agent, steered by chain-of-thought reasoning, which observes, internalizes and reconstructs the semantic essence of a sample, liberated from the classic pursuit of reversing the sample to its original state. As a proof of concept, we conduct experimental simulations using a multimodal generative dialogue agent and evaluates the methodology under a variety of attack scenarios. Experimental results demonstrate that the proposed framework consistently neutralizes both deductive and inductive adversarial illusions across diverse white-box and black-box attack scenarios.},
  keywords={Games;Predictive models;Noise reduction;Logic;Cognition;Adaptation models;Semantics;Object recognition;Generative AI;Data models;Artificial intelligence;cybersecurity;disillusion;imitation;multimodality},
  doi={10.1109/ACCESS.2025.3574016},
  ISSN={2169-3536},
  month={},}@ARTICLE{9416834,
  author={Xia, Feng and Sun, Ke and Yu, Shuo and Aziz, Abdul and Wan, Liangtian and Pan, Shirui and Liu, Huan},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Graph Learning: A Survey}, 
  year={2021},
  volume={2},
  number={2},
  pages={109-127},
  abstract={Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed, respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
  keywords={Deep learning;Learning systems;Machine learning algorithms;Artificial intelligence;Signal processing algorithms;Feature extraction;Task analysis;Deep learning;graph data;graph learning;graph neural networks (GNNs);machine learning;network embedding;network representation learning (NRL)},
  doi={10.1109/TAI.2021.3076021},
  ISSN={2691-4581},
  month={April},}@ARTICLE{9775011,
  author={Guo, Zhendong and Liu, Haitao and Ong, Yew-Soon and Qu, Xinghua and Zhang, Yuzhe and Zheng, Jianmin},
  journal={IEEE Transactions on Cybernetics}, 
  title={Generative Multiform Bayesian Optimization}, 
  year={2023},
  volume={53},
  number={7},
  pages={4347-4360},
  abstract={Many real-world problems, such as airfoil design, involve optimizing a black-box expensive objective function over complex-structured input space (e.g., discrete space or non-Euclidean space). By mapping the complex-structured input space into a latent space of dozens of variables, a two-stage procedure labeled as generative model-based optimization (GMO), in this article, shows promise in solving such problems. However, the latent dimension of GMO is hard to determine, which may trigger the conflicting issue between desirable solution accuracy and convergence rate. To address the above issue, we propose a multiform GMO approach, namely, generative multiform optimization (GMFoO), which conducts optimization over multiple latent spaces simultaneously to complement each other. More specifically, we devise a generative model which promotes a positive correlation between latent spaces to facilitate effective knowledge transfer in GMFoO. And furthermore, by using Bayesian optimization (BO) as the optimizer, we propose two strategies to exchange information between these latent spaces continuously. Experimental results are presented on airfoil and corbel design problems and an area maximization problem as well to demonstrate that our proposed GMFoO converges to better designs on a limited computational budget.},
  keywords={Optimization;Training;Convergence;Task analysis;Linear programming;Bayes methods;Generators;Bayesian optimization (BO);generative model-based optimization (GMO);multiform optimization (MFoO);transfer optimization (TO)},
  doi={10.1109/TCYB.2022.3165044},
  ISSN={2168-2275},
  month={July},}@INPROCEEDINGS{10447940,
  author={Xu, Weichen and Xu, Xinxin and Fu, Tianhao and Cao, Jian and Xu, Xiaoyang and Huang, Yuetian and Cao, Xixin and Zhang, Xing},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={SweepMM: A High-Quality Multimodal Dataset for Sweeping Robots in Home Scenarios for Vision-Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={5105-5109},
  abstract={Embodied intelligence based on vision-language models aims to learn from interactions and derive general intelligence. However, existing generalized vision-language models cannot understand domain knowledge in home scenarios due to the lack of sweeping robot multimodal datasets. In this paper, we propose the first multimodal dataset for sweeping robots, called SweepMM. We create textual data such as room type, scene descriptions, and moving recommendations using various approaches including rule-based, manual-based, and off-the-shelf model-based methods. Based on this dataset, we fine-tune the first generative pretrained model for sweeping robots, called SweepGPM. This model enables human-robot dialogue and surpasses previous state-of-the-art methods by 0.8% in room type recognition, 0.4% in obstacle detection, and 8.0% in lost item search, demonstrating the potential of embodied intelligence in sweeping robots.},
  keywords={Bridges;Visualization;Signal processing;Data models;Acoustics;Task analysis;Speech processing;Sweeping robot;Benchmark dataset;Vision-language model;Embodied intelligence},
  doi={10.1109/ICASSP48485.2024.10447940},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{11071131,
  author={Shao, Jun and Liu, Li and Shao, Qing},
  booktitle={2025 3rd International Conference on Data Science and Information System (ICDSIS)}, 
  title={Deep Learning Models of Artificial Intelligence in Computer Vision}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Deep learning models in computer vision face challenges such as high computational resource demands and limited generalization in practical scenarios. To address these issues, this study proposes a systematic optimization framework integrating transfer learning, self-supervised learning, model pruning, and data augmentation. Specifically, transfer learning leverages pre-trained models for efficient feature extraction, while self-supervised learning reduces dependency on labeled data through contrastive pre-training. Model pruning and quantization streamline network complexity, and advanced data augmentation (e.g., Mixup, CutMix) enhances robustness. Experiments across image classification, object detection, and semantic segmentation tasks demonstrate significant improvements: ResNet50 achieved 79.2% classification accuracy, pruned Faster R-CNN reduced parameters by 30% with minimal accuracy loss, and combined self-supervised learning with augmentation boosted semantic segmentation mIoU to 73.4%. These strategies effectively balance accuracy, efficiency, and adaptability, offering practical solutions for resource-constrained applications.},
  keywords={Deep learning;Computer vision;Accuracy;Computational modeling;Semantic segmentation;Transfer learning;Self-supervised learning;Object detection;Data models;Optimization;deep learning;computer vision;artificial intelligence;model optimization;object detection},
  doi={10.1109/ICDSIS65355.2025.11071131},
  ISSN={},
  month={May},}@INPROCEEDINGS{10923795,
  author={Llerena, Yanelys Fernández and Oliveira, João and Mendes, Andreia Fernandes and Crespo Ferreira, Miguel Ângelo and Pozo Pérez, José Rubén and Sousa, Nuno and Castilla, Yusbel Chávez and Ribeiro, Rui Pedro and Caetano, Inês and Magalhães, Luís Gonzaga and Reyes, Edel Garcia and Guevara Lopez, Miguel Angel},
  booktitle={2024 International Conference on Graphics and Interaction (ICGI)}, 
  title={Development of a Virtual Fitting Room Integrating Computer Vision, Artificial Intelligence and Virtual Reality Technologies}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This article presents a virtual fitting room system that improves customer experience by providing personalized image-based recommendations. The prototype designed takes a step forward in the state of the art by contemplating the creation of a customizable avatar based on the user's biometric characteristics and clothing style. Facial recognition models were implemented to predict gender and age, and segmentation and classification techniques are used to extract characteristics from the clothing the user is wearing. The work describes the progress and experiences in the development of some prototype modules and the possible methodologies that are being evaluated to develop a real-time web-based adaptation experience to represent the user's appearance and simulate a combination of multiple garments.},
  keywords={Computer vision;Solid modeling;Image segmentation;Face recognition;Fitting;Clothing;Prototypes;Predictive models;User experience;Real-time systems;Artificial intelligence;Virtual reality;Computer vision;Deep learning;Fashion recommendation;Machine learning;User experience;Virtual fitting room;Virtual try-on},
  doi={10.1109/ICGI64003.2024.10923795},
  ISSN={},
  month={Nov},}@ARTICLE{8466571,
  author={Chen, Longxiang and Hu, Xiaolong and Zhang, Zuping},
  journal={IEEE Access}, 
  title={Face Aging With Boundary Equilibrium Conditional Autoencoder}, 
  year={2018},
  volume={6},
  number={},
  pages={54834-54843},
  abstract={Since generative adversarial networks (GANs) were proposed in 2014, mode collapse has been a problem that affects many researchers when training GANs. With the reconstruction loss of an autoencoder, conditional adversarial autoencoder (CAAE) is free from mode collapse. However, its reconstruction loss will bring a saturation problem, in which the encoder maps every input image into just one latent variable. Combining the CAAE with a boundary equilibrium generative adversarial network, we propose a boundary equilibrium conditional autoencoder (BECAE) focusing on the face aging task. Our model is the first GANs that renders images through a discriminator. We also introduce some statistics to measure the level of the saturation problem. The results show that the BECAE has successfully solved the saturation problem and can generate face images of the same quality as the images generated by the CAAE.},
  keywords={Face;Gallium nitride;Generators;Aging;Generative adversarial networks;Image reconstruction;Training;Face aging;generative adversarial networks;boundary equilibrium;autoencoder;non-convergence;saturation problem},
  doi={10.1109/ACCESS.2018.2870150},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9768630,
  author={Choudhry, Mani Deepak and S, Jeevanandham and Rose, Biji and P, Sruthi Mol},
  booktitle={2022 First International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)}, 
  title={Machine Learning Frameworks for Industrial Internet of Things (IIoT): A Comprehensive Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Many industrial processes have been transformed by ict infrastructure. Artificial intelligence and machine learning algorithms have been needed by companies of all sizes, whether minor or major, to handle the terabytes of data created by sensing, actuation, production control systems, and web services. These data are large (terabytes) and diverse (picture, audio, video, graphics), necessitating the use of specific models and approaches for analysis and administration. Intelligent technologies create enormous quantities of data in massive industrial networks, hence IIoT frameworks demand clever, resilient big data analysis tools. Because of their cognitive acquisition and processing capabilities, artificial intelligence (AI) and machine learning (ML) techniques generate impressive outcomes in IIoT networks. The possibility of machine learning in IIoT applications is assessed in this research study, which also gives a comprehensive topology of IIoT and major enabling technologies. The theoretical underpinnings of numerous well-known ML algorithms, as well as several software and hardware frameworks for ML implementations, are then presented. The use of machine learning techniques in IIoT applications is briefly covered. Finally, this study identifies key problems as well as potential research directions.},
  keywords={Industries;Machine learning algorithms;Web services;Supply chains;Software algorithms;Machine learning;Maintenance engineering;Artificial Intelligence;IIoT;Machine Learning;Smart Business},
  doi={10.1109/ICEEICT53079.2022.9768630},
  ISSN={},
  month={Feb},}@ARTICLE{10113647,
  author={Hendra, Andi and Kanazawa, Yasushi},
  journal={IEEE Access}, 
  title={TP-GAN: Simple Adversarial Network With Additional Player for Dense Depth Image Estimation}, 
  year={2023},
  volume={11},
  number={},
  pages={44176-44191},
  abstract={We present a simple yet robust monocular depth estimation technique by synthesizing a depth map image from a single RGB input image using the advantage of generative adversarial networks (GAN). We employ an additional sub-model termed refiner to extract local depth features, then combine it with the global scene information from the generator to improve the GAN’s performance compared to the standard GAN architectural scheme. Notably, the generator is the first player to learn to synthesize depth images. The second player, the discriminator, classifies the generated depth. In the meantime, the third player, the refiner, enhances the final reconstructed depth. Complementing the GAN model, we apply a conditional generative network (cGAN) to lead the generator in mapping the input image to the respective depth representation. We further incorporate a structured similarity (SSIM) as our loss function for the generator and refiner in GAN training. Through extensive experiment validation, we confirmed the performance of our strategy on the publicly indoor NYU Depth v2 and KITTI outdoor data. Experiment results on the NYU depth v2 dataset show that our proposed approach achieves the best performance by 96.0% on threshold accuracy ( $\delta < 1.25^{2}$ ) and the second-best accuracy on all thresholds on the KITTI dataset. We discovered that our proposed method compares favorably to numerous existing monocular depth estimation strategies and demonstrates a considerable improvement in the accuracy of image depth estimation despite its simple network architecture.},
  keywords={Generative adversarial networks;Generators;Estimation;Image reconstruction;Task analysis;Convolutional neural networks;Depth estimation;single image;conditional GAN;generative adversarial network (GAN);third player GAN},
  doi={10.1109/ACCESS.2023.3272292},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10771581,
  author={Karimy, Aziz Ullah and Rasuli, Juma and Reddy, P Chandrasekhar and Joya, Musa and Hamdard, Ali Juma and Ghulami, Hassan Rahnaward},
  booktitle={2024 IEEE Global Humanitarian Technology Conference (GHTC)}, 
  title={A Review on the Feasibility of AI-Supported Education Platforms in Afghanistan: Addressing Barriers to Women and Girls' Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Access to education is a fundamental human right; however, girls and women in Afghanistan face numerous restrictions and barriers imposed by the Taliban regime. Despite these issues, employing technologies, particularly Artificial Intelligence (AI), provides convenient solutions to facilitate education and empower Afghan girls and women. In this paper, we explore how AI-supported educational platforms can assist in providing quality education to Afghan girls and women. It analyzes current digital delivery approaches with incorporated AI technology, including generative AI, Intelligent Tutoring System (ITS), chatbots, and SMS-based learning platforms, that are accessible on low-cost devices, and do not rely heavily on high-speed internet connections, and outlines suitable pathways to overcome the existing restrictions. By employing AI technology, a personalized learning environment can be created to deliver quality education based on the individual needs of Afghan girls and women. A survey was conducted to explore the feasibility of implementing an AI-supported educational platform in Afghanistan, considering challenges such as internet connectivity, availability of electric-ity, access to digital devices, and time constraints. Based on successfully implemented programs in another region, we listed recommendations for policymakers, NGOs, and stakeholders to introduce customized AI-supported educational platforms for Afghan girls and women.},
  keywords={Surveys;Sensitivity;Generative AI;Scalability;Education;Chatbots;Stakeholders;Cultural differences;Time factors;Sustainable development;Education;Artificial intelligence;Intelligent Tutoring Systems;Afghan girls and women;Afghanistan},
  doi={10.1109/GHTC62424.2024.10771581},
  ISSN={2473-5728},
  month={Oct},}@INPROCEEDINGS{10513421,
  author={Qin, Yuchao and Liu, Geping and Wu, Min},
  booktitle={2023 Twelfth International Conference of Educational Innovation through Technology (EITT)}, 
  title={Good or Bad? Explore the Application of ChatGPT in Education ——Based on Interviews and User Experience Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={158-163},
  abstract={The emergence of ChatGPT has garnered significant societal attention. While many researchers highlight the promising applications of ChatGPT in education, there are also dissenting voices. Although ChatGPT has been widely discussed in theory, it lacks practical verification. In order to verify the application effect of ChatGPT in the field of education, this study conducted interviews with ten education users who used ChatGPT for a long time. Additionally, we recruited three primary school teachers to use ChatGPT in their teaching activities and surveyed them about their experience. Findings reveal that ChatGPT's application in education is characterized by complex dual effects with its effectiveness contingent upon users' self-awareness and sense of responsibility. Enhancing novel digital literacy among educators and students stands as a crucial task for the era of generative artificial intelligence.},
  keywords={Technological innovation;Generative AI;Education;Chatbots;User experience;Interviews;Task analysis;ChatGPT;artificial intelligence;applying in education;new digital literacy},
  doi={10.1109/EITT61659.2023.00037},
  ISSN={2166-0549},
  month={Dec},}@INPROCEEDINGS{10933116,
  author={Kumar, JMSV Ravi and Dheeraj, Bellaganti and Gowtham, Geddada and Babu, Dokala Vivek and Kiran, Bonthu Asha and Srikanth, M.},
  booktitle={2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)}, 
  title={Image Search Engine with Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1217-1223},
  abstract={The Image Recognition Search Engine is one of the modern advances in artificial intelligence and image analysis and processing. This advanced system surpasses typical functionalities in image identification as it captures features to solve new problems faced in existing systems of digital media authentication, content designing, and improvement of images. The system core relies on advanced AI algorithms that can rasterically recognize and analyze various forms of image objects ranging from objects, faces, scenes, and brand logos converting what appears to be simple images into indexical, analyzable data. One feature is that the system can integrate deepfake detection technology, which is relevant in the context of increasing the amount of manipulated content in the media space. Using special algorithms to learn even the least detectable abnormality in images including the presence of unnatural distinctive facial characteristics and changes in lighting conditions, we obtain a highly effective tool for improving the reliability of image identification. Furthermore, the platform allows for new features to convert an image into a video by implementing frame interpolation and motion estimation through animating the images to allow users to create interesting clips for applications regarding marketing or any personal use.},
  keywords={Deepfakes;Visualization;Sentiment analysis;Image recognition;Face recognition;Search engines;Feature extraction;User experience;Artificial intelligence;Usability;Image Recognition;Deepfake Detection;Image Processing;AI Algorithms;Content Analysis;object detection},
  doi={10.1109/ICSADL65848.2025.10933116},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11077553,
  author={Cruz, Meenalosini Vimal and Chintapalli, Sri Shakthi Sarath and Bansal, Yogya and Usha, G. and Lup, Felix Hamza},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={Advancements and Challenges in Emotional Analysis Using Machine Learning: A Comprehensive Review of Facial Expression Recognition Models}, 
  year={2025},
  volume={},
  number={},
  pages={379-384},
  abstract={The field of emotional analysis using computer vision is one of the most important fields in artificial intelligence. The network has potential for many exciting areas in humancomputer interaction, health, and entertainment. This paper delves into different machine learning techniques applied to recognizing and interpreting emotions from facial expressions. It includes Support Vector Machines, Convolutional Neural Networks, Generative Adversarial Networks, Capsule Networks, and Vision Transformers. Although important advances have taken place, these models still have a few limitations: they struggle to generalize across different datasets; deal with occlusions and tilted faces; and may be computationally complex. In addition, these models do not offer interoperability and may have possible cultural biases. Also, the analysis underlines problems of recognition of microexpressions. Therefore, ways of overcoming such limitations must be targeted in the future development of more reliable and precise Emotion Recognition Systems. This paper aims to present current methodologies with an in-depth analysis of their effectiveness.},
  keywords={Support vector machines;Emotion recognition;Computer vision;Analytical models;Computational modeling;Face recognition;Transfer learning;Transformers;Convolutional neural networks;Cultural differences;Facial Expression Recognition;Convolutional Neural Networks (CNNs);Emotion Recognition Systems;Machine Learning in Affective Computing;Vision Transformers},
  doi={10.1109/AIRC64931.2025.11077553},
  ISSN={},
  month={May},}@ARTICLE{11097292,
  author={Kim, Jong-Hyun},
  journal={IEEE Access}, 
  title={Neural Network-Based Projective Grid Model for Learning Representation of Surface and Wave Foams}, 
  year={2025},
  volume={13},
  number={},
  pages={133635-133649},
  abstract={In this paper, we propose a projective grid model that enables learning representation of foam effects using artificial neural networks. In 3D fluid simulations, foam is one of the most representative secondary effects in water. Consequently, the processes of foam generation, advection, and dissolution are computationally expensive. However, no prior research has explored representing these processes using neural networks. This paper introduces a constitutive model for learning foam effects in projective space with neural networks. Learning 3D fluid simulation is a complex and multi-faceted challenge, but our approach simplifies the design of foam effects by leveraging a 2D projective space rather than a fully 3D space. The proposed projective grid model consists of the following components: 1) learning the conditions under which foam particles are generated in the projective shape, 2) distinguishing between surface foam and wave foam based on varying conditions instead of relying on a single foam texture, 3) learning the advection process of different foam types, and 4) learning the dissolution process through representation learning. As a result, our method enables efficient representation of 3D foam effects without the need for complex numerical calculations, demonstrating its effectiveness across various scenarios.},
  keywords={Three-dimensional displays;Fluids;Mathematical models;Computational modeling;Rendering (computer graphics);Solid modeling;Surface waves;Superresolution;Sea surface;Real-time systems;Fluid simulations;foam effects;artificial neural networks;projective grid;projection space;surface foam;wave foam},
  doi={10.1109/ACCESS.2025.3593120},
  ISSN={2169-3536},
  month={},}@ARTICLE{9757826,
  author={Wang, Mei and Deng, Weihong and Liu, Cheng-Lin},
  journal={IEEE Transactions on Image Processing}, 
  title={Unsupervised Structure-Texture Separation Network for Oracle Character Recognition}, 
  year={2022},
  volume={31},
  number={},
  pages={3137-3150},
  abstract={Oracle bone script is the earliest-known Chinese writing system of the Shang dynasty and is precious to archeology and philology. However, real-world scanned oracle data are rare and few experts are available for annotation which make the automatic recognition of scanned oracle characters become a challenging task. Therefore, we aim to explore unsupervised domain adaptation to transfer knowledge from handprinted oracle data, which are easy to acquire, to scanned domain. We propose a structure-texture separation network (STSN), which is an end-to-end learning framework for joint disentanglement, transformation, adaptation and recognition. First, STSN disentangles features into structure (glyph) and texture (noise) components by generative models, and then aligns handprinted and scanned data in structure feature space such that the negative influence caused by serious noises can be avoided when adapting. Second, transformation is achieved via swapping the learned textures across domains and a classifier for final classification is trained to predict the labels of the transformed scanned characters. This not only guarantees the absolute separation, but also enhances the discriminative ability of the learned features. Extensive experiments on Oracle-241 dataset show that STSN outperforms other adaptation methods and successfully improves recognition performance on scanned data even when they are contaminated by long burial and careless excavation.},
  keywords={Character recognition;Bones;Adaptation models;Training;Feature extraction;Writing;Transforms;Oracle character recognition;unsupervised domain adaptation;feature disentanglement;generative adversarial network},
  doi={10.1109/TIP.2022.3165989},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{8462093,
  author={Liu, Bin and Nie, Shuai and Zhang, Yaping and Ke, Dengfeng and Liang, Shan and Liu, Wenju},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Boosting Noise Robustness of Acoustic Model via Deep Adversarial Training}, 
  year={2018},
  volume={},
  number={},
  pages={5034-5038},
  abstract={In realistic environments, speech is usually interfered by various noise and reverberation, which dramatically degrades the performance of automatic speech recognition (ASR) systems. To alleviate this issue, the commonest way is to use a well-designed speech enhancement approach as the front-end of ASR. However, more complex pipelines, more computations and even higher hardware costs (microphone array) are additionally consumed for this kind of methods. In addition, speech enhancement would result in speech distortions and mismatches to training. In this paper, we propose an adversarial training method to directly boost noise robustness of acoustic model. Specifically, a jointly compositional scheme of generative adversarial net (GAN) and neural network-based acoustic model (AM) is used in the training phase. GAN is used to generate clean feature representations from noisy features by the guidance of a discriminator that tries to distinguish between the true clean signals and generated signals. The joint optimization of generator, discriminator and AM concentrates the strengths of both GAN and AM for speech recognition. Systematic experiments on CHiME-4 show that the proposed method significantly improves the noise robustness of AM and achieves the average relative error rate reduction of 23.38% and 11.54% on the development and test set, respectively.},
  keywords={Training;Gallium nitride;Speech enhancement;Acoustics;Speech recognition;Noise measurement;Generators;robust speech recognition;deep adversarial training;acoustic model;generative adversarial net},
  doi={10.1109/ICASSP.2018.8462093},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10172296,
  author={Wang, Xianghai and Li, Siyao and Zhao, Xiaoyang and Zhao, Keyun},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={BiG-FSLF: A Cross Heterogeneous Domain Few-Shot Learning Framework Based on Bidirectional Generation for Hyperspectral Image Change Detection}, 
  year={2023},
  volume={61},
  number={},
  pages={1-13},
  abstract={In recent years, hyperspectral image change detection (HSI-CD) based on deep learning has achieved high detection accuracy, but these methods obtain excellent detection results usually rely on having sufficient labeled samples to train the network. However, the production of HSI labels is difficult, costly, and inefficient. In practical tasks, often only a limited number of labeled samples can be obtained due to the limitation of timeliness. To address this problem, a cross-heterogeneous domain few-shot learning framework based on bidirectional generation (BiG-FSLF) is proposed for HSI-CD, which aims to solve the few-shot problem of HSI-CD by few-shot learning (FSL) and to assist HSI-FSL to perform better by obtaining learnable changed information (i.e., empirical knowledge) from another remote sensing data. Specifically, a multitask generation encoder (MLGenE) is designed to take on both the tasks of FSL and domain adaptation to achieve HSI-CD under the condition of cross-heterogeneous domain few-shot. First, we take any pair of image data in a very high-resolution image (VHRI) CD dataset as the source domain and HSI is used as the target domain, using sufficient labeled samples in the source domain and a small number of labeled samples in the target domain for FSL. Meanwhile, a bidirectional generation domain adaptation (BiGDA) method based on a generative adversarial strategy is proposed to achieve adaptive alignment of the two heterogeneous domains (source and target domains) feature distributions, to mitigate the impact of the domain shift problem inherent to cross-domain data on FSL. Abundant experiments with only five training samples on the publicly available popular HSI-CD datasets confirm that the proposed method can show great detection performance. The source code of the proposed framework will be released at https://github.com/lsylnnu/BiG-FSLF.},
  keywords={Training;Task analysis;Metalearning;Feature extraction;Convolutional neural networks;Hyperspectral imaging;Adaptation models;Change detection (CD);convolutional neural network (CNN);few-shot learning (FSL);generative adversarial;hyperspectral image (HSI)},
  doi={10.1109/TGRS.2023.3292249},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9858428,
  author={Huang, Shuqi and Jin, Huaiping and Yang, Biao and Liu, Haipeng},
  booktitle={2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)}, 
  title={Combining Virtual Sample Generation Based Data Enhancement and Multi-objective Optimization Based Selective Ensemble for Soft Sensor Modeling}, 
  year={2022},
  volume={},
  number={},
  pages={287-293},
  abstract={Soft sensor modeling technology realizes the real-time estimation of difficult-to-measure variables by constructing the mathematical model between secondary variables and primary variable. Nevertheless, sufficient and high-quality training samples are difficult to obtain owing to the high cost of data acquisition and low sampling rate. To solve this, a soft sensor modeling method, combining virtual sample generation based data enhancement and multi-objective optimization based selective ensemble (DESE), is proposed. First, a supervised variational autoencoder (SVAE) is constructed by introducing quality variable. Second, a generative model is built through the combination of SVAE and Wasserstein GAN with gradient penalty (WGAN-gp). Third, SV-WGANgp is trained on each sample subset, which is obtained by resampling, and a fixed number of virtual samples are generated. A set of base models is established for the expanded original samples subsequently. Finally, the multi-objective optimization method is utilized to prune these models, which satisfy both accuracy and diversity requirements. After integrating the selected base models, the final prediction results are obtained. Experimental results verify that, compared with the other three popular generation models, DESE significantly improves the prediction performance of soft sensor model by supplementing the original samples.},
  keywords={Training;Soft sensors;Diversity reception;Optimization methods;Predictive models;Prediction algorithms;Linear programming;Soft Sensor;Data Enhancement;Generative Model;Multi-objective Optimization;Selective Ensemble},
  doi={10.1109/DDCLS55054.2022.9858428},
  ISSN={2767-9861},
  month={Aug},}@ARTICLE{10947312,
  author={Nando, Yudi April and Mai, Ngoc-Dau and Chung, Wan-Young},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Self-Powered Meat Freshness Monitoring With GAN-Optimized 3-D Antenna and Ensemble Learning}, 
  year={2025},
  volume={74},
  number={},
  pages={1-14},
  abstract={Automated meat monitoring is crucial for ensuring safety, maintaining quality, reducing economic losses, adhering to regulations, managing inventory effectively, and minimizing environmental impact. This article presents a self-powered system for classifying meat freshness using ensemble learning and a battery-free sensor tag powered by energy harvested from a 3-D quasi-Yagi antenna (QYA). The QYA design features a slotted-bowtie structure at the driven element and a meandering structure at the reflector, optimized through generative adversarial networks (GANs) to address various parameters and reduce design time. Constructed from five 2-D QYAs arranged in a cube and an RF harvester PCB base, the 3-D QYA considerably enhances the energy harvesting performance. Compared to its 2-D counterpart, the 3-D QYA significantly enhances coverage area, gain, energy-scavenging efficiency, and transmission distance, achieving an average gain of 11.43 dBi over a 25-MHz band across prediction, simulation, and measurement. RF harvesting tests at 915 MHz achieved a maximum power conversion efficiency (PCE) of 68.2% at 9.5-dBm input power. This energy supports the high power demands of the integrated gas sensor for battery-free monitoring of meat freshness. Over five days, data on equivalent carbon dioxide, total volatile organic compounds (VOCs), temperature, and storage time were collected and analyzed using ensemble learning models with five primary learners. The system accurately classified meat freshness into fresh, semi-fresh, and rotten categories, tracking transitions between states with over 96% classification accuracy. Our results confirm the effectiveness of this system as a practical solution for industrial applications and for improving health and safety standards in meat consumption.},
  keywords={Monitoring;Three-dimensional printing;Radio frequency;Gas detectors;Ensemble learning;Energy harvesting;Accuracy;Temperature measurement;Temperature sensors;Sensors;3-D quasi-Yagi antenna (QYA);915-MHz radio frequency energy harvesting (RFEH);ensemble learning;generative adversarial networks (GANs);meat freshness classification},
  doi={10.1109/TIM.2025.3556162},
  ISSN={1557-9662},
  month={},}@ARTICLE{11018794,
  author={Tran, Van-Nhan and Choi, Piljoo and Le, Hoanh-Su and Lee, Suk-Hwan and Kwon, Ki-Ryong},
  journal={IEEE Open Journal of the Computer Society}, 
  title={DiffCoR: Exposing AI-Generated Image by Using Stable Diffusion Model Based on Consistent Representation Learning}, 
  year={2025},
  volume={6},
  number={},
  pages={1353-1365},
  abstract={Diffusion-based generative models have significantly advanced the field of image synthesis, presenting additional challenges regarding the integrity and authenticity of digital images. Consequently, the identification of AI-generated images has become a critical problem in image forensics. However, there is a lack of literature addressing the detection of images generated by diffusion models. In this article, our focus is on developing a model capable of detecting images generated through both GAN techniques and diffusion models. We propose DiffCoR, a novel detection method for identifying AI-generated images. It consists of two main modules: Stable Diffusion Processing (SDP) and Image Representation Learning (IRL). The SDP module uses a pre-trained Stable Diffusion model to reconstruct input images via reverse diffusion and captures subtle manipulations through reconstruction discrepancies. The IRL module applies self-supervised learning with Latent Consistency Loss (LCL) to extract robust, invariant features, ensuring consistent latent representations across augmented views. We also incorporate frequency domain analysis using Discrete Fourier Transform (DFT) to enhance manipulation detection. Additionally, we introduce ForensicsImage, a publicly available dataset of over 400,000 real and AI-generated images from LSUN-Bedroom, CelebA-HQ, CelebDFv2, and various diffusion models. Experiments on ForensicsImage and GenImage show that DiffCoR achieves state-of-the-art performance, with strong cross-dataset generalization, making it suitable for real-world use in digital forensics, content verification, and social media moderation.},
  keywords={Diffusion models;Image reconstruction;Frequency-domain analysis;Noise;Feature extraction;Faces;Deepfakes;Representation learning;Image synthesis;Discrete Fourier transforms;Multimedia forensics;image forensics;representation learning;generative AI;deep learning},
  doi={10.1109/OJCS.2025.3575507},
  ISSN={2644-1268},
  month={},}@ARTICLE{10682107,
  author={Rong, Yi and Mao, Yingchi and Cui, Huajun and He, Xiaoming and Chen, Mingkai},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Edge Computing Enabled Large-Scale Traffic Flow Prediction With GPT in Intelligent Autonomous Transport System for 6G Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-18},
  abstract={The Intelligent Autonomous Transport System in 6G (6G-IATS) refers to the coordination of 6G, Artificial Intelligence (AI), and intelligent transportation systems, which is expected to revolutionize future intelligent transportation systems. In 6G-IATS, large-scale traffic flow prediction, affiliated with time series prediction, holds significant value for transportation planning and urban management. As an emerging AI method, Large Language Models (LLMs) have emerged prominently in time series forecasting. Unfortunately, it is challenging to achieve accurate and efficient large-scale traffic flow prediction by LLMs in 6G-IATS, due to the two issues: a) these LLMs fail to capture the spatio-temporal correlations in a large-scale road network, leading to limited prediction accuracy, and b) they process a substantial amount of training data on the central server, which imposes low training efficiency. Jointly considering the two concerns, this paper proposes a novel LLM and edge computing-based architecture for large-scale traffic flow prediction in 6G-IATS, called Spatio-Temporal Generative Large Language Model on Edge (STGLLM-E). In this architecture, we first decompose the entire large-scale road network into several subgraphs. To capture the spatio-temporal correlations, an LLM-based method named Spatio-Temporal Generative Large Language Model (STGLLM) including Spatio-Temporal Module (STM) and Generative Large Language Model (GLLM) is proposed. Secondly, to improve the training efficiency of the STGLLM-E, an edge training strategy based on edge servers is devised. Experiments are conducted on two real-world traffic flow datasets. The experimental results illustrate that the STGLLM-E is superior to the baselines in the prediction accuracy and the efficiency of training.},
  keywords={Roads;6G mobile communication;Training;Servers;Correlation;Accuracy;Artificial intelligence;IATS;6G;large-scale traffic flow prediction;LLMs;edge computing},
  doi={10.1109/TITS.2024.3456890},
  ISSN={1558-0016},
  month={},}@INPROCEEDINGS{10734439,
  author={Vartziotis, Tina and Dellatolas, Ippolyti and Dasoulas, George and Schmidt, Maximilian and Schneider, Florian and Hoffmann, Tim and Kotsopoulos, Sotirios and Keckeisen, Michael},
  booktitle={2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)}, 
  title={Learn to Code Sustainably: An Empirical Study on Green Code Generation}, 
  year={2024},
  volume={},
  number={},
  pages={30-37},
  abstract={The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generated code considered in this study is produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code’s "green capacity", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.},
  keywords={Measurement;Training;Codes;Runtime;Computational modeling;Green products;Encoding;Sustainable development;Artificial intelligence;Software development management;LLM;Generative;Green Coding;Sustainability},
  doi={},
  ISSN={},
  month={April},}@INPROCEEDINGS{11143438,
  author={Su, Xin and Hou, Qiushuo and He, Ruisi and Simeone, Osvaldo},
  booktitle={2025 IEEE 26th International Workshop on Signal Processing and Artificial Intelligence for Wireless Communications (SPAWC)}, 
  title={Conformal Robust Beamforming Via Generative Channel Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Traditional approaches to outage-constrained beamforming optimization rely on statistical assumptions about channel distributions and estimation errors. However, the resulting outage probability guarantees are only valid when these assumptions accurately reflect reality. This paper tackles the fundamental challenge of providing outage probability guarantees that remain robust regardless of specific channel or estimation error models. To achieve this, we propose a two-stage framework: (i) construction of a channel uncertainty set using a generative channel model combined with conformal prediction, and (ii) robust beamforming via the solution of a min-max optimization problem. The proposed method separates the modeling and optimization tasks, enabling principled uncertainty quantification and robust decision-making. Simulation results confirm the effectiveness and reliability of the framework in achieving model-agnostic outage guarantees.},
  keywords={Wireless communication;Estimation error;Uncertainty;Array signal processing;Simulation;Channel estimation;Power system reliability;Reliability;Channel models;Optimization;Conformal prediction;channel estimation;robust beamforming;outage probability},
  doi={10.1109/SPAWC66079.2025.11143438},
  ISSN={1948-3252},
  month={July},}@INPROCEEDINGS{10400613,
  author={Zhang, Z. and Hu, Y.},
  booktitle={5th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2023)}, 
  title={Texture enhancement in infrared and visible image fusion using triple different discriminators}, 
  year={2023},
  volume={2023},
  number={},
  pages={230-234},
  abstract={Infrared and visible image fusion is the fusion of meaningful features from infrared and visible images to a high-quality image. During the fusion process, texture details and target contours are not clear resulting in low fused image quality. We propose a triple discriminator generative adversarial network that contains both rich textural detail and sharp contrast. The difference image can provide not only rich texture details but also clear target edges, which can contribute to preserving the unique features during the convolution process. Markov discriminator and loss function are designed to reduce the effect of high contrast on infrared image texture and enhance the extraction of texture details. Comparing qualitative and quantitative experiments with advanced fusion methods on publicly available datasets, our model has excellent visual perception and superior metric results. Using generative adversarial network model with triple discriminator to improve the image fusion performance and enhance the texture details and target contours of the fused image.},
  keywords={},
  doi={10.1049/icp.2023.2943},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10675220,
  author={Shao, Hongxiang and Xi, Yingqi and Peng, Jiangfeng},
  booktitle={2024 5th International Conference on Electronic Communication and Artificial Intelligence (ICECAI)}, 
  title={Generative Model-Based Reinforcement Learning Visual Servoing Approach for Mobile Robot Control}, 
  year={2024},
  volume={},
  number={},
  pages={526-529},
  abstract={The mobile robot uses the camera to capture the motion scene information to learn the relative relationship between the target pose and the robot pose. In image-based mobile robot visual servoing, model-free reinforcement learning methods' learning efficiency is often reduced due to continuous or large-scale state problems, leading to the curse of dimensionality. We believe that reinforcement learning based on prediction models is an efficient and promising alternative to model-free reinforcement learning. Therefore, we propose an intelligent control method for the mobile robot based on the generative recurrent neural network (RNN) and deep reinforcement learning (DRL) called GRMRL; the main part of the model encodes the image, receives the action vector, decodes the predicted image. This trained model can predict short-term future images based on current observations. For scalability and training cost reasons, we use covariance matrix adaptation evolutionary strategies (CMA-ES) to train the policy network in the control system. Various experiments are conducted to evaluate the proposed algorithm on a simulated mobile robot path planning environment called Carracing. In a low-data area of 10k, the performance of the proposed method exceeds most model-free algorithms and achieves high learning efficiency.},
  keywords={Training;Adaptation models;Recurrent neural networks;Accuracy;Scalability;Predictive models;Prediction algorithms;model-based reinforcement learning;mobile robot intelligent control;path planning;recurrent neural network},
  doi={10.1109/ICECAI62591.2024.10675220},
  ISSN={},
  month={May},}@INPROCEEDINGS{10928631,
  author={Islam, Md. Repon and Sheikh Sadi, Muhammad},
  booktitle={2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)}, 
  title={Generative AI-assisted Standalone Wearable Reading Glasses for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={173-176},
  abstract={This paper presents a novel, low-cost, wearable Optical Character Recognition (OCR) reading assistant glasses designed for the low-vision and visually impaired community. This standalone system uses a Radxa Zero 3W single board computer and a 5MP RGB camera. PaddleOCR engine, optimized through a series of preprocessing and postprocessing algorithms, used for effective text recognition. A finetuned lightweight 0.5B Large Language Model deployed on-device as a Generative AI chat assistant (Ask-LLM) to respond to the user query with full privacy. To convert the generated response into speech, the Festival Text-to-Speech (TTS) engine has been used. Also, Whisper CT2 Automatic Speech Recognition (ASR) has been utilized for effective and real-time voice transcription. The system recognizes English text with character recognition accuracy of 95.25% and Ask-LLM generates responses at 2~4 tokens/second. The hardware cost of the whole system is only $50 while works completely offline utilizing cutting-edge technologies. This paper contributes to the field of assistive technology by showcasing how carefully selected off-the-shelf components and open-source software can be engineered into a powerful, affordable, and practical solution.},
  keywords={Costs;Accuracy;Optical character recognition;Glass;Assistive technologies;Hardware;Text to speech;Character recognition;Open source software;Engines;Visually Impaired; Smart Glasses; Optical Character Recognition; LLM; Reading Assistant; PaddleOCR},
  doi={10.1109/RAAICON64172.2024.10928631},
  ISSN={},
  month={Nov},}@ARTICLE{9796124,
  author={Zhang, Wenwen and Wang, Jiangong and Wang, Yutong and Wang, Fei-Yue},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={ParaUDA: Invariant Feature Learning With Auxiliary Synthetic Samples for Unsupervised Domain Adaptation}, 
  year={2022},
  volume={23},
  number={11},
  pages={20217-20229},
  abstract={Recognizing and locating objects by algorithms are essential and challenging issues for Intelligent Transportation Systems. However, the increasing demand for much labeled data hinders the further application of deep learning-based object detection. One of the optimal solutions is to train the target model with an existing dataset and then adapt it to new scenes, namely Unsupervised Domain Adaptation (UDA). However, most of existing methods at the pixel level mainly focus on adapting the model from source domain to target domain and ignore the essence of UDA to learn domain-invariant feature learning. Meanwhile, almost all methods at the feature level ignore to make conditional distributions matched for UDA while conducting feature alignment between source and target domain. Considering these problems, this paper proposes the ParaUDA, a novel framework of learning invariant representations for UDA in two aspects: pixel level and feature level. At the pixel level, we adopt CycleGAN to conduct domain transfer and convert the problem of original unsupervised domain adaptation to supervised domain adaptation. At the feature level, we adopt an adversarial adaption model to learn domain-invariant representation by aligning the distributions of domains between different image pairs with same mixture distributions. We evaluate our proposed framework in different scenes, from synthetic scenes to real scenes, from normal weather to challenging weather, and from scenes across cameras. The results of all the above experiments show that ParaUDA is effective and robust for adapting object detection models from source scenes to target scenes.},
  keywords={Adaptation models;Representation learning;Feature extraction;Task analysis;Semantics;Generative adversarial networks;Object detection;Object detection;unsupervised domain adaptation;distribution alignment;domain-invariant representation},
  doi={10.1109/TITS.2022.3176397},
  ISSN={1558-0016},
  month={Nov},}@INPROCEEDINGS{9960025,
  author={Saihood, Ahmed and Karshenas, Hossein and Nilchi, Ahmad Reza Naghsh},
  booktitle={2022 12th International Conference on Computer and Knowledge Engineering (ICCKE)}, 
  title={Spatial-channel attention-based stochastic neighboring embedding pooling and long-short-term memory for lung nodules classification}, 
  year={2022},
  volume={},
  number={},
  pages={477-485},
  abstract={Handling lesion size and location variance in lung nodules are one of the main shortcomings of traditional convolutional neural networks (CNNs). The pooling layer within CNNs reduces the resolution of the feature maps causing small local details loss that needs processing by the following layers. In this article, we proposed a new pooling-based stochastic neighboring embedding method (SNE-pooling) that is able to handle the long-range dependencies property of the lung nodules. Further, an attention-based SNE-pooling model is proposed that could perform spatial and channel attention. The experimental results conducted on LIDC and LUNGx datasets show that the attention-based SNE-pooling model significantly improves the performance for the state of the art.},
  keywords={Knowledge engineering;Visualization;Computational modeling;Stochastic processes;Lung;Generative adversarial networks;Lesions;CNN;SNE-pooling;attention-based pooling;lung nodules},
  doi={10.1109/ICCKE57176.2022.9960025},
  ISSN={2643-279X},
  month={Nov},}@INPROCEEDINGS{11089421,
  author={Singh, C.Edwin and Allimuthu, Udayakumar and Matheswaran, Saravanan and Alekhya, P.Siva Naga and Reshma Farzana, SK and Pavithra, B.},
  booktitle={2025 11th International Conference on Communication and Signal Processing (ICCSP)}, 
  title={On Road Vehicle Breakdown Assistance by Using Machine Learning}, 
  year={2025},
  volume={},
  number={},
  pages={153-158},
  abstract={The On Road Vehicle Breakdown Application (ORVBA) functions as an optimal system to resolve remote automotive breakdowns. Primary end-users of the ORVBA platform will use the system to reach qualified professionals through an application system they can trust. The framework of ORVBA connects solely with certified mechanics who have obtained proper licenses. The ORVBA deals with the Cognitive Radio Networks (CRN), Enhanced Capsule GAN model, and Coupled Hidden Markov Model (CHMM). The continuous system contains specific user records that hold minimal value for other system users. These users remain ignorant about prospective mechanical failures that could manifest far from their regular service locations. Users can obtain access to emergency experts through the On Road Vehicle Breakdown Application (ORVBA) regardless of where they are located or what areas they are situated in. We suggest creating a mobile and web-based support system to fix this. This website lets technicians register their credentials and provide service information. The mobile help app requires two steps: installing the Android app and downloading it. The process automatically collects and stores user location data when connected to a database. The technology leverages user location data to display nearby service providers and stores them on their phone. The list of service providers appears when a user requests service. Even if their mobile devices lose internet connection, people whose automobiles malfunction will have service provider information.},
  keywords={Electric breakdown;Signal processing algorithms;Vehicular ad hoc networks;Hidden Markov models;Transportation;Signal processing;Generative adversarial networks;Automobiles;Optimization;Smart phones;Vehicle Failures;Vehicular ad hoc networks;Lane detection;Electric breakdown;intelligent vehicles;Global navigation satellite system;Evolutionary computation},
  doi={10.1109/ICCSP64183.2025.11089421},
  ISSN={2836-1873},
  month={June},}@ARTICLE{11153861,
  author={Lin, Wei-Zhi and Chen, Jen-Jee and Tseng, Yu-Chee},
  journal={IEEE Transactions on Mobile Computing}, 
  title={The Survey Hole Inpainting Problem: a Machine Learning Approach}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={This work considers the inpainting of missing data in an indoor field, such as geomagnetism and WiFi fingerprints. As opposed to typical image/video inpainting problems, this problem poses several new challenges. First, unlike images with rectangular shapes and fixed RGB channels, indoor geographic data are multi-channeled and highly influenced by building structures. Second, unlike natural objects with fixed shapes, each geographic field is distinct and geographic data are environmentsensitive, following complex physical laws. Consequently, learning from data in other fields is difficult. Third, such data may be obtained from manual surveys and crowdsourcing, which often results in weakly-labeled and noisy datasets. We model our field data as (i) manually surveyed labeled data with holes and (ii) crowdsourced weakly-labeled data without holes. We propose a two-level adversarial regularization inpainting model to conquer these challenges and validate our results with real field data.},
  keywords={Surveys;Fingerprint recognition;Data models;Trajectory;Location awareness;Computer architecture;Generators;Training;Generative adversarial networks;Pedestrians;Adversarial network;crowdsourcing;indoor localization;inpainting;remote sensing},
  doi={10.1109/TMC.2025.3607935},
  ISSN={1558-0660},
  month={},}@ARTICLE{9449880,
  author={Bianco, Michael J. and Gannot, Sharon and Fernandez-Grande, Efren and Gerstoft, Peter},
  journal={IEEE Access}, 
  title={Semi-Supervised Source Localization in Reverberant Environments With Deep Generative Modeling}, 
  year={2021},
  volume={9},
  number={},
  pages={84956-84970},
  abstract={Localization in reverberant environments remains an open challenge. Recently, supervised learning approaches have demonstrated very promising results in addressing reverberation. However, even with large data volumes, the number of labels available for supervised learning in such environments is usually small. We propose to address this issue with a semi-supervised learning (SSL) approach, based on deep generative modeling. Our chosen deep generative model, the variational autoencoder (VAE), is trained to generate the phase of relative transfer functions (RTFs) between microphones. In parallel, a direction of arrival (DOA) classifier network based on RTF-phase is also trained. The joint generative and discriminative model, deemed VAE-SSL, is trained using labeled and unlabeled RTF-phase sequences. In learning to generate and classify the sequences, the VAE-SSL extracts the physical causes of the RTF-phase (i.e., source location) from distracting signal characteristics such as noise and speech activity. This facilitates effective end-to-end operation of the VAE-SSL, which requires minimal preprocessing of RTF-phase. VAE-SSL is compared with two signal processing-based approaches, steered response power with phase transform (SRP-PHAT) and MUltiple SIgnal Classification (MUSIC), as well as fully supervised CNNs. The approaches are compared using data from two real acoustic environments - one of which was recently obtained at Technical University of Denmark specifically for our study. We find that VAE-SSL can outperform the conventional approaches and the CNN in label-limited scenarios. Further, the trained VAE-SSL system can generate new RTF-phase samples which capture the physics of the acoustic environment. Thus, the generative modeling in VAE-SSL provides a means of interpreting the learned representations. To the best of our knowledge, this paper presents the first approach to modeling the physics of acoustic propagation using deep generative modeling.},
  keywords={Acoustics;Location awareness;Direction-of-arrival estimation;Microphones;Data models;Task analysis;Position measurement;Source localization;semi-supervised learning;generative modeling;deep learning},
  doi={10.1109/ACCESS.2021.3087697},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10545003,
  author={Zhang, GuoXin and Tao, YiRan},
  booktitle={2024 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Construction of Artistic Intelligence System based on Multi-View Image Generation Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={1287-1293},
  abstract={This paper explores the development of an artistic intelligence system through the use of a multi-view image generation algorithm, demonstrating its versatility in e-commerce, virtual reality, medical imaging, security surveillance, and autonomous driving. An in-depth study of state-of-the-art algorithms, such as Pix2Vox and conditional generative networks, contributes to the development of 3D structure reassembly and viewpoint synthesis in computer vision and machine learning. The proposed methodology combines multi-view image analysis and a Generative Adversarial Network (GAN)-based image generation algorithm to improve the accuracy and sophistication of image generation. The artistic intelligence system provides computational integration for efficient design in industrial crafts, enabling rapid realization of patterns or 3D models by inputting design requirements and configuring parameters. The system's ability to store and leverage existing data streamlines design processes, increasing operational efficiency and adaptability. It facilitates the classification of stored data, creating a personalized design repository. Experimental validation proves the effectiveness of the system. Expert evaluations on realism, diverse image production metrics, and object recognition accuracy demonstrate the system's ability to generate realistic and diverse artistic content with accurate object identification. The study concludes by highlighting the potential impact of the proposed system on industrial craftsmanship and its role in fostering innovative intersections between art and artificial intelligence},
  keywords={Solid modeling;Machine learning algorithms;Three-dimensional displays;Image synthesis;Surveillance;Virtual reality;Production;Artistic intelligence;intelligence system;multi-view image;image generation algorithm;image processing},
  doi={10.1109/ICICT60155.2024.10545003},
  ISSN={2767-7788},
  month={April},}@ARTICLE{9260150,
  author={Abdollahi, Arnick and Pradhan, Biswajeet and Gite, Shilpa and Alamri, Abdullah},
  journal={IEEE Access}, 
  title={Building Footprint Extraction from High Resolution Aerial Images Using Generative Adversarial Network (GAN) Architecture}, 
  year={2020},
  volume={8},
  number={},
  pages={209517-209527},
  abstract={Building extraction with high accuracy using semantic segmentation from high-resolution remotely sensed imagery has a wide range of applications like urban planning, updating of geospatial database, and disaster management. However, automatic building extraction with non-noisy segmentation map and obtaining accurate boundary information is a big challenge for most of the popular deep learning methods due to the existence of some barriers like cars, vegetation cover and shadow of trees in the high-resolution remote sensing imagery. Thus, we introduce an end-to-end convolutional neural network called Generative Adversarial Network (GAN) in this study to tackle these issues. In the generative model, we utilized SegNet model with Bi-directional Convolutional LSTM (BConvLSTM) to generate the segmentation map from Massachusetts building dataset containing high-resolution aerial imagery. BConvLSTM combines encoded features (containing of more local information) and decoded features (containing of more semantic information) to improve the performance of the model even with the presence of complex backgrounds and barriers. The adversarial training method enforces long-range spatial label vicinity to tackle with the issue of covering building objects with the existing occlusions such as trees, cars and shadows and achieve high-quality building segmentation outcomes under the complex areas. The quantitative results obtained by the proposed technique with an average F1-score of 96.81% show that the suggested approach could achieve better results through detecting and adjusting the difference between the segmentation model output and the reference map compared to other state-of-the-art approaches such as autoencoder method with 91.36%, SegNet+BConvLSTM with 95.96%, FCN-CRFs with 95.36%% SegNet with 94.77%, and GAN-SCA model with 96.36% accuracy.},
  keywords={Training;Image segmentation;Buildings;Semantics;Generative adversarial networks;Feature extraction;Gallium nitride;Building extraction;GAN;remote sensing;SegNet},
  doi={10.1109/ACCESS.2020.3038225},
  ISSN={2169-3536},
  month={},}@ARTICLE{10246252,
  author={Sanjalawe, Yousef K. and Al-E’mari, Salam R.},
  journal={IEEE Access}, 
  title={Abnormal Transactions Detection in the Ethereum Network Using Semi-Supervised Generative Adversarial Networks}, 
  year={2023},
  volume={11},
  number={},
  pages={98516-98531},
  abstract={Numerous abnormal transactions have been exposed as a result of targeted attacks on Ethereum, such as the Ethereum Decentralized Autonomous Organization attack. Exploiting vulnerabilities in smart contracts, malicious users can pursue their own illicit objectives through abnormal transactions. Consequently, identifying these malevolent users, implicated in fraudulent activities and their attribution, becomes exceedingly complex. Cryptocurrency transactions used for malicious purposes, employing pseudo-anonymous accounts to send and receive ransom payments and accumulating funds under various identities, further highlight the need to control and detect these abnormal transactions for maintaining a high level of security within the Ethereum network. Although existing Intrusion Detection Systems (IDSs) help mitigate abnormal transaction occurrences, their performance necessitates improvement. To address this issue, this study presents a novel approach, named Abnormal Transactions Detection Using a Semi-Supervised Generative Adversarial Network (ATD-SGAN), which efficiently detects abnormal attacks within the Ethereum network. ATD-SGAN leverages a semi-supervised generative adversarial network for this purpose. The results demonstrate that ATD-SGAN significantly enhances the performance of state-of-the-art IDSs. It achieves an increase in detection accuracy from 3.78% to 11.05% and reduces the false alarm rate from 42.29% to 0.15%. Moreover, ATD-SGAN notably improves the F1-measure, ranging from 10.39% to 3.79%, compared to the current IDSs.},
  keywords={Blockchains;Feature extraction;Smart contracts;Security;Generative adversarial networks;Decentralized applications;Denial-of-service attack;Network security;Abnormal transactions;ethereum;feature selection;intrusion detection system;network security},
  doi={10.1109/ACCESS.2023.3313630},
  ISSN={2169-3536},
  month={},}@ARTICLE{9614187,
  author={Zhang, Xueqin and Wang, Jiyuan and Zhu, Shinan},
  journal={IEEE Access}, 
  title={Dual Generative Adversarial Networks Based Unknown Encryption Ransomware Attack Detection}, 
  year={2022},
  volume={10},
  number={},
  pages={900-913},
  abstract={Aiming at unknown or variant ransomware attack encrypted with SSL (Secure Sockets Layer)/ TLS (Transport Layer Security) protocol, a detection framework named TGAN-IDS (Transferred Generating Adversarial Network-Intrusion Detection System) based on dual generative adversarial networks is presented in this paper. In this framework, DCGAN (Deep Convolutional Generative Adversarial Network) is adopted to train a generator which has good performance to generate adversarial sample, and is transferred to the generator of TGAN. A pre-training model named PreD is built based on CNN (Convolutional Neural Network), which has good performance to do binary classification, and is transferred to the discriminator of TGAN. The generator and discriminator of TGAN play games in training process until the discriminator has a strong ability to detection unknown attack, and then it is output as an anomaly detector. In order to suppress the deterioration of normal sample detection ability during adversarial training of TGAN, a reconstruction loss function is introduced into the target function of TGAN. Experiments on a mixed dataset which is constructed by CICIDS2017 and other ransomware datasets show comparing with other deep learning network, such as AlexNet, ResNet and DenseNet etc., TGAN-IDS performs well in the indicators of detection accuracy, recall or F1-score etc. Also experiments on KDD99, SWaT and WADI datasets show that TGAN-IDS is suitable for other unencrypted unknown network attack detection.},
  keywords={Ransomware;Generative adversarial networks;Anomaly detection;Training data;Protocols;Generators;Intrusion detection;Transfer learning;Encryption;Ransomware;encrypted traffic;anomaly detection;GAN;transfer learning},
  doi={10.1109/ACCESS.2021.3128024},
  ISSN={2169-3536},
  month={},}@ARTICLE{8883161,
  author={Li, Zhijiang and Zhu, Haonan and Cao, Liqin and Jiao, Lei and Zhong, Yanfei and Ma, Ailong},
  journal={IEEE Access}, 
  title={Face Inpainting via Nested Generative Adversarial Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={155462-155471},
  abstract={Face inpainting aims to repaired damaged images caused by occlusion or cover. In recent years, deep learning based approaches have shown promising results for the challenging task of image inpainting. However, there are still limitation in reconstructing reasonable structures because of over-smoothed and/or blurred results. The distorted structures or blurred textures are inconsistent with surrounding areas and require further post-processing to blend the results. In this paper, we present a novel generative model-based approach, which consisted by nested two Generative Adversarial Networks (GAN), the sub-confrontation GAN in generator and parent-confrontation GAN. The sub-confrontation GAN, which is in the image generator of parent-confrontation GAN, can find the location of missing area and reduce mode collapse as a prior constraint. To avoid generating vague details, a novel residual structure is designed in the sub-confrontation GAN to deliver richer original image information to the deeper layers. The parent-confrontation GAN includes an image generation part and a discrimination part. The discrimination part of parent-confrontation GAN includes global and local discriminator, which benefits the reconstruction of overall coherency of the repaired image while obtaining local details. The experiments are executed over the publicly available dataset CelebA, and the results show that our method outperforms current state-of-the-art techniques quantitatively and qualitatively.},
  keywords={Generative adversarial networks;Generators;Semantics;Face;Image reconstruction;Gallium nitride;Image restoration;Face inpainting;deep neural network;nested GAN},
  doi={10.1109/ACCESS.2019.2949614},
  ISSN={2169-3536},
  month={},}@ARTICLE{10466546,
  author={Li, Xiaoying and He, Shouwu},
  journal={IEEE Access}, 
  title={Blind Image Quality Evaluation Method Based on Cyclic Generative Adversarial Network}, 
  year={2024},
  volume={12},
  number={},
  pages={40555-40568},
  abstract={The mission of blind image quality evaluation is currently a challenging computer vision problem. Due to the shortage of reference images, it is hard for blind image quality evaluation methods to achieve the same performance as full reference image quality evaluation methods. In addition, current quality evaluation methods are difficult to effectively forecast the quality scores of synthesized distorted images as well as real distorted images. To address such issues, this study proposed a cyclic generative adversarial network composed of a quality perception network and a quality regression network on the grounds of generative adversarial networks. For further enhancing the predictive performance of quality aware networks, this study proposed using attention blocks for adaptively fusing high-level semantic features and low-level semantic features. It extracted content and distortion information from images through an image quality evaluation method on the grounds of content perception and distortion inference. And according to the different properties of the extracted features, adaptive fusion blocks were used for adaptively fusing content features and distortion features. Experiments showcased that the Spearman order correlation coefficient and Pearson linear correlation coefficient obtained by the proposed method on multiple datasets were higher than other similar methods. At the same time, the proposed method has achieved good prediction results on various types of distorted images, and has surpassed other methods. The prediction accuracy of the proposed method on five types of distortion was 0.971, 0.963, 0.984, 0.971, and 0.926, respectively. The proposed method achieved the highest predictive accuracy on all distortion types in the LIVE dataset, with predicted accuracy values of 0.973, 0.965, 0.984, 0.963, and 0.944, respectively. In summary, the proposed method not only achieved good prediction accuracy, but also had strong generalization performance in cross dataset testing. This provides a scientific and effective research direction for blind image quality evaluation.},
  keywords={Image quality;Deep learning;Quality assessment;Performance evaluation;Feature extraction;Distortion measurement;Semantics;Generative adversarial networks;Correlation coefficient;Data mining;Computer vision;Blind images;deep learning;attention block;quality score;evaluation},
  doi={10.1109/ACCESS.2024.3375940},
  ISSN={2169-3536},
  month={},}@ARTICLE{10138200,
  author={Hatori, Koki and Takemura, Kenjiro},
  journal={IEEE Access}, 
  title={Conditional Generative Adversarial Network-Based Tactile Stimulus Generation for Ultrasonic Tactile Display}, 
  year={2023},
  volume={11},
  number={},
  pages={53531-53537},
  abstract={Tactile rendering is a promising technology that is necessary to integrate into virtual reality, augmented reality, mixed reality, and even metaverse environments. One of the key technologies for realizing tactile rendering is a reproduction or a display of tactile sensation. This study developed a model that generates an appropriate input signal to an ultrasonic tactile display using a conditional generative adversarial network. Sensory evaluation scores and vibration data acquired by a tactile sensor were used as training data for the conditional generative adversarial network-based models. In this study, different cluster analysis conditions were used to create the input information for the models. Each model generated the input signals for an ultrasonic tactile display, and the accuracy of the models was evaluated through sensory evaluation experiments. The results showed that model accuracy improved with moderate cluster classification and that the reproducibility of tactile sensation created with the models developed in this study was improved when compared with the reproducibility of tactile sensation created without the models.},
  keywords={Haptic interfaces;Machine learning;Generative adversarial networks;Sensors;Augmented reality;Virtual reality;Tactile display;machine learning;GAN;haptics;sensory reproduction},
  doi={10.1109/ACCESS.2023.3280860},
  ISSN={2169-3536},
  month={},}@ARTICLE{9119402,
  author={Ding, Chen and Kang, Wei and Zhu, Jiaqi and Du, Shuangyan},
  journal={IEEE Access}, 
  title={InjectionGAN: Unified Generative Adversarial Networks for Arbitrary Image Attribute Editing}, 
  year={2020},
  volume={8},
  number={},
  pages={117726-117735},
  abstract={Existing image-to-image translation methods usually incorporate encoder-decoder and generative adversarial networks to generate images. The encoder compresses an entire image into a static representation using a sequence of convolution layers until a bottleneck, and then, the intermediate features are decoded to the target image. However, the existence of bottleneck layer in those approaches still has limitations in the sharpness of details, distinct image translation and identity preservation, since different domain translations may be related to the global or local region in the input image. To address these issues, we propose a new model, InjectionGAN, based on a novel generative adversarial network (GAN) for arbitrary attribute transfer. Specifically, conditional on the target domain label, an auto-encoder-like network with multiple linear transformation and refinement connections are trained to translate the input image into the target domain. The connections block better shuttle the low-level information in the encoder to the decoder, which helps to preserve the structural information while modify the appearance slightly at the pixel level through adversarial training. The results on two popular datasets suggest that InjectionGAN achieves a better performance.},
  keywords={Generative adversarial networks;Task analysis;Generators;Training;Gallium nitride;Image reconstruction;Facial features;Facial arrtibute transfer;GAN;image-to-image translation},
  doi={10.1109/ACCESS.2020.3003139},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10650118,
  author={Zeng, Qingyuan and Gong, Yunpeng and Jiang, Min},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Studying adversarial attacks on artificial intelligence (AI) systems helps discover model shortcomings, enabling the construction of a more robust system. Most existing adversarial attack methods only concentrate on single-task single-model or single-task cross-model scenarios, overlooking the multi-task characteristic of artificial intelligence systems. As a result, most of the existing attacks do not pose a practical threat to a comprehensive and collaborative AI system. However, implementing cross-task attacks is highly demanding and challenging due to the difficulty in obtaining the real labels of different tasks for the same picture and harmonizing the loss functions across different tasks. To address this issue, we propose a self-supervised Cross-Task Attack framework (CTA), which utilizes co-attention and anti-attention maps to generate cross-task adversarial perturbation. Specifically, the co-attention map reflects the area to which different visual task models pay attention, while the anti-attention map reflects the area that different visual task models neglect. CTA generates cross-task perturbations by shifting the attention area of samples away from the co-attention map and closer to the anti-attention map. We conduct extensive experiments on multiple vision tasks and the experimental results confirm the effectiveness of the proposed design for adversarial attacks.},
  keywords={Training;Visualization;Perturbation methods;Semantic segmentation;Neural networks;Object detection;Multitasking;adversarial attack;cross-task;attention},
  doi={10.1109/IJCNN60899.2024.10650118},
  ISSN={2161-4407},
  month={June},}
