@ARTICLE{10663973,
  author={Zhang, Xiangrong and Chen, Yifan and Wang, Guanchun and Zhang, Yifang and Jiao, Licheng},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={EDDA:An Efficient Divide-and-Conquer Domain Adapter for Automatics Modulation Recognition}, 
  year={2025},
  volume={19},
  number={1},
  pages={140-153},
  abstract={The development of deep learning technology has injected new vitality into the task of automatic modulation recognition (AMR). Despite achieving promising progress, existing models tend to lose recognition capability in low-quality communication environments due to the neglect of latent distributions within the data, i.e., classifying samples in a single feature space, resulting in unsatisfactory performance. Motivated by this observation, this paper aims to rethink the modulation signals classification from a new perspective on the latent data distribution. To address this, we propose a novel efficient divide-and-conquer domain adapter (EDDA) for AMR tasks, significantly enhancing the existing model's performance in challenging scenarios, irrespective of its architecture. Specifically, we first follow a divide-and-conquer approach to divide the raw data into multiple sub-domain spaces by signal-to-noise ratio (SNR), and then encourage the domain adapter to estimate the latent distributions and learn domain internally-invariant feature projections. Subsequently, we introduce a dynamic strategy for updating domain labels to overcome the limitations of the initial domain label partition by SNR. Finally, we provide theoretical support for EDDA and validate its effectiveness on two widely used benchmark datasets, RadioML2016.10a and RadioML2016.10b. Experimental results show that EDDA achieves average accuracy improvements of 11.63% and 2.32% on the respective datasets. Theoretical and experimental results demonstrate the superiority and versatility of EDDA.},
  keywords={Modulation;Feature extraction;Adaptation models;Convolutional neural networks;Training;Signal to noise ratio;Accuracy;Deep learning;Deep learning;automatic modulation recognition;fine-tuning;sample distribution discrepancies;domain adaptation},
  doi={10.1109/JSTSP.2024.3453559},
  ISSN={1941-0484},
  month={Jan},}@INPROCEEDINGS{9415766,
  author={Cao, Jingyi and Peng, Chenglei and Li, Yang and Du, Sidan},
  booktitle={2021 13th International Conference on Knowledge and Smart Technology (KST)}, 
  title={A Shadow Detection Method for Retaining Key Objects in Complex Scenes}, 
  year={2021},
  volume={},
  number={},
  pages={90-95},
  abstract={The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual large-scale scenes, key objects covered by shadows are often regarded as shadows, which may harm computer vision tasks. In the paper, we are the first to propose the Object-aware Shadow Detection Network (OSD-Net) model for computer vision tasks in complex scenes. It introduces the direction-aware spatial context (DSC) module to detect shadows, uses semantic segmentation with Mask RCNN to extract key objects in the picture, and designs a function to perform mask fusion. Qualitative experiments have been performed to test OSD-Net on three public datasets commonly used in computer vision. Compared with popular shadow detection methods, OSD-Net is able to effectively protect the key targets in the picture from being misjudged as shadows, and ensure shadow detection accuracy.},
  keywords={Computer vision;Computational modeling;Semantics;Task analysis;Standards;shadow detection;OSD-Net;Mask RCNN;DSC;computer vision},
  doi={10.1109/KST51265.2021.9415766},
  ISSN={2374-314X},
  month={Jan},}@ARTICLE{10236448,
  author={Zhang, Xiaobo and Wang, Tao and Zhao, Xiaole and Wen, Dengmin and Zhai, Donghai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multitask-Guided Deep Clustering With Boundary Adaptation}, 
  year={2024},
  volume={35},
  number={5},
  pages={6089-6102},
  abstract={Multitask learning uses external knowledge to improve internal clustering and single-task learning. Existing multitask learning algorithms mostly use shallow-level correlation to aid judgment, and the boundary factors on high-dimensional datasets often lead algorithms to poor performance. The initial parameters of these algorithms cause the border samples to fall into a local optimal solution. In this study, a multitask-guided deep clustering (DC) with boundary adaptation (MTDC-BA) based on a convolutional neural network autoencoder (CNN-AE) is proposed. In the first stage, dubbed multitask pretraining (M-train), we construct an autoencoder (AE) named CNN-AE using the DenseNet-like structure, which performs deep feature extraction and stores captured multitask knowledge into model parameters. In the second phase, the parameters of the M-train are shared for CNN-AE, and clustering results are obtained by deep features, which is termed as single-task fitting (S-fit). To eliminate the boundary effect, we use data augmentation and improved self-paced learning to construct the boundary adaptation. We integrate boundary adaptors into the M-train and S-fit stages appropriately. The interpretability of MTDC-BA is accomplished by data transformation. The model relies on the principle that features become important as the reconfiguration loss decreases. Experiments on a series of typical datasets confirm the performance of the proposed MTDC-BA. Compared with other traditional clustering methods, including single-task DC algorithms and the latest multitask clustering algorithms, our MTDC-BA achieves better clustering performance with higher computational efficiency. Deep features clustering results demonstrate the stability of MTDC-BA by visualization and convergence verification. Through the visualization experiment, we explain and analyze the whole model data input and the middle characteristic layer. Further understanding of the principle of MTDC-BA. Through additional experiments, we know that the proposed MTDC-BA is efficient in the use of multitask knowledge. Finally, we carry out sensitivity experiments on the hyper-parameters to verify their optimal performance.},
  keywords={Task analysis;Feature extraction;Data models;Clustering algorithms;Correlation;Convolutional neural networks;Data mining;Boundary adaptation;clustering;deep learning;explainable method;multitask learning},
  doi={10.1109/TNNLS.2023.3307126},
  ISSN={2162-2388},
  month={May},}@ARTICLE{9186931,
  author={Shi, Yaxin and Pan, Yuangang and Xu, Donna and Tsang, Ivor W.},
  journal={Neural Computation}, 
  title={Multiview Alignment and Generation in CCA via Consistent Latent Encoding}, 
  year={2020},
  volume={32},
  number={10},
  pages={1936-1979},
  abstract={Multiview alignment, achieving one-to-one correspondence of multiview inputs, is critical in many real-world multiview applications, especially for cross-view data analysis problems. An increasing amount of work has studied this alignment problem with canonical correlation analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this letter studies multiview alignment from a Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multiview inputs by matching the marginalization of the joint distribution of multiview random variables under different forms of factorization. To realize our design, we present adversarial CCA (ACCA), which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis, based on conditional mutual information, reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.},
  keywords={},
  doi={10.1162/neco_a_01309},
  ISSN={0899-7667},
  month={Oct},}@INPROCEEDINGS{10822040,
  author={Wang, Chunshi and Teng, Shougan and Sun, Shaohua and Zhao, Bin},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={SymMatch: Symmetric Bi-Scale Matching with Self-Knowledge Distillation in Semi-Supervised Medical Image Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={2562-2569},
  abstract={With the development of medical image segmentation technology, high-quality automatic segmentation methods, particularly within semi-supervised learning frameworks, have become a research hotspot. This study introduces a new semi-supervised medical image segmentation algorithm called SymMatch. The algorithm effectively leverages limited labeled data along with a large amount of unlabeled data through a symmetrical network structure and knowledge distillation techniques. SymMatch applies a spectrum of perturbations, from weak to strong, at both image and feature levels, effectively leveraging the potential of unlabeled data. Additionally, by incorporating a bi-scale distillation loss, the model’s robustness and accuracy in handling complex medical imaging data are further enhanced. Experimental results show that SymMatch demonstrates superior performance across multiple recognized medical imaging datasets (such as ACDC, LA and PanNuke). Notably, even with very limited labeled data, it maintains high segmentation accuracy. These achievements not only advance the development of semi-supervised medical image segmentation technology but also provide new ideas and methods for future research in related technologies. Code is available at https://github.com/AiEson/SymMatch.},
  keywords={Knowledge engineering;Image segmentation;Accuracy;Image recognition;Perturbation methods;Semisupervised learning;Robustness;Labeling;Positron emission tomography;Biomedical imaging;Semi-Supervised Learning;Self-Knowledge Distillation;Medical Image Segmentation},
  doi={10.1109/BIBM62325.2024.10822040},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10545853,
  author={Li, Yanda and Guo, Zhaobo and Tan, Yuanjun and Ji, Gaoyang and Zhou, Dongbo},
  booktitle={2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)}, 
  title={An Improved Self-Supervised Learning Method for Controllable Content Generation in Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1502-1507},
  abstract={The widespread application of pretrained large language models for content generation has focused on overcoming the illusion of control in text generation. While fine-grained control at the word and phrase level allows for partially controlled text generation, the probability of producing content that aligns with human natural language patterns remains low. This article proposes an enhanced self-supervised learning approach to improve the quality of controllable text generation for this issue. Building upon the CoCon model, our approach ensures controllability in text generation at both the sentence and word granularity. It amplifies features in a high-dimensional space, filtering out irrelevant feature information to aid the model in generating more precise content. Experimental results on publicly available datasets demonstrate that our approach effectively reduces Perplexity, enhances model performance, and produces text that better conforms to natural language conventions.},
  keywords={Measurement;Training;Solid modeling;Filtering;Computational modeling;Natural languages;Training data;controllable text generation;pretrained large language model;self-supervised learning;Perplexity},
  doi={10.1109/ICCECT60629.2024.10545853},
  ISSN={},
  month={April},}@INPROCEEDINGS{10219694,
  author={Yang, Guoxing and Fu, Feifei and Fei, Nanyi and Wu, Haoran and Ma, Ruitao and Lu, Zhiwu},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={DiST-GAN: Distillation-based Semantic Transfer for Text-Guided Face Generation}, 
  year={2023},
  volume={},
  number={},
  pages={840-845},
  abstract={Recently, large-scale pre-training has achieved great success in multi-modal tasks and shown powerful generalization ability due to superior semantic comprehension. In the field of text-to-image synthesis, recent works induce large-scale pre-training with VQ-VAE as a discrete visual tokenizer, which can synthesize realistic images from arbitrary text inputs. However, the quality of images generated by these methods is still inferior to that of images generated by GAN-based methods, especially in some specific domains. To leverage both the superior semantic comprehension of large-scale pre-training models and the powerful ability of GAN-based models in photorealistic image generation, we propose a novel knowledge distillation framework termed DiST-GAN to transfer the semantic knowledge of large-scale visual-language pre-training models (e.g., CLIP) to GAN-based generator for text-guided face image generation. Our DiST-GAN consists of two key components: (1) A new CLIP-based adaptive contrastive loss is devised to ensure the generated images are consistent with the input texts. (2) A language-to-vision (L2V) transformation module is learned to transform token embeddings of each text into an intermediate embedding that is aligned with the image embedding extracted by CLIP. With these two novel components, the semantic knowledge contained in CLIP can thus be transferred to GAN-based generator which preserves the superior ability of photorealistic image generation in the mean time. Extensive results on the Multi-Modal CelebA-HQ dataset show that our DiST-GAN achieves significant improvements over the state-of-the-arts.},
  keywords={Measurement;Adaptation models;Visualization;Image synthesis;Impedance matching;Semantics;Transforms;Text-to-image generation;Knowledge distillation;Large-scale pre-training},
  doi={10.1109/ICME55011.2023.00149},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{9428276,
  author={Bai, Sikai and Gao, Junyu and Wang, Qi and Li, Xuelong},
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Multi-Domain Synchronous Refinement Network for Unsupervised Cross-Domain Person Re-Identification}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Unsupervised cross-domain person re-identification (re-ID) is a challenging task, because it is an open-set problem with completely unknown person identities in the target domain. Existing methods attempt to tackle the challenge by transferring image style across domains or generating pseudo labels in the target domain, whereas the valuable information in multiple domains (i.e., source domain, style-transferred data, and target domain) is not taken fully into consideration. To this end, we propose a novel multi-domain synchronous refinement (MDSR) network, where valuable knowledge from multiple domains is sufficiently exploited and refined to enforce the discriminative ability of the model. MDSR network contains two complementary modules dedicated to source-to-target domain adaptation and style-transferred data to the target domain adaptation, respectively. The domain adaptive knowledge from two modules is aggregated in the final stage. Extensive experiments verify our method achieves significant improvements over the state-of-the-art approaches on multiple unsupervised domain adaptative person re-ID tasks.},
  keywords={Knowledge engineering;Adaptation models;Conferences;Hybrid learning;Task analysis;Person re-identification;unsupervised domain adaptation;synchronous refinement},
  doi={10.1109/ICME51207.2021.9428276},
  ISSN={1945-788X},
  month={July},}@ARTICLE{10540622,
  author={Wang, Cong and Sun, Guodong and Wang, Cailing and Gao, Zixuan and Wang, Hongwei},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Monte Carlo-Based Restoration of Images Degraded by Atmospheric Turbulence}, 
  year={2024},
  volume={54},
  number={11},
  pages={6610-6620},
  abstract={Atmospheric turbulence can often introduce phase errors into a propagating light field, thus resulting in anisoplanatic and temporally varying blur and distortion of images. Restoring such images degraded by atmospheric turbulence is extremely ill-posed, due to multiple plausible solutions for a given input image. Most methods offer a deterministic estimation of clean images and require high-computational costs. To address these challenges, this article proposes a fast turbulence mitigation network (FTMNet). It is a lightweight model for atmospheric turbulence mitigation. Differing other methods, it does not employ a strategy for producing a single deterministic reconstruction. Instead, it leverages the Monte Carlo method to enhance restoration performance and produces a different and reasonable set of reconstructed images for a given input. As a result, FTMNet effectively mitigates atmospheric turbulence effect while maintaining low-inference time and computational resource requirements. Experimental results demonstrate that FTMNet shows high-inference speed, reaching 90 fps, and outperforms the state-of-the-art peers.},
  keywords={Atmospheric modeling;Computational modeling;Convolution;Distortion;Monte Carlo methods;Imaging;Image restoration;Atmospheric turbulence;deep learning;image restoration;Monte Carlo method},
  doi={10.1109/TSMC.2024.3399464},
  ISSN={2168-2232},
  month={Nov},}@ARTICLE{10937323,
  author={Cui, Xiaonan and Hu, Dinghan and Lai, Xiaoping and Jiang, Tiejia and Gao, Feng and Cao, Jiuwen},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Seizure Detection Framework via Multisubject Dynamic Adaptation and Structural Clustering}, 
  year={2025},
  volume={74},
  number={},
  pages={1-12},
  abstract={Intersubject variation seriously affects the generalization ability of seizure detection models. Most current models need to be calibrated and trained with annotated data before application, making them strongly dependent on subject-specific features and difficult to directly generalize on new subjects. To overcome this limitation, we propose a multisubject dynamic adaptation and structural clustering (SCMDA) framework to perform offline seizure detection tasks. First, the backbone network is designed as a combination of the temporal encoder and multiple dynamic attention transfer (DAT) modules, where DAT is a parallel structure of squeeze-and-excitation (SE) residual and dynamic residual transfer (DRT). The designed DAT module can enhance the discriminability of the latent space and blur the distribution boundaries between source subjects to reduce the negative impact of domain information on distribution alignment. Then, the model is optimized by jointly discriminative feature alignment of the latent space and structurally regularized clustering of the target domain. The cluster centroids are generated by learning the self-attention feature interaction of the target data in a feedforward manner. Finally, to evaluate the effectiveness of SCMDA, we conduct extensive tests on the public available TUH dataset and the Children’s Hospital, Zhejiang University School of Medicine (CHZU) dataset. The proposed method achieves 93.42% and 91.23% cross-subject classification accuracy on the TUH and CHZU datasets, outperforming the current state-of-the-art offline algorithms.},
  keywords={Electroencephalography;Feature extraction;Convolution;Brain modeling;Training;Kernel;Epilepsy;Adaptation models;Optimization;Time-frequency analysis;Multisubject dynamic transfer;seizure detection;structurally regularized clustering;unsupervised domain adaptation},
  doi={10.1109/TIM.2025.3551437},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{9533625,
  author={Zhang, Yinghui and Liu, Chun and Sun, Bo and He, Jun and Yu, Lejun},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={NIR-VIS Heterogeneous Face Synthesis via Enhanced Asymmetric CycleGAN}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Heterogeneous face image involves many fields, among which the near infrared (NIR) and visible (VIS) face image recognition is a hot field. NIR image has excellent imaging effect under extreme illumination conditions. Although few NIR images are available, unpaired image-to-image translation provides a solution for converting NIR-VIS to each other. However, NIR image contains relatively poor information, while VIS image contains relatively rich information, which results in asymmetry between NIR and VIS, and not suitable for unpaired translation based on two domains of the same complexity. In this paper, an enhanced asymmetric CycleGAN(EN-ASGAN) with edge retention module, auxiliary encoder module and generators equipped with different sizes is used to convert NIR-VIS face images. To validate the effective of EN-ASGAN, we conduct experiments on CASIA NIR-VIS 2.0 dataset. The experimental results are evaluated qualitatively and quantitatively and show that EN-ASGAN is effective for heterogeneous face generation in unpaired NIR-VIS images.},
  keywords={Image quality;Image resolution;Face recognition;Image edge detection;Neural networks;Lighting;Imaging;infrared face image;visible face image;Cycle-GAN;image translation;heterogeneous face recognition},
  doi={10.1109/IJCNN52387.2021.9533625},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10924851,
  author={Wang, Jianing and Zhang, Wan and Liu, Yichen and Hua, Zheng and Yao, Yuqiong and Hao, Shengjia and Zhao, Zhicao and Li, Lin},
  booktitle={2024 IEEE Smart World Congress (SWC)}, 
  title={Urban Area Detection Based on Cross-Domain Lightweight Parallel Fusion Network for Orbita “Zhuhai-1” Hyperspectral Satellite}, 
  year={2024},
  volume={},
  number={},
  pages={1836-1843},
  abstract={In order to ensure the stabilization issues of efficient deployment in orbit and cross-domain real-time detection. In this paper, a lightweight domain adaptation parallel fusion network (LAPNet) for urban detection of Orbita “Zhuhai-1” (OHS) remote sensing data has been proposed. Point convolution and dynamic convolution are employed to fully exploit spatial-spectral fusion features from local-global and pixel-semantic perspective. At the same time, cross-scene domain alignment from the public dataset to the target dataset is achieved by the adversarial domain adaptation strategy. Based on the hyperspectral remote sensing data from the “Zhuhai-1” satellite, we created three urban detection datasets for the experiment, where the public dataset and the dataset generated by hyperspectral reconstruction algorithm as the pre-training datasets for urban area detection. To verify the effectiveness of LAPNet for domain adaptation, we evaluate the LAPNet for the urban area detection task on the target domain “Zhuhai- 1 “ hyperspectral dataset. Compared to related comparative algorithms, the LAPNet improves the urban detection performance and efficiency for hyperspectral remote sensing satellite on each dataset with effectively reducing the number of model parameters.},
  keywords={Adaptation models;Satellites;Convolution;Urban areas;Reconstruction algorithms;Feature extraction;Orbits;Real-time systems;Convolutional neural networks;Hyperspectral imaging;Urban area detection;Hyperspectral;”Zhuhai-1”;satellite;Lightweight;Convolutional neural networks;Domain adaptation learning},
  doi={10.1109/SWC62898.2024.00282},
  ISSN={2993-396X},
  month={Dec},}@INPROCEEDINGS{10800140,
  author={Huang, Wen and Han, Bing and Chen, Zhengyang and Wang, Shuai and Qian, Yanmin},
  booktitle={2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP)}, 
  title={Prototype and Instance Contrastive Learning for Unsupervised Domain Adaptation in Speaker Verification}, 
  year={2024},
  volume={},
  number={},
  pages={383-387},
  abstract={Speaker verification system trained on one domain usually suffers performance degradation when applied to another domain. To address this challenge, researchers commonly use feature distribution matching-based methods in unsupervised domain adaptation scenarios where some unlabeled target domain data is available. However, these methods often have limited performance improvement and lack generalization in various mismatch situations. In this paper, we propose Prototype and Instance Contrastive Learning (PICL), a novel method for unsupervised domain adaptation in speaker verification through dual-level contrastive learning. For prototype contrastive learning, we generate pseudo labels via clustering to create dynamically updated prototype representations, aligning instances with their corresponding class or cluster prototypes. For instance contrastive learning, we minimize the distance between different views or augmentations of the same instance, ensuring robust and invariant representations resilient to variations like noise. This dual-level approach provides both high-level and low-level supervision, leading to improved generalization and robustness of the speaker verification model. Unlike previous studies that only evaluated mismatches in one situation, we have conducted relevant explorations on various datasets and achieved state-of-the-art performance currently, which also proves the generalization of our method.},
  keywords={Degradation;Adaptation models;Noise;Prototypes;Contrastive learning;Robustness;unsupervised domain adaptation;speaker verification;contrastive learning;mismatch},
  doi={10.1109/ISCSLP63861.2024.10800140},
  ISSN={},
  month={Nov},}@ARTICLE{11016953,
  author={Yang, Binbin and Han, Jianhong and Hou, Xinghai and Zhou, Dehao and Liu, Wenkai and Bi, Fukun},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={FSDA-DETR: Few-Shot Domain-Adaptive Object Detection Transformer in Remote Sensing Imagery}, 
  year={2025},
  volume={63},
  number={},
  pages={1-16},
  abstract={Few-shot domain-adaptive object detection (FSDAOD) aims to transfer knowledge from a source domain to a target domain with limited labeled data, which faces severe challenges in the field of remote sensing. To address this issue, numerous convolutional neural network (CNN)-based domain adaptation methods employ style transfer and feature alignment to mitigate domain shifts, but limited target domain samples are prone to yielding equivocal optimization and are maladaptive. Furthermore, the sparsity of targets and the complexity of backgrounds in remote sensing imagery contribute to confusing feature alignment. Moreover, detection transformer (DETR)-based detectors have achieved remarkable progress in unsupervised domain adaptation (UDA) but remain unexplored in FSDAOD. To address these challenges, we introduce FSDA-DETR, the first DETR-based strong baseline designed for the FSDAOD of remote sensing imagery. Specifically, we propose a cross-domain style rectification (CSR) module that rectifies the styles of the target domain to align with the source domain by storing and dynamically updating the weighted-fusion source-domain style bases. To further strengthen the detector’s cross-domain detection performance, we propose a category-aware feature alignment (CFA) module that performs fine-grained masking on object regions of rectified backbone features corresponding to different categories and utilizes adversarial training for domain-invariant feature extraction. Extensive experiments on three cross-domain benchmarks, comprising six diverse datasets, demonstrate that FSDA-DETR outperforms state-of-the-art methods. For instance, in the optical-to-synthetic aperture radar (SAR) benchmark, FSDA-DETR achieves 74.5% mean average precision (mAP) with only 1% of target-domain training data. The code and datasets are available at https://github.com/wsybb252237/FSDA-DETR},
  keywords={Remote sensing;Feature extraction;Object detection;Detectors;Training;Adaptation models;Benchmark testing;Visualization;Training data;Prototypes;Domain adaptive;few-shot;object detection;remote sensing imagery},
  doi={10.1109/TGRS.2025.3574245},
  ISSN={1558-0644},
  month={},}@ARTICLE{11078301,
  author={Hua, Dingli and Chen, Qingmao and Wu, Zhiliang and Zuo, Yifan and Wen, Wenying and Fang, Yuming},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Perceptual Transform Fusion of Infrared and Visible Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Infrared and visible image fusion aims to generate fused images with rich textures and clear target representations. Existing methods generally assume high-quality input images, thus overlooking issues such as reduced contrast and loss of details in visible images under low-light conditions. The naive enhance-then-fuse strategy cannot perform fuse-oriented image enhancement, which always reaches a sub-optimal result. To address this challenge, we propose a perceptual transform fusion of infrared and visible images, which simultaneously optimizes low-light enhancement and image fusion. Specifically, to improve computational efficiency and optimize key feature representations while suppressing noise interactions caused by lighting variations, we introduce a lightweight adaptive sparse Transformer block (ASTBlock). This model adaptively integrates sparse and dense attention mechanisms to enhance feature representations and employs a feed-forward network to eliminate redundant information, thereby ensuring the quality of image fusion. Subsequently, to retain significant details while reducing the impact of noise introduced by low-light enhancement, we incorporate discrete wavelet transform (DWT) for feature decomposition and fusion, further enhancing the representation capability and feature preservation of fused images. Meanwhile, to tackle the issues of insufficient contrast and hidden details in low-light conditions, we design an illumination perception module and an illumination consistency loss to improve the contrast and clarity of fused images. Experimental results on multiple public benchmark datasets for quality assessment and downstream tasks, e.g., pedestrian detection, demonstrate that our method significantly outperforms the state-of-the-art (SOTA) methods. The code is available at https://github.com/hinmouc/PIVFusion.},
  keywords={Image fusion;Noise;Lighting;Transformers;Discrete wavelet transforms;Feature extraction;Marine vehicles;Adaptation models;Visualization;Fuses;Infrared-visible fusion;adaptive Transformer;discrete wavelet transform;illumination perception},
  doi={10.1109/TCSVT.2025.3588203},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{11095434,
  author={Bhushan, R Hema and K, Amritha G and Ambiga, Pranav and Malgikar, Sathvik S and V.R., Badri Prasad},
  booktitle={2025 17th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, 
  title={Using Instruction-Following LLM Hidden States as Conditioning for Video Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Video generation has applications in several fields. With the advent of Generative AI, we see extensive research being conducted on video generation using AI. Through this project, we experiment the usage of LLM Hidden states as conditioning to train a Video Latent Diffusion Model to study their ability of passing richer semantic information about the video samples. We performed a comparative study of context retention abilities of LLMs in case of embeddings and hidden states separately. We create a pipeline with three major components - the LLM, a custom Bridge Network and the Diffusion UNet. We conduct our study using two different datasets - the Captioned Moving MNIST and a subset of the Sakuga-42M dataset. We conclude by evaluating our model variants on standard benchmarks and metrics, and state our findings, which could serve as ground for future work.},
  keywords={Visualization;PSNR;Large language models;Semantics;Refining;Pipelines;Diffusion models;Prompt engineering;Standards;Videos;Video Generation;Generative AI;Artificial Intelligence;Large Language Model;Multimodal;Prompt Engineering;Diffusion;Variational Autoencoder;Hidden States;FVD;CLIP Score;UNet;Latent},
  doi={10.1109/ECAI65401.2025.11095434},
  ISSN={2688-0253},
  month={June},}@ARTICLE{11018135,
  author={Wang, Jia and Li, Peipei and Xiang, Liuyu and Wang, Rui and Zhang, Zhili and Tian, Qing and He, Zhaofeng},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={FDNet: A Frequency-Aware Decomposition Network for Robust Face Super-Resolution Against Adversarial Attacks}, 
  year={2025},
  volume={20},
  number={},
  pages={6175-6189},
  abstract={Face super-resolution (FSR) is a crucial step in the face analysis pipeline, achieving remarkable progress by applying deep neural networks (DNNs). However, DNN-based FSR models are not robust enough and may suffer significant performance degradation due to subtle adversarial perturbations. In addition, the high-frequency details of images restored by existing models are insufficient, especially at large upsampling factors. In this paper, we propose a frequency-aware decomposition network (FDNet) for robust face super-resolution, which aims to defend against adversarial attacks and obtain face images with fidelity. Observing that the noise introduced by adversarial attacks is often intricately mixed with the high-frequency information of the input image, we decompose and process the features of different frequencies separately to eliminate harmful perturbations and enhance high-frequency information. Specifically, by leveraging the frequency-aware capability of empirical mode decomposition (EMD), we propose an EMD-based multi-branch structure. The framework implicitly compels different branches to adaptively extract features from distinct frequency bands, limiting the adversarial noise into decoupled components restricted to specific branches. It also improves the recovery of high-frequency information, which is conducive to producing more credible results. Furthermore, we introduce a high-frequency noise suppressor capable of randomly eliminating imperceptible noise in the high-frequency components. Quantitative and qualitative results demonstrate the superior robustness of our proposed method against adversarial attacks, showing better fidelity in image reconstruction compared to state-of-the-art FSR methods, especially for upscaling factors of 8 and 16.},
  keywords={Noise;Perturbation methods;Superresolution;Faces;Face recognition;Robustness;Feature extraction;Fast Fourier transforms;Frequency-domain analysis;Training;Face super-resolution;adversarial attack;empirical mode decomposition;frequency-aware decomposition;multi-branch structure},
  doi={10.1109/TIFS.2025.3574956},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10665642,
  author={Yu, Junru and Xu, Risheng and Wang, Xiao and Mu, Chaoxu},
  booktitle={2024 14th Asian Control Conference (ASCC)}, 
  title={TAIL-DRL:Adaptive Lane Change Decision Making Of Autonomous Vehicles In Mixed Traffic}, 
  year={2024},
  volume={},
  number={},
  pages={381-386},
  abstract={Adaptive decision making is critical for safety and comfort of autonomous vehicles (AVs) driving in mixed traffic situations. However, existed decision-making algorithms focus on learning directly from expert behaviors while ignore the temporal efficiency and adaptiveness to environment changes, decreasing the flexibility of AVs. Therefore, this paper proposed a deep reinforcement learning (DRL) framework with temporal attention mechanism. First, we incorporate imitation strategies to learn from expert demonstrations to overcome the initial strategy search inefficiency in DRL, thus capture the implicit decision-making mechanisms that hidden in expert behavior data. Second, a temporal attention mechanism is embeded into the DRL framework, while utilizes an extended sequence of historical observations to analyze the temporary driving styles of other traffic participants, further facilitating smoother lane change operations of AVs. Last but not the least, we model high-level lane change decisions as composition of discrete sub-operations of acceleration and angular velocity of steering. This strategy enhances the control precision of AVs, enabling rapid and secure lane-change actions. Thousands of experiments were conducted in the CARLA simulator and the results show that our algorithm significantly outperform the state-of-the-art DRL algorithms and behavioral planners in multiple lane change scenarios.},
  keywords={Adaptation models;Attention mechanisms;Roads;Decision making;Aerospace electronics;Deep reinforcement learning;Angular velocity;autonomous vehicle;lane change decision-making;deep reinforcement learning;temporal attention mechanism},
  doi={},
  ISSN={2770-8373},
  month={July},}@INPROCEEDINGS{10607615,
  author={Sani, Siva RamaKrishna and Manvitha Reddy Mallireddy, Sai and Reddy Renati, Naveen Kumar and Leela Sai Surya, Peela},
  booktitle={2024 4th International Conference on Pervasive Computing and Social Networking (ICPCSN)}, 
  title={Style Synthesis: AI-Powered Dress Try-On Experience}, 
  year={2024},
  volume={},
  number={},
  pages={73-78},
  abstract={In the world of internet retail and fashion, virtual try-on systems have become a game changing innovation. This research introduces a revolutionary deep learning-based method for virtual try-on. The method first segments the clothing from the source image using segmentation techniques, then creates an interactive image to provide a virtual fitting experience, and finally combines accurate posture recognition using pretrained models with the imposition of the garment texture on the dense pose it predicted. Realtime experimenting with different clothing items, sizes, and positions is possible because to the interactive user interface. The technology is more user friendly because it can accommodate a variety of body types. This virtual try-on system, powered by deep learning, has the potential to significantly change customer involvement across a variety of industries, especially in fashion and ecommerce, by transforming the fashion retail scene.},
  keywords={Industries;Visualization;Technological innovation;Accuracy;Social networking (online);Clothing;User interfaces;Virtual try-on systems;Deep learning-based method;Virtual fitting;Posture Recognition;Garment texture;Dense pose;Segmentation},
  doi={10.1109/ICPCSN62568.2024.00020},
  ISSN={},
  month={May},}@INPROCEEDINGS{10645420,
  author={Mou, Luntian and Sun, Yihan and Tian, Yunhan and He, Ruichen and Gao, Feng and Li, Zijin and Jain, Ramesh},
  booktitle={2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={MemoMusic 4.0: Personalized Emotion Music Generation Conditioned by Valence and Arousal as Virtual Tokens}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={MemoMusic aims at recommending or generating emotion music to meet the emotional requirements of individual listeners and navigate them to more positive emotional states. For MemoMusic 4.0, valence and arousal are applied as virtual tokens for emotion conditioned music generation. And predicting various music features is approached as multi-task learning. Additionally, adaptive weights are assigned to the loss of relevant features based on emotional conditions. Experimental results have validated the emotional regulation ability of MemoMusic 4.0 and the effectiveness of valence and arousal as virtual tokens for emotion music generation.},
  keywords={Training;Adaptation models;Correlation;Navigation;Conferences;Process control;Music;MemoMusic 4.0;personalized emotion music generation;valence;arousal;virtual token},
  doi={10.1109/ICMEW63481.2024.10645420},
  ISSN={2995-1429},
  month={July},}@INPROCEEDINGS{10872427,
  author={HuXiaoHui and Fang, Fang and LiaoZhangJuan and HuangLinTao and HeChuang and HuangWenLi},
  booktitle={2024 2nd International Conference on Computer, Vision and Intelligent Technology (ICCVIT)}, 
  title={Transmission-Lines Foreign Objects Detection Using MSYOLO}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Foreign objects detection on transmission lines has the problems that the accuracy is low due to the images captured by UAVs often have small targets, and most models can hardly meet the requirements of real-time detection. Thispaper proposes a novel multiscale-adaptive YOLO-based network model with SIoU loss (MSYOLO) for precise detection of foreign objects on transmission lines. First, an enhanced scale-aware pyramid network which combined dual focus feature aggregation network and a unified feature enhancement module was proposed to improve the detection rate of small targets. Second, SIoU and Fast NMS was introduced to further correct and accelerate the bounding box regression. Experimental results show that our proposed method reaches 86.65 AP50, 46.22%AR and 149 FPS, which is superior to the other anchor-based methods of YOLO series.},
  keywords={YOLO;Computer vision;Power transmission lines;Accuracy;Feature extraction;Propagation losses;Real-time systems;Foreign objects detection on transmission lines;enhanced scale-aware pyramid network(ESPN);SIoU;Fast NMS},
  doi={10.1109/ICCVIT63928.2024.10872427},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9297814,
  author={Lin, Chin-Teng and Huang, Kuan-Chih and Pal, Nikhil R. and Cao, Zehong and Liu, Yu-Ting and Fang, Chieh-Ning and Hsieh, Tsung-Yu and Lin, Yang-Yin and Wu, Shang-Lin},
  booktitle={2020 International Conference on Fuzzy Theory and Its Applications (iFUZZY)}, 
  title={Adaptive Subspace Sampling for Class Imbalance Processing-Some clarifications, algorithm, and further investigation including applications to Brain Computer Interface}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Kohonen's Adaptive Subspace Self-Organizing Map (ASSOM) learns several subspaces of the data where each subspace represents some invariant characteristics of the data. To deal with the imbalance classification problem, earlier we have proposed a method for oversampling the minority class using Kohonen's ASSOM. This investigation extends that study, clarifies some issues related to our earlier work, provides the algorithm for generation of the oversamples, applies the method on several benchmark data sets, and makes an application to a Brain Computer Interface (BCI) problem. First we compare the performance of our method using some benchmark data sets with several state-of-the-art methods. Finally, we apply the ASSOM-based technique to analyze a BCI based application using electroencephalogram (EEG) datasets. Our results demonstrate the effectiveness of the ASSOM-based method in dealing with imbalance classification problem.},
  keywords={Classification algorithms;Kernel;Self-organizing feature maps;Brain-computer interfaces;Prototypes;Mathematical model;Training data;Imbalanced Learning;Oversampling;Synthetic Sample Generation;Subspace;EEG;Classification},
  doi={10.1109/iFUZZY50310.2020.9297814},
  ISSN={2377-5831},
  month={Nov},}@ARTICLE{10758762,
  author={Nie, Jianhui and Wang, Yaning and Shen, Zhiwei and Lu, Xiaohui},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={DeepSharpen: An Unsupervised Sharp Feature Recovery Method Based on Quadric Projection}, 
  year={2025},
  volume={74},
  number={},
  pages={1-11},
  abstract={Sharp features are integral components of point cloud data, containing rich surface information that is often compromised due to ambiguous light reflection during optical measurements, such as structured light and RGB cameras. This article aims to address the critical issue of recovering distorted sharp features, which is essential for enhancing the accuracy of measurements from such devices. To achieve this, we propose DeepSharpen, an unsupervised method that first performs feature region point cloud segmentation. Next, we fit the segmented point cloud to quadratic surfaces and infer the intersection lines as the true positions of the features. Finally, we refine additional points within the feature regions through loss functions based on shape and adjacency consistency to accomplish feature sharpening. Our experimental results demonstrate that DeepSharpen not only accurately recovers the damaged sharp features but also exhibits strong generalization capability and noise robustness.},
  keywords={Point cloud compression;Vectors;Surface treatment;Accuracy;Surface fitting;Feature extraction;Fitting;Estimation;Loss measurement;Image edge detection;Deep learning;point cloud;quadric;sharp recovery;unsupervised},
  doi={10.1109/TIM.2024.3502838},
  ISSN={1557-9662},
  month={},}@ARTICLE{11175505,
  author={Lu, Yuwu and Hu, Xue and Huang, Haoyu and Lai, Zhihui and Li, Xuelong},
  journal={IEEE Transactions on Multimedia}, 
  title={Semantic Dual-Adversarial Network for Blended-Target Domain Adaptation}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={The popularity of blended-target domain adaptation (BTDA) is growing since target data in the real world often come from multiple domains with different data distributions. Most BTDA studies adapt directly from the source domain to the target domains without considering which kinds of semantic information embedded in images should be explored. Therefore, some irrelevant semantic information is inevitably used, which leads to negative transfer. To address these issues, we propose a semantic dual-adversarial network (SDN) method for BTDA. Specifically, to suppress irrelevant semantic information, we adopt a min-max game strategy between the classifier and the feature extractor. The classifier tries to maximize the prediction distribution discrepancy, whereas the extractor endeavors to minimize this discrepancy. In this process, irrelevant semantic information is suppressed and the principal semantic information is emphasized. To align the categorical distributions, we train a category-aware domain discriminator and a feature extractor with category labels. In addition, we introduce a random ratio-based feature fusion scheme to augment the source domain, which can decrease domain gaps. At last, we propose a weighted negative self-supervised learning method to enhance the model's generalization. Extensive experiments on multiple benchmarks showcase that our method significantly outperforms the prior state-of-the-art methods in BTDA.},
  keywords={Semantics;Feature extraction;Adaptation models;Training;Data models;Self-supervised learning;Noise measurement;Incremental learning;Deep learning;Computational modeling;Multi-target domain adaptation;image classification;adversarial learning;transfer learning},
  doi={10.1109/TMM.2025.3613144},
  ISSN={1941-0077},
  month={},}@ARTICLE{10685491,
  author={Liu, Jinfan and Yan, Yichao and Li, Junjie and Zhao, Weiming and Chu, Pengzhi and Sheng, Xingdong and Liu, Yunhui and Yang, Xiaokang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={IPAD: Industrial Process Anomaly Detection Dataset}, 
  year={2025},
  volume={35},
  number={1},
  pages={380-393},
  abstract={Video anomaly detection (VAD) is a challenging task aiming to recognize anomalies in video frames, and existing large-scale VAD researches primarily focus on road traffic and human activity scenes. In industrial scenes, there are often a variety of unpredictable anomalies, and the VAD method can play a significant role in these scenarios. However, there is a lack of applicable datasets and methods specifically tailored for industrial production scenarios due to concerns regarding privacy and security. To bridge this gap, we propose a new dataset, IPAD, specifically designed for VAD in industrial scenarios. The industrial processes in our dataset are chosen through on-site factory research and discussions with engineers. This dataset covers 16 different industrial devices and contains over 6 hours of both synthetic and real-world video footage. Moreover, we annotate the key feature of the industrial process, i.e., periodicity. Based on the proposed dataset, we introduce a period memory module and a sliding window inspection mechanism to effectively investigate the periodic information in a basic reconstruction model. Our framework leverages LoRA adapter to explore the effective migration of pretrained models, which are initially trained using synthetic data, into real-world scenarios. Our proposed dataset and method will fill the gap in the field of industrial video anomaly detection and drive the process of video understanding tasks as well as smart factory deployment. Project page: https://ljf1113.github.io/IPAD_VAD.},
  keywords={Anomaly detection;Synthetic data;Pedestrians;Image reconstruction;Production facilities;Data models;Tablet computers;Deep learning;video anomaly detection;dataset;reconstruction model},
  doi={10.1109/TCSVT.2024.3465517},
  ISSN={1558-2205},
  month={Jan},}@INPROCEEDINGS{11094013,
  author={Zhou, Shuangfan and Zhou, Chu and Lyu, Youwei and Guo, Heng and Ma, Zhanyu and Shi, Boxin and Sato, Imari},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution}, 
  year={2025},
  volume={},
  number={},
  pages={16081-16090},
  abstract={Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks.},
  keywords={Stokes parameters;Accuracy;Superresolution;Pipelines;Neural networks;Cameras;Hardware;Pattern recognition;Image reconstruction;Videos},
  doi={10.1109/CVPR52734.2025.01499},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10944178,
  author={Liu, Xiwei and Kassab, Mohamad and Xu, Min and Ho, Qirong},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={$\mathcal{J}$-Invariant Volume Shuffle for Self-Supervised Cryo-Electron Tomogram Denoising on Single Noisy Volume}, 
  year={2025},
  volume={},
  number={},
  pages={568-577},
  abstract={Cryo-Electron Tomography (Cryo-ET) enables detailed 3D visualization of cellular structures in near-native states but suffers from low signal-to-noise ratio due to imaging constraints. Traditional denoising methods and supervised learning approaches often struggle with complex noise patterns and the lack of paired datasets. Self-supervised methods, which utilize noisy input itself as a target, have been studied; however, existing Cryo-ET self-supervised denoising methods face significant challenges due to losing information during training and the learned incomplete noise patterns. In this paper, we propose a novel self-supervised learning model that denoises Cryo-ET volumetric images using a single noisy volume. Our method features a U-shape $\mathcal{J}$ -invariant blind spot network with sparse centrally masked convolutions, dilated channel attention blocks, and volume-unshuffle/shuffle technique. The volume-unshuffle/shuffle technique expands receptive fields and utilizes multi-scale representations, significantly improving noise reduction and structural preservation. Experimental results demonstrate that our approach achieves superior performance compared to existing methods, advancing Cryo-ET data processing for structural biology research. Code is available at https://github.com/Xiwei-web/SelfCryoET.},
  keywords={Training;Solid modeling;Three-dimensional displays;Biological system modeling;Noise reduction;Supervised learning;Self-supervised learning;Tomography;Noise measurement;Signal to noise ratio;computer vision;cryo-et;self-supervised;denoising;3d images processing},
  doi={10.1109/WACV61041.2025.00065},
  ISSN={2642-9381},
  month={Feb},}@ARTICLE{11021679,
  author={Xiao, Yun and Zhang, Chunlei and Jiang, Bo and Chen, Yuan and Tang, Jin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Multimodal Remote Sensing Image Registration via Modality Perception and Self-Supervised Position Estimation}, 
  year={2025},
  volume={63},
  number={},
  pages={1-14},
  abstract={Multimodal remote sensing image registration ensures that images from different sensors or modalities are spatial and informational consistent for effective comparison and analysis. However, due to the nonlinear modality gaps that exist between images, it is difficult to focus solely on spatial positional differences while ignoring the modality gaps. In this article, to address this issue, we propose a new framework for multimodal registration network, named MMRNet. The proposed framework comprises the following main aspects. First, a novel self-supervised positional misalignment estimator (PME) is designed for multimodal image registration. PME can efficiently overcome the modality gaps and learn the positional differences between multimodal images more reliably, optimizing the registration loss by minimizing the positional differences directly. Then, a new paradigm of modality translation, termed modality perception module (MPM), is introduced to effectively learn modality gaps and perform modality translation in the case of positional misalignment. Finally, we further design the modality perception guidance loss to supervise the modality translation task, which can encourage the fidelity of the generated pseudo-modality images. Our registration network integrates both rigid registration model and nonrigid registration model. The experimental results demonstrate that the proposed registration framework can obtain obviously superior performance in both rigid and nonrigid image registration tasks on optical-synthetic aperture radar (SAR) data, optical-map data, and optical-infrared data. The code and relevant dataset will be made publicly available at https://github.com/Ahuer-Lei/MMRNet.},
  keywords={Image registration;Optical sensors;Optical imaging;Remote sensing;Feature extraction;Translation;Accuracy;Nonlinear optics;Optical signal processing;Optical reflection;Image registration;modality translation;multimodal learning;remote sensing processing;rigid and nonrigid transformation},
  doi={10.1109/TGRS.2025.3576290},
  ISSN={1558-0644},
  month={},}@ARTICLE{11072218,
  author={Wang, Jiaxiang and Zheng, Aihua and Liu, Lei and Li, Chenglong and He, Ran and Tang, Jin},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Adaptive Interaction and Correction Attention Network for Audio-Visual Matching}, 
  year={2025},
  volume={20},
  number={},
  pages={7558-7571},
  abstract={Audio-visual matching techniques aim to recognize and match information across different identities by learning a similarity metric across modalities. However, modal differences arise from insufficient cross-modal correlations and noise interference, which substantially hinder the performance of traditional deep metric learning methods in audio-visual matching tasks. To address the modal differences issue, we propose a novel Adaptive Interactive and Correction Attention Network (AICANet). This network efficiently captures deep information connections, generating modality-consistent feature embeddings within a unified metric framework. The core of AICANet is its two-pronged approach to reducing modal differences. First, we propose the Adaptive Interactive Attention (AIA) module, which flexibly establishes associations among cross-modal local features using dynamically generated pseudo-labels. Second, we propose the Adaptive Correction Attention (ACA) mechanism, which employs an adaptive threshold to de-interference effectively and accurately adjust the representation of local feature associations. Notably, the ACA mechanism is suitable for both intra-modal and inter-modal refined attention correction. Additionally, we design a relative distance stretching metric loss ( ${\mathcal {L}}_{RDSM}$ ), which reinforces the similarity invariance of feature embeddings in a uniform space and enhances matching accuracy. Extensive tests on the VoxCeleb and VoxCeleb2 datasets demonstrate that AICANet outperforms leading existing algorithms across several evaluation metrics, validating its superior performance. The codes can be found at https://github.com/w1018979952/AICANet.},
  keywords={Measurement;Visualization;Interference;Correlation;Semantics;Noise;Filters;Attention mechanisms;Face recognition;Extraterrestrial measurements;Audio-visual matching;adaptive interaction attention;adaptive correction attention;modal differences},
  doi={10.1109/TIFS.2025.3586484},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{10899311,
  author={Wang, Yan and Zhang, Lan and Wang, Xianwen and Ding, Ke and Yan, Jin},
  booktitle={2024 10th International Conference on Big Data Computing and Communications (BigCom)}, 
  title={Large-Scale Multi-Camera Person Trajectory Tracking Based on Low Sampling Rate of Camera}, 
  year={2024},
  volume={},
  number={},
  pages={157-164},
  abstract={With the development of smart cities, particularly on campuses and in industrial parks, the rapid increase in the number of cameras has boosted the demand for multi-target, multi-camera tracking technology. However, current work on multi-target multi-camera tracking often involves a small number of cameras with limited monitoring coverage and short duration, insufficient for the analytical tasks required in smart parks. Moreover, existing works rely on high frame rate video streams as the data source, which, despite providing rich information, demand high storage and computational resources, limiting their application in long-term, large-scale trajectory tracking scenarios. In response to these challenges, we propose a new application scenario: large-scale multi-camera pedestrian tracking based on low-sampling-rate images. By utilizing low-sampling-rate surveillance images instead of videos and we have designed a Multi-dimensional Fusion Identity Verification architecture and a new trajectory tracking process to address the challenges brought about by large-scale camera tracking scenarios, such as environmental factors, diverse camera installation conditions, and the similarity in clothing among a large number of covered individuals. Our approach was tested in a real-world surveillance system with 2,456 cameras covering over 494.2 acres. Despite a significant reduction in data volume by over 60 times and an increase in camera count by 200 times compared to using video data sources of the real world such as WildTrack, our method still attained a pedestrian tracking accuracy of 75.8% and a recall rate of 57.4%. These results affirm our method's effectiveness and feasibility in large-scale environments.},
  keywords={Pedestrians;Accuracy;Trajectory tracking;Smart cities;Surveillance;Soft sensors;Computer architecture;Streaming media;Cameras;Streams;Multi-target multi-camera tracking;Person re-identification},
  doi={10.1109/BIGCOM65357.2024.00030},
  ISSN={2767-2174},
  month={Aug},}@ARTICLE{10578064,
  author={Tong, Xingyu and Xiao, Yang and Tan, Bo and Yang, Jianyu and Cao, Zhiguo and Tianyi Zhou, Joey and Yuan, Junsong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={You Will Never Walk Alone: One-Shot 3D Action Recognition With Point Cloud Sequence}, 
  year={2024},
  volume={34},
  number={11},
  pages={11464-11477},
  abstract={In this work, we pay the first effort to address one-shot 3D action recognition in point cloud sequence, without skeleton information. The main contribution lies in two folders. First, a novel one-shot classification approach that considers the feature distribution of 3D action is proposed. We find that, for different 3D actions their dimensional-wise feature distributions are generally in Gaussian form and similar action categories hold approximate feature distributions. Accordingly, K-nearest base classes’ mean value and covariance matrix information help to form one-shot novel class’s pseudo feature distribution. To alleviate the potential ambiguous problem within nearest neighbor search, we divide the base classes into subsets via C-means clustering to facilitate the similarity measure to novel class. Meanwhile, the feature distribution of base class’s whole set and subsets will be jointly considered for generating novel class’s pseudo feature distribution. Multi-dimensional Gaussian sampling is conducted on the acquired pseudo feature distribution for feature-level data augmentation, to make one-shot novel class “never walk alone” for leveraging classifier training. Secondly to better characterize fine-grained 3D action, a temporal attention method is proposed, via introducing vision Transformer (ViT) to capture action’s discriminative short-term motion pattern with densely sampled short-term 3DV (3D dynamic voxel) features along temporal dimension. Experiments on NTU RGB+D 120 and 60 verify superiority of our approach. It outperforms state-of-the-art skeleton-based methods by 13.9% at most. The source code is available at https://github.com/Tong-XY/YNWA.},
  keywords={Three-dimensional displays;Point cloud compression;Skeleton;Data augmentation;Training;Feature extraction;Transformers;One-shot 3D action recognition;3D dynamic voxel;vision transformer;distribution calibration},
  doi={10.1109/TCSVT.2024.3421304},
  ISSN={1558-2205},
  month={Nov},}@ARTICLE{11021006,
  author={Qi, Mengnan and Mao, Shasha and Zhang, Yimeng and Gu, Jing and Gou, Shuiping and Jiao, Licheng and Zhang, Yuming},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={DASCE: Long-Tailed Data Augmentation Based Sparse Class-Correlation Exploitation}, 
  year={2025},
  volume={37},
  number={8},
  pages={4497-4511},
  abstract={The long-tailed data distribution frequently occurs in the real-world scenarios, whereas deep learning is not effective enough for such distribution. In order to improve the effectiveness for the long-tailed data, data augmentation is widely used to balance the distribution of classes by generating new samples. However, most existing studies are designed from the perspective of the class-independence assumption by default, ignoring the effect of interrelation among classes for data augmentation, which causes that some generated samples may be unrepresentative and useless for balancing the class-distribution. Inspired by this, we propose a new data augmentation method based the sparse class-correlation exploitation in this paper, which can generate more representative samples by utilizing the class-correlation, to effectively balance the class-distribution for the long-tailed data. In the proposed method, a sparse class-correlation exploration module is first proposed to explore the potential correlations among multiple classes for boosting the classification performance. Based on the class-correlations, the pivotal seed-samples are generated by maximizing the sparse representation of challenging samples. Meanwhile, an ambiguity-filtered translation module is designed to generate more representative new samples for the target classes based the obtained seed-samples by enhancing the class-consistency and suppressing the deviation from the target classes. In addition, we introduce the self-supervised feature and fuse it with the discriminative feature to explore more accurate class-correlations. Experimental results illustrate that the proposed method obtains better performance only with a small number of generated samples than the state-of-the-art methods.},
  keywords={Heavily-tailed distribution;Data augmentation;Correlation;Automobiles;Birds;Translation;Training;Deep learning;Data models;Visualization;Long-tailed data distribution;deep learning;data augmentation;sparse class-correlation exploitation},
  doi={10.1109/TKDE.2025.3573899},
  ISSN={1558-2191},
  month={Aug},}@INPROCEEDINGS{11096281,
  author={Grini, Anass and Taheri, Oumaima and El Khamlichi, Btissam and El Fallah-Seghrouchni, Amal},
  booktitle={2025 21st International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability}, 
  year={2025},
  volume={},
  number={},
  pages={771-776},
  abstract={While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.},
  keywords={Computational modeling;Network intrusion detection;Machine learning;Computer architecture;Telecommunication traffic;Smart systems;Robustness;Numerical models;Internet of Things;Resource management;Network Intrusion Detection;Internet of Things;Adversarial attacks;Cybersecurity;Machine Learning},
  doi={10.1109/DCOSS-IoT65416.2025.00117},
  ISSN={2325-2944},
  month={June},}@INPROCEEDINGS{9412126,
  author={Sun, Linhui and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={PEAN: 3D Hand Pose Estimation Adversarial Network}, 
  year={2021},
  volume={},
  number={},
  pages={1251-1258},
  abstract={Despite recent emerging research attention, 3D hand pose estimation still suffers from the problems of predicting inaccurate or invalid poses which conflict with physical and kinematic constraints. To address these problems, we propose a novel 3D hand pose estimation adversarial network (PEAN) which can implicitly utilize such constraints to regularize the prediction in an adversarial learning framework. PEAN contains two parts: a 3D hierarchical estimation network (3DHNet) to predict hand pose, which decouples the task into multiple subtasks with a hierarchical structure; a pose discrimination network (PDNet) to judge the reasonableness of the estimated 3D hand pose, which back-propagates the constraints to the estimation network. During the adversarial learning process, PDNet is expected to distinguish the estimated 3D hand pose and the ground truth, while 3DHNet is expected to estimate more valid pose to confuse PDNet. In this way, 3DHNet is capable of generating 3D poses with accurate positions and adaptively adjusting the invalid poses without additional prior knowledge. Experiments show that the proposed 3DHNet does a good job in predicting hand poses, and introducing PDNet to 3DHNet does further improve the accuracy and reasonableness of the predicted results. As a result, the proposed PEAN achieves the state-of-the-art performance on three public hand pose estimation datasets.},
  keywords={Training;Solid modeling;Three-dimensional displays;Pose estimation;Kinematics;Predictive models;Pattern recognition},
  doi={10.1109/ICPR48806.2021.9412126},
  ISSN={1051-4651},
  month={Jan},}@INPROCEEDINGS{10651458,
  author={Chen, Min and Pan, Yi and Pu, Zhiqiang and Yi, Jianqiang and Wang, Shijie and Liu, Boyin},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Extraction and Transfer of General Deep Feature in Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Knowledge transfer from teacher agent to student agent in reinforcement learning addresses the issue of sample inefficiency resulting from random exploration. However, existing approaches usually assume that the policy or state-action pairs (demonstrations) of teacher is accessible, which may not be met in reality due to privacy and other limitations. This paper proposes a novel knowledge transfer method based solely on the state sequences of teacher. Specifically, We encode raw state into general deep feature through learning state evaluation and domain discrimination tasks based on teachers’ experience, and share the encoder among different student agents in various scenarios. We validate our method in Starcraft II and Google Research Football environment. The experimental results show that our method significantly accelerates the learning process of agents and improves their convergence performance, showcasing the "depth" of the feature. Additionally, the feature successfully generalize to teachers’ unseen scenarios and unfamiliar roles, demonstrating the "generality" of the feature.},
  keywords={Privacy;Accelerated aging;Neural networks;Reinforcement learning;Feature extraction;Internet;Knowledge transfer;reinforcement learning;multi-agent reinforcement learning;transfer learning;domain adaption},
  doi={10.1109/IJCNN60899.2024.10651458},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11088854,
  author={Lyu, Shuang and Miao, Wentao and Zhao, Yinjun and Gao, Wei and Lyu, Rongxing},
  booktitle={2025 IEEE 2nd International Conference on Deep Learning and Computer Vision (DLCV)}, 
  title={Super-Resolution Remote Sensing Image Classification Based on Residual Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a deep learning-based superresolution reconstruction method for remote sensing images, targeting the shortage of high-resolution datasets in classification tasks. To address the challenge of using lowresolution images for high-resolution classification, a superresolution generator based on residual networks is developed. This model reconstructs low-resolution remote sensing images into high-resolution ones, effectively expanding the dataset. The study focuses on five types of military aircraft (B1, B52, C17, C130, and KC 135). The reconstruction performance is evaluated using peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and reconstruction accuracy. Results show that the reconstruction accuracy reaches 93.5%, while the classification accuracy increases from 76.3% for lowresolution images to 89.1% after super-resolution processing. This enhancement significantly boosts the classification model's accuracy and generalization ability. Additionally, sensitivity analysis of key parameters in the reconstruction process is conducted, offering practical insights. The proposed method effectively leverages historical remote sensing data, resolving the bottleneck of insufficient datasets for classification tasks.},
  keywords={Deep learning;Visualization;Accuracy;PSNR;Superresolution;Military aircraft;Image reconstruction;Remote sensing;Residual neural networks;Image classification;Remote sensing images;Super-resolution reconstruction;Deep learning;Residual Networks},
  doi={10.1109/DLCV65218.2025.11088854},
  ISSN={},
  month={June},}@INPROCEEDINGS{11015228,
  author={V.Nethriya and S.Rithika and M.Revathi and Patturose J., Gold Beulah},
  booktitle={2024 International Conference on Communication, Computing, Smart Materials and Devices (ICCCSMD)}, 
  title={LegalClarity: AI-Powered Document Analysis for Informed Decision-Making}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Many individuals and businesses often sign important documents without thoroughly reading or understanding them due to their complexity and length. This can lead to unforeseen consequences such as financial losses and legal complications. To tackle this issue, we propose an AI-powered solution designed to simplify the review process and highlight critical points, thereby making it easier for users to understand the documents they sign. Our system employs Optical Character Recognition (OCR) to transform scanned documents into machine-readable text and uses Natural Language Processing (NLP) techniques for text preprocessing. Utilizing open-source pretrained models like BERT, we have developed a supervised learning model that accurately identifies and highlights potential issues. Moreover, a text summarization feature condenses the document and emphasizes key points. The user-friendly interface includes an interactive dashboard for document uploads and a mobile application for real-time feedback. This innovative solution aims to enhance transparency and decision-making, ultimately protecting users from unforeseen consequences and promoting informed decisions.},
  keywords={Reviews;Law;Optical character recognition;Decision making;Supervised learning;Text summarization;Transforms;Natural language processing;Telecommunication computing;Smart materials;Efficient Document Review;Improved Understanding;Risk Mitigation;Financial Protection;Legal Clarity;Time Savings;Decision Confidence},
  doi={10.1109/ICCCSMD63546.2024.11015228},
  ISSN={},
  month={Dec},}@ARTICLE{10925891,
  author={Wu, Yuan and Bai, Shoudu and Hu, Qingyong and Wang, Bo and Li, Min and Hu, Xinrong and Chen, Yanjiao},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Ubicon-BP: Towards Ubiquitous, Contactless Blood Pressure Detection Using Smartphone}, 
  year={2025},
  volume={24},
  number={8},
  pages={7680-7692},
  abstract={Blood pressure (BP) is a critical physiological parameter closely associated with severe diseases such as heart failure and kidney damage. Current methods either require additional or dedicated hardware, or closing touching to the devices, causing discomfort and inconvenience. Therefore, a convenient, contactless BP measurement solution is highly desired. In this work, we present Ubicon-BP, a ubiquitous, device-free, and contactless BP detection application. Ubicon-BP calculates BP based on the pulse transit time (PTT), a key feature that is medically proven correlated with BP. However, using smartphone sensors to contactless calculate PTT is non-trivial since it requires a micro-second level precision for cardiac event detection. To address this issue, we propose leveraging the acoustic sensors in smartphone to detect vibrations caused by heart valve movements, as well as camera sensors to measure finger pulses. To accurately measure heartbeat signal that are susceptible to motion, we first improve the sensing granularity of acoustic signals and then introduce the IQ-MVED model to eliminate motion interference. Furthermore, when recovering pulse signals from video signals, issues such as poor generalization performance arise. Consequently, we propose the TS-CAN and meta-learning models to obtain personalized pulse signals. Finally, we transform the extracted time-frequency features from the recovered heartbeats and pulse signals to the corresponding BP. Comprehensive testing involving 50 subjects reveal a standard deviation error of $ 4.27 \; \text{mmHg}$4.27mmHg for diastolic pressure and $ 6.36 \; \text{mmHg}$6.36mmHg for systolic pressure, respectively.},
  keywords={Heart beat;Sensors;Biomedical monitoring;Monitoring;Acoustics;Feature extraction;Cameras;Blood pressure;Acoustic measurements;Acoustic sensors;BP detection;smartphone;pulse transit time (PTT);heartbeat sensing},
  doi={10.1109/TMC.2025.3551315},
  ISSN={1558-0660},
  month={Aug},}@INPROCEEDINGS{10961762,
  author={Sonaniya, Aman and Jain, Vikas Kumar and Vishnu, Devraj and Qureshi, Shahana Gajala},
  booktitle={2024 First International Conference on Data, Computation and Communication (ICDCC)}, 
  title={Ensemble Approach to Optimize Network Intrusion Detection: A Comparative Study of Machine Learning Classifiers}, 
  year={2024},
  volume={},
  number={},
  pages={271-277},
  abstract={As networked systems grow quickly, detecting network breaches is becoming more and more important. Network Intrusion Detection Systems (NIDS) are critical tools for the protection of networks from hostile activity, and machine learning based approaches are being increasingly used to improve NIDS effectiveness. This work offers a hybrid ensemble machine learning model that combines three robust classification methods: Random Forest, Gradient Boosting and Support Vector Machines. We test the ensemble model on the KDD Cup 1999 dataset, a standard NIDS benchmark. The model performs better than individual classifiers with an accuracy of 99.14%, precision of 99.05%, recall of 99.14%, and an F1 score of 99.01%. Besides accuracy, precision, recall, and F1 score, the ROC-AUC analysis shows that the ensemble model can discern different types of network attacks. Hyperparameter adjustment and cross validation are investigated in order to optimize model performance better. This work demonstrates the great potential of the ensemble learning approaches to enhance network intrusion detection.},
  keywords={Support vector machines;Adaptation models;Analytical models;Accuracy;Network intrusion detection;Telecommunication traffic;Feature extraction;Robustness;Ensemble learning;Tuning;Intrusion Detection Systems;Machine Learning;Ensemble Learning;Network Security;Hyperparameter Tuning},
  doi={10.1109/ICDCC62744.2024.10961762},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10669437,
  author={Wang, Zezhong and Huang, Jinhui and Wu, Zekai and Chen, Joanna A. and Ge, Ming and Lai, Edmond},
  booktitle={2024 Prognostics and System Health Management Conference (PHM)}, 
  title={A Data-Driven GRU-GPR Method for Predicting the Remaining Useful Life of Lithium-Ion Battery}, 
  year={2024},
  volume={},
  number={},
  pages={72-79},
  abstract={Remaining useful life prediction is crucial in the prognostics and health management of lithium-ion batteries. This paper combines the gated recurrent unit with Gaussian process regression to predict the remaining useful life of batteries based on limited training data. The proposed method can capture the non-linear degradation of capacity and provide corresponding uncertainty estimation. The approach begins with relevant feature selection by least absolute shrinkage and selection operator. Subsequently, these selected features undergo empirical model decomposition for denoising, resulting in intrinsic mode functions that serve as inputs to the predictive model. The validation experiments are conducted based on batteries from the data repository of NASA. Comparative results demonstrate that the developed model achieves low RUL prediction errors of 0.4, outperforming other benchmark methods. Furthermore, the pro-posed method exhibits robustness in predicting complex declining capacity patterns, even with limited training data.},
  keywords={Lithium-ion batteries;Degradation;Uncertainty;Noise reduction;NASA;Training data;Predictive models;Remaining useful life;Lithium-ion battery;Gated recurrent unit;GPR;Predict},
  doi={10.1109/PHM61473.2024.00021},
  ISSN={2166-5656},
  month={May},}@ARTICLE{10879458,
  author={Mao, Yongwei and Wu, Jinjian and Liu, Yongxu and Li, Leida and Dong, Weisheng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Scene-Modulated High-Order Statistical Representation Learning for No-Reference Super-Resolution Image Quality Assessment}, 
  year={2025},
  volume={35},
  number={7},
  pages={6589-6601},
  abstract={With the rapid development of single image super-resolution (SR) technology, there is an urgent need to develop a fair no reference Super-Resolution image Quality Assessment (SRQA) method. Existing no reference SRQA methods primarily concentrate on SR artifacts including structural distortion and texture distortion by extracting spatial features, but ignore the inductive bias of Deep Neural Network (DNN)-based SR models. As a result, they function effectively for interpolation-based and dictionary-based algorithms, but struggle to perform as effectively with DNN-based SR algorithms. We found that the visual content generated by DNN-based SR models under different inductive biases often carries a content-invariant model-specific style, which can be captured by the correlations between hierarchical representation channels. To that end, we propose a novel Scene-modulated High-order Statistical Representation network (SmHSR) built on a multi-scale over-complete transformation. We quantify the perceptual quality of SR images as the shift of high-order statistical properties in their multi-scale over-complete representation, where intra-channel statistics are used to capture spatial correlations and inter-channel statistics are used to capture the inductive bias of SR models. In addition, the scene information implicit in the deep over-complete representation is used to modulate the high-order statistical properties, which simulates the top-down regulation of cognition on perception. Under the modulation of scene information, SmHSR can learn more sophisticated scene-aware statistical representation. The MultiLayer Perceptron (MLP) is used to map the high-order statistical representation to an overall quality. We test our method on multiple SR image quality databases. Experimental results show that our method outperforms the state-of-the-art SRQA methods.},
  keywords={Correlation;Visualization;Image quality;Image reconstruction;Training;Superresolution;Feature extraction;Prediction algorithms;Measurement;Data mining;Super-resolution image quality assessment;statistical representation learning;scene-modulation},
  doi={10.1109/TCSVT.2025.3541103},
  ISSN={1558-2205},
  month={July},}@ARTICLE{11153603,
  author={Zheng, Juepeng and Li, Guowen and Wen, Yibin and Zhang, Jinxiao and Dong, Runmin and Fu, Haohuan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={In this article, we first tackle a more realistic domain adaptation (DA) setting: source-free blending-target DA (SF-BTDA), where we cannot access to source-domain data while facing mixed multiple target domains without any domain labels in prior. Compared to existing DA scenarios, SF-BTDA generally faces the coexistence of different label shifts in different targets, along with noisy target pseudolabels generated from the source model. In this article, we propose a new method called evidential graph contrastive alignment (EGCA) to decouple the blending-target domain and alleviate the effect of noisy target pseudolabels. First, to improve the quality of pseudo target labels, we propose a calibrated evidential learning (CEL) module to iteratively improve both the accuracy and certainty of the resulting model and adaptively generate high-quality pseudo target labels. Second, we design a graph contrastive learning with the domain distance matrix and confidence-uncertainty criterion, to minimize the distribution gap of samples of the same class in the blending-target domain, which alleviates the coexistence of different label shifts in blended targets. We conduct a new benchmark based on three standard DA datasets, and EGCA outperforms other methods with considerable gains and achieves comparable results compared with those that have domain labels or source data in prior.},
  keywords={Data models;Adaptation models;Noise measurement;Earth;Accuracy;Hands;Deep learning;Contrastive learning;Training;Systems modeling;Contrastive learning;domain adaptation (DA);evidential learning;source-free},
  doi={10.1109/TNNLS.2025.3603224},
  ISSN={2162-2388},
  month={},}@INPROCEEDINGS{10248680,
  author={Liu, Yongxin and Zhao, Haifeng and Zhang, Shaojie and Fu, Yanping},
  booktitle={2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={3D Reconstruction and Texture Optimization Based on Differentiable Rendering}, 
  year={2023},
  volume={},
  number={},
  pages={77-83},
  abstract={Due to noise and quantization errors inherent in depth cameras, RGB-D based 3D reconstruction algorithms inevitably suffer from defects such as texture blurring and ghosting. However, current texture optimization methods tend to only focus on optimizing one aspect, such as camera pose, geometry, or texture, without considering the highly coupled relationship between them. As a result, it is difficult to achieve global optimality. To address this issue, we propose a joint optimization framework based on differentiable rendering that simultaneously optimizes camera pose, geometry, and texture. By fully exploiting the complementary relationship between these aspects, we are able to reconstruct textures with high fidelity while recovering fine geometric details. First, we propose an optimized geometry model combined with adaptive subdivision, where we selectively subdivide the model only in texture-rich regions to increase the number of vertex facets. Then, we perform geometry optimization to recover high-frequency geometric details evenly in the presence of large geometric errors without incurring computational costs. After the pose correction and geometric model optimization, we use pixel regeneration pipelines to generate clear texture atlases. Comparative evaluations with state-of-the-art texture optimization methods on a public dataset demonstrate the effectiveness of our proposed approach in reducing errors in the reconstruction of 3D models, generating clear textures, and recovering high-frequency geometric details.},
  keywords={Geometry;Adaptation models;Solid modeling;Three-dimensional displays;Quantization (signal);Computational modeling;Geometric modeling;Texture optimization;Joint optimization;Differentiable rendering;Adaptive subdivision},
  doi={10.1109/ICSP58490.2023.10248680},
  ISSN={},
  month={April},}@INPROCEEDINGS{1275919,
  author={Fenglei Hou and Bingxi Wang},
  booktitle={International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003}, 
  title={Text-independent speaker recognition using probabilistic SVM with GMM adjustment}, 
  year={2003},
  volume={},
  number={},
  pages={305-308},
  abstract={There are two most popular techniques in pattern recognition, discriminative classifiers and generative model classifiers. Combining them together could improve the performance of the recognition system. We present a novel method for text-independent speaker recognition. This system uses the output of the Gaussian mixture model to adjust the probabilistic output of the support vector machine. The new probabilistic SVM/GMM model based speaker recognition system is tested on the NIST 2003 speaker recognition evaluation database. Results on text-independent speaker identification and verification are provided to demonstrate the effectiveness of such systems.},
  keywords={Speaker recognition;Support vector machines;Hidden Markov models;Support vector machine classification;Artificial neural networks;NIST;Information science;System testing;Databases;Pattern recognition},
  doi={10.1109/NLPKE.2003.1275919},
  ISSN={},
  month={Oct},}@ARTICLE{10602503,
  author={Kalateh, Sepideh and Estrada-Jimenez, Luis A. and Nikghadam-Hojjati, Sanaz and Barata, Jose},
  journal={IEEE Access}, 
  title={A Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges}, 
  year={2024},
  volume={12},
  number={},
  pages={103976-104019},
  abstract={Emotion recognition involves accurately interpreting human emotions from various sources and modalities, including questionnaires, verbal, and physiological signals. With its broad applications in affective computing, computational creativity, human-robot interactions, and market research, the field has seen a surge in interest in recent years. This paper presents a systematic review of multimodal emotion recognition (MER) techniques developed from 2014 to 2024, encompassing verbal, physiological signals, facial, body gesture, and speech as well as emerging methods like sketches emotion recognition. The review explores various emotion models, distinguishing between emotions, feelings, sentiments, and moods, along with human emotional expression, categorized in both artistic and non-verbal ways. It also discusses the background of automated emotion recognition systems and introduces seven criteria for evaluating modalities alongside a current state analysis of MER, drawn from the human-centric perspective of this field. By selecting the PRISMA guidelines and carefully analyzing 45 selected articles, this review provides comprehensive perspectives into existing studies, datasets, technical approaches, identified gaps, and future directions in MER. It also highlights existing challenges and current applications of the MER.},
  keywords={Emotion recognition;Physiology;Mood;Feature extraction;Cultural differences;Guidelines;Multimodal sensors;Artificial intelligence;Affective computing;Deep learning;Machine learning;Multimodal emotion recognition;artificial intelligence;affective computing;emotion recognition;deep learning;machine learning;emotion expression},
  doi={10.1109/ACCESS.2024.3430850},
  ISSN={2169-3536},
  month={},}@ARTICLE{10439178,
  author={Jeong, Dawun and Hong, Youngtaek and Lee, Jina and Lee, Seul Bi and Cho, Yeon Jin and Shim, Hackjoon and Chang, Hyuk-Jae},
  journal={IEEE Access}, 
  title={Improving the Reproducibility of Computed Tomography Radiomic Features Using an Enhanced Hierarchical Feature Synthesis Network}, 
  year={2024},
  volume={12},
  number={},
  pages={27648-27660},
  abstract={Radiomics has gained popularity as a quantitative analysis method for medical images. However, computed tomography (CT) scans are performed using various parameters, such as X-ray dose and reconstruction kernels, which is a fundamental reason for the lack of reproducibility of radiomic features. This study evaluated whether the proposed network improves the reproducibility of radiomic features across various CT protocols and reconstruction kernels. We set five CT scan protocols and two reconstruction kernels to create various noise settings for the obtained CT images with an abdominal phantom. We developed an enhanced hierarchical feature synthesis (EHFS) network to improve the reproducibility of radiomic features across various CT protocols and reconstruction kernels. Eight hundred and nineteen radiomic features were extracted, including first-order, second-order, and wavelet features. Reproducibility was assessed using Lin’s concordance correlation coefficient (CCC) on internal and external testing. We considered a radiomic feature with CCC  $\ge0.85$  as a high-agreement feature. As a result, the average number of reproducible features increased in all protocols, from 241 ± 38 to 565 ± 11 in internal testing. In external testing, consisting of a new phantom and unseen protocol, 239 ± 74 reproducible features were in source images and 324 ± 16 were in generated images. The EHFS network is a novel approach to improving the reproducibility of radiomic features. It outperforms existing methods in reproducibility and generalization, as demonstrated by comprehensive experiments on both internal and external datasets. Our deep-learning-based CT image conversion could be a solution for standardization in ongoing radiomics research.},
  keywords={Radiomics;Computed tomography;Reproducibility of results;Protocols;Kernel;Image reconstruction;Feature extraction;Artificial intelligence;Radiomics;Artificial intelligence;computed tomography;radiomics;reproducibility},
  doi={10.1109/ACCESS.2024.3366989},
  ISSN={2169-3536},
  month={},}@ARTICLE{11003075,
  author={Khalaf, Abdulrahman Dira and Hamdan, Hazlina and Abdul Halin, Alfian and Manshor, Noridayu},
  journal={IEEE Access}, 
  title={Segmentation and Classification of Skin Cancer Diseases Based on Deep Learning: Challenges and Future Directions}, 
  year={2025},
  volume={13},
  number={},
  pages={90163-90184},
  abstract={Deep Learning (DL) techniques have significantly improved the diagnostic accuracy in healthcare, particularly for detecting and classifying skin cancer. Such technological advancements will assist healthcare professionals in delivering more accurate, efficient, and timely diagnoses, ultimately improving patient outcomes and facilitating early detection and treatment. Medical imaging technologies such as magnetic resonance imaging (MRI) and computed tomography (CT) are critical for diagnosing dermatological conditions. However, interpreting these images can be challenging due to overlapping structures and varying image quality. This study explores the application of DL in skin cancer diagnosis, focusing on advances in image segmentation and classification. DL-based models are reviewed specifically by convolutional neural networks (CNNs), and evaluations on their effectiveness for skin lesion detection are provided. This study also examines the critical challenges of deploying DL models in clinical practice, covering issues including dataset diversity, model interpretability, and real-world implementation feasibility. It further explores the selection of network architectures and data preprocessing techniques, emphasizing their influence on model performance. In summary, this study identifies research gaps and suggests future directions for enhancing DL models for dermatological applications.},
  keywords={Skin cancer;Skin;Accuracy;Image segmentation;Lesions;Biomedical imaging;Medical services;Artificial intelligence;Reviews;Computational modeling;Augmentation;classification;convolutional neural networks;deep learning;detection;understandable artificial intelligence;segmentation;skin cancer},
  doi={10.1109/ACCESS.2025.3569170},
  ISSN={2169-3536},
  month={},}@ARTICLE{10693421,
  author={Jain, Priti R. and Quadri, S. M. K. and Khattar, Anuradha},
  journal={IEEE Access}, 
  title={EM-UDA: Emotion Detection Using Unsupervised Domain Adaptation for Classification of Facial Images}, 
  year={2024},
  volume={12},
  number={},
  pages={140262-140276},
  abstract={Facial expressions can be used to interpret human feelings. They can be successfully used to assess the mood of a person. Accurate prediction of moods can prove to be of immense help in several areas including the mental health of an individual. Most methods proposed for facial emotion recognition use supervised learning. Research in utilizing facial expressions to assess the mental health of an individual is decelerated by the lack of availability of annotated data. Some unsupervised studies in this area are multimodal- text, images and at times questionnaires. These questionnaires and text are often used to validate results obtained using images. The proposed work is an attempt to design an unsupervised adversarial domain adaptation-based model EM-UDA to automate recognition of emotions from facial images so as to handle the issue of scarcity of labeled data. The model classifies facial images as depicting negative emotions or positive emotions. The proposed EM-UDA model achieves an accuracy of 83.9% and an F1-Score of 82.8% when trained on AffectNet, CK+ and tested on CK+. It attains an accuracy of 74.55% and a F1-Score of 74.87% when trained on AffectNet, FER 2013 and tested on FER 2013.},
  keywords={Emotion recognition;Accuracy;Adaptation models;Face recognition;Social networking (online);Supervised learning;Mood;Artificial intelligence;Convolutional neural networks;Artificial intelligence;convolutional neural network;domain adaptation;emotion detection},
  doi={10.1109/ACCESS.2024.3467990},
  ISSN={2169-3536},
  month={},}@ARTICLE{1323802,
  author={Yizhou Wang and Song-Chun Zhu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Analysis and synthesis of textured motion: particles and waves}, 
  year={2004},
  volume={26},
  number={10},
  pages={1348-1363},
  abstract={Natural scenes contain a wide range of textured motion phenomena which are characterized by the movement of a large amount of particle and wave elements, such as falling snow, wavy water, and dancing grass. In this paper, we present a generative model for representing these motion patterns and study a Markov chain Monte Carlo algorithm for inferring the generative representation from observed video sequences. Our generative model consists of three components. The first is a photometric model which represents an image as a linear superposition of image bases selected from a generic and overcomplete dictionary. The dictionary contains Gabor and LoG bases for point/particle elements and Fourier bases for wave elements. These bases compete to explain the input images and transfer them to a token (base) representation with an O(10/sup 2/)-fold dimension reduction. The second component is a geometric model which groups spatially adjacent tokens (bases) and their motion trajectories into a number of moving elements-called "motons". A moton is a deformable template in time-space representing a moving element, such as a falling snowflake or a flying bird. The third component is a dynamic model which characterizes the motion of particles, waves, and their interactions. For example, the motion of particle objects floating in a river, such as leaves and balls, should be coupled with the motion of waves. The trajectories of these moving elements are represented by coupled Markov chains. The dynamic model also includes probabilistic representations for the birth/death (source/sink) of the motons. We adopt a stochastic gradient algorithm for learning and inference. Given an input video sequence, the algorithm iterates two steps: 1) computing the motions and their trajectories by a number of reversible Markov chain jumps, and 2) learning the parameters that govern the geometric deformations and motion dynamics. Novel video sequences are synthesized from the learned models and, by editing the model parameters, we demonstrate the controllability of the generative model.},
  keywords={Motion analysis;Video sequences;Dictionaries;Inference algorithms;Layout;Snow;Monte Carlo methods;Photometry;Solid modeling;Birds;Index Terms- Textured motion;generative model;texton;statistical learning;object tracking;stochastic gradient.},
  doi={10.1109/TPAMI.2004.76},
  ISSN={1939-3539},
  month={Oct},}@INPROCEEDINGS{10765986,
  author={Escobar, Alma-Delia Otero and Suárez Jasso, Elsa and Anota, Mayra-Mineva Méndez},
  booktitle={2024 IEEE International Conference on Engineering Veracruz (ICEV)}, 
  title={Comparative Analysis of Generative Language Tools for Higher Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The development of technology has marked great changes in all areas, especially in education, the incorporation of various tools supported by artificial intelligence has brought positive and negative consequences, in many cases there is a lack of knowledge of these and therefore they have not been properly used. This study attempts to evaluate and compare some generative language tools through a literature search and their practical use. Using content analysis, the seven selected tools were compared considering the criteria of: Efficiency, Quality, Usability, Scalability and Costs. The results show that tools such as ChatGPT, GitHub Copilot, Whisper, Hugging Face and Grammarly stand out for their high efficiency, quality, usability and scalability, being the most robust options for various educational applications. While tools such as Google Bard and Khan Academy's AI Tutor offer good evaluations in usability and scalability, with relatively low costs, making them accessible and effective in educational environments. Some limitations were identified in tools such as Talk to Books and ChatPDF that present challenges in efficiency and usability, limiting their applicability compared to more advanced tools. In addition, GPT for Slides, although functional for presentations, shows limitations in scalability and security. It was identified that there is a positive correlation between efficiency and quality in most tools, although there are exceptions where these aspects are not aligned. It is concluded that generative tools in Higher Education are crucial to modernize teaching and are aligned with the UN Sustainable Development Goals. Choosing the right tool should consider factors such as security, costs and ease of use to maximize its educational impact.},
  keywords={Costs;Correlation;Scalability;Education;Chatbots;Security;Usability;Sustainable development;Faces;Software development management;analysis;tools;higher education;generative language},
  doi={10.1109/ICEV63254.2024.10765986},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10578808,
  author={Anderson, Neil and McGowan, Aidan and Hanna, Philip and Cutting, David and Galway, Leo and Collins, Matthew},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Using ChatGPT in Software Development Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative Artificial Intelligence (AI) and Large Language Models (LLMs) such as ChatGPT are revolutionizing the landscape of learning and teaching. They excel in understanding and creating natural language texts, thereby captivating students with their quick and well-crafted responses. While some perceive AI simply as a tool to reduce workload, our study appreciates these technologies for their ability to beautifully augment human capabilities. In this study, we tasked ChatGPT with designing a relational database for an online food delivery system, similar to an early university computer science assignment. This paper explains the attention mechanism, which is a crucial component in LLMs, enabling them to focus on specific parts of the presented input (prompt) and enhances their ability to ‘understand’ context. Through a series of iterative prompt refinements, we evaluate ChatGPT's effectiveness in developing this database, with a goal to enhance the accuracy and relevance of its responses. Our findings reveal both the benefits and limitations of using LLMs in education, highlighting their potential to significantly enrich the learning experience.},
  keywords={Generative AI;Large language models;Natural languages;Relational databases;Learning (artificial intelligence);Chatbots;Iterative methods;AI-enhanced learning;Generative AI;Language models;Attention mechanism},
  doi={10.1109/EDUCON60312.2024.10578808},
  ISSN={2165-9567},
  month={May},}@INPROCEEDINGS{9660463,
  author={Liu, Yinan and Li, Yiyang and Li, Lei},
  booktitle={2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)}, 
  title={Analysis and Research of Abstractive Automatic Summarization Based on Sequential Facts}, 
  year={2021},
  volume={},
  number={},
  pages={334-338},
  abstract={Automatic summarization is a task of converting text, and the summary result obtained should be able to accurately describe the facts that occurred in the original text. But so far, there are a lot of factual errors in the results obtained by generative summary models, resulting in low quality and poor readability. We believe that adding factual information in the encoding stage can effectively improve the readability of the summary and generate more accurate facts. To this end, we propose an abstractive summary model based on sequential facts and conduct experiments on the CNN/Daily Mail dataset. Experiments have proved that the integration of factual information can effectively improve the ROUGE value and factual accuracy of the summary.},
  keywords={Conferences;Encoding;Task analysis;Postal services;Automatic Summarization;Sequential Fact;Seq2Seq;Abstractive Summary},
  doi={10.1109/IC-NIDC54101.2021.9660463},
  ISSN={2575-4955},
  month={Nov},}@INPROCEEDINGS{10759178,
  author={Gao, Xirong and Binti Mohammad Noh, Liza Marziana and Mu, Xian and Wu, Chenhao and Liu, Jiang},
  booktitle={2024 IEEE International Conference on Imaging Systems and Techniques (IST)}, 
  title={Style Transfer of Chinese Opera Character Paintings}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Traditional opera character painting blends Chinese color ink painting with conventional opera, requiring both advanced opera art understanding and Chinese painting skills. However, creating these paintings demands specific artistic skills and the ability to integrate elements of opera art. Addressing these challenges is essential for the long-term development of opera character paintings and the promotion of traditional Chinese culture. Recent advancements in artificial intelligence (AI), particularly in image style transfer technology, offer new methods for generating opera character paintings. This paper uses our collected dataset of traditional opera character photographs and their paintings, employing an adversarial generative network (GAN) for implicit feature learning. Consequently, real opera character photographs are transformed into distinctive Chinese color ink paintings. To enhance the quality of the generated images, the encoding process incorporates feature mapping and attention module integration, which help reduce unimportant areas. Additionally, adaptive normalization techniques are used to improve neural network stability, accelerate convergence, and strengthen generalization capabilities. Simulation experiments demonstrate that the proposed method outperforms existing CNN and CycleGAN-based methods, indicates promising transfer performance and highlights the potential for the artistic creation of opera character paintings.},
  keywords={Representation learning;Art;Image color analysis;Neural networks;Ink;Generative adversarial networks;Mathematical models;Stability analysis;Artificial intelligence;Painting;Cultural heritage;Opera character painting;Style Transfer;Generative Adversarial Network;Attention},
  doi={10.1109/IST63414.2024.10759178},
  ISSN={2832-4234},
  month={Oct},}@INPROCEEDINGS{10390884,
  author={Peng, Yang and Lu, Peng and Yin, Wang},
  booktitle={2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)}, 
  title={Enhance Image Style Transfer with Depth Spatial Modulation}, 
  year={2023},
  volume={},
  number={},
  pages={147-151},
  abstract={Style transfer technology is an image editing technique that creates new images by blending the content features of content reference images and the style features of style reference images. In past style transfer algorithms, the transfer results often failed to strictly follow the guidelines given by the content and style references, resulting in discrepancies between the contours of the generated objects and the content references, as well as instances where the colors within the objects bled out. In this work, we attempt to augment traditional style transfer algorithms with a structural branch, filling in structural information using spatial modulation. As this approach does not disrupt the existing architecture of the generative network, it maintains a degree of universality. Experimental results suggest that our proposed method can enhance the transfer effectiveness of existing style transfer networks in terms of authenticity, content, and style.},
  keywords={Image color analysis;Modulation;Filling;Task analysis;Guidelines;Style Transfer;Computer Vision;Spatial Modulation;Deep Learning},
  doi={10.1109/IC-NIDC59918.2023.10390884},
  ISSN={2575-4955},
  month={Nov},}@ARTICLE{10110326,
  author={Ross, Nimel Sworna and Shibi, C. Sherin and Mustafa, Sithara Mohamed and Gupta, Munish Kumar and Korkmaz, Mehmet Erdi and Sharma, Vishal S. and Li, Z.},
  journal={IEEE Sensors Journal}, 
  title={Measuring Surface Characteristics in Sustainable Machining of Titanium Alloys Using Deep Learning-Based Image Processing}, 
  year={2023},
  volume={23},
  number={12},
  pages={13629-13639},
  abstract={A crucial method of maintenance in the manufacturing industry is machine vision-based fault diagnostics and condition monitoring of machine tools. The friction that occurs between the tool and the workpiece has a greater influence on the surface properties of the material. Effective problem diagnosis is necessary for machine systems to continue operations safely. Data-driven approaches have recently exhibited great promise for intelligent fault diagnosis. Unfortunately, the data collected under real-world conditions may be imbalanced, making diagnosis difficult. In dry, minimum quantity lubrication (MQL), and cryogenic circumstances, the method of failure detection of the proposed design is novel. The purpose of this interrogation is to evaluate the roughness profiles obtained from the machined surfaces and class separation. Markov transition field (MTF) is adopted to encode the surface profiles. In addition to this, conditional generative adversarial network (CGAN) for augmentation and bidirectional long-short term memory (BLSTM), multilayer perceptron (MLP), and 2-D-convolutional neural network (CNN) models are used for surface profile classification and correlation with process parameters. According to the study’s finding, the 2-D-CNN was significantly more accurate than the models in predicting surface profiles, with an average accuracy of above 99.6% in both training and testing. In the limelight, the suggested approach can demonstrate to be quite useful for categorizing and proposing appropriate machining circumstances, specifically in situations with minimal data.},
  keywords={Surface treatment;Machining;Surface morphology;Markov processes;Surface roughness;Rough surfaces;Predictive models;Conditional generative adversarial network (CGAN);cryogenic;deep learning (DL);machining;Markov transition field (MTF)},
  doi={10.1109/JSEN.2023.3269529},
  ISSN={1558-1748},
  month={June},}@ARTICLE{9852204,
  author={Vaccari, Ivan and Carlevaro, Alberto and Narteni, Sara and Cambiaso, Enrico and Mongelli, Maurizio},
  journal={IEEE Access}, 
  title={eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics}, 
  year={2022},
  volume={10},
  number={},
  pages={83949-83970},
  abstract={Machine learning (ML) algorithms are nowadays widely adopted in different contexts to perform autonomous decisions and predictions. Due to the high volume of data shared in the recent years, ML algorithms are more accurate and reliable since training and testing phases are more precise. An important concept to analyze when defining ML algorithms concerns adversarial machine learning attacks. These attacks aim to create manipulated datasets to mislead ML algorithm decisions. In this work, we propose new approaches able to detect and mitigate malicious adversarial machine learning attacks against a ML system. In particular, we investigate the Carlini-Wagner (CW), the fast gradient sign method (FGSM) and the Jacobian based saliency map (JSMA) attacks. The aim of this work is to exploit detection algorithms as countermeasures to these attacks. Initially, we performed some tests by using canonical ML algorithms with a hyperparameters optimization to improve metrics. Then, we adopt original reliable AI algorithms, either based on eXplainable AI (Logic Learning Machine) or Support Vector Data Description (SVDD). The obtained results show how the classical algorithms may fail to identify an adversarial attack, while the reliable AI methodologies are more prone to correctly detect a possible adversarial machine learning attack. The evaluation of the proposed methodology was carried out in terms of good balance between FPR and FNR on real world application datasets: Domain Name System (DNS) tunneling, Vehicle Platooning and Remaining Useful Life (RUL). In addition, a statistical analysis was performed to improve the robustness of the trained models, including evaluating their performance in terms of runtime and memory consumption.},
  keywords={Machine learning algorithms;Reliability;Adversarial machine learning;Prediction algorithms;Machine learning;Data models;Safety;Object detection;Machine learning;detection algorithms;adversarial machine learning;reliable},
  doi={10.1109/ACCESS.2022.3197299},
  ISSN={2169-3536},
  month={},}@ARTICLE{10926849,
  author={Huang, Shudong and Feng, Wentao and Tang, Chenwei and He, Zhenan and Yu, Caiyang and Lv, Jiancheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Partial Differential Equations Meet Deep Neural Networks: A Survey}, 
  year={2025},
  volume={36},
  number={8},
  pages={13649-13669},
  abstract={Many problems in science and engineering can be mathematically modeled using partial differential equations (PDEs), which are essential for fields like computational fluid dynamics (CFD), molecular dynamics, and dynamical systems. Although traditional numerical methods like the finite difference/element method are widely used, their computational inefficiency, due to the large number of iterations required, has long been a challenge. Recently, deep learning (DL) has emerged as a promising alternative for solving PDEs, offering new paradigms beyond conventional methods. Despite the growing interest in techniques like physics-informed neural networks (PINNs), a systematic review of the diverse neural network (NN) approaches for PDEs is still missing. This survey fills that gap by categorizing and reviewing the current progress of deep NNs (DNNs) for PDEs. Unlike previous reviews focused on specific methods like PINNs, we offer a broader taxonomy and analyze applications across scientific, engineering, and medical fields. We also provide a historical overview, key challenges, and future trends, aiming to serve both researchers and practitioners with insights into how DNNs can be effectively applied to solve PDEs.},
  keywords={Artificial neural networks;Surveys;Mathematical models;Finite element analysis;Computational modeling;Training;Artificial intelligence;Market research;Computational fluid dynamics;Partial differential equations;Deep learning (DL);deep neural networks (DNNs);partial differential equations (PDEs);survey;universal approximators},
  doi={10.1109/TNNLS.2025.3545967},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{10265112,
  author={John, Thrupthi Ann and Balasubramanian, Vineeth N. and Jawahar, C. V.},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science}, 
  title={Explaining Deep Face Algorithms Through Visualization: A Survey}, 
  year={2024},
  volume={6},
  number={1},
  pages={15-29},
  abstract={Although current deep models for face tasks surpass human performance on some benchmarks, we do not understand how they work. Thus, we cannot predict how it will react to novel inputs, resulting in catastrophic failures and unwanted biases in the algorithms. Explainable AI helps bridge the gap, but currently, there are very few visualization algorithms designed for faces. This work undertakes a first-of-its-kind meta-analysis of explainability algorithms in the face domain. We explore the nuances and caveats of adapting general-purpose visualization algorithms to the face domain, illustrated by computing visualizations on popular face models. We review existing face explainability works and reveal valuable insights into the structure and hierarchy of face networks. We also determine the design considerations for practical face visualizations accessible to AI practitioners by conducting a user study on the utility of various explainability algorithms.},
  keywords={Face recognition;Visualization;Surveys;Artificial intelligence;Explainable AI;Artificial neural networks;Deep neural networks;face understanding;explainability;accountability;transparency;interpretability;XAI;fairness;survey},
  doi={10.1109/TBIOM.2023.3319837},
  ISSN={2637-6407},
  month={Jan},}@INPROCEEDINGS{10575255,
  author={Abraham, Shawn and Ewards, Vinodh and Terence, Sebastian},
  booktitle={2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Interactive Video Virtual Assistant Framework with Retrieval Augmented Generation for E-Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1192-1199},
  abstract={In the era of a new digital world, there has been a decline in social interactions among humans in a physical setting. These concerns have made educational institutions focus on holistic development, which is recognized to be crucial for the overall growth and performance of a student. Nevertheless, concerns like inadequate student teacher and peer interactions, in an online setting can have adverse effects in the long run. This paper presents an analysis and the benefits of utilizing an interactive virtual assistant framework as a means to abridge the growing disconnect of modern technologies from the traditional classroom setting, closing in on the gap brought about by virtual learning medium. This research study examines the online and hybrid learning approach, highlighting its drawbacks in comparison to conventional teaching methodologies while trying to improve the current situation. The virtual assistant framework presented in this study makes use of cutting-edge technologies in the field of NLP dialogue systems and video generation using Wav2Lip, (a lip-sync model) and GFPGAN model for face-specific enhancement. The Virtual assistant is developed and demonstrated as a learning aid in both hybrid and physical classrooms. Further applications and performance improvements of the framework are discussed.},
  keywords={Electronic learning;Virtual assistants;Computational modeling;Education;Learning (artificial intelligence);Hybrid learning;Retrieval Augmented Generation (RAG);Vector store;Large Language Model (LLM);Lip-sync;Generative Adversarial Network (GAN);Video Chatbot;Interactive Virtual Assistant (IVA);interactive learning},
  doi={10.1109/ICAAIC60222.2024.10575255},
  ISSN={},
  month={June},}@INPROCEEDINGS{10803434,
  author={Hamed, Ahmed Abdeen and Fandy, Tamer E. and Wu, Xindong},
  booktitle={2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={Accelerating Complex Disease Treatment Through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer}, 
  year={2024},
  volume={},
  number={},
  pages={354-359},
  abstract={The objective of this research is to introduce a network specialized in predicting drugs that can be repurposed by investigating real-world evidence sources, such as clinical trials and biomedical literature. Specifically, it aims to generate drug combination therapies for complex diseases (e.g., cancer, Alzheimer's). We present a multilayered network medicine approach, empowered by a highly configured ChatGPT prompt engineering system, which is constructed on the fly to extract drug mentions in clinical trials. Additionally, we introduce a novel algorithm that connects real-world evidence with disease-specific signaling pathways (e.g., KEGG database). This sheds light on the repurposability of drugs if they are found to bind with one or more protein constituents of a signaling pathway. Our network medicine framework, empowered by GenAI, shows promise in identifying drug combinations with a high degree of specificity, knowing the exact signaling pathways and proteins that serve as targets. It is noteworthy that ChatGPT successfully accelerated the process of identifying drug mentions in clinical trials, though further investigations are required to determine the relationships among the drug mentions.},
  keywords={Drugs;Proteins;Protein engineering;Databases;Clinical trials;Chatbots;Breast cancer;Prompt engineering;Artificial intelligence;Alzheimer's disease;Network Medicine;Drug Repurposing;Generative AI;LLMs;Multilayered Network;Signaling Pathways;Breast Cancer},
  doi={10.1109/MedAI62885.2024.00053},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8995289,
  author={Li, Miao and Tang, Hongyin and Jin, Beihong and Zong, Chengqing},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={A New Effective Neural Variational Model with Mixture-of-Gaussians Prior for Text Clustering}, 
  year={2019},
  volume={},
  number={},
  pages={1390-1395},
  abstract={Text clustering is one of the fundamental tasks in natural language processing and text data mining. It remains challenging because texts have complex internal structure besides the sparsity in the high-dimensional representation. In the paper, we propose a new Neural Variational model with mixture-of-Gaussians prior for Text Clustering (abbr. NVTC) to reveal the underlying textual manifold structure and cluster documents effectively. NVTC is a deep latent variable model built on the basis of the neural variational inference. In NVTC, the stochastic latent variable, which is modeled as one obeying a Gaussian mixture distribution, plays an important role in establishing the association of documents and document labels. On the other hand, by joint learning, NVTC simultaneously learns text encoded representations and cluster assignments. Experimental results demonstrate that NVTC is able to learn clustering-friendly representations of texts. It significantly outperforms several baselines including VAE+GMM, VaDE, LCK-NFC, GSDPMM and LDA on four benchmark text datasets in terms of ACC, NMI, and AMI. Furthermore, NVTC learns effective latent embeddings of texts which are interpretable by topics of texts, where each dimension of latent embeddings corresponds to a specific topic.},
  keywords={Text mining;Manifolds;Hands;Semantics;Stochastic processes;Benchmark testing;Natural language processing;Artificial intelligence;Text clustering;Deep generative model;Neural variational inference;Latent variable model},
  doi={10.1109/ICTAI.2019.00195},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10056542,
  author={Shibata, Hiroki and Takama, Yasufumi},
  booktitle={2022 International Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, 
  title={Proposal of Consistent Learning Model with Exchange Monte Carlo}, 
  year={2022},
  volume={},
  number={},
  pages={197-202},
  abstract={This paper proposes a consistent learning model based on Exchange Monte Carlo Method. The paper also gives discussion with respect to experiments on the synthesized case. Learning model is currently focusing on the model with the interface for input and output. On that model, preparing a dataset remains in human's work, and there is still not sufficient research how to prepare a valuable dataset efficiently. Exchange Monte Carlo is used widely for both purposes of optimization and estimation of a probability distribution, and it has the ability to combine any probability model in one and sample the model's state efficiency from the combined probability model. From this point of view, when we consider the three models, i.e., real space, learnt model, and model of parameter distribution, we can combine them and construct the consistent model that explains the phenomena of learning consistently. It is supposed that those samples give us valuable dataset and set of the learnt parameter, when the parameter space also modeled with Bayesian inference framework. With these ideas mentioned so far, this paper proposes a generalized consistent probability model of the real space, learning model, and parameters' distribution of the learning model. To challenge to the sampling problem on high-dimensionality of the consistent model, Exchange Monte Carlo and Hamiltonian dynamics are employed. Experiments show the proposed method works on the synthesized case, that is, the original distribution is approximated well and parameter is optimized too, only by sampling from the consistent model without preparing dataset.},
  keywords={Monte Carlo methods;Extraterrestrial phenomena;Focusing;Estimation;Learning (artificial intelligence);Probability distribution;Bayes methods;Exchange Monte Carlo;Machine learning;Probability model;Hamiltonian dynamics;Generative model},
  doi={10.1109/TAAI57707.2022.00044},
  ISSN={2376-6824},
  month={Dec},}@INPROCEEDINGS{10860248,
  author={Chen, Yuming and Yashtini, Maryam},
  booktitle={2024 4th International Conference on Artificial Intelligence, Virtual Reality and Visualization}, 
  title={Detecting AI Generated Images Through Texture and Frequency Analysis of Patches}, 
  year={2024},
  volume={},
  number={},
  pages={103-110},
  abstract={The significant improvement in AI image generation in recent years poses serious threats to social security, as AI generated misinformation may infringe upon political stability, personal privacy, and digital copy rights of artists. Building an AI generated image detector that accurately identifies generated image is crucial to maintain the social security and property rights of artists. This paper introduces preprocessing pipeline that uses positional encoded azimuthal integrals for image patches to create fingerprints that encapsulate distinguishing features. We then trained a multi-head attention model with 97.5% accuracy on classification of the fingerprints. The model also achieved 80% accuracy on images generated by AI models not presented in the training dataset, demonstrating the robustness of our pipeline and the potential of broader application of our model.},
  keywords={Training;Solid modeling;Accuracy;Image coding;Computational modeling;Pipelines;Fingerprint recognition;Security;Artificial intelligence;Fake news;Generative AI;Detector;Texture;Frequency;Positional Encoding;Multi-head Attention;AI Security},
  doi={10.1109/AIVRV63595.2024.10860248},
  ISSN={},
  month={Nov},}@ARTICLE{8528393,
  author={Zeng, Jiangfeng and Ma, Xiao and Zhou, Ke},
  journal={IEEE Access}, 
  title={CAAE++: Improved CAAE for Age Progression/Regression}, 
  year={2018},
  volume={6},
  number={},
  pages={66715-66722},
  abstract={Face age progression/regression has garnered substantial active research interest due to its tremendous impact on a wide-range of practical applications like searching for missing individuals with photos of childhood, entertainment, and so on. Most existing face aging models have proven to be successful and effective in learning the transformation between age groups with the aid of paired samples, i.e., face images of the same person at different ages. Considering the expensive cost of collecting paired datasets, Conditional Adversarial Autoencoder (CAAE) is designed for face aging task without paired samples and first achieves face age progression and regression in a holistic framework. However, only rough wrinkles are generated because of the insufficient discriminative and generative ability. To tackle this problem, in this paper, we develop a novel generative model based on CAAE, dubbed CAAE++, which defeats the previous CAAE mainly for two enhancements: 1) an auxiliary classifier is added on top of the discriminator, which allows a single discriminator not only distinguishes real images from synthetics but also classifies them into the target age group; and 2) a pre-trained deep face recognition model and a pre-trained age estimation model are exploited to preserve identity and age similarity, respectively. We train CAAE++ on UTKFace dataset and test on FGNET dataset. Experimental results demonstrate the efficacy of our proposed method in terms of fidelity.},
  keywords={Face;Aging;Biological system modeling;Generative adversarial networks;Gallium nitride;Training;Generators;Age progression/regression;generative adversarial networks;conditional adversarial autoencoder;image generation;generators;face recognition},
  doi={10.1109/ACCESS.2018.2877706},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10217736,
  author={Saad Eldien, Noha A. and Essam Ali, Raghda and Moussa, Farid Ali},
  booktitle={2023 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Real and Fake Face Detection: A Comprehensive Evaluation of Machine Learning and Deep Learning Techniques for Improved Performance}, 
  year={2023},
  volume={},
  number={},
  pages={315-320},
  abstract={The progress of digital manipulation methods has led to the creation of incredibly realistic fake faces, making it more challenging for humans to differentiate between genuine and fabricated ones while our brains are wired to interpret facial features, the use of sophisticated technology and artificial intelligence is blurring the line between real and manipulated images. As a consequence, techniques such as deep learning are becoming popular in distinguishing between real and fake faces with greater accuracy and reliability. In this study, a combination of machine learning and deep learning approaches was utilized to develop models for the detection of genuine and fabricated faces. The initial model implemented was an artificial neural network model, which utilized a Fourier-based technique for feature extraction. The model underwent training and testing using a benchmark dataset labeled as "Real and Fake Faces", and produced an accuracy rate of 0.57. ResNet18 approach involved the utilization of multiple models of convolutional neural networks that were trained on the same dataset and their results enhanced the overall precision of the classification process. The implementation of ResNet18 in deep learning has greatly enhanced the overall performance by achieving a substantially elevated accuracy level of 0.77.},
  keywords={Deep learning;Training;Sensitivity;Machine learning algorithms;Computational modeling;Benchmark testing;Media;Deep Learning;Machine Learning;Real and Fake Faces;Face Detection;ANN;ResNet18},
  doi={10.1109/IMSA58542.2023.10217736},
  ISSN={},
  month={July},}@ARTICLE{10458951,
  author={Wu, Zhiheng and Wu, Zhengxing and Chen, Xingyu and Lu, Yue and Yu, Junzhi},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Self-Supervised Underwater Image Generation for Underwater Domain Pre-Training}, 
  year={2024},
  volume={73},
  number={},
  pages={1-14},
  abstract={The rapid progress in computer vision has presented new opportunities for enhancing the visual capabilities of underwater robots. However, most deep learning-based visual perception algorithms often underperform due to the scarcity of underwater datasets. To address this issue, we propose an underwater image synthesis method for pre-training in the underwater domain. By leveraging self-supervised learning, we simulate the physical imaging process of underwater scenes, allowing for style transfer from in-air images to underwater images using a reduced amount of underwater data. Furthermore, we propose a pre-training strategy that utilizes synthetic underwater images to enhance underwater visual perception. Finally, abundant experiments are conducted, including quantitative and qualitative comparisons. The results validate the effectiveness and superiority of the proposed underwater image synthesis method, highlighting the substantial improvement in underwater environment perception achieved through the underwater domain pre-training (UDP) strategy.},
  keywords={Image synthesis;Object detection;Learning systems;Visual perception;Self-supervised learning;Semantic segmentation;Image processing;Underwater technology;Object detection;pre-training;self-supervised learning;semantic segmentation;underwater image generation},
  doi={10.1109/TIM.2024.3373105},
  ISSN={1557-9662},
  month={},}@ARTICLE{10720876,
  author={Yuan, Xiaoyan and Wang, Wei and Li, Xiaohe and Zhang, Yuanting and Hu, Xiping and Deen, M. Jamal},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={CATransformer: A Cycle-Aware Transformer for High-Fidelity ECG Generation From PPG}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Electrocardiography (ECG) is the gold standard for monitoring heart function and is crucial for preventing the worsening of cardiovascular diseases (CVDs). However, the inconvenience of ECG acquisition poses challenges for long-term continuous monitoring. Consequently, researchers have explored non-invasive and easily accessible photoplethysmography (PPG) as an alternative, converting it into ECG. Previous studies have focused on peaks or simple mapping to generate ECG, ignoring the inherent periodicity of cardiovascular signals. This results in an inability to accurately extract physiological information during the cycle, thus compromising the generated ECG signals' clinical utility. To this end, we introduce a novel PPG-to-ECG translation model called CATransformer, capable of adaptive modeling based on the cardiac cycle. Specifically, CATransformer automatically extracts the cycle using a cycle-aware module and creates multiple semantic views of the cardiac cycle. It leverages a transformer to capture detailed features within each cycle and the dynamics across cycles. Our method outperforms existing approaches, exhibiting the lowest RMSE across five paired PPG-ECG databases. Additionally, extensive experiments are conducted on four cardiovascular-related tasks to assess the clinical utility of the generated ECG, achieving consistent state-of-the-art performance. Experimental results confirm that CATransformer generates highly faithful ECG signals while preserving their physiological characteristics.},
  keywords={Electrocardiography;Transformers;Monitoring;Fast Fourier transforms;Adaptation models;Cardiovascular diseases;Time series analysis;Biomedical monitoring;Accuracy;Transforms;ECG;Cardiovascular diseases;PPG;Cardiac cycle;Transformer},
  doi={10.1109/JBHI.2024.3482853},
  ISSN={2168-2208},
  month={},}@ARTICLE{10950078,
  author={Weng, Meimei and Liu, Jianjun and Yang, Jinlong and Wu, Zebin and Xiao, Liang},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Range–Null Space Decomposition With Frequency-Oriented Mamba for Spectral Superresolution}, 
  year={2025},
  volume={18},
  number={},
  pages={10292-10306},
  abstract={Spectral superresolution (SSR) is a technique aimed at reconstructing hyperspectral images (HSIs) from images with low spectral resolution. Previous methods combining mathematical models with deep learning have shown promising performance for HSI reconstruction. However, these methods still have limitations when dealing with complex scenes, especially in terms of data consistency and realness. To address these issues, we propose a model-driven SSR network that integrates range-null space decomposition with deep learning. Specifically, we solve for the range space (R-Space) part and null space (N-Space) part to reconstruct the desired HSI with consistency and realness. The R-Space is primarily iteratively derived from the input multispectral image to ensure reliable data consistency, while the N-Space reflects the true distribution of the target HSI, and its proper representation helps improve visual quality. To enhance N-Space exploration, we construct a frequency-oriented N-Space learning module that leverages Mamba and self-attention to separately extract spatial and spectral information in the frequency domain. In addition, we introduce a structure tensor term and a multikernel maximum mean discrepancy term in the loss function to constrain R-Space and N-space, respectively. Experimental results show that the proposed method achieves excellent performance.},
  keywords={Image reconstruction;Transformers;Superresolution;Hyperspectral imaging;Feature extraction;Frequency-domain analysis;Data mining;Convolution;Visualization;Optimization;Hyperspectral image (HSI);Mamba;range–null space decomposition (RNSD);spectral superresolution (SSR)},
  doi={10.1109/JSTARS.2025.3558399},
  ISSN={2151-1535},
  month={},}@ARTICLE{10374367,
  author={Xu, Xu and Lv, Wenrui and Wang, Wei and Zhang, Yushu and Chen, Junxin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Empowering Semantic Segmentation With Selective Frequency Enhancement and Attention Mechanism for Tampering Detection}, 
  year={2024},
  volume={5},
  number={6},
  pages={3270-3283},
  abstract={Nowadays, massive amounts of multimedia contents are exchanged in our daily life, while tampered images are also flooding the social networks. Tampering detection is therefore becoming increasingly important for multimedia integrity, and it is generally realized by designing specific convolutional neural networks. From a new perspective, this article proposes two pluggable modules for empowering existing semantic segmentation models for tampering detection. First, a selective frequency enhancement (SFE) module is developed to suppress the semantic information and selectively enhance the tamper information. Second, a boundary enhanced attention (BEA) module is designed to highlight the edge information of tempered area. Our SFE and BEA modules are combined with five mainstream semantic segmentation networks for performance evaluation. The experiment results demonstrate that our modules are able to empower the semantic segmentation networks for tampering detection, and their combinations even perform better than state-of-the-art algorithms in certain datasets.},
  keywords={Semantic segmentation;Forgery;Image edge detection;Social networking (online);Semantics;Discrete cosine transforms;Splicing;Convolutional neural networks;image forensics;image tampering detection;semantic segmentation},
  doi={10.1109/TAI.2023.3347178},
  ISSN={2691-4581},
  month={June},}@INBOOK{10897175,
  author={Islam, Mohammad Rubyet},
  booktitle={Generative AI, Cybersecurity, and Ethics}, 
  title={Ethical Design and Development}, 
  year={2025},
  volume={},
  number={},
  pages={163-178},
  abstract={Summary <p>As generative artificial intelligence (GenAI) increasingly integrates into various aspects of our lives, its ethical design and development become crucial. This chapter underscores the necessity of embedding ethical considerations in the GenAI development process through stakeholder engagement, transparency, and continuous monitoring. It highlights the roles of developers, ethicists, legal experts, and users in creating fair and accountable GenAI systems. Emphasis is placed on addressing biases, ensuring robustness and security, adhering to regulatory compliance, and fostering human&#x2010;centric design. Through interdisciplinary collaboration and comprehensive ethical training, the chapter outlines strategies to align GenAI technologies with societal values and ethical standards.</p>},
  keywords={Ethics;Artificial intelligence;Training;Stakeholders;Prevention and mitigation;Predictive models;Robustness;Guidelines;Documentation;Decision making},
  doi={10.1002/9781394279326.ch6},
  ISSN={},
  publisher={Wiley},
  isbn={9781394279319},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10897175},}@INPROCEEDINGS{9580069,
  author={Sawant, Sahil and Vishwakarma, Ankit and Sawant, Prerana and Bhavathankar, Prasenjit},
  booktitle={2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Analytical and Sentiment based text generative chatbot}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={Chatbots have existed for a decade and they have been used for many objective specific tasks in the industrial area extensively. The rise of deep neural nets has enhanced further growth of these chatbots. The chatbots are used to understand the sentences and decipher the meaning and continue the conversations depending on the need but they don't capture the emotion of the users. While conversing with a user, especially for the industries which implement these chatbots, maintaining a good interpersonal relationship is everything. With a chatbot which not only provides answers to the user of it but also understands it on a sentimental level, this can be achieved. Through our paper we focus on building a chatbot which generates responses on the basis of the emotions of the users in order to recreate a more sympathetic and human-like aspect towards the user. We have used the feed forward neural network architecture that fits in perfectly for a fast and accurate response. We have used the knowledge on Natural Language Processing to pre-process the data in a suitable format. The model is trained to understand the emotion based on the sentences and understand the meaning with a dataset specifically curated by the authors of the paper. The accuracy achieved on the model was 93.45% with a response generation time of 50 milliseconds.},
  keywords={Architecture;Neural networks;Buildings;Computer architecture;Chatbots;Time factors;Feeds;artificial intelligence;chatbot;natural language processing;neural networks;sentiment analysis},
  doi={10.1109/ICCCNT51525.2021.9580069},
  ISSN={},
  month={July},}@INPROCEEDINGS{9803802,
  author={Riedel, M. and Book, M. and Neukirchen, H. and Cavallaro, G. and Lintermann, A.},
  booktitle={2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO)}, 
  title={Practice and Experience using High Performance Computing and Quantum Computing to Speed-up Data Science Methods in Scientific Applications}, 
  year={2022},
  volume={},
  number={},
  pages={281-286},
  abstract={High-Performance Computing (HPC) can quickly process scientific data and perform complex calculations at extremely high speeds. A vast increase in HPC use across scientific communities is observed, especially in using parallel data science methods to speed-up scientific applications. HPC enables scaling up machine and deep learning algorithms that inherently solve optimization problems. More recently, the field of quantum machine learning evolved as another HPC related approach to speed-up data science methods. This paper will address primarily traditional HPC and partly the new quantum machine learning aspects, whereby the latter specifically focus on our experiences on using quantum annealing at the Juelich Supercomputing Centre (JSC). Quantum annealing is particularly effective for solving optimization problems like those that are inherent in machine learning methods. We contrast these new experiences with our lessons learned of using many parallel data science methods with a high number of Graphical Processing Units (GPUs). That includes modular supercomputers such as JUWELS, the fastest European supercomputer at the time of writing. Apart from practice and experience with HPC co-design applications, technical challenges and solutions are discussed, such as using interactive access via JupyterLab on typical batch-oriented HPC systems or enabling distributed training tools for deep learning on our HPC systems.},
  keywords={Deep learning;Training;Annealing;High performance computing;Data science;Writing;Quantum annealing;High-Performance Computing;Software Frame-work;Machine Learning;Deep Learning;Quantum Computing},
  doi={10.23919/MIPRO55190.2022.9803802},
  ISSN={2623-8764},
  month={May},}@INPROCEEDINGS{9730353,
  author={Xu, Junjie H. and Huang, Hong and Ling, Xiaoling and Paliyawan, Pujana},
  booktitle={2022 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Toward Collaborative Game Commentating Utilizing Pre-Trained Generative Language Models}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={In this paper, we propose a novel task of collaborative game commentating, an artificial intelligence agent capable of collaboratively commentating with a human commentator in Live-Streaming of Esports. To this end, we propose a collaborative game commentating system that employs a pre-trained language model trained using commentaries by professional commentators, along with metadata including title and tags. The conducted experiments show that (1) fine-tuned Text-to-Text Transfer Transformer (T5) model, a state-of-the-art generative language model, could produce more clearer and precise commentary and better recall the words from the reference commentary, as it effectively improves the scores on evaluation metrics that are widely used for concise text generation task after tuning the model. (2) The more information used for the current method fusion of information, the clearer and more precise generated commentary is. However, it performs worse to recall the words from reference commentary.},
  keywords={Measurement;Fuses;Conferences;Collaboration;Games;Transformers;Task analysis;Collaborative Game Commentating;Natural Language Generation;Human Computer Interaction},
  doi={10.1109/ICCE53296.2022.9730353},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10500260,
  author={Vardhan Khurdula, Harsha and Hajiarbabi, Mohammadreza},
  booktitle={SoutheastCon 2024}, 
  title={Unleashing the Potential of Custom Chat Generative Pre-Trained Transformer and Inordinately Deep Neural Networks in Pneumonia Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={412-420},
  abstract={This paper introduces an innovative approach to pneumonia detection, utilizing a custom tuned Chat GPT-based expert system alongside deep learning models. While addressing key limitations of existing methodologies, particularly in data preprocessing and model evaluation metrics, this research offers an end-to-end diagnostic solution that significantly enhances accuracy and user engagement. The integration of a Generative Large Language Transformer (Custom Chat-GPT) enables detailed symptom analysis leading to a more intuitive diagnostic process. In-depth experiments validate this approach over traditional methods, marking a substantial advancement in the automation of medical diagnosis through artificial intelligence. This work not only sets new performance benchmarks but also highlights the potential of AI in medical diagnostics, offering insights into the future of healthcare technology.},
  keywords={Technological innovation;Pneumonia;Sensitivity;Benchmark testing;Predictive models;Transformers;Natural language processing;Pneumonia Diagnosis;Deep Neural Networks (DNNs);Chest X-ray Analysis;Medical Image Classification;Large Language Models (LLMs)},
  doi={10.1109/SoutheastCon52093.2024.10500260},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{10048054,
  author={True, Pascal and Graef, Thomas and Menge, Matthias},
  booktitle={VDE High Voltage Technology; 4. ETG-Symposium}, 
  title={Detection of partial discharges in high voltage DC cable systems using current pulse waveform analyses and generative adversarial networks}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Since common phase related partial discharge (PRPD) detection techniques used in high voltage (HV) AC systems, can only be used to a limited extend in high voltage DC systems, other techniques are required. One common approach is the current pulse waveform analyses (CPWA). The CPWA is used to extract parameters from the pulse waveform of the partial discharge (PD). The target of these method is to detect, classify, and interpret physical properties of different partial discharges due to waveform parameters like rise time, fall time, pulse width, local charge, energy, etc. Instead of using classical algorithms to extract features from the current pulse waveform and analyses these features in an analytical manner, the signal is directly given to an artificial intelligence (AI) based system. First the measured signal is windowed to fixed size vectors, which supply the input of the AI Network. The Network is able to extract features from the given signal by itself. AI systems need much of measured signals for training. A Generative Adversarial Networks (GAN) trained with a reduced dataset is used to produce additional signals for training of other Neural Networks (NN). The generator of the GAN can later be used as simulation tool, while the discriminator model can be used to decide if a signal is an PD or not. First results applying the GAN principle to PD signals are presented.},
  keywords={},
  doi={},
  ISSN={},
  month={Nov},}@INBOOK{10953467,
  author={Martinez-Ramon, Manel and Ajith, Meenu and Kurup, Aswathy Rajendra},
  booktitle={Deep Learning: A Practical Introduction}, 
  title={Bibliography}, 
  year={2024},
  volume={},
  number={},
  pages={365-386},
  abstract={},
  keywords={},
  doi={10.1002/9781119861898.biblio},
  ISSN={},
  publisher={Wiley},
  isbn={9781119861874},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953467},}@INPROCEEDINGS{11100292,
  author={Yuan, Shuai and Niu, Dan and Zhao, Huatao and Liu, Shiyuan and Jin, Zhou and Sun, Changyin},
  booktitle={2025 International Symposium of Electronics Design Automation (ISEDA)}, 
  title={BBP-DNS: Batch-Block Parallelism and Dual-NoC Scheduling for Accelerating GPT on Edge Devices}, 
  year={2025},
  volume={},
  number={},
  pages={114-119},
  abstract={Transformer-based GPT models in artificial intelligence have exhibited remarkable performance advantages across generative tasks. However, edge-side deployment of GPT models faces significant challenges due to the limited memory and computational resources of edge devices. The lack of specialized compiler toolchains often necessitates manual compilation, resulting in complicated deployment processes. To address these challenges, we propose a Batch-Block Parallelization and Dual-NoC Scheduling algorithm(BBP-DNS) for edge devices, which automates tensor partitioning and batch scheduling to enhance data locality and reduce overheads in data storage, movement, and kernel scheduling, thereby improving inference speed. To mitigate hardware constraints in accelerator memory, bandwidth, and compute resources, we extend tensor parallelism with a fine-grained batch-level scheduling strategy. Additionally, we introduce a novel operator mapping methodology that automates accelerator data management, addressing the inefficiencies of traditional manual compilation workflow. Experimental results demonstrate BBP-DNS’s superior performance, achieving a 30x performance improvement of a GPT single-layer block on simulator of that evaluated by TVM and PyTorch on CPUs.},
  keywords={Performance evaluation;Tensors;Processor scheduling;Computational modeling;Pipelines;Transformers;Scheduling;Partitioning algorithms;Resource management;Parallel algorithms;Neural Network;In-Memory Computing;Network-on-Chip;Model Deployment;Tensor Parallelism},
  doi={10.1109/ISEDA65950.2025.11100292},
  ISSN={},
  month={May},}@INPROCEEDINGS{10297258,
  author={Wang, Minzheng and Cao, Jia and Kong, Qingchao and Luo, Yin},
  booktitle={2023 IEEE International Conference on Intelligence and Security Informatics (ISI)}, 
  title={Boosting Domain-Specific Question Answering Through Weakly Supervised Self-Training}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Question answering systems have emerged as an important research area in the field of natural language processing, enabling the provision of accurate answers to user queries in a more efficient and user-friendly manner. These systems have much practical significance especially in security-related applications, where intelligence analysts can easily access pertinent information from diverse sources to accelerate the decision-making process. In the context of security-related scenarios, users tend to interact with a question answering system with specific domain-oriented purposes. However, most existing question answering systems focus on open-domain situations with abundant labeled data as well as structured data, and domain-specific question answering methods are in urgent need. Domain-specific question answering faces the challenge of the low-resource issue caused by data scarcity. To address this challenge, in this paper, we propose a weakly supervised self-training method for domain-specific question answering based on the Retriever-Reader framework. For the retriever module, during the self-training process, we develop two strategies for generating pseudo-labels to augment the labeled dataset, including high confidence sampling and random negative sampling. For the reader module, we adopt the pre-trained language model and fine-tune the generative reader using limited labeled datasets. To evaluate our proposed method, we construct the first Chinese financial question answering dataset of textual document. Experimental results demonstrate that our proposed method can significantly improve the performances of the baseline method through the self-training process.},
  keywords={Training;Decision making;Boosting;Question answering (information retrieval);Security;Iterative methods;Informatics;question answering system;Retriever-Reader framework;weakly supervised learning;self-training},
  doi={10.1109/ISI58743.2023.10297258},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9589486,
  author={Wen, Xiangyu and Jiang, Wei and Zhan, Jinyu and Bian, Chen and Song, Ziwei},
  booktitle={2021 International Conference on Embedded Software (EMSOFT)}, 
  title={Work-in-Progress: Generative Strategy based Backdoor Attacks to 3D Point Clouds}, 
  year={2021},
  volume={},
  number={},
  pages={23-24},
  abstract={3D deep learning has been applied in safety-critical scenarios, e.g., autonomous driving. Several works have raised the security problems of 3D deep learnings mainly from the perspective of adversarial attacks. In this paper, we propose a novel backdoor attack method to threaten 3D deep learning without the original training data. Several neurons are selected and made sensitive to backdoor triggers. The backdoor triggers are generated by reversing neural network, and the shape of which is constrained to map the objects in the physical world. Sufficient training data can be also generated by reverse engineering. Finally, retraining with the generated 3D trigger and training data is applied to inject backdoors, which is in no need of accessing the original training process and data.},
  keywords={Deep learning;Training;Three-dimensional displays;Shape;Reverse engineering;Neurons;Training data;Computing methodologies→Artificial intelligence},
  doi={},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10152386,
  author={Boukhamla, Assia and Bouziane, Mohamed Hatem and Laib, Akram and Azizi, Nabiha and Rouabhi, Roumaissa and Merah, Aya and Chaib, Rim},
  booktitle={2023 International Conference on Control, Automation and Diagnosis (ICCAD)}, 
  title={GANs Investigation for Multimodal Medical Data Interpretation : Basic Architectures and Overview}, 
  year={2023},
  volume={},
  number={},
  pages={01-06},
  abstract={Medical imaging technologies have drastically changed the way healthcare professionals diagnose and treat patients. However, medical imaging datasets are limited in terms of size and diversity, which can lead to inaccurate diagnosis and treatment plans. To overcome these limitations, Generative Adversarial Networks (GANs) have been used to generate new medical images based on existing datasets. GANs are a type of deep learning artificial intelligence that can use existing datasets to create new synthetic images that look almost indistinguishable from real images. GANs can be used to generate medical images of organs, tissues, and diseases that are not present in existing datasets, providing healthcare professionals with more accurate and diverse datasets to diagnose and treat patients. GANs for medical image synthesis is a powerful tool that can revolutionize the healthcare industry. In this paper an investigation of the most used Gans architecture are presented. a state of the art of the latest works applying Gans are also discussed based on the adopted modality.},
  keywords={Deep learning;Training;Text mining;Industries;Image synthesis;Natural languages;Medical services;Generative Adversarial Networks (GANs);medical imaging;image synthesis;Image generation;medical text mining},
  doi={10.1109/ICCAD57653.2023.10152386},
  ISSN={2767-9896},
  month={May},}@INPROCEEDINGS{10370304,
  author={Dhiman, Neha and Singh, Hakam and Thakur, Abhishek},
  booktitle={2023 International Conference on Electrical, Electronics, Communication and Computers (ELEXCOM)}, 
  title={An Efficient Approach for Image Forgery Detection Using Deep Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The advent of digital platforms revolutionized sharing of personal images and videos on social media. Intruders extract and modify these images to gain popularity or favor. There is freely available software that changes these images very quickly. If the image is forged, the algorithm should detect it and stop to publish on social media. Copy-move and splicing forgery are prevalent methods of image forgery. To combat these forgeries, a real-time deep learning-based approach has been developed, providing enhanced social security in today's society. This approach can effectively detect and identify counterfeit images shared on various social media platforms. The real-time forgery detection algorithms are developed using supervised learning. This algorithm is tested on publicly available datasets such as DVMM, CMFD, and Colombia.},
  keywords={Deep learning;Machine learning algorithms;Social networking (online);Neural networks;Prediction algorithms;Feature extraction;Forgery;Machine Learning (ML);Color Illumination (CI);Convolution Neural Network (CNN);Deep Learning (DL);Splicing Forgery (SF);Copy-move Forgery (CMF)},
  doi={10.1109/ELEXCOM58812.2023.10370304},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10602407,
  author={Gao, Feng and Zhang, Song and You, Zhipeng and Cao, Sen},
  booktitle={2024 IEEE 2nd International Conference on Power Science and Technology (ICPST)}, 
  title={Fault Diagnosis of AC Transmission System via GoogleNet}, 
  year={2024},
  volume={},
  number={},
  pages={1052-1057},
  abstract={The application of intelligent detection means in AC transmission systems is crucial to ensure the reliable operation of power systems and improve the quality of power supply. The traditional detection methods rely on human expertise and prior knowledge, which may not be efficient in complex transmission systems. Therefore, it is essential to develop an intelligent detection method that can automatically learn from the system's behavior and detect potential faults. To address this issue, we propose an intelligent fault detection method based on machine learning. This method relies on the abnormal operation database of AC transmission systems and combines it with the experience of first-line power grid technicians. It forms a set of reliable intelligent knowledge graph fault detection methods. Firstly, the substation fault recording data (single-phase grounding, relative phase short-circuit, valve short-circuit) is converted into two-dimensional time-frequency images using wavelet transform. Then, data enhancement technology is used to expand the fault sample data. Finally, the GoogleNet network is employed for AC transmission system fault diagnosis. To verify the effectiveness of the proposed method, we compared its performance with traditional methods such as KNN and decision tree. The test results demonstrate that the classification accuracy of GoogleNet on the training set is 93%, and the classification accuracy on the test set is 82%. These results suggest that the proposed method is an effective solution for AC transmission system fault diagnosis. The proposed intelligent fault detection method based on machine learning can effectively identify faults in AC transmission systems. It combines the experience of power grid technicians with an advanced machine learning algorithm to form a reliable and efficient fault detection system. The experimental results demonstrate the high reliability and good effectiveness of GoogleNet in AC transmission system fault diagnosis. Therefore, this method has the potential to improve power system reliability and enhance power quality.},
  keywords={Fault diagnosis;Wavelet transforms;Training;Time-frequency analysis;Image recognition;Fault detection;Valves;GoogleNet;AC Transmission System;Fault Diagnosis},
  doi={10.1109/ICPST61417.2024.10602407},
  ISSN={},
  month={May},}@INPROCEEDINGS{10699123,
  author={Zhang, Zhenyu},
  booktitle={2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={Big Data Based Network Security Awareness Model using Modified Lion Swarm Optimization with Fully Convolutional Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Globally, Cyber security has become a major issue because of growing security risks in networks, the Internet, websites, and organizations. So, it requires an advanced Intrusion Detection Systems (IDS) to protect sensitive data and vital infrastructure against such security risks. In prior researches, to identify online threats or network attacks, various network security perception models have been created utilizing Machine Learning (ML) techniques. But such systems fall short in identifying unusual attacks/assaults frequently. For efficient intrusion detection in big-data circumstances, this research proposed a new Deep Learning (DL) based network security perception model termed Modified Lion Swarm Optimisation with Fully Convolutional Neural Network (MLSO-FCNN). The present research employed the CICIDS-2019 dataset and makes use of Independent Component Analysis (ICA) and z-score normalization to enhance discriminative capacity and extract features. When compared to traditional methods such as CNN, Deep Neural Networks (DNNs) Swarm Intelligent FCNN (SIFCNN), the MLSO-FCNN method achieves high intrusion detection accuracy of ${99.98\%}$.},
  keywords={Accuracy;Transfer learning;Intrusion detection;Organizations;Network security;Feature extraction;Data models;Convolutional neural networks;Particle swarm optimization;Optimization;big data;cyber security;fully convolutional neural network;network security perception model;modified lion swarm optimization},
  doi={10.1109/NMITCON62075.2024.10699123},
  ISSN={},
  month={Aug},}@ARTICLE{11127007,
  author={Zhang, Quanshi and Zhang, Hao and Chen, Yiting and Ren, Qihan and Ren, Jie and Cheng, Xu and Xiang, Liyao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Interpretable Rotation-Equivariant Multiary-Valued Network for Attribute Obfuscation}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={This paper focuses on the problem of preventing information leakage in neural networks, i.e., assuming that attackers have obtained intermediate-layer features of a neural network, and preventing attackers from inverting these features to the input with private information. We propose a generic method to slightly revise each arbitrary traditional neural network into a multiary-valued rotation-equivariant neural network (RENN) for preventing information leakage. Specifically, we convert realvalued features in the network into multi-ary features, and each element in the feature vector is a multi-ary number. We hide the input information into a certain phase of the multi-ary feature, and rotate the multi-ary feature for attribute obfuscation in the encryption process. The rotation axis and angle can be considered as the private key. In this way, even when attackers have obtained network parameters and intermediate-layer features, they still cannot extract input information without knowing the rotation information. More crucially, the encryption operation does not damage the spatial correlations between features, so that the encrypted features can be easily processed by convolution operations in the neural network without difficulties. In order to implement successful encryption and decryption, the RENN is designed to satisfy the rotation equivariance property. To this end, we propose a set of rules to revise classic operations in the neural network to ensure the rotation equivariance property. Besides, we prove that the $d$-ary RENN is downward compatible with the $d^{\prime }$-ary RENN when $d^{\prime } \lt d$. In experiments, the RENN significantly boosts the capacity of preventing information leakage, yet with only mild degradation of classification accuracy, compared to traditional neural networks. Besides, the computational cost is much less than the homomorphic encryption.},
  keywords={Neural networks;Information leakage;Feature extraction;Cryptography;Quaternions;Privacy;Protection;Computational efficiency;Training;Homomorphic encryption;Convolutional Neural Networks;privacy protection;preventing information leakage},
  doi={10.1109/TPAMI.2025.3599592},
  ISSN={1939-3539},
  month={},}@ARTICLE{9633236,
  author={Li, Jing and Huang, Qingwang and Du, Yingjun and Zhen, Xiantong and Chen, Shengyong and Shao, Ling},
  journal={IEEE Transactions on Image Processing}, 
  title={Variational Abnormal Behavior Detection With Motion Consistency}, 
  year={2022},
  volume={31},
  number={},
  pages={275-286},
  abstract={Abnormal crowd behavior detection has recently attracted increasing attention due to its wide applications in computer vision research areas. However, it is still an extremely challenging task due to the great variability of abnormal behavior coupled with huge ambiguity and uncertainty of video contents. To tackle these challenges, we propose a new probabilistic framework named variational abnormal behavior detection (VABD), which can detect abnormal crowd behavior in video sequences. We make three major contributions: (1) We develop a new probabilistic latent variable model that combines the strengths of the U-Net and conditional variational auto-encoder, which also are the backbone of our model; (2) We propose a motion loss based on an optical flow network to impose the motion consistency of generated video frames and input video frames; (3) We embed a Wasserstein generative adversarial network at the end of the backbone network to enhance the framework performance. VABD can accurately discriminate abnormal video frames from video sequences. Experimental results on UCSD, CUHK Avenue, IITB-Corridor, and ShanghaiTech datasets show that VABD outperforms the state-of-the-art algorithms on abnormal crowd behavior detection. Without data augmentation, our VABD achieves 72.24% in terms of AUC on IITB-Corridor, which surpasses the state-of-the-art methods by nearly 5%.},
  keywords={Feature extraction;Probabilistic logic;Video sequences;Image reconstruction;Anomaly detection;Training;Optical losses;Abnormal crowd behavior detection;conditional variational auto-encoder;optical flow network;motion loss;Wasserstein generative adversarial network},
  doi={10.1109/TIP.2021.3130545},
  ISSN={1941-0042},
  month={},}@ARTICLE{9158525,
  author={Shi, Lukui and Wang, Ziyuan and Pan, Bin and Shi, Zhenwei},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={An End-to-End Network for Remote Sensing Imagery Semantic Segmentation via Joint Pixel- and Representation-Level Domain Adaptation}, 
  year={2021},
  volume={18},
  number={11},
  pages={1896-1900},
  abstract={It requires pixel-by-pixel annotations to obtain sufficient training data in supervised remote sensing image segmentation, which is a quite time-consuming process. In recent years, a series of domain-adaptation methods was developed for image semantic segmentation. In general, these methods are trained on the source domain and then validated on the target domain to avoid labeling new data repeatedly. However, most domain-adaptation algorithms only tried to align the source domain and the target domain in the pixel level or the representation level, while ignored their cooperation. In this letter, we propose an unsupervised domain-adaptation method by Joint Pixel and Representation level Network (JPRNet) alignment. The major novelty of the JPRNet is that it achieves joint domain adaptation in an end-to-end manner, so as to avoid the multisource problem in the remote sensing images. JPRNet is composed of two branches, each of which is a generative-adversarial network (GAN). In one branch, pixel-level domain adaptation is implemented by the style transfer with the Cycle GAN, which could transfer the source domain to a target domain. In the other branch, the representation-level domain adaptation is realized by adversarial learning between the transferred source-domain images and the target-domain images. The experimental results on the public data sets have indicated the effectiveness of the JPRNet.},
  keywords={Gallium nitride;Image segmentation;Semantics;Remote sensing;Feature extraction;Adaptation models;Training;Domain adaptation;generative-adversarial network (GAN);remote sensing;semantic segmentation},
  doi={10.1109/LGRS.2020.3010591},
  ISSN={1558-0571},
  month={Nov},}@ARTICLE{9760725,
  author={Tan, Hongchen and Liu, Xiuping and Yin, Baocai and Li, Xin},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={DR-GAN: Distribution Regularization for Text-to-Image Generation}, 
  year={2023},
  volume={34},
  number={12},
  pages={10309-10323},
  abstract={This article presents a new text-to-image (T2I) generation model, named distribution regularization generative adversarial network (DR-GAN), to generate images from text descriptions from improved distribution learning. In DR-GAN, we introduce two novel modules: a semantic disentangling module (SDM) and a distribution normalization module (DNM). SDM combines the spatial self-attention mechanism (SSAM) and a new semantic disentangling loss (SDL) to help the generator distill key semantic information for the image generation. DNM uses a variational auto-encoder (VAE) to normalize and denoise the image latent distribution, which can help the discriminator better distinguish synthesized images from real images. DNM also adopts a distribution adversarial loss (DAL) to guide the generator to align with normalized real image distributions in the latent space. Extensive experiments on two public datasets demonstrated that our DR-GAN achieved a competitive performance in the T2I task. The code link: https://github.com/Tan-H-C/DR-GAN-Distribution-Regularization-for-Text-to-Image-Generation.},
  keywords={Semantics;Generators;Task analysis;Image synthesis;Training;Visualization;Stability analysis;Adversarial machine learning;Text processing;Distribution normalization;generative adversarial network;semantic disentanglement mechanism;text-to-image (T2I) generation},
  doi={10.1109/TNNLS.2022.3165573},
  ISSN={2162-2388},
  month={Dec},}@ARTICLE{10023990,
  author={Tan, Hongchen and Yin, Baocai and Wei, Kun and Liu, Xiuping and Li, Xin},
  journal={IEEE Transactions on Multimedia}, 
  title={ALR-GAN: Adaptive Layout Refinement for Text-to-Image Synthesis}, 
  year={2023},
  volume={25},
  number={},
  pages={8620-8631},
  abstract={We propose a novel Text-to-Image Generation Network, Adaptive Layout Refinement Generative Adversarial Network (ALR-GAN), to adaptively refine the layout of synthesized images without any auxiliary information. The ALR-GAN includes an Adaptive Layout Refinement (ALR) module and a Layout Visual Refinement (LVR) loss. The ALR module aligns the layout structure (which refers to locations of objects and background) of a synthesized image with that of its corresponding real image. In ALR module, we proposed an Adaptive Layout Refinement (ALR) loss to balance the matching of hard and easy features, for more efficient layout structure matching. Based on the refined layout structure, the LVR loss further refines the visual representation within the layout area. Experimental results on two widely-used datasets show that ALR-GAN performs competitively at the Text-to-Image generation task.},
  keywords={Layout;Semantics;Visualization;Task analysis;Training;Generators;Adaptation models;Generative adversarial network;text-to-image synthesis;information consistency constraint;object layout refinement},
  doi={10.1109/TMM.2023.3238554},
  ISSN={1941-0077},
  month={},}@ARTICLE{9915620,
  author={Liu, Yunfan and Li, Qi and Deng, Qiyao and Sun, Zhenan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Towards Spatially Disentangled Manipulation of Face Images With Pre-Trained StyleGANs}, 
  year={2023},
  volume={33},
  number={4},
  pages={1725-1739},
  abstract={Generative Adversarial Networks with style-based generators could successfully synthesize realistic images from input latent code. Moreover, recent studies have revealed that interpretable translations of generated images could be obtained by linearly traversing in the latent space. However, in most existing latent spaces, linear interpolation often leads to ‘spatially entangled modification’ in the manipulation result, which is undesirable in many real-world applications where local editing is required. To solve this problem, we propose to manipulate the latent code in the ‘style space’ and analyze its advantage in achieving spatial disentanglement. Furthermore, we point out the weakness of simply interpolating in the style space and propose ‘Style Intervention’, a lightweight optimization-based algorithm, to further improve the visual fidelity of manipulation results. The performance of our method is verified with the task of attribute editing on high-resolution face images. Both qualitative and quantitative results demonstrate the advantage of image translation in the style space and the effectiveness of our method on both real and synthetic images.},
  keywords={Codes;Generators;Semantics;Aerospace electronics;Faces;Visualization;Space exploration;Generative adversarial networks;style-based generators;facial attribute manipulation},
  doi={10.1109/TCSVT.2022.3213662},
  ISSN={1558-2205},
  month={April},}@INPROCEEDINGS{9207487,
  author={Xu, Rui and Yan, Weizhong},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Comparison of GANs-Based Approaches for Combustor System Fault Detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={In manufacturing industry, anomaly detection (AD) has been widely applied to monitor asset operation status and provide decision support for proactive maintenance. However, the extreme complexities of many industrial assets have raised many challenges to the classical anomaly detection approaches. For the past several years, generative adversarial networks (GANs) have achieved breakthrough in a variety of applications, such as image generation and video prediction. Some initial applications of GANs in image-related AD problems also show promising results. In this paper, we investigated the performance of three GANs-based anomaly detection approaches for a specific industrial use case - fault detection of the combustion system of gas turbines in power plants. The results show that under the framework of semi-supervised learning, the three approaches do not perform well on the one-year field data collected from a gas turbine. However, if a small portion of fault data is provided for training, we observe that the performance of GANs is significantly improved.},
  keywords={Gallium nitride;Generators;Turbines;Anomaly detection;Cost function;Training;Generative adversarial networks (GANs);anomaly detection;semi-supervised learning;supervised learning;combustor system;gas turbine;manufacturing industry},
  doi={10.1109/IJCNN48605.2020.9207487},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10375579,
  author={Tan, Hongchen and Yin, Baocai and Xu, Kaiqiang and Wang, Huasheng and Liu, Xiuping and Li, Xin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Attention-Bridged Modal Interaction for Text-to-Image Generation}, 
  year={2024},
  volume={34},
  number={7},
  pages={5400-5413},
  abstract={We propose a novel Text-to-Image Generation Network, Attention-bridged Modal Interaction Generative Adversarial Network (AMI-GAN), to better explore modal interaction and perception for high-quality image synthesis. The AMI-GAN contains two novel designs: an Attention-bridged Modal Interaction (AMI) module and a Residual Perception Discriminator (RPD). In AMI, we mainly design a multi-scale attention mechanism to exploit semantics alignment, fusion, and enhancement between text and image, to better refine details and context semantics of the synthesized image. In RPD, we design a multi-scale information perception mechanism with our proposed novel information adjustment function, to encourage the discriminator to better perceive visual differences between the real and synthesized image. Consequently, the discriminator will drive the generator to improve the visual quality of the synthesized image. Besides, based on these novel designs, we can design two versions, a single-stage generation framework (AMI-GAN-S), and a multi-stage generation framework (AMI-GAN-M), respectively. The former can synthesize high-resolution images because of its low computational cost; the latter can synthesize images with realistic detail. Experimental results on two widely used T2I datasets showed that our AMI-GANs achieve competitive performance in T2I task.},
  keywords={Semantics;Task analysis;Visualization;Computational modeling;Image synthesis;Generators;Layout;Generative adversarial network;text-to-image synthesis;attention-bridged modal interaction;residual perception discriminator},
  doi={10.1109/TCSVT.2023.3347971},
  ISSN={1558-2205},
  month={July},}@INPROCEEDINGS{10146669,
  author={Kumar, Aman and Rizvi, Danish Raza},
  booktitle={2023 1st International Conference on Intelligent Computing and Research Trends (ICRT)}, 
  title={Knowledge Weightage Calculation: an AI & ML based smart modelling for text- content summarization and quantification}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The need for naturalistic interaction and communication between humans and machines is essential for high-level artificial intelligence. Matured Natural Language Processing is the first step towards attaining artificial intelligence by automation equal to human beings. It has been a hot and active area of research among researchers for a long. In the advanced forms of natural language processing-based intelligent application building must-have capabilities of generating summaries, understanding context, representing and combining multiple information, and knowledge extraction. Artificial Intelligence and various computing ways (Statistical Reasoning, Machine Learning, and Deep Learning) for achieving it, have been proven the next big weapon for attaining a true sense of intelligent automation and smart system building. Many tasks related to natural language processing have shown consistent and comparable high performances since the introduction of deep learning modelling. Using sequence-to-sequence models has been advantageous and demonstrated promising outcomes in many other areas, including machine translation, speech recognition, picture captioning, etc. This paper aims to propose a novel framework to extract important features from an unstructured text by removing the noisy, irrelevant, and redundant components, and to provide accountability for the information delivered by retrieving image-based text, deep text summarization by knowledge weight calculation, and representation- based on the extracted features.},
  keywords={Deep learning;Intelligent automation;Weapons;Speech recognition;Feature extraction;Market research;Data mining;nlp;deep text summarization;deep learning;knowledge weight calculation;natural language processing},
  doi={10.1109/ICRT57042.2023.10146669},
  ISSN={},
  month={Feb},}@ARTICLE{11151764,
  author={Rao, Wenhao and Guo, Jiayang and Zhu, Chunran and Xu, Meiyan and Xiao, Naian and Pan, Yijie and Zhang, Ling and Ye, Xiaowen and Jiang, Jun and Wang, Xiaolu and Gu, Peipei and Chen, Duo},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Interictal Epileptiform Discharge Detection using Dual-domain Features and GAN}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Interictal Epileptiform Discharge is essential for identifying epilepsy. However, the unpredictable and non-stationary nature of electroencephalogram (EEG) patterns poses considerable challenges for reliable identification. Manual interpretation of EEG is subjective and time-consuming. With advancements in machine learning and deep learning, computer-aided approaches for automated IED detection have been rapidly developed. The state-of-the-art convolutional neural network (CNN)-based methods have shown promising results but struggle to capture long-term dependencies in time-series data. In contrast, Transformer excels at modeling sequential information through self-attention mechanisms, overcoming the CNN limitations. This study proposes an IED Detector (IEDD) that integrates convolutional layers and a Transformer to detect IEDs. The IEDD initially employs convolutional layers to extract local features of IEDs, followed by a Transformer to model long-term dependencies. To further extract spatial features, EEG data are represented as a three-dimensional tensor with embedded channel topology, where a CNN captures spatial features at each sampling point and a Long Short-Term Memory (LSTM) network models their temporal evolution. Additionally, due to the scarcity of IED data, a novel Transformer-based Generative Adversarial Network (GAN) is developed to augment the IED dataset. Experimental results show the proposed approach achieves an average accuracy of $96.11\%$ on the augmented Dataset 1 and $95.25\%$ on Dataset 2 for binary classification, with an average sensitivity of $87.26\%$ and precision of $89.96\%$ for multi-label classification. These findings provide valuable insights into advancing deep learning and Transformer-based approaches for automated IED detection.},
  keywords={Electroencephalography;Epilepsy;Feature extraction;Brain modeling;Transformers;Sleep;Hospitals;Convolutional neural networks;Long short term memory;Recording;Dual-domain features;Generative adversarial network;IED detection;LSTM;Transformer},
  doi={10.1109/JBHI.2025.3605257},
  ISSN={2168-2208},
  month={},}@INBOOK{9950409,
  author={},
  booktitle={Big Data Analytics and Intelligent Systems for Cyber Threat Intelligence}, 
  title={11 Detecting High-quality GAN-generated Face Images using Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={235-252},
  abstract={In recent years, a considerable amount of effort has been devoted to cyber-threat protection of computer systems which is one of the most critical cybersecurity tasks for single users and businesses since even a single attack can result in compromised data and sufficient losses. Massive losses and frequent attacks dictate the need for accurate and timely detection methods. Current static and dynamic methods do not provide efficient detection, especially when dealing with zero-day attacks. For this reason, big data analytics and machine intelligence-based techniques can be used. This book brings together researchers in the field of big data analytics and intelligent systems for cyber threat intelligence CTI and key data to advance the mission of anticipating, prohibiting, preventing, preparing, and responding to internal security. The wide variety of topics it presents offers readers multiple perspectives on various disciplines related to big data analytics and intelligent systems for cyber threat intelligence applications. Technical topics discussed in the book include: &#x2022; Big data analytics for cyber threat intelligence and detection &#x2022; Artificial intelligence analytics techniques &#x2022; Real-time situational awareness &#x2022; Machine learning techniques for CTI &#x2022; Deep learning techniques for CTI &#x2022; Malware detection and prevention techniques &#x2022; Intrusion and cybersecurity threat detection and analysis &#x2022; Blockchain and machine learning techniques for CTI},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770227773},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9950409},}@ARTICLE{9052758,
  author={Lin, Yutian and Wu, Yu and Yan, Chenggang and Xu, Mingliang and Yang, Yi},
  journal={IEEE Transactions on Image Processing}, 
  title={Unsupervised Person Re-identification via Cross-Camera Similarity Exploration}, 
  year={2020},
  volume={29},
  number={},
  pages={5481-5490},
  abstract={Most person re-identification (re-ID) approaches are based on supervised learning, which requires manually annotated data. However, it is not only resource-intensive to acquire identity annotation but also impractical for large-scale data. To relieve this problem, we propose a cross-camera unsupervised approach that makes use of unsupervised style-transferred images to jointly optimize a convolutional neural network (CNN) and the relationship among the individual samples for person re-ID. Our algorithm considers two fundamental facts in the re-ID task, i.e., variance across diverse cameras and similarity within the same identity. In this paper, we propose an iterative framework which overcomes the camera variance and achieves across-camera similarity exploration. Specifically, we apply an unsupervised style transfer model to generate style-transferred training images with different camera styles. Then we iteratively exploit the similarity within the same identity from both the original and the style-transferred data. We start with considering each training image as a different class to initialize the Convolutional Neural Network (CNN) model. Then we measure the similarity and gradually group similar samples into one class, which increases similarity within each identity. We also introduce a diversity regularization term in the clustering to balance the cluster distribution. The experimental results demonstrate that our algorithm is not only superior to state-of-the-art unsupervised re-ID approaches, but also performs favorably compared with other competing unsupervised domain adaptation methods (UDA) and semi-supervised learning methods.},
  keywords={Cameras;Training;Annotations;Task analysis;Generative adversarial networks;Manuals;Measurement},
  doi={10.1109/TIP.2020.2982826},
  ISSN={1941-0042},
  month={},}@ARTICLE{9552486,
  author={Zheng, Aihua and Wang, Ming and Li, Chenglong and Tang, Jin and Luo, Bin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Entropy Guided Adversarial Domain Adaptation for Aerial Image Semantic Segmentation}, 
  year={2022},
  volume={60},
  number={},
  pages={1-14},
  abstract={Recent advances on aerial image semantic segmentation mainly employ the domain adaption to transfer knowledge from the source domain to the target domain. Despite the remarkable achievement, most methods focus on the global marginal distribution alignment to reduce the domain shift between source and target domains, leading to a wrong mapping of the well-aligned features. In this article, we propose an effective unsupervised domain adaptation approach, which relies on a novel entropy guided adversarial learning algorithm, for aerial image semantic segmentation. In specific, we perform local feature alignment between domains by learning a self-adaptive weight from the target prediction probability map to measure the interdomain discrepancy. To exploit the meaningful structure information among semantic regions, we propose to utilize the graph convolutions for long-range semantic reasoning. Comprehensive experimental results on the benchmark dataset of aerial image semantic segmentation and natural scenes demonstrate the superior performance of the proposed method compared to the state-of-the-art methods.},
  keywords={Semantics;Image segmentation;Adversarial machine learning;Entropy;Generative adversarial networks;Adaptation models;Layout;Aerial image;graph convolutional network (GCN);information entropy;semantic segmentation;unsupervised domain adaptation (UDA)},
  doi={10.1109/TGRS.2021.3113581},
  ISSN={1558-0644},
  month={},}@ARTICLE{10605126,
  author={Yang, Bin and Hu, Yuxuan and Liu, Xiaowen and Li, Jing},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={CEFusion: An Infrared and Visible Image Fusion Network Based on Cross-Modal Multi-Granularity Information Interaction and Edge Guidance}, 
  year={2024},
  volume={25},
  number={11},
  pages={17794-17809},
  abstract={Infrared and visible image fusion (IVF) aims to generate a fused image with abundant texture details and salient thermal radiation targets, which can not only preserve the necessary scene information for traffic vision tasks, but also highlight the imperceptible targets that are crucial in intelligent transportation system (ITS). However, the existing image fusion methods often lack the information interactions between cross-modal features and among cross-granularity features, and they usually ignore the importance of edge information to the image, which affects the quality of the fused image. To this end, this study proposes an IVF network based on cross-modal multi-granularity information interaction and edge guidance, termed as CEFusion. On the one hand, a triple-branch scene fidelity module is designed to fuse the different modal features extracted by the encoder. This module can adequately mine difference information and infrared salient information of the cross-modal features through cross-modal information interaction. On the other hand, a progressive cross-granularity interaction feature enhancement module is employed to achieve the information interaction among cross-granularity features, which can further enrich the texture and structure information in the fused features. In addition, a novel edge loss function is proposed to guide the network to retain the edge information from source images. Extensive comparative and generalization experiments demonstrate that our CEFusion superior to the state-of-the-art methods in preserving texture details and thermal radiation targets. More importantly, the performance of our method in the high-level vision task suggests that it can provide reliable assistance for the ITS applications.},
  keywords={Feature extraction;Image fusion;Image edge detection;Transformers;Generative adversarial networks;Intelligent transportation systems;Information management;Image analysis;Intelligent transportation systems;image fusion;information interaction;edge guidance;scene fidelity},
  doi={10.1109/TITS.2024.3426539},
  ISSN={1558-0016},
  month={Nov},}@ARTICLE{10063231,
  author={Lyu, Jun and Li, Guangyuan and Wang, Chengyan and Cai, Qing and Dou, Qi and Zhang, David and Qin, Jing},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multicontrast MRI Super-Resolution via Transformer-Empowered Multiscale Contextual Matching and Aggregation}, 
  year={2024},
  volume={35},
  number={9},
  pages={12004-12014},
  abstract={Magnetic resonance imaging (MRI) possesses the unique versatility to acquire images under a diverse array of distinct tissue contrasts, which makes multicontrast super-resolution (SR) techniques possible and needful. Compared with single-contrast MRI SR, multicontrast SR is expected to produce higher quality images by exploiting a variety of complementary information embedded in different imaging contrasts. However, existing approaches still have two shortcomings: 1) most of them are convolution-based methods and, hence, weak in capturing long-range dependencies, which are essential for MR images with complicated anatomical patterns and 2) they ignore to make full use of the multicontrast features at different scales and lack effective modules to match and aggregate these features for faithful SR. To address these issues, we develop a novel multicontrast MRI SR network via transformer-empowered multiscale feature matching and aggregation, dubbed McMRSR $^{++}$ . First, we tame transformers to model long-range dependencies in both reference and target images at different scales. Then, a novel multiscale feature matching and aggregation method is proposed to transfer corresponding contexts from reference features at different scales to the target features and interactively aggregate them. Furthermore, a texture-preserving branch and a contrastive constraint are incorporated into our framework for enhancing the textural details in the SR images. Experimental results on both public and clinical in vivo datasets show that McMRSR $^{++}$  outperforms state-of-the-art methods under peak signal to noise ratio (PSNR), structure similarity index measure (SSIM), and root mean square error (RMSE) metrics significantly. Visual results demonstrate the superiority of our method in restoring structures, demonstrating its great potential to improve scan efficiency in clinical practice.},
  keywords={Magnetic resonance imaging;Iron;Transformers;Feature extraction;Superresolution;Image restoration;Generative adversarial networks;Feature matching and aggregation;multicontrast magnetic resonance imaging (MRI);super-resolution (SR);transformers},
  doi={10.1109/TNNLS.2023.3250491},
  ISSN={2162-2388},
  month={Sep.},}@ARTICLE{10032807,
  author={Blázquez-García, Ane and Wickstrøm, Kristoffer and Yu, Shujian and Mikalsen, Karl Øyvind and Boubekki, Ahcène and Conde, Angel and Mori, Usue and Jenssen, Robert and Lozano, Jose A.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Selective Imputation for Multivariate Time Series Datasets With Missing Values}, 
  year={2023},
  volume={35},
  number={9},
  pages={9490-9501},
  abstract={Multivariate time series often contain missing values for reasons such as failures in data collection mechanisms. Since these missing values can complicate the analysis of time series data, imputation techniques are typically used to deal with this issue. However, the quality of the imputation directly affects the performance of downstream tasks. In this paper, we propose a selective imputation method that identifies a subset of timesteps with missing values to impute in a multivariate time series dataset. This selection, which will result in shorter and simpler time series, is based on both reducing the uncertainty of the imputations and representing the original time series as good as possible. In particular, the method uses multi-objective optimization techniques to select the optimal set of points, and in this selection process, we leverage the beneficial properties of the Multi-task Gaussian Process (MGP). The method is applied to different datasets to analyze the quality of the imputations and the performance obtained in downstream tasks, such as classification or anomaly detection. The results show that much shorter and simpler time series are able to maintain or even improve both the quality of the imputations and the performance of the downstream tasks.},
  keywords={Time series analysis;Task analysis;Uncertainty;Machine learning;Probabilistic logic;Gaussian processes;Multitasking;Multivariate time series;missing data;imputation;irregular sampling},
  doi={10.1109/TKDE.2023.3240858},
  ISSN={1558-2191},
  month={Sep.},}@ARTICLE{9309366,
  author={Fang, Jie and Cao, Xiaoqian and Wang, Dianwei and Xu, Shengjun},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Multitask Learning Mechanism for Remote Sensing Image Motion Deblurring}, 
  year={2021},
  volume={14},
  number={},
  pages={2184-2193},
  abstract={As a fundamental preprocessing technique, remote sensing image motion deblurring is important for visual understanding tasks. Most conventional approaches formulate the image motion deblurring task as a kernel estimation. Because the kernel estimation is a highly ill-posed problem, many priors have been applied to model the images and kernels. Even though these methods have obtained relatively better performances, they are usually time-consuming and not robust for different conditions. To address this problem, we propose a multitask learning mechanism for remote sensing image motion deblurring in this article, which contains an image restoration subtask and an image texture complexity recognition one. First, we consider the image motion deblurring problem as a domain transformation problem, from the blurred domain to a clear one. Specifically, the blurred domain represents the data space consisted of blurring images, and the definition of clear domain is similar. Second, we design a novel weighted attention map loss to enhance the reconstruction capability of the restoration subbranch for difficult local regions. Third, based on the restoration subbranch, a recognition subbranch is incorporated into the framework to guide the deblurring process, which provides the auxiliary texture complexity information to help the optimization of restoration subbranch. Additionally, in order to optimize the proposed network, we construct three large-scale datasets, and each sample in the dataset contains a clear image, a blurred image, and its texture label obtained by corresponding texture complexity. Finally, the experimental results on three constructed datasets demonstrate the robustness and the effectiveness of the proposed method.},
  keywords={Complexity theory;Image restoration;Task analysis;Kernel;Remote sensing;Mathematical model;Generative adversarial networks;Domain transformation;image deblurring;multitask learning mechanism},
  doi={10.1109/JSTARS.2020.3047636},
  ISSN={2151-1535},
  month={},}
