@INPROCEEDINGS{10899902,
  author={Hu, Zhuohuan and Yu, Richard and Zhang, Zizhou and Zheng, Haoran and Liu, Qianying and Zhou, Yining},
  booktitle={2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)}, 
  title={Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={973-977},
  abstract={This paper leverages machine learning algorithms to forecast and analyze financial time series. The process begins with a denoising autoencoder to filter out random noise fluctuations from the main contract price data. Then, one-dimensional convolution reduces the dimensionality of the filtered data and extracts key information. The filtered and dimensionality-reduced price data is fed into a GANs network, and its output serve as input of a fully connected network. Through cross-validation, a model is trained to capture features that precede large price fluctuations. The model predicts the likelihood and direction of significant price changes in real-time price sequences, placing trades at moments of high prediction accuracy. Empirical results demonstrate that using autoencoders and convolution to filter and denoise financial data, combined with GANs, achieves a certain level of predictive performance, validating the capabilities of machine learning algorithms to discover underlying patterns in financial sequences.},
  keywords={Analytical models;Machine learning algorithms;Accuracy;Autoencoders;Time series analysis;Predictive models;Filtering algorithms;Information filters;Real-time systems;Cryptocurrency;CNN;GANs;Cryptocurrency;Prediction},
  doi={10.1109/ICAIRC64177.2024.10899902},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10420967,
  author={Guna, Priya},
  booktitle={2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)}, 
  title={Enhancing Image Super-Resolution with Convolutional Neural Network Ensembles}, 
  year={2023},
  volume={},
  number={},
  pages={881-885},
  abstract={The goal of image super-resolution (SR) is to increase the resolution and quality of low-resolution images, and it is a crucial problem in computer vision. Recent years have seen incredible progress in SR problems using Convolutional Neural Networks (CNNs). However, boosting SR performance even further remains difficult. Using the strength of Convolutional Neural Network ensembles, this research introduces a new method for image super-resolution. By pooling the advantages of several different CNN models, we may produce an ensemble that is more accurate and robust than any of the individual models. Here, we present a flexible ensemble framework for integrating multiple SR designs into a single system for comprehensive problem-solving. We show that our ensemble approach improves image super-resolution outcomes through a thorough experimental assessment on industry-standard benchmark datasets. We also explore other methods for training and fusing the ensemble members, such as consensus-based methods and adaptive weighting schemes. To achieve optimal SR performance, these methods aim to collect synergistic data from multiple independently operating networks. Our ensemble-based method not only achieves state-of-the-art super-resolution results, but it also shows resistance to noise and other degradation factors. To further illuminate the process of image super-resolution, we also offer some information on the interpretability of ensemble decisions. this research offers a fresh and efficient approach to improving image super-resolution using ensembles of Convolutional Neural Networks. Our method not only improves the current state of the art in SR, but it also points to a promising way for making super-resolution methods more robust and reliable in real-world settings.},
  keywords={Image quality;Degradation;Computational modeling;Superresolution;Robustness;Convolutional neural networks;Ensemble learning;image super-resolution;convolutional neural network;component;Ensembles;deep learning},
  doi={10.1109/ICCSAI59793.2023.10420967},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10674420,
  author={Yuan, Min and Zhang, Yixiong and Shi, Jianghong},
  booktitle={2024 IEEE 4th International Conference on Software Engineering and Artificial Intelligence (SEAI)}, 
  title={A Two-Stage Model for Self-Supervised Interference Suppression of Automotive Radar in Autonomous Driving}, 
  year={2024},
  volume={},
  number={},
  pages={337-342},
  abstract={Automotive radar is widely used in autonomous driving and advanced driver assistance systems. With the dense deployment of automotive radars on traffic roads, mutual inter-ference is unavoidable and endangers driving safety. In recent studies, sparse methods based on morphological component analysis (MCA) have been utilized to suppress the mutual interference of automotive radars. However, the energy distribution is hard to balance when the signal is decomposed and reconstructed. Thus, it negatively affects the radar signal's morphological characteristics, leading to residual interference, signal loss, and target missing. To solve the problem, a two-stage model for self-supervised interference suppression is proposed based on the temporal convolutional denoising autoencoder (TCDAE) and the finely tuned U-net. In the first stage, the target echo distorted by the interfering signal is preprocessed by the adaptive interference part elimination (AIE) module. Then, the TCDAE module is used for signal imputation to maintain the signal energy as much as possible. In the second stage, the imputed target echo is transmitted into the range-Doppler (RD) map using the RD map generation module. The finely tuned U-net separates the target from the RD map distorted by interference. The real-data experiment on the road implies that the proposed method outperforms the state-of-the-art (SOTA) methods. The target can be recovered and the increased signal-to-interference plus noise ratio (SINR) of the desired target is 40 dB.},
  keywords={Interference suppression;Training;Convolution;Roads;Frequency-domain analysis;Noise reduction;Radar;automotive radar;interference suppression;temporal convolutional denoising autoencoder;U-net},
  doi={10.1109/SEAI62072.2024.10674420},
  ISSN={},
  month={June},}@INPROCEEDINGS{10762296,
  author={Xu, Xinhe and Wang, Zhuoer and Zhang, Yihan and Liu, Yizhou and Wang, Zhaoyue and Xu, Zhihao and Zhao, Muhan and Luo, Huaiying},
  booktitle={2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={Style Transfer: From Stitching to Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={526-530},
  abstract={This article compares two style transfer methods in image processing: the traditional method, which synthesizes new images by stitching together small patches from existing pattern images, and a modern machine learning-based approach that uses a segmentation network to isolate foreground objects and apply style transfer solely to the background. The traditional method excels in creating artistic abstractions but can struggle with seamlessness, whereas the machine learning method preserves the integrity of foreground elements while enhancing the background, offering improved aesthetic quality and computational efficiency. Our study indicates that machine learning-based methods are more suited for real-world applications where detail preservation in foreground elements is essential.},
  keywords={Learning systems;Visualization;Image segmentation;Image color analysis;Neural networks;Machine learning;Coherence;Computational efficiency;Software engineering;Object Segmentation;Style Transfer;Color Transfer;Texture Transfer},
  doi={10.1109/ICBASE63199.2024.10762296},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10098100,
  author={Zhang, Yuhong and Shu, Haitao and Bu, Chenyang and Hu, Xuegang},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={A Zero-shot Learning Method with a Multi-Modal Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={391-395},
  abstract={Zero-shot learning aims to recognize unseen-classes using some seen-class samples as training set. It is challenging owing to that the feature representations of unseen-class samples are unavailable. Existing methods transfer the mapping from seen-classes to unseen-classes with the correlation as a bridge, in which, the semantic representations are used to discriminate the classes. However, the unavailability of visual representations for unseen-classes and the insufficient discrimination of semantic representations make the zero-shot learning challenging. Therefore, the visual representations are learned as complements to semantic representations to construct a multi-modal knowledge graph (KG), and a zero-shot learning method based on multi-modal KG is proposed in this paper. Specially, a semantic KG is introduced to capture the correlation of classes, and with the correlation, the visual feature representations of all classes are learned. Then, the discriminative visual representations and the semantic representations are used together to construct a multi-modal KG. With the multi-modal KG, the classifier for seen-classes is transferred to unseen classes. Extensive experimental results show the effectiveness of our method.},
  keywords={Learning systems;Representation learning;Training;Bridges;Visualization;Correlation;Semantics;Zero-shot Learning;Knowledge Graph;Multi-modal Representation},
  doi={10.1109/ICTAI56018.2022.00064},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{9687957,
  author={Ye, Hanmin and Xue, Lian and Chen, Xiaohui and Liu, Wenjie},
  booktitle={2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Research on the Method of Landscape Image Style Transfer based on Semantic Segmentation}, 
  year={2021},
  volume={2},
  number={},
  pages={1171-1175},
  abstract={The traditional style transfer technique in landscape image has the problem of semantic mismatching in different scenes. In this paper, based on the convolution and landscape style transfer algorithm of neural network, and analyses the semantic segmentation algorithm in theory, based on the normalized statistics of image style transfer algorithm (BN-NST) algorithm was designed on the basis of landscape style transfer algorithm based on semantic segmentation (BS-NST), solve the problem of semantic matching accuracy in different scenarios. In order to verify the algorithm, this paper uses the improved Deeplabv3+ network model and the convolutional neural network algorithm to process the input content image and style image, and the obtained target image has the advantages of authenticity and appreciation of landscape photos.},
  keywords={Image segmentation;Image texture;Convolution;Splicing;Semantics;Neural networks;Distortion;landscape image style transfer;convolutional neural network;semantic segmentation},
  doi={10.1109/ICIBA52610.2021.9687957},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10491652,
  author={Cao, Zhiyuan and Liu, Hu},
  booktitle={2023 International Conference on Artificial Intelligence and Automation Control (AIAC)}, 
  title={ICFusion: An Infrared and Visible Image Fusion Network Based on Illumination Aware and Convolutional Attention}, 
  year={2023},
  volume={},
  number={},
  pages={151-155},
  abstract={Infrared and visible image fusion plays a pivotal role in image enhancement, aiming to merge the unique attributes of both imaging modalities: the comprehensive scene understanding of infrared image and the detailed visual cues of visible image. However, many current fusion techniques are designed for single-illumination settings, overlooking the challenges of diverse illuminance conditions and the intricacies of modality fusion. In this paper, we present ICFusion, a cutting-edge image fusion network anchored by illumination aware and convolutional attention mechanisms. This framework integrates two key modules: the Illumination Probability Computation Module (IPCM) to gauge scene illuminance and the Convolutional Attention Fusion Module (CAFM) to discern latent features and reduce redundancy. By capitalizing on illumination aware, our approach deftly combines information from both imaging modes. Experimental outcomes underscore ICFusion’s edge over contemporary methods, especially in retaining fine textural nuances.},
  keywords={Visualization;Automation;Image edge detection;Redundancy;Lighting;Imaging;Probability;Image fusion;multimodal fusion;attention mechanism;illumination-based},
  doi={10.1109/AIAC61660.2023.00030},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9550795,
  author={Daodong, Zhang and Yikai, Peng and Hongping, Pu},
  booktitle={2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Design of DDS Signal Generator Based on FPGA}, 
  year={2021},
  volume={},
  number={},
  pages={51-54},
  abstract={The signal generator is a kind of electronic equipment that can generate standard signals. Compared with the traditional frequency synthesizer, the DDS signal generator has the advantages of low cost, low power consumption, high resolution and fast conversion time. Signal generator is a key technology to realize the digitization of equipment, so it is widely used in the field of telecommunication and electronic instruments such as communications and electronic measurement. This paper designs a signal generator based on FPGA and combined with DDS technology to realize a certain frequency and phase waveform signal. The feasibility and practicability of the designed signal generator are verified by using Vivado simulation software to generate 295.3kHz and 390.6kHz sine wave and triangle wave signals.},
  keywords={Frequency synthesizers;Frequency modulation;Costs;Signal generators;Stability analysis;Software;Transistors;FPGA;direct digital frequency synthesizer;signal generator},
  doi={10.1109/PRAI53619.2021.9550795},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10463360,
  author={Khan, Shafaq and Palanisamy, Lavanya Shanmugapuram and Raghuraman, Manish},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Federated Learning - a Novel Approach for Predicting Diseases in Unprecented Areas}, 
  year={2024},
  volume={},
  number={},
  pages={058-063},
  abstract={According to the World Health Organization (WHO), approximately 75 percent of deaths in rural areas are attributed to delayed disease diagnosis. Some diseases exhibit unforeseeable symptoms, leading to life-threatening conditions. Moreover, emerging and rare diseases with recognizable mani-festations in advanced stages pose challenges due to physicians' limited knowledge. In this context, federated learning emerges as a privacy-conscious machine learning approach, ideally suited for smart healthcare applications. It facilitates collaboration among multiple hospitals to conduct training without the need to share raw data, thereby preserving sensitive information. The proposed solution showcases the feasibility of using federated learning to predict diseases based on frontal chest X-rays. The iterative training process of federated learning, occurring at predefined intervals, significantly enhances efficiency, allowing doctors to include specific symptoms for early predictions of novel diseases. A comprehensive evaluation using RESNET-50 on frontal chest X-rays demonstrated that the federated learning approach improves the efficiency of disease detection compared to a normal model by atleast 2 percent.},
  keywords={Training;Federated learning;Hospitals;Organizations;Predictive models;Medical diagnosis;Iterative methods},
  doi={10.1109/ICAIIC60209.2024.10463360},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10604480,
  author={Zhou, Zhuozhi and Lan, Jinhui},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={A Dual Cross Attention Transformer Network for Infrared and Visible Image Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={494-499},
  abstract={Infrared and visible image fusion task aims to decompose and combine complementary information from both sensors. To overcome the lack of global intensity balance in fusion images, we proposed our joint transformer network with feature enhancement and stack cross attention (SCA) layer. Firstly, axis-based self-attention layers are applied to extract shallow features. Then, feature enhancement layer extracts feature from spatial and channel perspectives into stacks. Subsequently, the SCA layer employs cross attention for feature interaction between modalities and cross-layer attention for reassembling feature stacks to targeted pattern, which adaptively generates cross modality and feature layer relationships, respectively. Moreover, to tackle the issue of maintaining fusion by results-oriented metrics, we conduct decomposition loss to constrain above procedure by controlling cross modality correlation. Therefore, modality-specific and modality-general features are divided properly, facilitating feature reconstruction in the decoder. Finally, qualitative results show that our method preserves abundant texture and precise intensity from source images. Quantitative experimental results demonstrate that our fusion network achieves the state-of-the-art fusion performance, especially in mutual information (MI).},
  keywords={Measurement;Infrared image sensors;Sensor fusion;Feature extraction;Transformers;Sensors;Task analysis;Image fusion;Attention mechanism;Trans-former;Sensors;Feature extraction},
  doi={10.1109/ICAIBD62003.2024.10604480},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{10604547,
  author={Ge, Wenqiang and Liu, Peishun and Zhang, Mingyu and Zhang, Zhen and Lai, Yiwan},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={DiskTransformer: A Transformer Network for Hard Disk Failure Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={327-332},
  abstract={The hard disk drive is one of the most commonly damaged components in data centers, which can cause a lot of losses such as unexpected shutdowns and data loss. Therefore, the prediction of hard disk drive failures has received widespread attention in data center management. Existing work has made remarkable progress in the accuracy of failure prediction. However, the prediction performance for long-term failure and small-sample disks is not satisfactory. To address this issue, we propose a new method for hard disk failure prediction. The framework consists of a time-series feature extraction network and a prediction network. The time-series feature extraction network is composed of a Temporal Convolutional Network used to extract different dimensional relationships and a set of Long Short Term Memory networks used to extract independent dimensional time-series features. The prediction network uses a Transformer model, which can fully utilize the high-quality fused features extracted by the time-series feature extraction network for disk drive failure prediction. Experimental results on public datasets have demonstrated that our proposed method can not only predict long-term failure but also has reliable prediction performance when facing small-sample disk data.},
  keywords={Data centers;Disk drives;Predictive models;Feature extraction;Hard disks;Transformers;Data models;hard disk drive;failure prediction;time-series feature extraction;Transformer;feature fusion},
  doi={10.1109/ICAIBD62003.2024.10604547},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{8588493,
  author={Mandai, Yusaku and Kaneko, Tomoyuki},
  booktitle={2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, 
  title={Alternative Multitask Training for Evaluation Functions in Game of Go}, 
  year={2018},
  volume={},
  number={},
  pages={132-135},
  abstract={For the game of Go, Chess, and Shogi (Japanese Chess), deep neural networks (DNNs) have contributed to building accurate evaluation functions, and many studies have attempted to create the so-called value network, which predicts the reward of a given state. A recent study of the value network for the game of Go has shown that a two-headed neural network with two different objectives can be trained effectively and performs better than a single-headed network. One of the two heads is called a value head and the other head, the policy head, predicts the next move at a given state. This multitask training makes the network more robust and improves the generalization performance. In this paper, we show that a simple discriminator network is an alternative target of multitask learning. Compared to the existing deep neural network, our proposed network can be designed more easily because of its simple output. Our experimental results showed that our discriminative target also makes the learning stable and the evaluation function trained by our method is comparable to the training of existing studies in terms of predicting the next move and playing strength.},
  keywords={Games;Training;Neural networks;Magnetic heads;Training data;Computer architecture;Feature extraction;neural networks;joint training;computer go},
  doi={10.1109/TAAI.2018.00037},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{10332100,
  author={Jia, Lanjie and Chen, Zhili and Shao, Dan and Abba, Abubakar Adamu},
  booktitle={2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Breast Cancer Subtype Classification Based on PET/CT Bimodal Imaging Feature Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={25-29},
  abstract={In this paper, the powerful feature extraction capability of convolutional neural networks is used to extract advanced latent features of breast lesions, and the classification of breast cancer subtype is realized based on the features extracted from PET/CT bimodal images. The DenseNet network model is used to construct the PET/CT imaging feature extraction network. Since the PET/CT bimodal imaging data from breast cancer patients is very limited, the transfer learning method is adopted to solve the problem of insufficient data and prevent overfitting during training. In order to combine PET/CT bimodal imaging features, two feature fusion approaches are proposed, which are offline feature fusion and online feature fusion. For offline feature fusion, principal component analysis is employed to reduce the dimensionality of the deep features extracted by the feature extraction network from individual modalities. The reduced features are then concatenated and forwarded into a classifier for breast cancer subtype classification. For the online feature fusion, the feature maps output by the PET/CT feature extraction networks are directly concatenated and then input into the classification subnetwork for breast cancer subtype classification. The validity is evaluated using clinical data in practice. The PET/CT dual-modality features can improve the accuracy of breast cancer classification, and the experimental results show that the accuracy of online feature fusion is the highest.},
  keywords={Training;Transfer learning;Imaging;Feature extraction;Breast cancer;Pattern recognition;Lesions;bimodal imaging;deep learning;feature fusion;breast cancer subtype classification},
  doi={10.1109/PRAI59366.2023.10332100},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10062898,
  author={Zhao, Jing},
  booktitle={2022 International Conference on Virtual Reality, Human-Computer Interaction and Artificial Intelligence (VRHCIAI)}, 
  title={Tomb Mural Image Enhancement based on Improved CycleGAN}, 
  year={2022},
  volume={},
  number={},
  pages={242-247},
  abstract={For the problem of fading or discoloration, the E-CycleGAN model is proposed to realize the digital restoration of tomb murals in color. Specifically, first, StyleGAN3 network is used to generate the tomb mural face; then use empty convolution to replace the original convolution in CycleGAN, to expand the detail information of the mural face, and finally, based on CycleGAN Loss, add Identity Loss to ensure that the content of the original image does not change. Repair was performed on the self-built tomb chamber mural face dataset, and the experimental results showed that the NIQE index decreased by 1.72% on average. It proves that the network has obtained better restoration results in the color restoration of the tomb murals.},
  keywords={Human computer interaction;Solid modeling;Convolution;Image color analysis;Virtual reality;Maintenance engineering;Image restoration;Tomb chamber mural;digital restoration;CycleGAN;NIQE},
  doi={10.1109/VRHCIAI57205.2022.00049},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8638469,
  author={Eng, Jeng Hong and Saudi, Azali and Sulaiman, Jumat},
  booktitle={2018 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={Numerical Evaluation of Quarter-Sweep SOR Iteration for Solving Poisson Image Blending Problem}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Solving Poisson equation for seamless image blending is one of the powerful image editing tools. Thus, the aim of this paper is to evaluate the capability of the Quarter-Sweep Successive Over Relaxation (QSSOR) iterative methods in solving this problem. The finite difference discretization scheme is used to formulate the Poisson approximation equation. The concept and implementation of the proposed methods are presented. Finally, the numerical experimentations showed that QSSOR method has the best performance in blending time and number of iterations while generating the same quality of images compared to the Full-Sweep SOR (FSSOR) and Half-Sweep SOR (HSSOR).},
  keywords={Iterative methods;Poisson equations;Laplace equations;Mathematical model;Image resolution;Indexes;Quarter-Sweep SOR iteration;Poisson equation;Poisson image blending},
  doi={10.1109/IICAIET.2018.8638469},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10604567,
  author={Shen, Ziyang},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Machine Learning Model for Heart Failure Prediction with Feature Selection and Data Expansion}, 
  year={2024},
  volume={},
  number={},
  pages={6-11},
  abstract={Heart failure is a serious cardiovascular disease whose treatment and management require accurate predictive models. However, traditional feature selection methods may face difficulties in dealing with heart failure data, especially when the data size is small. Therefore, this thesis combines the large language model to overcome the drawbacks of small sample data, while this study proposes a new collaborative filtering-based feature selection method, which is intended to be used in the construction of heart failure machine learning models to alleviate the preference of different models for feature selection in order to ensure that the selected features have better generality. In the study of experimental data from 299 patients, we used visualisation techniques to improve the transparency and credibility of the scheme, and the experimental results further validate the feasibility and efficiency of the scheme. This research result provides new support and guidance for the prediction and treatment of heart failure.},
  keywords={Training;Heart;Solid modeling;Accuracy;Large language models;Machine learning;Predictive models;heart failuret;integration;feature screening;machine learning;survival analysis},
  doi={10.1109/ICAIBD62003.2024.10604567},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{9619245,
  author={Liu, Wei and Zhang, Yuan and Zhou, Le and Lyu, Yuting},
  booktitle={2021 3rd International Conference on Industrial Artificial Intelligence (IAI)}, 
  title={Difference of Gaussian Convolutional Sparse Principal Component Thermography for Defect Signal Enhance in Composite Materials}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={Pulsed thermography (PT) is a well-established non-destructive testing technique for the subsurface defect detection in Carbon Fiber Reinforced Polymer (CFRP). Among the analysis methods for the thermographic data, principal component thermography (PCT) and sparse principal component thermography (SPCT) are recommended for visualization enhancement of defect signals. However, since the methods of PCT and SPCT are performed directly based on the characteristic matrix model of the original thermal images, their results are heavily affected by the noise and uneven background signals inside the images. To solve the problem above, a new method known as difference of Gaussian convolutional sparse principal component thermography (DoG-SPCT) is proposed in this paper. The method first separates defect signals from the interference with a DoG filter, and then extracts features for defective areas by SPCT to enhance visualization of defects. In the experimental part, one CFRP specimen with subsurface defects is detected by PT and the proposed DoG-SPCT is evaluated for the defect visualization enhancing purpose. The result of the experiment shows that the DoG filter can separate the defect components from the noise and uneven background signals, so that the features for defective regions can be effectively extracted in the following SPCT.},
  keywords={Visualization;Convolution;Loading;Dogs;Feature extraction;Thermal conductivity;Thermal noise;Pulsed Thermography;Carbon Fiber Reinforced Polymer;Subsurface Defect Detection;Difference of Gaussian Filters;Sparse Principal Component Thermography},
  doi={10.1109/IAI53119.2021.9619245},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10574608,
  author={Suresh, Abishek and Bharathi, R and Vijayakumar, Vaidehi},
  booktitle={2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)}, 
  title={Enhanced Deep Dehazing for Haze Removal in License Plates}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper introduces Enhanced Deep Dehazing (EDD), a novel deep learning model that overcomes the limitations of traditional image dehazing methods. Unlike approaches that rely on complex transmission map estimation, EDD utilizes an Encoder-Decoder architecture for direct data-driven learning. EDD effectively handles diverse hazy conditions that degrade image quality across various applications. It utilizes convolutional layers for feature extraction and deconvolutional layers for image reconstruction, employing ReLU activation and batch normalization to improve training stability and convergence. A case study on the application of EDD to car license plate dehazing, a critical task for autonomous vehicles and surveillance systems, is presented. EDD was trained and tested on the Foggy-Hazy Licence Plates dataset, with experimental results demonstrating superior efficiency compared to previous works, thus underscoring its effectiveness and potential in dehazing foggy/hazed license plates and highlighting its applicability in realworld scenarios.},
  keywords={Image quality;Training;Surveillance;Dynamics;Streaming media;Real-time systems;Robustness;Convolution;Deep Learning;Neural Network;ReLU;Dehazing},
  doi={10.1109/AIIoT58432.2024.10574608},
  ISSN={},
  month={May},}@INPROCEEDINGS{10842786,
  author={Sharma, Ankita and Bajaj, Rohit and Sahu, Rakesh},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={Intelligent Approach for Anomaly Detection using Machine learning Techniques in Industrial Control system}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Anomaly detection is crucial for cybersecurity in industrial control systems due to the continuous injection of malicious payloads by attackers. This study examines the effectiveness of various machine learning algorithms in identifying these payloads throughout the system. The experiments utilize three datasets: the Hardware in Loop HAI dataset, the Gas Pipeline dataset, and the ICS cyber-attack dataset. Python programming and a high-performance GPU facilitate the experimental procedures. This comprehensive analysis, based on these experiments, highlights that the CNN-Dense net and Random Forest algorithms deliver the highest performance in terms of accuracy, precision, recall, and Receiver Operating Characteristic (ROC) curve across all datasets. It is important to note that other algorithms also perform comparably to CNN-Dense net and Random Forest. Consequently, the choice of machine learning algorithm should be guided by the specific data generated by the application system.},
  keywords={Industries;Machine learning algorithms;Accuracy;Industrial control;Pipelines;Receivers;Critical infrastructure;Random forests;Anomaly detection;Payloads;Machine Learning Detection Techniques;Data injection attack;Classification},
  doi={10.1109/IDICAIEI61867.2024.10842786},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10551086,
  author={Zeng, Biqing and Liang, Jianchun},
  booktitle={2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Multi-Encoder with Entity-Aware Embedding Framework for Distantly Supervised Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={495-500},
  abstract={Distantly supervised relation extraction (DSRE) inherently faces challenges arising from labeling noise due to the distant supervision assumption. Most existing studies employ Piece-wise CNN (PCNN) to extract semantic features from sentences, often neglecting to comprehensively capture contextual features. As a consequence, valuable information is lost, leading to a decline in relation prediction performance. In this paper, we propose a Multi-Encoder with Entity-Aware Embedding Framework for Distantly Supervised Relation Extraction (MEEA), designed to enhance the prediction of entity relations by effectively capturing comprehensive contextual features. Specifically, MEEA employs a novel entity-aware word embedding method that employs an attention fusion mechanism to integrate relative position information and information of two entities, which can emphasize the importance of entity pair in RE. Meanwhile, we adopt a multi-encoder framework that utilizes PCNN and two distinct sentence encoders to extract diverse features. Subsequently, these features are effectively fused through an attention fusion mechanism to capture global contextual dependencies comprehensively. Experiments demonstrate that our proposed MEEA framework exhibits significant enhancements over previous methods when applied to NYT10, GDS, and Wiki-20m datasets.},
  keywords={Computational modeling;Semantics;Noise;Big Data;Feature extraction;Data models;Data mining;Distantly supervised relation extraction;contextual features;Multi-Encoder;attention Fusion},
  doi={10.1109/ICCBD-AI62252.2023.00091},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8959907,
  author={Peng, Guan-Fu and Yang, Yi-Shian and Tsai, Chen-Yu and Soo, Von-Wun},
  booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)}, 
  title={Generate Modern Chinese Poems from News Based on Text Style Transfer Using GAN}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, we investigate techniques that can transfer a news story into a poem. We train cycle-GAN that can conduct text style transfer from news style to poem style even lack of parallel corpus. We compare teacher forcing and free-running modes of training as well as different attention mechanisms in the GAN and cycle-GAN architectures. We found that there is a trade-off between degree of style transfer and content preserving that can be controlled by the ratio of reconstruction and transfer using different training modes of the discriminator and the generator. We show that both GAN and cycle-GAN can be trained to convert news into poems to some extent using non-parallel corpus.},
  keywords={Gallium nitride;Decoding;Training;Computer science;Mathematical model;Generators;Deep learning;deep learning;text style transfer;poem generation;GAN;cycle GAN},
  doi={10.1109/TAAI48200.2019.8959907},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{10574701,
  author={Chitra, T. and Hemanth, S V and Karthikeyan, S. and Reddy, Vallem Ranadheer and Banupriya, K.M. and Amirthayogam, G.},
  booktitle={2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)}, 
  title={Predicting Lung Cancer Disease Using Optimized Weighting-Based Enhanced Neural Network Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Lung cancer is a highly lethal disease, claiming the lives of approximately 1 million individuals annually. Cancer is characterized by abnormal and rapid cell growth, making it difficult to control, though early diagnosis through Computed Tomography (CT) scan images can significantly improve chances of survival. However, detecting lung cancer early is challenging and can increase the risk of complications such as infection, inflammation, and tumor growth in the lungs. Moreover, the current methods for analyzing lung cancer prognosis have low accuracy. To solve this problem, we proposed an Optimized Weighting-Based Enhanced Neural Network (OWENN) method to classify patients based on attributes accurately. Furthermore, they utilize the Adaptive Median Filter (AMF) technique during image pre-processing to calculate the mean value of each pixel and remove noise. Moreover, Improved Particle Swarm Optimization (IPSO) can be implemented to extract tumors from lung images efficiently. Finally, the OWENN method improves the accuracy of cancer or non-cancer detection by classifying patients based on their selected attributes. Afterward, the experimental results indicate that the suggested OWENN approach can attain lung cancer prediction by evaluating sensitivity, precision, accuracy, and time delay.},
  keywords={Image segmentation;Accuracy;Computed tomography;Noise;Neural networks;Lung cancer;Lung;Lung cancer;CT image;Machine learning;Optimized Weighting-Based Enhanced Neural Network;and image processing},
  doi={10.1109/AIIoT58432.2024.10574701},
  ISSN={},
  month={May},}@INPROCEEDINGS{8785627,
  author={Liu, Peng and Hong, Ying and liu, Yan},
  booktitle={2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Single Image Super-Resolution using Adaptive Upsampling Convolutional Network}, 
  year={2019},
  volume={},
  number={},
  pages={726-730},
  abstract={These methods based on deep convolutional neural networks have achieved dramatic improvements in image super-resolution reconstruction. In this paper, we propose an adaptive upsampling convolutional network for single image super-resolution. The proposed network is mainly based on the adaptive upsampling convolutional unit (AUCU) which is composed of convolutional layers, parametric rectified linear units, sub-pixel convolution layer and the adaptive shortcuts. In the AUCU model, different weights will be assigned to different inputs of the adaptive shortcuts. And these weights are obtained adaptively from training. Owing to the special structure of AUCU, the proposed algorithm can recovery the fine texture details for a large upscaling factor. Besides the AUCU, bicubic interpolation algorithm is also used for the super-resolution restoration during the reconstruction process. In order to enhance the quality of the reconstructed image, a perceptual loss function is proposed for training the feed-forward networks. The proposed loss function consists of two parts of loss: feature loss and MSE loss. The experimental results on the pubic benchmark datasets demonstrate that the proposed algorithm outperforms many other state-of-the-art super-resolution methods.},
  keywords={Image reconstruction;Training;Image resolution;Convolution;Image restoration;Feature extraction;Signal resolution;deep convolutional neural networks;adaptive upsampling convolutional network;single image superresolution;peak signal-to-noise ratio;structure similarity index metrics},
  doi={10.1109/ITAIC.2019.8785627},
  ISSN={},
  month={May},}@INPROCEEDINGS{9643353,
  author={Wan, Shouhong and Zhang, Peiyi and Jin, Peiquan and Ding, Pengcheng},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={A Part Invariance Network for Cross-Domain Person Re-identification}, 
  year={2021},
  volume={},
  number={},
  pages={575-581},
  abstract={This paper addresses a domain adaptive person re-identification network to learn representations across domains for a challenging task: exploiting more robust features from a labeled source domain dataset and an unlabeled target domain dataset. Although convolutional networks have achieved high accuracy in single-domain re-identification task, the performance of trained models slumped in a new dataset. Current methods focus on global features, but part-based features can offer more details of pedestrians, which was been proved in recent works. In this paper, we consider local features as part invariance to reduce the gap between domains and learn discriminative features for cross-domain person re-identification. A part invariance network is proposed which implements a memory to store features extracted in the target domain to reduce camera and neighborhood invariance to improve the adaptability of the model. Experiments conducted show that the invariance proposed in this paper outperforms the existing methods by a large margin.},
  keywords={Training;Adaptation models;Adaptive systems;Conferences;Feature extraction;Cameras;Task analysis;cross-domain;part invariance;re-id},
  doi={10.1109/ICTAI52525.2021.00092},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10704483,
  author={Pho, Julius Gerald and Gouw, Christian Arifin and Lucky, Henry and Suhartono, Derwin},
  booktitle={2024 International Conference on Artificial Intelligence, Blockchain, Cloud Computing, and Data Analytics (ICoABCD)}, 
  title={Utilizing Data Augmentation Methods to Generalize DeepFake Classifier}, 
  year={2024},
  volume={},
  number={},
  pages={143-148},
  abstract={DeepFakes are fabricated audiovisual media. As technology advances, the tools for making DeepFakes have increased. Due to this, making DeepFakes are more accessible to a lot of people. As a result, some people use it irresponsibly such as hoaxes and scams. To combat this, a reliable DeepFake detector is needed. EfficientNet-B0has been used and has the highest success rate in detecting DeepFakes. Moreover, utilizing dynamic face cutout has only been proven to increase the accuracy rate of detecting DeepFakes. Because of this, we decide to implement EfficientNet-B0and face augmentation with the hopes of having a reliable DeepFake detection model. For comparison, three experiments using three different models were done. The three models include EfficientNet-B0 with Multi-task Cascaded Convo-lutional Networks (MTCNN), EfficientNet-B0 with MTCNN and Face-CutOut, and EfficientNet-B0 with MTCNN and a common data augmentation method. The experiments showed that The EfficientNet-B0and MTCNN model yielded the best results, reaching above the 90% mark on the Accuracy, AUC-ROC, and Fl-Score. When face augmentation was implemented, the results faced a slight decrease of around 1% on the Accuracy, AUC-ROC, and Fl-Score compared to the model using only EfficientNet-B0 and MTCNN. However, the EfficientNet-B0 and MTCNN model implemented with Face-Cutout has the best AUC-ROC score, above the model using only EfficientNet-B0 and MTCNN, which has a score of 97.82%. The results demonstrate that data augmentation is effective in enhancing model generalization.},
  keywords={Training;Deepfakes;Accuracy;Machine learning;Media;Data augmentation;Multitasking;Data models;Reliability;Faces;EfficientNet-B0 DeepFake classifier;Multi-task Cascaded Convolutional Networks (MTCNN);Face-Cutout Data augmentation},
  doi={10.1109/ICoABCD63526.2024.10704483},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10490868,
  author={Zhang, Jing and Yang, Gang and Liu, Aiping and Chen, Xun},
  booktitle={2023 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={A Data Augmentation Method Based on Multi-Modal Image Fusion for Detection and Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In the field of computer vision, effective data augmentation plays a crucial role in enhancing the robustness and generalization capability of visual models. This paper proposes a novel data augmentation method based on multimodal image fusion. Unlike traditional augmentation approaches, the proposed method focuses on synthesizing the fused samples that contain complementary scene characteristics from different modalities while actively suppressing useless and redundant information. To evaluate the effectiveness of our method, the experiments were conducted in the contexts of both object detection and semantic segmentation. The experimental results demonstrate that our method can significantly improve the accuracy of visual models than original samples.},
  keywords={Visualization;Computer vision;Semantic segmentation;Object detection;Data augmentation;Transformers;Robustness;object detection;semantic segmentation;data augmentation;image fusion},
  doi={10.1109/ICSMD60522.2023.10490868},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10165161,
  author={Jiang, Yaodong and Sheng, Wen and Cheng, Dongsheng and Xiang, Long and Song, Ruoyu and Jiang, Wei},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Individual Recognition of Big Data Radar Digital Waveform Based on Long Short-Term Memory Network}, 
  year={2023},
  volume={3},
  number={},
  pages={857-862},
  abstract={Individual recognition of Big Data radar digital waveform is based on the classification of target individuals using radar target waveform. Traditional recognition methods suffer from problems such as low accuracy and complex recognition processes. To address these issues, this paper proposes an individual recognition method for radar digital waveform based on Long Short-Term Memory (LSTM) network. The actual collected radar target digital waveform is simulated to generate big data of radar digital waveform, and then the basic frequency domain feature data is constructed. The LSTM network is used to extract the individual data features of the radar, and the network parameters are iteratively trained using the SGD optimization algorithm to achieve effective recognition of individual radar signals. Experimental results show that the radar signal individual recognition method based on LSTM has significantly improved accuracy compared to SVM and XGBOOST methods.},
  keywords={Support vector machines;Target recognition;Frequency-domain analysis;Radar;Big Data;Feature extraction;Radar signal processing;individual recognition of radar digital waveform;big data;long short-term memory;fourier transform;confusion matrix},
  doi={10.1109/ICIBA56860.2023.10165161},
  ISSN={},
  month={May},}@INPROCEEDINGS{10123535,
  author={Yamazaki, Masayuki and Tsuji, Kei and Mori, Eigo},
  booktitle={2022 5th International Conference on Artificial Intelligence for Industries (AI4I)}, 
  title={Real Time Analysis on Bus Passenger for Unmanned Door Operation using Overhead Fisheye Cameras}, 
  year={2022},
  volume={},
  number={},
  pages={27-30},
  abstract={In this study, we propose a robust method for ensuring safe unmanned bus door operation using commonly available surveillance cameras. Bus door operation is a critical function of bus services because passengers can be injured unless they work appropriately. Our method leverages only image processing technologies and is composed of an object detector, an optical flow analyzer, and a pose estimator. We carefully combined the modules so that they judged the readiness of the door operation accurately and in real time. We confirmed that our proposed method can ensure safe bus door operation using only two industry fisheye surveillance cameras installed on a modified Toyota Coaster microbus. Our system runs at 10 fps using a commonly available GPU desktop.},
  keywords={Industries;Image recognition;Surveillance;Graphics processing units;Detectors;Cameras;Real-time systems;smart bus;robot bus;intelligent door;in-vehicle;fisheye image recognition;action recognition;edge computing},
  doi={10.1109/AI4I54798.2022.00014},
  ISSN={2770-4718},
  month={Sep.},}@INPROCEEDINGS{10085429,
  author={Kumari, Ritesh and Garg, Hitendra},
  booktitle={2023 International Conference on Artificial Intelligence and Smart Communication (AISC)}, 
  title={An Image Copy-Move Forgery Detection based on SURF and Fourier-Mellin Transforms}, 
  year={2023},
  volume={},
  number={},
  pages={515-519},
  abstract={Image forgery is widespread nowadays on social media. The problem worsened with advanced editing software, making forgery very hard to detect. A natural image consists of different features. During forgery detection, these features are extracted to find any manipulation in the image. Two main approaches under copy-move forgery detection are block-based and key-based techniques. The paper proposes exploiting a combined approach based on block-based and key-point techniques such as speed-up robust feature (SURF) and Fourier-Mellin transform (FMT). The image is first categorized into smooth and textured parts. Surf is applied to textural areas of the image, while FMT coefficients are exploited from smooth regions. Dense linear fitting (DLF) and random sampling consensus (RANSAC) are used separately to eliminate the false matching points and outliers. Finally, mathematical morphology is adapted to generate the binary map for both parts of the image to locate the forgery area. Experimental results prove that the suggested model is robust against blurring, scaling, and compression attacks.},
  keywords={Image coding;Image resolution;Social networking (online);Splicing;Morphology;Transforms;Feature extraction;Copy-move;Forgery detection;CMFD;SURF;FMT},
  doi={10.1109/AISC56616.2023.10085429},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9696123,
  author={Cao, Yifeng and Wu, Yuefan and Tian, Zhenyu and Yu, Xuan},
  booktitle={2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={An auxiliary tool for preliminary tests of skin cancer : A self-modifying meta-learning method for clean and noisy data}, 
  year={2021},
  volume={},
  number={},
  pages={172-176},
  abstract={Deep learning is a popular method when it comes to disease detection problems. As for skin cancer, a rather common kind of disease, however, unexpected rare cases often occur with few written records and referential materials, resulting in a disadvantageous situation for a usual neural network to learn. Hence, J in this paper, we propose a self-modifying meta-learning model which combines the idea of meta-learning with curriculum learning. Applying this mechanism, our model will first train on data of common diseases and then adapt the model to rare disease classification. Moreover, despite the existence of natural noise in data, like manually mistaken labels, our model can still handle which. We evaluate our algorithm on ISIC 2018 skin lesion classification dataset. Employing only 5 samples from each class, we achieve our accuracy up to 79.2%. Apart from that, when predicting data with a 20% noisy rate, our model can also adapt to classify unseen classes by accuracy of 76.2%. The further utilization for our model can be limited to skin cancer detection and diagnosis and extend to be applied to all kinds of diseases, serving as helpful assistants for medical workers, which would be a win-win for both patients and doctors.},
  keywords={Adaptation models;Neural networks;Superresolution;Data models;Robustness;Convolutional neural networks;Task analysis;Skin-cancer;Deep Learning;Meta Learning;Noise Robustness},
  doi={10.1109/ICBASE53849.2021.00040},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9415272,
  author={Ahn, Jongsik and Kim, Min Young},
  booktitle={2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Positional estimation of invisible drone using acoustic array with A-shaped neural network}, 
  year={2021},
  volume={},
  number={},
  pages={320-324},
  abstract={Image-based object detection is a commonly used algorithm for anti-drone surveillance system. However, there is a disadvantage that it cannot be detected if the target is not visible within the image. In this paper, we propose drone position estimation algorithm using acoustic array to detect objects complementing the difficulty of estimating sudden directional shifts in hiding, occurrence situations and quickly out of the vision of the camera. Sound data is converted into an image via mel-spectrogram to facilitate image sensor and sound sensor fusion and the drone position is estimated via the Convolution Neural Network. The proposed neural network is the A-shape neural network, which consists of up-sampling and down-sampling. Through these methods, we achieve RMSE of 13.045 pixels and show that the location of the drone can be estimated efficiently.},
  keywords={Surveillance;Neural networks;Estimation;Object detection;Sensor fusion;Acoustic arrays;Cameras;Acoustic;Anti-Drone System;Surveillance System;Mel-Spectrogram;Convolution Neural Network},
  doi={10.1109/ICAIIC51459.2021.9415272},
  ISSN={},
  month={April},}@INPROCEEDINGS{9525592,
  author={Gao, Tian and Li, Meian and Xue, Lixia and Chen, Hao and Zhu, Haojie and Bao, Jingwen},
  booktitle={2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Sensitivity of camera’s height based on coordinate variation of monocular vision ranging model}, 
  year={2021},
  volume={},
  number={},
  pages={10-14},
  abstract={Using computer vision technology to perceive the distance of obstacles is an important technology in intelligent driving decision-making. Monocular vision ranging technology has a wide application range and reduces the requirement of camera quality, so it is a hot research direction in the field of visual ranging. In practical application, the change of the camera’s external parameters will have a certain impact on the ranging accuracy. This paper will study the sensitivity of the monocular vision ranging model based on the coordinate change to the camera height. We propose a conjecture that the product of the model’s formula accuracy and the height measurement accuracy is equal to the final ranging accuracy of the model. This conjecture will be preliminarily verified through experiments in this paper, and an error correction mechanism is proposed to reduce the impact of camera height changes on ranging accuracy.},
  keywords={Visualization;Computer vision;Sensitivity;Automation;Decision making;Cameras;Distance measurement;monocular vision ranging;sensitivity analysis;error correction;camera calibration},
  doi={10.1109/AIEA53260.2021.00010},
  ISSN={},
  month={May},}@INPROCEEDINGS{9688267,
  author={Fan, Min and Cai, Ziyun and Zhang, Tengfei and Wang, Baoyun},
  booktitle={2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Adversarial Domain Adaptation via Class Correlation}, 
  year={2021},
  volume={2},
  number={},
  pages={468-472},
  abstract={Adversarial learning has been embedded in deep networks as a transferable representation of domain adaptation. The existing adversarial domain adaptation methods may not pay attention to the correlation between categories. In this paper, we propose an adversarial domain adaptation based on class correlation. In addition to the commonly used adversarial loss, we added a loss based on class correlation to reduce the probability of misclassification. We prevent the diversity of the network from decreasing by restricting the eigenvalues of the class correlation matrix.},
  keywords={Training;Correlation;Conferences;Big Data;Eigenvalues and eigenfunctions;Entropy;Adversarial machine learning;adversarial learning;domain adaptation;class correlation},
  doi={10.1109/ICIBA52610.2021.9688267},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10963274,
  author={Zhou, Houying and Deng, Yanjun and Wang, Hao},
  booktitle={2024 5th International Conference on Computers and Artificial Intelligence Technology (CAIT)}, 
  title={Rethinking Perturbation-Based Training-Free Method for Deepfake Face Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Training-free deepfake image detection aims to discern whether inputs are authentic or synthetic by directly conduct evaluation on testing samples. Prior approaches predominantly measure the similarity between original images and perturbation-generated versions. Leveraging pre-trained foundation models like DINOv2, these methods typically deliver remarkable detection performance on natural images. However, their effectiveness diminishes significantly when applied to deepfake face detection, particularly for faces of varying resolutions. Additionally, the computational demands are high due to the complexity of these foundational models, hindering the application in real-world scenarios. To overcome these challenges, we elaborate a simple yet effective Upsampling-Perturbation-Downsampling method for training-free deepfake face detection. This approach enhances both the robustness against diverse input resolutions and the efficiency of detection process. Extensive experiments on our augmented DeepFakeFaceForensics dataset demonstrate that our approach significantly outperforms state-of-the-art methods.},
  keywords={Computers;Deepfakes;Image resolution;Foundation models;Computational modeling;Robustness;Complexity theory;Face detection;Faces;Testing;deepfake face detection;perturbation-based;training-free;resolution-robust;efficient detection},
  doi={10.1109/CAIT64506.2024.10963274},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11041109,
  author={D, Prabhakaran and Balaji G, Sai and A, Varsha and Dev R, Rishi},
  booktitle={2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)}, 
  title={Deep Learning-Based Hybrid Model for Childbirth Mode Prediction Using Maternal Health Data}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The current study proposes the use of a hybrid deep learning model to predict delivery modalities (normal, caesarean, emergency, and aided delivery) from maternal health data. The model employs Convolutional Neural Networks (CNNs) for feature extraction and Long Short-Term Memory (LSTM) networks for time series analyses to learn sequential and feature-based patterns in pregnancy-related data that yield reliable predictions. The LSTM module may capture any sequential relationships in the event of pregnancy, while the CNN architecture is responsible for most feature extraction from maternal health data, contributing towards predicting delivery modality with accuracy. The model may take further advantage of Bayesian hyperparameter tuning to enhance model resilience and generalizability. XGBoost and Random Forest have been chosen as the benchmark models to evaluate them. Moreover, from the experimental data, the newly proposed hybrid LSTM-CNN model seems to outperform conventional machine-learning techniques with a much more accurate prediction capability. The conclusion regarding the improved performance of the model is corroborated through metrics such as recall, accuracy, precision, and F1 measures. Hence, through providing more accurate and reliable childbirth predictions, this research posits hybrid deep learning models to be capable of predicting maternal health outcomes and endorses their feasibility in a clinical setting.},
  keywords={Deep learning;Pediatrics;Accuracy;Medical services;Predictive models;Feature extraction;Data models;Reliability;Long short term memory;Tuning;CNN;LSTM;deep learning;hybrid models;Bayesian hyperparameter tuning;maternal health},
  doi={10.1109/AIMLA63829.2025.11041109},
  ISSN={},
  month={April},}@INPROCEEDINGS{10490566,
  author={Jin, Miaomiao and Ren, Ziliang and Wei, Wenhong and Chen, Qian and An, Ni},
  booktitle={2023 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Human Motion Prediction Based on Graph Convolutional Networks and Multilayer Perceptron}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In the field of computer vision, human motion prediction is a classic problem with many great uses. Methods for predicting human movement aim to process and analyze human movement data for predicting future human movement. There are complex problems of loss constraint and training process in this method. For solving the above problems, reduce the training and prediction time and provide a new perspective, this paper uses a lightweight and efficient graph convolutional network combined with multi-layer perceptron model, including a fully connected layer graph convolutional network. What's more, for further improving the prediction accuracy, we also compare some paper models with this model and find that this model has better accuracy performance. Finally, experiments on Human3.6M dataset verify the effectiveness of the proposed method, the proposed method can accurately predict the future human behavior, the accuracy can reach 90.2%.},
  keywords={Training;Computer vision;Data analysis;Convolution;Transforms;Predictive models;Multilayer perceptrons;Human motion prediction;graph convolutional networks;Time information;Spatial dependence;discrete cosine transform},
  doi={10.1109/ICSMD60522.2023.10490566},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10864162,
  author={Zhang, Xinze},
  booktitle={2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Gait Prediction Study Under Medical Knee Pads Based on Arctic Puffin Optimization and VMD Decomposition}, 
  year={2024},
  volume={},
  number={},
  pages={352-355},
  abstract={In recent years, various types of knee pads have emerged, and the study of gait data under knee pads has gradually become a research hotspot. However, the prediction research of gait data has been suffering from the dilemma of insufficient accuracy. Based on this, this paper proposes an improved gait data prediction model that combines the bidirectional gated recurrent unit (BiGRU), variational modal decomposition (VMD) and Arctic puffin optimization (APO) algorithms. The experimental results show that the prediction performance of the VMD+APO+BiGRU-based model significantly outperforms that of the pre-improvement model, as well as other classical time series prediction methods. The evaluation metrics of the model on both the training and test sets show extremely high accuracy, especially in the goodness-of-fit R2 up to 0.99853, which validates the effectiveness of the model in gait recognition and disease monitoring under medical knee pads.},
  keywords={Knee;Measurement;Training;Accuracy;Predictive models;Prediction algorithms;Data models;Arctic;Optimization;Gait recognition;Medical knee pads; Gait data;VMD decomposition;APO optimization},
  doi={10.1109/ICAICE63571.2024.10864162},
  ISSN={},
  month={Nov},}@INBOOK{9562711,
  author={Reznik, Leon},
  booktitle={Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Data Science Work For and Against Computer Security}, 
  title={Index}, 
  year={2022},
  volume={},
  number={},
  pages={337-342},
  abstract={},
  keywords={},
  doi={10.1002/9781119771579.index},
  ISSN={},
  publisher={IEEE},
  isbn={9781119771555},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9562711},}@INPROCEEDINGS{10854052,
  author={Li, Gang and Sun, Yan and Fu, Hai and Sun, Yaowen},
  booktitle={6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024)}, 
  title={AI-driven solutions for proactive network security and threat detection}, 
  year={2024},
  volume={2024},
  number={},
  pages={756-760},
  abstract={This paper introduces the Adaptive Threat Detection Network (ATDN) designed to address the complex and diverse cyber threats in university networks. ATDN leverages reinforcement learning, multi-modal data fusion, and adversarial training to improve the detection of both known and unknown threats. We evaluated ATDN using three open-source datasets—NSL-KDD, CICIDS2017, and UNSW-NB15—and compared its performance against several baseline algorithms. The results show that ATDN significantly outperforms traditional models in terms of accuracy, precision, recall, and F1-Score. The ablation study highlights the key roles of reinforcement learning and multi-modal data fusion in the model’s performance. ATDN efficiently handles the complex traffic patterns in university networks, maintaining low latency for real-time response. Future research will focus on extending ATDN to handle encrypted traffic and incorporating additional data sources to further strengthen its ability to mitigate modern network threats.},
  keywords={},
  doi={10.1049/icp.2024.4311},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10528426,
  author={Liang, Chen and Wang, Yande and Zheng, Haoyu and Dong, Liang and Tan, Wenhao},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Ensemble Network for Alzheimer’s Disease Diagnosis}, 
  year={2023},
  volume={},
  number={},
  pages={572-576},
  abstract={Alzheimer’s disease (AD) is a common progressive neurodegenerative disorder characterized by a gradual decline in patients’ behavior, memory, and cognition. However, due to the complexity and low quality of medical images, a single neural network may not fully leverage the relevant features within the images. To address this issue, this paper proposes a multi-network ensemble architecture aiming to improve diagnostic accuracy. In the proposed approach, we employ three different neural networks to process a medical image. Each network has a unique structure and feature extraction capability to better capture relevant information in the images, and their outputs are fused via a fusion layer to obtain the final consensus output. Additionally, we utilize cross-entropy as the loss function to measure the discrepancy between the integrated output and the ground truth, and optimize the network parameters through backpropagation. After the end-to-end training, the resulting model is capable of providing accurate diagnostic results. Experimental results demonstrate that this method can better capture relevant features in medical images, thereby enhancing diagnostic accuracy. It shows potential in the diagnosis of Alzheimer’s disease, providing physicians with a reliable auxiliary tool.},
  keywords={Training;Solid modeling;Three-dimensional displays;Neural networks;Reliability engineering;Loss measurement;Medical diagnosis;Alzheimer’s disease diagnosis;medical images;multi-network ensemble;neural network},
  doi={10.1109/ACAIT60137.2023.10528426},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11042456,
  author={Reddy, G. Vishnu Vardhan and Manvitha, Y. and Teja, G. Ravi and Maresh, G. and De, Arnab},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Development of Attendance Monitoring System using Facial Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={With the increasing need for automated and efficient attendance monitoring, traditional methods, such as human roll calls and RFID-based systems have limitations in terms of scalability and security. In this paper, a Face Recognition-Based Attendance Management System is presented that reliably recognizes faces using the local binary pattern histogram (LBPH) approach. The three main design components of the system are an attendance management graphical user interface, a training module, and a real-time recognition module. The training module builds an LBPH-based classifier by preprocessing images using OpenCV and extracting the features from the face and storing them in histogram format. The identification module uses a face detection method based on the Haar cascade to identify individuals in real time and map their identities against a recorded dataset. The system, which was created using Python, OpenCV, Tkinter, and Pandas, offers a secure, contactless, and efficient alternative to conventional attendance methods. Experimental results demonstrate the usability of the system and its applicability in the real world in academic environments.},
  keywords={Training;Histograms;Accuracy;Face recognition;Lighting;Feature extraction;Real-time systems;Face detection;Usability;Monitoring;Face Recognition;Attendance Management System;Local Binary Pattern Histograms (LBPH) Algorithm;OpenCV;Real-time Face Detection;Haar Cascade},
  doi={10.1109/RMKMATE64874.2025.11042456},
  ISSN={},
  month={May},}@INPROCEEDINGS{10947883,
  author={Rambhia, Jeenal and Sutar, Rajendra},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={Cogni-Sonance: Navigating the Neurological Orchestra of Imagined Speech}, 
  year={2024},
  volume={},
  number={},
  pages={178-183},
  abstract={The silent expression of language in the mind, or imagined speech, is an intriguing yet mysterious feature of human thought. This paper offers a thorough analysis of the illusory speech using Electroencephalography signals. We examine the complex interactions between brain oscillations and functional connectivity patterns related to imagined speech tasks, building on recent developments in EEG methodology and processing tools. In order to clarify the cortical networks and activation patterns involved in the production and processing of inner speech, we investigate the spatiotemporal dynamics of EEG signals. The dataset contains 3 different paradigms of imagination i.e. imagined digits, imagined alphabets and imagined images. The bagged tree model has given the best classification accuracy. For three class the classification accuracy is 56.7% and for binary class the maximum obtained accuracy is 70.1%. We have also emphasized on having lighter models which have less training time. EEG research provides us with a window into the inner workings of the human mind and may open up new avenues for helping those who struggle with language comprehension or speech. Through the use of EEG to decode the neurological foundations of imagined speech, we have found the source localization for all the different modalities of imagination. we hope to shed light on human inner monologue and open new avenues for creative applications in neurorehabilitation, brain-computer interfaces, and mental health therapies.},
  keywords={Electrodes;Accuracy;Cognitive neuroscience;Speech recognition;Machine learning;Speech enhancement;Brain modeling;Electroencephalography;Real-time systems;Brain-computer interfaces;EEG;Imagined speech;Cognitive Neuroscience;Mental Imagery;Neural Decoding;Machine Learning},
  doi={10.1109/GlobalAISummit62156.2024.10947883},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10604478,
  author={Feng, Kaiwen and Wu, Yuling},
  booktitle={2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={An Image Forgery Detection Network with Edge and Noise Feature Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={455-458},
  abstract={Many current forged images are deceptively similar to the real ones, but deep learning techniques can still recognize subtle artifacts in them. However, many deep learning models are hard to understand, and their detection results may not be trustworthy. In this paper, we provide an image forgery detection network framework with interpretability, and design a lightweight image forgery detection network with edge and noise fusion named ENFIDNet. By slightly modifying the original network, the active output of what features and key regions the image forgery detection network model has focused on is ensured without compromising performance. The experimental results present that the network framework designed by us is not only able to actively provide a visual interpretation, but also offers a reference for the network modification.},
  keywords={Deep learning;Adaptation models;Visualization;Accuracy;Image edge detection;Computational modeling;Noise;image forgery detection;interpretability;feature fusion},
  doi={10.1109/ICAIBD62003.2024.10604478},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{11035480,
  author={Du, Bianxia and He, Zhengshan and Zhang, Wenjing and Hu, Qiao},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={A Knowledge Graph-Driven Framework for Privacy Leakage Risk Assessment in Open-Source Information}, 
  year={2025},
  volume={},
  number={},
  pages={1040-1044},
  abstract={Open-source information is characterized by openness, transparency, sharing, and free usability. While these attributes promote knowledge dissemination and technological advancement, they also introduce potential security risks, particularly data leakage. This study aims to analyze and alert privacy leakage risks in open-source information by constructing an event logic graph of open-source information elements. First, we analyze privacy leakage cases in specific domains and build a privacy leakage event logic graph using expert knowledge. Next, we employ keyword analysis to extract key terms from open-source information and integrate them with the constructed knowledge graph to investigate leakage risks and mitigation measures across information processes. These efforts provide novel perspectives and methodologies for understanding and controlling security risks in open-source information.},
  keywords={Seminars;Privacy;Prevention and mitigation;Knowledge graphs;Logic;Security;Data mining;Usability;Protection;Information technology;Open-source information;Data leakage;Event logic graph},
  doi={10.1109/AINIT65432.2025.11035480},
  ISSN={},
  month={April},}@INPROCEEDINGS{10823023,
  author={Qian, Yuanyuan},
  booktitle={2024 6th International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={English Speech Recognition System Based on Long Short Term Memory Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={229-233},
  abstract={This paper discusses the development and execution of an English speech recognition system utilizing the Long Short-Term Memory (LSTM) algorithm. LSTM, a specific form of recurrent neural network (RNN), is extensively employed in deep learning due to its ability to manage long-term dependencies. It has found broad applications in speech recognition tasks. The system adopts an end-to-end architecture, starting from the front-end signal processing module, capturing the key features of audio signals using feature extraction techniques such as Mel Frequency Cepstral Coefficients (MFCCs), and then training the LSTM model on these features to achieve accurate transcription of the input speech. In the model training phase, a large dataset of English speech was used, including standard TIMIT pronunciation dictionaries, to ensure that the model could maintain good generalization ability under different accents and pronunciation conditions. Experimental results show that the error rate of the proposed LSTM-based speech recognition system is significantly lower than that of traditional HMM-based methods on various test sets, especially in handling recordings with background noise, demonstrating stronger robustness. Furthermore, the system also shows good real-time processing capabilities, which are expected to be widely applied in future intelligent voice assistants, telephone customer service systems, etc. However, the system still has certain limitations in handling non-standard English pronunciations and multi-language mixed scenarios, and future work will focus on solving these problems and further improving the system's flexibility and accuracy.},
  keywords={Training;Recurrent neural networks;Accuracy;Hidden Markov models;Signal processing algorithms;Speech recognition;Feature extraction;Telephone sets;Long short term memory;Standards;Speech recognition;Deep learning;Long-term memory network;Language model},
  doi={10.1109/ICAICA63239.2024.10823023},
  ISSN={2833-8413},
  month={Nov},}@INPROCEEDINGS{11011682,
  author={Arora, Rahul and Agarwal, Manan and N A S, Vinoth and Maheswari K M, Uma and R, Brindha},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Vehicle License Plate Detection and Recognition using YOLOv9 and Easy OCR}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The objective of the paper is to propose an approach for a real-time accurate Automatic Number Plate Recognition(ANPR) system, which recognizes vehicle license plates. The system is based on a common vision concept that combines Optical character recognition (OCR) and YOLOv9 deep learning algorithm. Intended to overcome challenges including poor light, partial obstruction, and different angles for high-speed vehicle plate recognition over multiple regions. This means that not only is the architecture future-friendly for license plate designs, but it will also work with existing infrastructure so this can be deployed in traffic management, law enforcement, and security use cases.},
  keywords={Deep learning;Accuracy;Law enforcement;Training data;Real-time systems;Robustness;Data models;Security;License plate recognition;Intelligent transportation systems;automatic number plate recognition;YOLOv9;deep learning;optical character recognition;and intelligent transportation systems},
  doi={10.1109/ICDSAAI65575.2025.11011682},
  ISSN={},
  month={March},}@INPROCEEDINGS{10920495,
  author={Xinqi, Yang and Zhiyuan, Dai and Mingxuan, Yan and Yuyang, Jiang and Zhenyu, Liu and Zhi, Tao},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Enhancement of Photoacoustic Microscopy Images by Hybrid Activations and Half Instance Normalization}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Photoacoustic Microscopy (PAM) imaging combines the high contrast of optical imaging with the deep penetration of ultrasound imaging, offering the advantages of high resolution and depth imaging. However, light scattering and ultrasound attenuation during the PAM imaging process limit imaging depth and resolution. In addition, environmental and system noise can affect the quality and stability of photoacoustic signals, resulting in artifacts and noise in the images. While previous research has primarily focused on photoacoustic image reconstruction, studies on PAM image enhancement remain relatively scarce. In this paper, we propose an algorithm based on the Hybrid Activation and Half Normalization (HAIN) block and a multi-stage U-Net. We also designed a Supervised Multi-Attention (SMA) module to connect the two stages. By combining channel attention and pixel attention, and incorporating ground truth supervision, the SMA module effectively extracts crucial global and detailed information. Experiments show that our proposed HAINet achieved an average peak signal-to-noise ratio of 37.38 dB and a structural similarity of 0.972 in the PAM image enhancement task. HAINet also outperformed the comparative experiments in the PAM image denoising task.},
  keywords={Ultrasonic imaging;Image resolution;PSNR;Microscopy;Noise reduction;Noise;Imaging;Stability analysis;Image enhancement;Signal resolution;Photoacoustic microscopy imaging;multi-stage U-Net;hybrid activation;supervised multi-attention},
  doi={10.1109/ICSMD64214.2024.10920495},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10581652,
  author={Wen, Pin and Chai, Qinqin and Wang, Wu},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Identification of Anoectochilus Roxburghii Origins Based on Imbalanced Dataset}, 
  year={2024},
  volume={},
  number={},
  pages={950-955},
  abstract={Anoectochilus roxburghii from different origins has different nutritional content and different price. Achieving origin identification is of great significance to the development and standardization of the Anoectochilus roxburghii industry However, it is influenced by complex factors, including the specific strain and growth environment. Achieving a high accuracy in origin identification presents significant challenges and may not always meet the stringent requirements. To account for the unique characteristics of Anoectochilus roxburghii dataset from different origins, such as limited sample size, imbalanced samples, and numerous sample interference factors, a method based on improved SMOTE and CatBoost is designed to address the need for precise origin identification. First, a Fourier transform near infrared spectrometer was used to collect sample information of Anoectochilus roxburghii from three different origins, and then the improved SMOTE algorithm was used to balance the dataset. Finally, CatBoost classifier was used to identify the different origins. Comparative experimental results show that the method proposed in this article has the highest identification accuracy, reaching more than 97%, which is 6.9% and 2.8% higher than using original data and the original SMOTE algorithm respectively. The model constructed can efficiently identify Anoectochilus roxburghii of different origins and can be served as a useful reference for quality supervision of Anoectochilus roxburghii.},
  keywords={Support vector machines;Seminars;Industries;Accuracy;Fourier transforms;Standardization;Interference;CatBoost;SMOTE;Near infrared spectroscopy;Anoectochilus roxburghii},
  doi={10.1109/AINIT61980.2024.10581652},
  ISSN={},
  month={March},}@INPROCEEDINGS{11160528,
  author={Xie, Yize and Lu, Yunjun and Xv, Zixi and Chen, Kebin},
  booktitle={2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Cond-Enabled Deep Learning Reconstruction for Ooda-Oriented Heterogeneous Network Topologies}, 
  year={2025},
  volume={},
  number={},
  pages={536-539},
  abstract={The OODA (Observation-Orientation-Decision-Action) heterogeneous network is a complex functional entity composed of three types of entities—S, D, and I—as nodes, with information links serving as edges. Specifically, $S$ represents the sensor node, $D$ denotes the decision node, and I corresponds to the execution nodes. The OODA heterogeneous network can integrate multimodal data and support dynamic decision-making, offering significant application potential in areas such as military command, emergency management, system management, and enterprise planning. Reconstructing the topology of the OODA heterogeneous network enables managers to infer the complete network topology based on the relationships between the edges and nodes of an existing incomplete network, which holds substantial practical significance. With the further development of deep learning, it becomes feasible to obtain low-dimensional vector representations of nodes while simultaneously considering their attribute information. In network science, the three key themeslink prediction, network reconstruction, and dynamics prediction-have traditionally been studied independently. This is because, in many real-world scenarios, only partial node time series data or incomplete structural data of the network can be observed. This paper leverages the CoND deep learning framework [1] to infer network structures and perform dynamics prediction using such incomplete data. The CoND deep learning system comprises three main components: the state estimation module, the network inference module, and the dynamics learning module. The first two modules aim to complete the incomplete data-node time series data and network topology structurewhile the third module focuses on learning the dynamic evolution patterns of the system. Finally, we compare our approach with traditional GNN (Graph Neural Network) methods to demonstrate the superiority of our model and further investigate its performance through case studies.},
  keywords={Deep learning;Time series analysis;Complex networks;Prediction algorithms;Heterogeneous networks;Graph neural networks;Vectors;Topology;Planning;State estimation;OODA Heterogeneous Network;State Estimation;Network Inference;Dynamics Prediction},
  doi={10.1109/AIEA66061.2025.11160528},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9731187,
  author={Zhao, Mo and Ma, Ya and Li, Zhendong and Liu, Hao},
  booktitle={2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Cross-Dataset Pose Estimation of Faces In The Wild}, 
  year={2021},
  volume={},
  number={},
  pages={718-724},
  abstract={In this paper, we propose a domain generalization method for cross-dataset pose estimation of faces captured in wild conditions. Conventional methods mainly devote efforts on extracting discriminative features to reason the three-dimension pose. Due to the large distribution discrepancies between widely-used synthetic training and real-world testing data, it is challenging to seek a domain-generalized feature space especially for the new test samples in real-world applications. To alleviate the influence of dataset bias, our model aims to learn the domain-invariant features across different domains. In detail, a carefully-designed domain discriminator is plugged to the features extracted from different domains, meanwhile the feature encoder is trained to enforce features from different domains confused by game-theorem iterations. With the adversarial manner, our model learns a generalized pose-relevant feature space shared across different domains. Extensive experimental results on the standard benchmark under the cross-dataset setting indicate the superiority of our method in comparisons with most state of the arts.},
  keywords={Training;Learning systems;Adaptation models;Solid modeling;Three-dimensional displays;Pose estimation;Benchmark testing;Head Pose Estimation;Domain Adaptation;Adversarial Learning},
  doi={10.1109/ACAIT53529.2021.9731187},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9643350,
  author={Wang, Chenyi and Guo, Jie and Qiu, Weidong and Huang, Zheng and Yang, Yuhang},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Terroristic Content Detection using a Multi-scene classification system}, 
  year={2021},
  volume={},
  number={},
  pages={896-901},
  abstract={The spread of terroristic images on the World Wide Web will bring significant risks to social security. Terroristic image detection technology can help image filtering. Otherwise, due to the lack of high-quality terrorist image dataset, deep learning based recognition methods have not been popularized. Furthermore, characteristics of occlusions and diversity of scenes, automatic approaches to terroristic content detection need to be well-designed. In this paper, a multi-model system is intended to detect for various types of terroristic content. For an input image, the system will locate and identify the terrorist organization leader or flag, determine whether the text information in the image belongs to terrorist slogan, and distinguish a terroristic picture from an ordinary one. For a terroristic image, the system will further detect sensitive objects, e.g. guns or flames, and subdivide the scene of brutal terrorist events. We build a terroristic dataset containing over 41,000 images for experimentation. The experiment result shows the accuracy of the multi-model terroristic content detection system.},
  keywords={Training;Deep learning;Image recognition;Terrorism;Conferences;Fires;Organizations;terroristic content;deep convolutional network;object detection;long-tailed classification},
  doi={10.1109/ICTAI52525.2021.00144},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10361163,
  author={Yan, Hang},
  booktitle={2023 2nd International Conference on Data Analytics, Computing and Artificial Intelligence (ICDACAI)}, 
  title={A New CycleGAN-Based Style Transfer Method}, 
  year={2023},
  volume={},
  number={},
  pages={712-719},
  abstract={Image style transfer is a research hotspot in computer graphics and computer vision. Especially in recent years, style transfer has been applied in many fields such as art creation, film and television production, and social networking. Therefore, in view of the problem that the image texture generated by the existing style transfer network CycleGAN is blurred, and the style transfer cannot only be performed for a specific foreground, a new CycleGAN-based style transfer method by adding a self-attention layer and semantic segmentation is proposed. The workflow of the network is to first process the image through the optimized CycleGAN to obtain the generated result, send the generated result to the semantic segmentation network to obtain the mask image, perform AND operation based on the mask image to obtain the foreground and background, and finally merge to obtain the output result. The experiments are carried out on two public image datasets, horse2zebra and MNIST. The results show that compared with the original CycleGAN network, this method has a faster texture learning speed and generates clearer textures, and moreover, it solves the problem that the original CycleGAN network cannot perform style transfer only for the foreground.},
  keywords={Analytical models;TV;Target recognition;Social networking (online);Semantic segmentation;Image edge detection;Computational modeling;CycleGAN;Self-Attention;Semantic Segmentation;Style Transfer},
  doi={10.1109/ICDACAI59742.2023.00142},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11165842,
  author={Lin, Qiaonan},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Knowledge Graph Reasoning and Completion Strategies under Few-Shot Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Knowledge graph (KG) reasoning and completion under few-shot learning conditions faces significant difficulties because of the limited number of labelled triplets for rare or new relations. Relevant strategies need to develop generalization from limited information while preserving semantic consistency across different relational structures, which is the aim of this study. The Hierarchical Relational Learning with Transfer for Knowledge Graph Reasoning and Completion (HiRe) framework implements a learning hierarchy which contains transfer learning throughout its stages to achieve strong knowledge exchange between high-resource and low-resource relations. The reasoning system consists of three interconnected learning tiers which include entity-level and triplet-level and context-level learning. The entity-level representations enhance their capabilities through two operations: combination of neighbouring information and utilization of transferred learning from comparable entities. Transformer-based encoders process support triplets to generate relational meta-representations which model semantic relationships within the triplets at the triplet level. Context-level reasoning uses attention mechanisms combined with contrasts to match query and reference triplets through the application of semantic patterns it has acquired from related tasks. Multiple levels of representation within the structure unify to form a hierarchal organization which allows accurate reasoning even when supervision is minimal. This methodology scales up transfer learning from beginning to end to deliver effective and adaptable few-shot KG reasoning along with completion capabilities.},
  keywords={Attention mechanisms;Accuracy;Scalability;Semantics;Transfer learning;Knowledge graphs;Transformer cores;Transformers;Cognition;Few shot learning;Few-shot Learning;Knowledge Graph Reasoning;Transfer Learning;Hierarchical Relational Learning;Semantic Consistency},
  doi={10.1109/ACDSA65407.2025.11165842},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9339126,
  author={Li, Xiaoli and Zhou, Shuailing},
  booktitle={2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={GLAGAN image inpainting algorithm based on global and local consistency}, 
  year={2020},
  volume={9},
  number={},
  pages={646-650},
  abstract={Image inpainting is an important part in the field of image processing, and its purpose is to complete the damaged area according to the pixel information not lost in the image. At present, although the image inpainting algorithm based on deep learning can repair the missing area, there are still problems that the effective information of the far area cannot be obtained, and the edge of the repair area is blurred or even distorted. In view of the above problems, this paper proposes a GLAGAN image inpainting algorithm based on global and local consistency, so that the repaired image can be semantically consistent globally and locally. In the generation network, the dilated convolution is used to initially repair the missing area, and the cross attention module is used to obtain the correlation between the repaired area and the known area, and the feature weight is calculated to further repair the damaged image. Then through the global discriminator and local discriminator to conduct adversarial training to improve the consistency of the image repair results. Experimental results show that the repair effect of the algorithm is more real and natural, and it has been further improved in both subjective evaluation and objective evaluation.},
  keywords={Training;Correlation;Convolution;Image edge detection;Maintenance engineering;Image restoration;Information technology;image inpainting;image processing;deep learning;cross attention module},
  doi={10.1109/ITAIC49862.2020.9339126},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10708944,
  author={Qiao, Shuang},
  booktitle={2024 3rd International Conference on Artificial Intelligence and Autonomous Robot Systems (AIARS)}, 
  title={Automatic Generation and Optimization Design of Embroidery Patterns Based on Image Processing}, 
  year={2024},
  volume={},
  number={},
  pages={565-570},
  abstract={With the improvement of living environment and the improvement of people's quality of life, embroidery, as a traditional handicraft art, is gradually receiving more attention and love from people. Deep learning (DL) has demonstrated strong capabilities in image generation and processing, particularly in generating visually reasonable and highly detailed images. This provides new opportunities for the automatic generation and optimization design of embroidery patterns. Traditional embroidery pattern generation methods may rely on rules, templates, or manual adjustments, while DL methods can automatically learn complex patterns and structures in images by training large amounts of data, improving the efficiency of embroidery pattern generation and optimization. The DL model can process a large amount of data in a short period of time and automatically generate and optimize patterns. This article designs an automatic generation and optimization algorithm for embroidery patterns based on image processing, using DL technology to learn and generate innovative and artistic embroidery patterns from a large amount of embroidery image data. The experimental results show that the algorithm proposed in this paper can achieve automatic generation and optimization of embroidery patterns, meeting the needs of users for unique and beautiful embroidery patterns.},
  keywords={Training;Deep learning;Computer vision;Image synthesis;Semantic segmentation;Manuals;Product design;Data models;Quality assessment;Optimization;Image processing;Embroidered patterns;Automatic generation and optimization},
  doi={10.1109/AIARS63200.2024.00109},
  ISSN={},
  month={July},}@INPROCEEDINGS{10168669,
  author={Chen, Wei-Jyun and Chiu, Ching-Te and Lin, Ting-Chun},
  booktitle={2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS)}, 
  title={Landmark-Based Adversarial Network for RGB-D Pose Invariant Face Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Even though numerous studies have been conducted, face recognition still suffers from poor performance in pose variance. Besides fine appearance details of the face from RGB images, we use depth images that present the 3D contour of the face to improve recognition performance in large poses. At first, we propose a dual-path RGB-D face recognition model which learns features from separate RGB and depth images and fuses the two features into one identity feature. We add associate loss to strengthen the complementary and improve performance. Second, we proposed a landmark-based adversarial network to help the face recognition model extract the pose-invariant identity feature. Our landmark-based adversarial network contains a feature generator, pose discriminator, and landmark module. After we use 2-stage optimization to optimize the pose discriminator and feature generator, we removed the pose factor in the feature extracted by the generator. We conduct experiments on KinectFaceDB, RealSensetest and LiDARtest. On KinectFaceDB, we achieve a recognition accuracy of 99.41%, which is 1.31% higher than other methods. On RealSensetest, we achieve a classification accuracy of 92.57%, which is 30.51% higher than other methods. On LiDARtest, we achieve 98.21%, which is 21.88% higher than other methods.},
  keywords={Training;Three-dimensional displays;Image recognition;Fuses;Circuits and systems;Face recognition;Feature extraction;face recognition;pose invariant face recognition;pose invariant feature;adversarial network;facial landmark},
  doi={10.1109/AICAS57966.2023.10168669},
  ISSN={2834-9857},
  month={June},}@INPROCEEDINGS{11108443,
  author={Yu, Ming},
  booktitle={2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)}, 
  title={Automobile Accident Claims Fraud Prediction Based on Machine Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Automobile accident fraud represents a significant challenge for the insurance industry, causing massive annual losses and undermining market health. Conventional fraud detection methods fall short against novel and complex fraud schemes. This study uses an Oracle database of 15,420 auto accident claims from an related company. During data preprocessing, we handled missing values, removed outliers, and applied SMOTE for data balancing. Then, we built five machine learning models: LR, SVM, KNN, RF, and XGBoost. The XGBoost model outperformed the others in fraud identification. This research offers an efficient auto accident fraud detection algorithm and a data-driven risk control solution for insurers. Future work could explore time - series features, online learning mechanisms, and privacy - protection strategies to enhance the adaptability and reliability of fraud detection systems.},
  keywords={Radio frequency;Adaptation models;Privacy;Machine learning;Predictive models;Fraud;Automobiles;Reliability;Protection;Accidents;component;fraud detection;machine learning models;XGBoost;future research directions},
  doi={10.1109/PRMVAI65741.2025.11108443},
  ISSN={},
  month={June},}@INPROCEEDINGS{11163088,
  author={Bijith, Pranav and Liu, Andrew and Collomb, Leonard and Gardiner, Alexander and Farrington, Sebastian},
  booktitle={2025 IEEE 12th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Robust Child Speech Classification Leveraging Augmentation and Speaker Embeddings}, 
  year={2025},
  volume={12},
  number={},
  pages={1212-1216},
  abstract={Classifying child speech is challenging due to the limited availability of large, publicly accessible datasets recorded in consistent environments. This study aimed to develop a classifier to differentiate child and adult speech using datasets such as LibriSpeech, VoxCeleb, Common Voice, OGI Kids, CMU Kids, and MyST. Initial attempts using diarization and embeddings from ECAPA-TDNN, Whisper, and HuBERT led to misclassification due to dataset-specific clustering caused by recording artifacts. To address this, data augmentation with impulse responses was applied, improving generalization. A fine-tuned gradient boosting classifier trained on ECAPA-TDNN embeddings achieved 93.5% accuracy. Future efforts will explore enhanced augmentation and model tuning for further improvements.},
  keywords={Voice activity detection;Training;Visualization;Accuracy;Computational modeling;Training data;User interfaces;Boosting;Data augmentation;Tuning;Child Speech Classification;Embeddings;Speech Processing;HuBERT;Whisper;SpeechBrain;Gradient Boosting;Data Augmentation},
  doi={10.1109/ITAIC64559.2025.11163088},
  ISSN={2693-2865},
  month={May},}@INPROCEEDINGS{10672087,
  author={Aymen, Farah and Monir, Hanin and Pester, Andreas},
  booktitle={2024 5th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={Large Vision Models: How Transformer-based Models excelled over Traditional Deep Learning Architectures in Video Processing}, 
  year={2024},
  volume={},
  number={},
  pages={50-54},
  abstract={Large vision models (LVMs), particularly vision transformers (ViTs), stand at the forefront of computer vision ad-vancements, demonstrating exceptional capabilities in processing and understanding visual data at a large scale. These models, with their deep learning frameworks and extensive parameter spaces, excel in tasks from object detection to complex scene comprehension, surpassing traditional models like CNNs and GANs. This paper explores the progression of LVMs, emphasizing the advantages of ViTs in video summarization and prediction. It highlights the limitations of CNNs, including their vulnerability to adversarial attacks and difficulties with minor image variations, and commends ViTs for their effective handling of long-range dependencies through self-attention mechanisms. The paper also examines LVM applications in both supervised and unsupervised video summarization, and introduces multimodal approaches that integrate visual, textual, and audio data, underlining the superiority of ViTs in a variety of computer vision tasks due to their advanced learning capabilities.},
  keywords={Deep learning;Computer vision;Visualization;Computational modeling;Memory management;Object detection;Transformers;ViTs;CNNs;GANs;attention;transformers;Video Summarization;Video Prediction},
  doi={10.1109/AIRC61399.2024.10672087},
  ISSN={},
  month={April},}@INPROCEEDINGS{9797545,
  author={Zheng, Yu and Cui, Jiandong and Zhong, Han and Choi, Dong-Hyuk},
  booktitle={2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Intelligent Repair Method of Old Movie Speckle Noise Based on AI Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={53-56},
  abstract={Speckle noise in old movies is caused by erasure or other reasons, which affects the quality of video images. Therefore, an intelligent repair method of speckle noise in old movies based on AI deep learning is proposed. Based on the analysis of the characteristics of speckle noise in old films, the image noise is filtered by bilateral filtering, and the image features are obtained by combining a convolution neural network to obtain the perceptual loss data. On this basis, the jump connection is added to the deep learning network structure of image restoration. Taking the loss function as the training object, the high-quality restoration of speckle noise is realized by optimizing the loss function. The test results show that the design method can ensure that the average PSNR value of the repaired image can reach more than 40 under lower and shorter training iterations, and the effect is obvious.},
  keywords={Deep learning;Training;Films;Neural networks;Speckle;Maintenance engineering;Motion pictures;AI deep learning;speckle noise;bilateral filtering;convolutional neural network;jump connection;loss function},
  doi={10.1109/ICAICE54393.2021.00019},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11144528,
  author={Xu, Haoran and Cao, Qi},
  booktitle={2025 2nd International Conference on Artificial Intelligence and Digital Technology (ICAIDT)}, 
  title={Co-Evolution of Multilingual Word Vectors and Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={388-391},
  abstract={Multilingual word vector technology has experienced an evolution from word alignment to unsupervised alignment to multilingual aligned word vectors; and language modelling has evolved from monolingual models to multilingual models based on the Transformer architecture. Both of them are synergistically applied to improve cross-language generalization capability and low-resource language modelling, showing significant advantages. The synergistic evolution of multilingual word vectors and language models promotes the development of cross-language intelligent systems, which will further enhance the machine's ability to understand and generate multilingual scenarios in the future.},
  keywords={Buildings;Transformers;Vectors;Multilingual;Intelligent systems;multilingual word vectors;language models;crosslanguage generalization;low-resource languages},
  doi={10.1109/ICAIDT66272.2025.00083},
  ISSN={},
  month={April},}@INPROCEEDINGS{10962843,
  author={Zhu, Jinchang and Cai, Zhaoqi and Lin, Siyu},
  booktitle={2024 5th International Conference on Computers and Artificial Intelligence Technology (CAIT)}, 
  title={Multi-scale Feature Fusion and Multi-task Learning for Cross-age Face Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={76-81},
  abstract={Cross-age face recognition has long been a critical challenge in the field of biometric identification, as facial appearance can change significantly with age. Addressing the issue of how to mitigate the influence of age-related variations during model training and enabling the model to produce age-invariant identity features is crucial. In this paper, we propose Multi-scale & Multi-task Cross-age Face Recognition (MMCFR) that incorporates several key techniques, including multi-scale feature fusion and Triplet loss, to improve the model's ability to extract fundamental facial features. These techniques help ensure the robustness of the features by capturing both local and global facial information at different scales. Moreover, during the subsequent multi-task learning process, we introduce an attention mechanism to guide the model in focusing selectively on age-independent features. This allows the model to better differentiate identity-related attributes from age-related changes, thus refining the feature extraction process. In addition, adversarial learning is employed to further enhance the model's ability to extract discriminative features, ensuring that the generated feature representations are more distinct and less susceptible to variations due to aging. Our proposed models demonstrate superior performance compared to conventional approaches, as evidenced by improved results on widely-used benchmark datasets such as CACD-VS, FG-NET, and LFW. Furthermore, our models show potential for broader applications, including the recognition of individuals in old photographs and the identification of missing persons over time. Experimental results on the AgeDB-30 datasets and CALFW datasets further validate the effectiveness of our approach, confirming its robustness and versatility in handling cross-age face recognition tasks.},
  keywords={Training;Attention mechanisms;Face recognition;Computational modeling;Biological system modeling;Refining;Neural networks;Feature extraction;Multitasking;Robustness;Cross-age face recognition;Multi-scale feature fusion;Triplet loss;Attention mechanism},
  doi={10.1109/CAIT64506.2024.10962843},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9836918,
  author={Zhu, Zihua and Yang, Chunshan},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Design of Signal Generation based on Embedded Chip and its Error Analysis}, 
  year={2022},
  volume={10},
  number={},
  pages={2431-2435},
  abstract={Signal generator is widely used in the field of electronics. In this paper, an embedded chip STM32F103C8T6 and direct synthesis of frequency chip AD9833 are used to design a signal generator that output is sinusoidal wave, triangular wave and square wave. Key and decoder are used to switch waveform and adjust frequency in the range of 1–5 MHz. Finally, error analysis of the output waveform frequency is performed.},
  keywords={Frequency synthesizers;Error analysis;Switches;Organic light emitting diodes;Signal generators;Distortion;Frequency measurement;STM32F103C8T6;Sine wave;Square wave;Triangle wave;AD9833},
  doi={10.1109/ITAIC54216.2022.9836918},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{11035451,
  author={Wang, Xiaoxiao},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Automatic Detection System for Public Safety Events Combining Deep Neural Networks and Pattern Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1005-1009},
  abstract={With the increasing complexity of public safety incidents, traditional incident detection methods are unable to meet the requirements of efficient and accurate detection, making the construction of automated detection system become particularly important. In this paper, we propose an automatic detection system for public safety events based on Vision Transformer (ViT), and optimize its performance through some innovative methods. Specifically, the model uses ViT to extract image features, and takes advantage of its long-distance dependency modeling to improve the spatial information understanding ability of events. In order to further optimize the timing modeling, a lightweight Temporal Convolution Module (TCM) is introduced, which can efficiently process the timing information of video frames and enhance the representation ability of timing features through an adaptive adjustment mechanism. In addition, combined with the multi-level Space-Time Self-Attention Mechanism, the dynamic weighting of spatio-temporal information is realized, which further improves the detection accuracy of the model in complex scenes. The experimental results show that the proposed method achieves higher accuracy and realtime performance than the existing technology on multiple public safety event datasets.},
  keywords={Seminars;Adaptation models;Computer vision;Accuracy;Convolution;Feature extraction;Transformers;Public security;Timing;Optimization;Vision Transformer;Temporal Convolution Module;Space-Time Self-Attention Mechanism;Public Safety Events},
  doi={10.1109/AINIT65432.2025.11035451},
  ISSN={},
  month={April},}@INPROCEEDINGS{10332102,
  author={Lv, Tianyi and Yu, Wenhui and Rui, Guangchao and Jia, Haijie and Jiang, Jun and Zhang, Xiang},
  booktitle={2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Underwater Image Enhancement Based on Shallow Underwater Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={341-345},
  abstract={In recent years, due to the importance of underwater image enhancement in underwater robot, underwater vehicle and ocean engineering, more and more extensive research has been done. It has evolved from implementing physics-based solutions to using very cutting edge cnn and GANs. However, these cutting-edge algorithms often come at the cost of high computing power and time, which reduces the efficiency and portability of underwater working equipment using these algorithms. At the same time, these models have harsh requirements on data sets, leading to high cost of training and unfriendly to many underwater operations. Therefore, this paper aims to propose a lightweight neural network structure, Shallow underwater neural network. These neural networks associate the original image directly with the output of each convolutional layer, preserving the original features while enhancing the image and avoiding gradient descent. The experimental results show that the model has a good effect on image enhancement, and the structure is lightweight.},
  keywords={Training;Costs;Target recognition;Oceans;Image edge detection;Computational modeling;Neural networks;Underwater image;Skip connection;CNN},
  doi={10.1109/PRAI59366.2023.10332102},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10868752,
  author={Nie, Roujun and Liu, Weiyue and Huang, Henan and Wei, Yuxin and Zeng, Hao and Zhao, Youbing and Ma, Tongqing and Sui, Huiyun and Sun, Xuxue},
  booktitle={2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={SmartTD: A Web Platform for Tie-Dye Pattern Generation and Digital Innovation}, 
  year={2024},
  volume={4},
  number={},
  pages={1476-1480},
  abstract={Today, the utilization of modern technological means to advance traditional tie-dye techniques has emerged as a critical issue that requires immediate attention. To address this, we propose an approach that integrates information technology with traditional craftsmanship through a web platform known as SmartTD. SmartTD efficiently extracts and analyzes existing tie-dye patterns to generate new and creative tie-dye artworks. This tie-dye pattern generation not only significantly expands users’ creative options but also infuses new vitality into traditional handicrafts. In addition, SmartTD retraces the historical background of tie-dye and its unique development trajectories in different regions of China, providing abundant technical documentation and details. Users can simulate the actual tie-dye process through virtual experiences offered on SmartTD, thereby gaining a more comprehensive understanding of the cultural value of this ancient craft. The integrated application of these methodologies provides a novel practical solution for the digital preservation of traditional handicrafts while simultaneously opening up broader avenues for cultural education and dissemination.},
  keywords={Technological innovation;Education;Documentation;Big Data;Market research;Trajectory;Cultural differences;History;Digital preservation;Information technology;GenAI;tie-dye;traditional craftsmanship;digi-tization},
  doi={10.1109/ICIBA62489.2024.10868752},
  ISSN={},
  month={Dec},}@INBOOK{10494655,
  author={},
  booktitle={Applying Artificial Intelligence in Cybersecurity Analytics and Cyber Threat Detection}, 
  title={Index}, 
  year={2024},
  volume={},
  number={},
  pages={327-333},
  abstract={},
  keywords={},
  doi={10.1002/9781394196470.index},
  ISSN={},
  publisher={Wiley},
  isbn={9781394196456},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10494655},}@INPROCEEDINGS{9797618,
  author={Sun, Kun and Jing, Mingli and Hu, Yuliag and Jiao, Yao},
  booktitle={2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Image style transfer based on improved convolutional neural}, 
  year={2021},
  volume={},
  number={},
  pages={575-579},
  abstract={Image style transfer is a hot issue in the field of computer vision. It refers to the process that an image (content image) has a specified artistic style (style image) through the algorithm, which has an important application value. The complex spatial structure in the process of style transfer will make the details blurred and the local structure of the image distorted. In this paper, a new feature detection network is introduced for feature extraction of style content and style content, which has fewer simple parameters. In the transformation network, the adaptive Instance standardization layer is added after the convolution layer to improve the retention ability of the spatial structure of the content image. Large convolution kernels are replaced to reduce the number of model parameters while maintaining the same receptive field. The network model built in this paper can realize the rapid migration of various styles, strengthen the structural characteristics of images, and improve the detail effect of stylized images significantly.},
  keywords={Adaptation models;Adaptive systems;Convolution;Feature detection;Neural networks;Standardization;Feature extraction;Image processing;style transfer;deep learning;convolutional neural network},
  doi={10.1109/ICAICE54393.2021.00114},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11050475,
  author={Cheema, Prabhdeep and Guven, Erhan},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Optimizing Recommendations Using Fine-Tuned Llms}, 
  year={2025},
  volume={},
  number={},
  pages={151-156},
  abstract={As digital media platforms strive to meet evolving user expectations, delivering highly personalized and intuitive movies and media recommendations has become essential for attracting and retaining audiences. Traditional systems often rely on keywordbased search and recommendation techniques, which limit users to specific keywords and a combination of keywords. This paper proposes an approach that generates synthetic datasets by modeling real-world user interactions, creating complex chat-style data reflective of diverse preferences. This allows users to express more information with complex preferences, such as mood, plot details, and thematic elements, in addition to conventional criteria like genre, title, and actor-based searches. In today's search space, users cannot write queries like “I want a Crime/Mystery movie set up in Florida.” Building on these contributions, we evaluate synthetic datasets for diversity and effectiveness in training and benchmarking models, particularly in areas often absent from traditional datasets. This approach enhances personalization and accuracy by enabling expressive and natural user queries. It establishes a foundation for the next generation of conversational AI-driven search and recommendation systems in digital entertainment.},
  keywords={Training;Mood;Entertainment industry;Media;Benchmark testing;Motion pictures;Data models;Recommender systems;Next generation networking;Synthetic data;synthetic datasets;conversational search;fine-tuning techniques},
  doi={10.1109/CAI64502.2025.00031},
  ISSN={},
  month={May},}@INBOOK{10950983,
  author={Xiao, Perry},
  booktitle={Artificial Intelligence Programming with Python: From Zero to Hero}, 
  title={Index}, 
  year={2022},
  volume={},
  number={},
  pages={659-681},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119820949},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950983},}@INPROCEEDINGS{10551080,
  author={Lin, Shaofu and Zhou, Shiwei and Xu, Zhe and Chen, Jianhui},
  booktitle={2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Extracting Behavior Events from Epidemic Daily Reports by Combining Deep Learning and Adversarial Training}, 
  year={2023},
  volume={},
  number={},
  pages={360-367},
  abstract={Extracting emergencies from network information has become an important means of public security. However, existing event extraction methods rely on large-scale annotated corpora and are difficult to be effectively applied to extract epidemic behavior events, which is a brand-new task of event extraction. Firstly, an epidemic provenance model is defined to comprehensively model epidemic information in epidemic daily reports. On this basis, we propose an epidemic extraction model of epidemic behavior events, called CBEventMine, that combines deep learning and adversarial training. A relatively simple model structure of BiLSTM+CRF+sigmoid is adopted to achieve fast convergence of the model on small sample data, and an adversarial learning mechanism is added to alleviate the overfitting problem of the model in few-slot learning. The experimental results demonstrate that the proposed model CBEventMine could effectively extract epidemic event than the existing state-of-the-art methods.},
  keywords={Training;Deep learning;Epidemics;Lightweight structures;Data models;Adversarial machine learning;Public security;Event extraction;BiLSTM;CRF;Adversarial learning;Epidemic daily reports},
  doi={10.1109/ICCBD-AI62252.2023.00067},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11159669,
  author={Dong, Shuaibo and Sun, Shibao and Zhao, Pengcheng},
  booktitle={2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Deep Learning with Multi-Attention for Unsupervised Industrial Anomaly Detection}, 
  year={2025},
  volume={},
  number={},
  pages={5-8},
  abstract={Unsupervised anomaly detection (UAD) for industrial multivariate time series (MTS) is challenged by complex dynamics and subtle anomalies, where conventional models suffer from low recall. To address this, we propose MPARN-AAE, a novel adversarial autoencoder. Its core is the Multi-Path Attention Recurrent Network (MPARN) module, which integrates an enhanced Temporal Convolutional Network (TCN) with multidimensional attention to capture complex spatio-temporal features. This, combined with a dynamic adversarial training strategy and an enhanced scoring mechanism, boosts detection sensitivity. Evaluations on the SWaT and WADI benchmarks demonstrate that our method significantly outperforms existing baselines. The superior F1-score is driven by a marked increase in recall, validating its enhanced sensitivity and robustness for practical industrial applications},
  keywords={Training;Sensitivity;Time series analysis;Autoencoders;NASA;Benchmark testing;Robustness;Spatiotemporal phenomena;Convolutional neural networks;Anomaly detection;Anomaly Detection;Multivariate Time Series;Adversarial Autoencoder;Temporal Convolutional Network;Attention Mechanism},
  doi={10.1109/AIEA66061.2025.11159669},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9820420,
  author={Mo, Kangquan and Zhang, Pingjian},
  booktitle={2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Interference Line Removal Using A Progressive Model}, 
  year={2022},
  volume={},
  number={},
  pages={402-405},
  abstract={Interference lines in the text image greatly affects the recognition of the text, thus, removing interference lines can improve the text recognition accuracy of the text image and the robustness of the text recognition model. This paper proposes a progressive model for interference line removal based on a recurrent neural network. The model utilizes the cyclic structure of RNN to perform multiple stages of interference line removal, which are responsible for removing part of the interference lines. In each stage, dilated feature extraction module is adopted to gradually extract richer image features. The feature extraction module aggregates dilated convolutions with different dilation rates, which ensures the extraction of receptive field of various scales. To make full use of shallow feature and speed up network training, dense connectivity is added between feature extraction blocks. The experimental results show that the proposed model can effectively remove interference lines in text images, and achieves better performance compared to other models.},
  keywords={Training;Recurrent neural networks;Image recognition;Text recognition;Interference;Big Data;Feature extraction;text image;interference line removal;progressive model},
  doi={10.1109/ICAIBD55127.2022.9820420},
  ISSN={},
  month={May},}@INPROCEEDINGS{11035025,
  author={Wang, Meng and Liu, Jianxiang and Kong, Deshuo and Zhu, Qian},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={A Two-Branch Infrared and Visible Image Fusion Network Based on Attention Guidance}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Infrared and visible image fusion aims to enhance image quality by integrating the complementary information of the two images. Due to the significant differences between the two modalities, it is easy to ignore the effective interaction between global and detail information during the information fusion process, resulting in the loss of some important contextual information. Therefore, in this paper, we propose an attentionguided two-branch fusion network (AGFuse) based on attention guidance. Specifically, we design a residual attention module (RAM) for shallow information extraction. Then, we introduce a two-branch feature extractor, where the base encoder extracts higher-level global features by combining convolution with a selfattention mechanism, while the detail encoder captures fine information in the image by separating and fusing detailed features. Further, we introduce the CGAFusion module, which weightedly fuses the global and detail features extracted by the encoder and outputs the fused image after optimized processing by the Transformer Block. A large number of experiments on mainstream data sets show that the proposed method is superior to other representative advanced methods.},
  keywords={Seminars;Attention mechanisms;Statistical analysis;Random access memory;Feature extraction;Transformers;Information retrieval;Data mining;Information technology;Image fusion;image fusion;image enhancement;Transformer;attention mechanism;infrared images},
  doi={10.1109/AINIT65432.2025.11035025},
  ISSN={},
  month={April},}@INPROCEEDINGS{10900177,
  author={Ye, Zhaoji and Liu, Weiguang and Du, Geguo and Zhou, Feixiang},
  booktitle={2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)}, 
  title={A Hybrid CNN-Transformer Architecture for Noise Cancellation in Complex Environments}, 
  year={2024},
  volume={},
  number={},
  pages={349-352},
  abstract={This paper proposes a speech enhancement algorithm that fuses a convolutional neural network (CNN) with a Transformer. It aims to effectively suppress background interference in complex noise environments and improve the clarity and intelligibility of speech signals. A weighted source-to-distortion ratio (wSDR) loss function is used to obtain the training gradient of the balanced model against the speech and noise signals by adjusting the parameter alpha. The results of the verification on the AISHELL-1 dataset show that the CNN+Transformer model has achieved an average PESQ score of 2.518 and an STOI score of 0.7234 under most test conditions.},
  keywords={Training;Fuses;Computational modeling;Interference;Speech enhancement;Transformers;Feature extraction;Noise cancellation;Convolutional neural networks;Robots;Deep Learning;Convolutional Neural Network;Noise Suppression},
  doi={10.1109/ICAIRC64177.2024.10900177},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10406337,
  author={Thakur, Nileshsingh V. and Yenurkar, Ganesh K. and Aherrao, Astha and Aherrao, Arya and Landge, Samiksha and Katre, Shrushti},
  booktitle={2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI)}, 
  title={Medical Image Fusion Using Discrete Wavelet Transform: In view of Deep Learning}, 
  year={2023},
  volume={1},
  number={},
  pages={1-6},
  abstract={Medical image fusion technique plays a major role in various clinical applications by deriving the multi modal information of medical images. This paper describes an approach for wavelet based medical image fusion of Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) images. Both the types of images are decomposed using wavelets. Various wavelet families are explored and nine different combinations of approximation coefficients and detail coefficients of CT and MRI images are used to get the fused image. These generated fused images are obtained from deep learning implementation point of view. After obtaining the results, it is observed that the max-max combination of approximation coefficients and detail coefficients is producing the better result in comparison with other type of combinations.},
  keywords={Deep learning;Magnetic resonance imaging;Computed tomography;Transforms;Discrete wavelet transforms;Image fusion;Biomedical imaging;Medical Image Fusion;Wavelet Transform;Image Decomposition},
  doi={10.1109/IDICAIEI58380.2023.10406337},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10605231,
  author={Lei, Mingyuan and Garg, Neha Priyadarshini and Gupta, Meenakshi and Cham, Tat-Jen},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Imitating human joystick control ability using style and content disentanglement}, 
  year={2024},
  volume={},
  number={},
  pages={695-698},
  abstract={Ability to extract user’s joystick input style from a few demonstrations and applying it to various navigation tasks can be useful for automated testing of shared control algorithms in simulation. This can reduce the need for user studies, leading to time and cost benefits, especially when human subjects suffer from disabilities. State-of-the-art imitation learning methods require demonstration for every task making them unsuitable for this use case. Methods like adversarial motion prior (AMP) and learning from play (Play-LMP) provide an alternative. However, data used to train AMP’s discriminator is generated by task-specific policies which can limit its ability to apply a style to other tasks. Also, it tries to optimize both style and task reward simultaneously which can lead to style not being imitated in favor of task completion. Instead of learning style through a discriminator, Play-LMP tries to extract style by learning latent representations. However, this is done in an unsupervised manner making it hard to enforce the correlation between the learned latent representation and the user’s input style. In this work, we investigate how the supervised latent optimization based disentanglement approach (used to disentangle images to its style and content latent), can be used to extract style from a few human demonstrations and applied to different tasks with simulator in the loop. Our initial results on synthetic data show that this can be a promising approach.},
  keywords={Navigation;Wheelchairs;Predictive models;Generators;Data models;Trajectory;Task analysis;style transfer;learning from demonstration;joystick digital twin},
  doi={10.1109/CAI59869.2024.00135},
  ISSN={},
  month={June},}@INPROCEEDINGS{11166439,
  author={Wetterwald, Pierre and Saarela, Mirka and Vozel, Benoît},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Needs assessment in crisis contexts: Generating a synthetic population from limited demographic data}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={In crisis situations, demographic data can be difficult to collect. This limits the effectiveness of humanitarian interventions by preventing an accurate assessment of needs. We aim to propose a robust statistical methodology for modeling a synthetic population. It must be able to reflect the dynamics and needs of the affected population. It also makes it possible to create synthetic environments to anticipate scenarios and their responses. The method has been applied to the case of internally displaced persons in Somalia.},
  keywords={Measurement;Accuracy;Explainable AI;Statistical analysis;Demography;Social sciences;Mathematical models;Filling;Data models;Covariance matrices;synthetic population;humanitarian crisis;mathematical demography;applications of statistics to social sciences;Somalia;explainable AI},
  doi={10.1109/ACDSA65407.2025.11166439},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11034615,
  author={Guo, Yuhao and Li, Guanyu and Xie, Chenyu and Sun, Qian},
  booktitle={2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)}, 
  title={Evolution and Perspectives of Speech Synthesis Technology: From Parametric Synthesis to the Era of Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={350-353},
  abstract={Speech synthesis technology has evolved over six decades from rule-based to data-driven and now knowledge-driven approaches. This paper reviews its development, from early parametric methods (e.g., Formant, LPC) to modern Large Language Models (LLMs), highlighting key breakthroughs and paradigm shifts. Traditional methods relied on handcrafted acoustic rules, while deep learning (e.g., WaveNet, Tacotron) enabled end-to-end natural-sounding synthesis. Recently, LLMs like VALL-E and Voicebox have transformed speech generation into conditional language modeling, enhancing zero-shot cloning and cross-language synthesis. We propose a three-stage” division of this evolution and discuss three key features of speech synthesis in the LLM era: (1) joint text-to-speech modeling, (2) dynamic style control, and (3) adaptive generation with limited samples. Despite achieving near-human naturalness (MOS 4.5), current systems face challenges like high computational cost, insufficient emotional expressiveness, and security risks. Future trends include lightweight deployment, multimodal generation, and self-learning systems, with ethical guidelines needed to address deepfake risks.},
  keywords={Deep learning;Adaptation models;Reviews;Large language models;Speech enhancement;Market research;Text to speech;Internet;Security;Guidelines;speech synthesis;text-to-speech (TTS);deep learning;large language modeling;technology evolution},
  doi={10.1109/ICAID65275.2025.11034615},
  ISSN={},
  month={April},}@INPROCEEDINGS{10730104,
  author={Zhang, ZhiQi and Shen, Bin and He, Wei and Liu, LongJie},
  booktitle={2024 3rd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)}, 
  title={Deepening Coarse-to-Fine: Evolution of Efficient Strategies for Single-Image Deblurring}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={In view of the wide application and remarkable results of the coarse-to-fine strategy in the field of single image deblurring, this paper discusses the optimization path of the strategy in depth, aiming at improving the efficiency of deblurring and reducing the computational redundancy. Different from the traditional method of enhancing image sharpness by stacking multi-layer networks, we innovatively reconstruct the coarse to fine processing flow and propose the Multi-Scale Feature Fusion U-NET (MSFF-Unet). The model receives multi-scale inputs through a single encoder, greatly simplifying the training process, and utilizes a shared decoder to directly generate multi-scale clear image outputs in parallel, simulating the effect of cascading U-net without the need for actual cascading structures and significantly reducing computational complexity. In addition, we introduce an asymmetric feature aggregation strategy, which effectively integrates cross-scale feature information and further enhances the deblurring effect. Comprehensive experimental verification on standard dataset such as GoPro shows that MSFF-Unet not only achieves significant improvement in image defuzzification quality, but also significantly reduces computing cost. Its performance exceeds many existing advanced technologies, demonstrating innovative potential and significant advantages in the field of image defuzzification. This research provides a new perspective and solution for building a more efficient and accurate image deblurring system.},
  keywords={Training;Computational modeling;Stacking;Redundancy;Image restoration;Decoding;Information technology;Standards;Optimization;Periodic structures},
  doi={10.1109/AICIT62434.2024.10730104},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11009555,
  author={Sun, Chang and Li, Chengxuan},
  booktitle={2025 2nd International Conference on Smart Grid and Artificial Intelligence (SGAI)}, 
  title={TFFM-YOLO: An Efficient High-Altitude Multi-Target Detection Algorithm on UAV Infrared Photography}, 
  year={2025},
  volume={},
  number={},
  pages={839-843},
  abstract={Infrared detection and tracking have emerged as pivotal aspects in the domain of object detection in recent years. However, detecting small objects continues to pose a significant challenge due to their diminutive size and the tendency for indistinct feature details to result in their omission. Moreover, information pertaining to small objects is often lost during downsampling operations. Deep learning-based detection methods have been employed to address these challenges. In this work, we propose an efficient aerial target detection algorithm, TFFM-YOLO, designed to increase detection accuracy and enhance image details within convolutional networks. Firstly, we integrate an Enhanced Receptive Field Augmentation Module (ERFB) into the backbone, aiming to mitigate background interference and amplify target features. Furthermore, we have developed a Triple Feature Fusion Module (TFFM) while leveraging three feature maps from different levels for additional fusion before their input into the detection network. Finally, the normalized Wasserstein distance is employed to optimize the balance of small target thresholds, thereby enhancing the probability of accurate bounding box detection. On the Hit-UAV dataset, our method achieves an 11.5% improvement in mAP@0.5 with minimal changes in the number of parameters compared to the original algorithm.},
  keywords={YOLO;Photography;Thermal expansion;Accuracy;Interference;Feature extraction;Autonomous aerial vehicles;Smart grids;Convolutional neural networks;Detection algorithms;infrared thermal image;UAV;multi-scale feature fusion;YOLO},
  doi={10.1109/SGAI64825.2025.11009555},
  ISSN={},
  month={March},}@INPROCEEDINGS{10920512,
  author={Li, Jingde and Shen, Changqing and Shi, Juanjuan and Wang, Dong and Huang, Weiguo and Zhu, Zhongkui},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Adversarial Domain Bias Removal Network for Cross-condition Bearing Fault Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Recently, domain adaptation has been extensively utilized to address the issue of cross-condition bearing fault diagnosis. Traditional domain adaptation methods typically optimize by reducing the disparity in sample distribution between the source conditions and the target conditions, without directly focusing on the model's contribution to the transfer conditions. When using traditional domain adaptation methods for training, it is possible that the classification features from the source conditions and the noise features from the target conditions share a similar distribution. To avoid ineffective transfer of the model, emphasize the ultimate goal of transfer learning, and ultimately enhance the model's diagnostic reliability under target conditions, a novel adversarial transfer paradigm, Adversarial Domain Bias Removal Network (ADBRN), has been proposed. ADBRN prioritizes the improvement of the model's diagnostic performance on target domain samples and explicitly enhancing the reliability of test results on target domain samples. Furthermore, this paper theoretically validates the positive correlation between the L2 norm of prediction vectors and prediction confidence.},
  keywords={Fault diagnosis;Training;Adaptation models;Limiting;Transfer learning;Noise;Focusing;Reliability theory;Vectors;Testing;fault diagnosis;domain adaptation;adversarial learning},
  doi={10.1109/ICSMD64214.2024.10920512},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11041316,
  author={S, Ranjith Kumar and P, Subhash and R, Anitha},
  booktitle={2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)}, 
  title={Enhanced Convolutional Neural Networks for Adaptive IoT Botnet Attack Detection Using Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The proliferation of Internet of Things (IoT) devices has revolutionized various industries but has also exposed critical vulnerabilities in network security, making IoT networks prime targets for botnet attacks. This project addresses the challenge by developing a hybrid detection system that integrates Convolutional Neural Networks (CNN) and Reinforcement Learning (RL) for efficient and accurate botnet detection. Utilizing the N-BaIoT dataset, the system is designed to process large volumes of network traffic data, enabling the identification of anomalous behavior indicative of botnet activities. The architecture begins with data collection and preprocessing to ensure the input is normalized and structured for machine learning models. CNNs are employed for feature extraction, leveraging their ability to automatically learn complex patterns in IoT traffic data. This is followed by fully connected layers for classification, predicting whether a traffic instance is benign or botnet-related. The system achieves high accuracy through the hierarchical combination of convolutional layers and dense layers, ensuring robust detection capabilities across diverse traffic patterns. To enhance adaptability and decision-making, an RL agent is integrated into the pipeline. The RL agent employs a feedback mechanism, continuously learning and optimizing the model's performance based on real-time classifications.},
  keywords={Adaptation models;Accuracy;Botnet;Reinforcement learning;Telecommunication traffic;Network security;Traffic control;Real-time systems;Internet of Things;Convolutional neural networks;Internet of Things (IoT);Botnet Detection;Network Security;Hybrid Detection System;Convolutional Neural Networks (CNN);Reinforcement Learning (RL);N-BaIoT Dataset;Real-Time Optimization;Adaptive Learning},
  doi={10.1109/AIMLA63829.2025.11041316},
  ISSN={},
  month={April},}@INPROCEEDINGS{10956570,
  author={Teng, Aiguo and Zha, Junjie and Shan, Xinwen and Zhu, Jiajia and Lu, Jiaxin},
  booktitle={2025 International Conference on Electrical Automation and Artificial Intelligence (ICEAAI)}, 
  title={Research on Deep Learning-Based Compression Processing Technology for UAV Inspection Images}, 
  year={2025},
  volume={},
  number={},
  pages={1395-1399},
  abstract={With the wide application of channel visualization technology and UAV inspection in the power system, the image intelligent recognition technology has achieved initial success in the power transmission and distribution profession. However, the multiple use of channel visualization devices and UAV inspections has led to the accumulation and storage of super-large-scale datasets, and the unstructured data storage resources are becoming increasingly tight. In this paper, a deep learning-based image compression processing technique for UAV inspection is proposed to address this situation. The research includes feature extraction and analysis of UAV inspection images, compression algorithm design, and image recovery quality assessment. Through experimental verification, the technique can significantly reduce storage requirements while maintaining high image recovery quality, providing a feasible solution for the efficient management of large-scale image data in power systems. This research has important practical significance, which can reduce the storage pressure of unstructured platform and improve the practical level of the system.},
  keywords={Industries;Deep learning;Image coding;Power transmission;Memory;Data visualization;Inspection;Autonomous aerial vehicles;Real-time systems;Quality assessment;Deep learning;drone inspection;image compression;data storage optimization},
  doi={10.1109/ICEAAI64185.2025.10956570},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10452505,
  author={Ananthi, N and Anjana, S and Dhanu Nagarajan, P and Barath Raj, D},
  booktitle={2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Classification of Chest X-Ray Images using Various Deep Learning Techniques to Identify Covid-19}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The healthcare community is deeply concerned about the novel coronavirus (2019-nCoV) that is spreading throughout the world. Since its discovery in late December 2019, the number of cases transferred from Wuhan, China to other countries has increased, causing ongoing changes in the medical environment. With the rapid surge of confirmed cases worldwide, initial focus centered on a Wuhan food market as the outbreak's source. However, a recent overview of primary clinical cases published in The Lancet challenges this hypothesis. Among the diverse strategies employed to curb the virus, closely monitoring its progression through international genomic sequencing has emerged as a crucial measure. Controlling the virus's spread stands as an immensely challenging task, underscoring the necessity for highly accurate health surveillance to enable prompt responses.},
  keywords={COVID-19;Sequential analysis;Surveillance;Task analysis;Surges;X-ray imaging;Biomedical imaging;Deep learning;Convolution Neural Network;Alex net;Resnet;VGG Net},
  doi={10.1109/ICDSAAI59313.2023.10452505},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11047720,
  author={Jiang, Chen and Li, Bo and Liu, Bin and He, Zhifen},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={FaceDiffusion: Facial Portrait Generation with 3D Parameter Constraints}, 
  year={2025},
  volume={},
  number={},
  pages={866-870},
  abstract={While current 2D portrait generation methods focus more on texture-level and identity features, they suffer from inconsistencies in 3D. Some methods can generate pictures of human from different angles, but when concentrated on the same person, it's still possible to distinguished the inconsistencies at naked-eye level, other methods are fine-tuned on several target pictures, although they perform better on texture and semantic but still lack geometric constraints. We address the problem of optimization and control the portrait generation process with more refined regulatory mechanisms by introducing 3D Morphable Face Models(3DMMs).},
  keywords={Training;Solid modeling;Three-dimensional displays;Image synthesis;Semantics;Text to image;Process control;Diffusion models;Faces;Optimization;Diffusion;face generation;3d-aware generation},
  doi={10.1109/AIITA65135.2025.11047720},
  ISSN={},
  month={March},}@INPROCEEDINGS{10823010,
  author={Zhao, Xiaoye and Yan, Shuai},
  booktitle={2024 6th International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Research on Optimization Model of Biosignal Processing Feature Recognition System Based on Self-Supervised Learning}, 
  year={2024},
  volume={},
  number={},
  pages={355-359},
  abstract={This study aims to build a bio signal processing feature recognition system based on self-supervised learning to improve the accuracy of signal processing and system optimization efficiency. First, through the algorithm design of the self-supervised learning model, the problems of large noise and uneven feature distribution in biological signals are solved, and the recognition accuracy of the model for features is enhanced. The model uses unlabeled data to improve the robustness of feature extraction by generating pseudo labels and multi-task learning. Secondly, this system uses a multi-layer convolutional neural network combined with an adaptive pooling layer to optimize the feature mapping process and reduce the interference of redundant data. In the system simulation link, the experiment uses a real biological signal data set and focuses on evaluating the accuracy, recall rate and computational efficiency of the model under different optimization strategies. The simulation results show that the system improves the feature recognition accuracy by 13.4% compared with the traditional supervised learning method in various indicators, and the processing speed is increased by 23.7%, achieving a significant optimization of computational efficiency based on ensuring accuracy.},
  keywords={Adaptation models;Accuracy;Biological system modeling;Signal processing algorithms;Self-supervised learning;Signal processing;Feature extraction;Biology;Computational efficiency;Optimization;Self-supervised learning;biological signal processing;feature recognition;system optimization},
  doi={10.1109/ICAICA63239.2024.10823010},
  ISSN={2833-8413},
  month={Nov},}@INPROCEEDINGS{9985856,
  author={Mei, Feng and Lv, Jiguang and Cao, Yang},
  booktitle={2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)}, 
  title={Parameter Update Framework based on Unitized Stochastic Gradient Descent Algorithm in Federated Learning}, 
  year={2022},
  volume={},
  number={},
  pages={656-660},
  abstract={Although the aim of federated learning is to solve the problem of data island and privacy preserving, it is found that federated learning itself also has the risk of privacy leakage due to the corrupted participants. In this paper, a collaborative federated learning framework is proposed to address the problem of privacy leakage during model training caused by honest-but-curious servers. The framework is based on the unitized stochastic gradient descent algorithm to selectively update the parameters to protect the gradient. The experimental results indicate that the proposed framework makes the server failed to infer explicit information from the model, which further improves the degree of privacy protection and ensures the accuracy of model training.},
  keywords={Training;Privacy;Data privacy;Federated learning;Stochastic processes;Collaboration;Inference algorithms;federated learning;privacy preserving;unitized stochastic gradient descent},
  doi={10.1109/ICBAIE56435.2022.9985856},
  ISSN={},
  month={July},}@INPROCEEDINGS{10983704,
  author={Zakizadeh Irdmousa, Mina and Delfan, Niloufar and Moshiri, Behzad},
  booktitle={2025 International Conference for Artificial Intelligence, Applications, Innovation and Ethics (AI2E)}, 
  title={ApneaTraNet: A Deep Fusion Model for Detection of Sleep Apnea-Hypopnea Using ECG Signals}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Obstructive sleep apnea-hypopnea (OSAH) poses significant health risks and traditional diagnosis methods are costly and hospital-based. This study presents an innovative deep learning framework for wearable solutions. Based on single-lead electrocardiogram (ECG) signals, the proposed model employs a ResNet34 architecture extended with bidirectional gated recurrent unit and transformer layers. This hybrid sequential fusion enables robust feature extraction, crucial for identifying distinct breathing events, such as apnea, hypopnea, and normal, within 5-second ECG segments. Our approach was evaluated on datasets from two publicly available benchmark datasets. Based on 10-fold cross-validation, the model achieved 93.79 % accuracy for classifying apnea, hypopnea, and normal sleep states. For the binary classification task of differentiating apnea from normal sleep, the proposed model achieved 97.19% accuracy. These performance measures indicate that the proposed model outperformed all existing studies for sleep apnea detection. This underscores the potential for automated OSAH detection using a wearable device, offering a more feasible and cost-effective alternative to traditional methods.},
  keywords={Deep learning;Performance evaluation;Technological innovation;Accuracy;Medical services;Electrocardiography;Transformers;Sleep apnea;Real-time systems;Wearable devices;Obstructive sleep apnea;Electrocardiogram;Transformers;Deep Learning;Deep Fusion},
  doi={10.1109/AI2E64943.2025.10983704},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11050519,
  author={Nguyen, Quoc Dung and Kim, Hakil},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Controllable Diffusion Model for Generating Multimodal Biometric Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={High-quality, diversified, and large-scale datasets are crucial for creating reliable deep-learning models for biometric applications. Unfortunately, there is a shortage of well-labeled data. This paper introduces a text-conditional biometric imaging generation framework, addressing the complexities associated with multi-modality considerations. The proposed framework harnesses cutting-edge diffusion probabilistic models to produce multi-modal biometric images at high resolutions, seamlessly aligning with biometric language prompts. The experimental results unequivocally validate the efficacy of the proposed framework in generating a diverse array of highly realistic synthetic biometric images while consistently maintaining a commendable level of fidelity when juxtaposed with their respective reference datasets. The contributions of this study offer substantial potential for propelling advancements in biometric imaging research.},
  keywords={Biometrics;Training;Image resolution;Image synthesis;Biological system modeling;Imaging;Propulsion;Diffusion models;Data augmentation;Reliability;diffusion models;image synthesis;data augmentation;multi-model generation},
  doi={10.1109/CAI64502.2025.00143},
  ISSN={},
  month={May},}@INPROCEEDINGS{11042773,
  author={Santhoshi, M. and Sneha, T.},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Deep Neural Networks for Extraction of Buildings from Satellite Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Building extraction from high-resolution satellite images is a crucial task for urban planning, disaster management, and land-use monitoring. This study uses deep learning techniques for accurate segmentation and localization of buildings using network architectures such as U-Net with MobileNet, and Transfer learning models including YOLOv8. This paper analyzes performance accuracy, precision and scalability of these deep learning models. This study shows how advanced deep learning methods may be used to extract buildings from satellite photos with accuracy and scalability. The aim if this paper is to provide a scalable and reliable method for accurately extracting buildings so that satellite images may be analysed effectively for a variety of real-world uses.},
  keywords={Deep learning;Image segmentation;Accuracy;Scalability;Buildings;Transfer learning;Urban planning;Disaster management;Satellite images;Telecommunication network reliability;Building extraction;Deep learning;Convolutional Neural Networks (CNNs);U-Net;YOLOv8;image segmentation;Transfer learning},
  doi={10.1109/RMKMATE64874.2025.11042773},
  ISSN={},
  month={May},}@INPROCEEDINGS{10405440,
  author={Huang, Yunbo and He, Xin and Mi, Tao and Yu, Guangjin and Wang, Xue and Liu, Yuan},
  booktitle={2023 5th International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Enhancement Method for Tool Wear State Textual Dataset Based on Improved GAAE Model}, 
  year={2023},
  volume={},
  number={},
  pages={352-357},
  abstract={During the cutting process of end milling cutters, the cutting surface will wear out, and the degree of wear is usually divided into three stages: initial wear stage, normal wear stage, and severe wear stage. Obviously, the sampling data in the normal wear stage is much larger than that in the initial wear stage and severe wear stage, resulting in an imbalance in the data set under the tool wear label, which will reduce the accuracy of the deep learning network model in predicting the tool wear state. To address this issue, this article proposes an enhanced method for tool wear condition monitoring dataset based on improved GAAE, which leverages the reconstruction accuracy of GAAE and the sample control ability of cGAN to fully exploit the advantages of both models. The sensor collect the vibration signal during the milling process, convert the vibration signal into spectral data and input them into GAAE. GAAE learns the data distribution characteristics through the autoencoder to generate initial sample data of the tool wear state. The generated samples are input together with the condition vector into the discriminator of cGAN. The discriminator of cGAN further distinguishes between generated samples and real samples, and introduces the condition vector to identify the specific characteristics or attributes of the samples. Afterwards, the enhanced dataset is input into a deep learning network model for classification, testing the usability of the generated data. The experimental results show that training a deep learning network model with enhanced tool wear state data sets can effectively improve the accuracy of the model for tool wear state monitoring, with a prediction accuracy of 96.7%.},
  keywords={Deep learning;Vibrations;Computational modeling;Milling;Predictive models;Data models;Usability;End mill;Tool Wear stage;Imbalanced dataset;Enhancement method;GAAE;cGAN;Deep learning network model},
  doi={10.1109/ICAICA58456.2023.10405440},
  ISSN={2833-8413},
  month={Nov},}@INBOOK{10710528,
  author={Togelius, Julian},
  booktitle={Artificial General Intelligence}, 
  title={INDEX}, 
  year={2024},
  volume={},
  number={},
  pages={223-228},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262380157},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10710528},}@INPROCEEDINGS{11047653,
  author={Lou, Yujia and Liu, Jie and Sheng, Yuan and Wang, Jiawei and Zhang, Yiwei and Ren, Yaokun},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={Addressing Class Imbalance with Probabilistic Graphical Models and Variational Inference}, 
  year={2025},
  volume={},
  number={},
  pages={1238-1242},
  abstract={This study proposes a method for imbalanced data classification based on deep probabilistic graphical models (DPGMs) to solve the problem that traditional methods have insufficient learning ability for minority class samples. To address the classification bias caused by class imbalance, we introduce variational inference optimization probability modeling, which enables the model to adaptively adjust the representation ability of minority classes and combines the class-aware weight adjustment strategy to enhance the classifier's sensitivity to minority classes. In addition, we combine the adversarial learning mechanism to generate minority class samples in the latent space so that the model can better characterize the category boundary in the high-dimensional feature space. The experiment is evaluated on the Kaggle “Credit Card Fraud Detection” dataset and compared with a variety of advanced imbalanced classification methods (such as GAN-based sampling, BRF, XGBoost-Cost Sensitive, SAAD, HAN). The results show that the method in this study has achieved the best performance in AUC, Precision, Recall and F1-score indicators, effectively improving the recognition rate of minority classes and reducing the false alarm rate. This method can be widely used in imbalanced classification tasks such as financial fraud detection, medical diagnosis, and anomaly detection, providing a new solution for related research.},
  keywords={Training;Adaptation models;Graphical models;Sensitivity;Computational modeling;Probabilistic logic;Data models;Adversarial machine learning;Fraud;Computational efficiency;Deep probabilistic graphical models;Imbalanced data classification;Variational inference;Adversarial learning},
  doi={10.1109/AIITA65135.2025.11047653},
  ISSN={},
  month={March},}@INPROCEEDINGS{10920594,
  author={Du, Zhengyu and Liu, Dongdong and Cui, Lingli},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={A Novel Deep Transfer Adversarial Dictionary Learning Strategy for Bearing Cross-Domain Fault Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Dictionary learning (DL) has gradually demonstrated its unique advantages in many fields with its powerful feature extraction and data representation capabilities. However, it still has some problems. For example, DL is susceptible to the time-shift properties of vibration signals, which is very common in industry equipment. Secondly, due to the lack of effective transfer learning strategies, the performance of DL in the field of cross-domain diagnosis is very limited. To overcome these drawbacks, a novel deep transfer adversarial dictionary learning (DTADL) strategy is proposed in this paper. First, a sample convolution module is constructed to extract shift-invariant features, and then a new deep dictionary module is designed, in which the iterative soft thresholding and the gradient descent method are used to train the dictionary for extracting the class-specific representations from the convolution module further. Besides, an adversarial domain predictor module, which includes a gradient flipping layer is designed for predicting samples from source or target domains and obtaining domain adversarial losses, which can be used to encourage domain confusion in the sparse representation space. The effectiveness of DTADL is verified on two bearing datasets, which achieved recognition rates of 99.60% and 97.10% in the experiment of transferring diagnosis between two datasets, respectively. In addition, DTADL is also compared with other traditional transfer learning methods, which also demonstrates the superiority of the proposed method.},
  keywords={Fault diagnosis;Vibrations;Technological innovation;Dictionaries;Sensitivity;Convolution;Sparse approximation;Transfer learning;Rolling bearings;Feature extraction;cross-domain diagnosis;deep dictionary module;adversarial domain predictor},
  doi={10.1109/ICSMD64214.2024.10920594},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10692725,
  author={Wu, Xinchun and Wang, Xiao and Huang, Xiaobing and Zhang, Xiaojun and Ju, Zhaoyang},
  booktitle={2024 6th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)}, 
  title={Image Dehazing Scheme Based On Haze-line And DCP Prior}, 
  year={2024},
  volume={},
  number={},
  pages={690-693},
  abstract={In visual research, researchers strive to understand how optical sensors capture image information and how the brain interprets and processes this information. However, natural atmospheric haze introduces serious visual challenges in images. Haze causes loss of image details, color distortion, and blurring of depth information, posing a significant challenge for many intelligent device applications. This paper combines the strengths of the Haze-line and DCP algorithms, complementing their weaknesses, to develop a method that accurately restores detailed object information in hazy images without introducing distortion or artifacts. By constraining atmospheric light, the method aims to approximate the true atmospheric light value for each pixel in the image. It involves solving the transmission twice and imposing constraints to maximize the restoration of transmission values, which often deviate significantly from real transmission rates due to differences in depth of field. Peak signal-to-noise ratio and structural similarity are used to evaluate the dehazing performance. Experimental results demonstrate that compared to other algorithms, this approach achieves effective dehazing with restored detail while minimizing distortion in the resulting hazefree images.},
  keywords={Optical losses;Visualization;PSNR;Automation;Image color analysis;Optical distortion;Distortion;Image restoration;Optical sensors;Internet of Things;Image Dehazing;Haze-line;Dark Channel Prior;Atmospheric light;Transmission rate},
  doi={10.1109/IoTAAI62601.2024.10692725},
  ISSN={},
  month={July},}@INPROCEEDINGS{11166133,
  author={Arzhmand, Erfan and Hosseini, Fatemeh and Rashid, Hossein},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Multimodal Scoring of Website Content Using Vision-Language Models for SEO Objectives}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Businesses are increasingly using personal websites to introduce themselves and sell their products or services. This has intensified competition among websites to attract a larger audience. As a result, website content production aimed at improving SEO scores has become a booming market. This research seeks to measure the alignment between website content and its topic using vision-language models, ranking websites accordingly. The results indicate that the proposed ranking method enhances accuracy compared to purely NLP-based approaches.},
  keywords={Accuracy;Production;Machine learning;Data mining;Videos;Business;NLP;VLM;SEO;machine learning;ranking},
  doi={10.1109/ACDSA65407.2025.11166133},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11047763,
  author={Ying, Qi},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={CFBN: Cross-Receptive Field Buffering Network for Underwater Image Enhancement}, 
  year={2025},
  volume={},
  number={},
  pages={882-885},
  abstract={The purpose of underwater image enhancement (UIE) is to reduce degraded underwater images into images with clear textures, vibrant colors, and similarities to real ground images. However, the scattering and absorption of underwater light can cause images to appear blue and green, as well as a decrease in clarity. In recent years, many UIE methods focused on the combination of both CNN and Transformer have been proposed. However, simply stacking CNN and Transformer is not conducive to feature interaction. To address these challenges, we propose a Cross-receptive Field Buffering Network (CFBN). Specifically, the CNN and Transformer Cross Complementary (CTCC) is proposed to combine the strengths of CNN and transformers including post-processing of the features after the Swin-Transformer, which helps overcome the gaps between networks caused by direct stacking. Moreover, the features extracted by the U-Net encoder are not necessarily effective. Therefore, Skip Attention Connection (SAC) is introduced, which adds attention to the skip connections which preserves more image details and their correlations, resulting in significant performance improvements. Our proposed CFBN is compared with various well-performing methods in common datasets and achieves better results in terms of visual quality, and color restoration.},
  keywords={Visualization;Image color analysis;Stacking;Noise reduction;Scattering;Transformers;Feature extraction;Excavation;Image restoration;Image enhancement;Cross-receptive;Deep learning (DL);underwater image enhancement},
  doi={10.1109/AIITA65135.2025.11047763},
  ISSN={},
  month={March},}@INPROCEEDINGS{11048065,
  author={Wang, Shuo and Li, Saifei and Yao, Tao and Zhang, Yongle},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={A Lightweight U-Net for Dehazing with Multi-Scale Feature Fusion and Auxiliary Loss}, 
  year={2025},
  volume={},
  number={},
  pages={793-797},
  abstract={Many learning-based defogging techniques enhance defogging efficacy by augmenting network depth, enlarging convolutional kernels, or incorporating attention mechanisms, yet frequently escalate computing expenses. We present a lightweight defogging architecture, MFAU-Net, that substantially decreases computational overhead while ensuring good defogging efficacy. In comparison to U-Net, MFAU-Net enhances computing efficiency by reducing the number of network scale layers and integrating deep convolution with pointwise convolution. To mitigate the potential feature loss resulting from the diminished number of scale layers, we have designed a multi-scale feature fusion module to augment the feature extraction efficacy. Furthermore, two auxiliary loss parsing layers boost gradient propagation, expedite convergence, and optimize multi-scale feature use. A training technique that integrates mean square error with gradient loss is employed to guarantee global consistency and accurately restore edge features. Experimental results show that MFAU-Net maintains good dehazing performance while achieving faster inference speed than most methods, making it suitable for advanced vision tasks.},
  keywords={Training;Deep learning;Convolution;Image edge detection;Mean square error methods;Feature extraction;Propagation losses;Image restoration;Kernel;Convergence;U-Net;deep learning;feature fusion;image dehazing},
  doi={10.1109/AIITA65135.2025.11048065},
  ISSN={},
  month={March},}@INPROCEEDINGS{10762133,
  author={Wang, Jiajia and Jiang, Ying},
  booktitle={2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={An enhanced time series anomaly detection model based on frequency domain analysis}, 
  year={2024},
  volume={},
  number={},
  pages={740-744},
  abstract={In recent years, detecting anomalies in time series data has an important function in industries including industrial monitoring, banking, and healthcare, and has attracted a lot of concern. Time series data typically exhibit varying intra and inter series correlations, leading to complex and intertwined dependencies, which traditional methods often struggle to capture, resulting in poor performance when dealing with complex timevarying patterns. This work presents an improved model for anomaly detection in time series data that leverages frequency domain analysis and element-by-element multiplication operations, and introducing Convolutional Block Attention Module(CBAM) to further optimize feature representation, thereby effectively enhancing the model’s capability to capture intricate temporal variations. Comprehensive experiments on two public datasets, NAB and UCR, show that the model we proposed significantly surpasses existing models. The improved model exhibits notable improvements in F1 score, recall, and precision, underscoring its capability to improve time series anomaly detection models’ performance.},
  keywords={Industries;Analytical models;Frequency-domain analysis;Time series analysis;Medical services;Feature extraction;Data models;Anomaly detection;Monitoring;Software engineering;frequency domain analysis;time series;element-wise multiplication;anomaly detection},
  doi={10.1109/ICBASE63199.2024.10762133},
  ISSN={},
  month={Sep.},}
