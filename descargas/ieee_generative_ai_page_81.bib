@INPROCEEDINGS{10327515,
  author={Xu, Lu and Guo, Xinyue and Han, Xingyu and Huang, Yuting and Wang, Hui and Fan, Zizhu},
  booktitle={2023 5th International Conference on Industrial Artificial Intelligence (IAI)}, 
  title={Lane Detection based on Generative Adversarial Networks and Attention Mechanism}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Lane detection plays a crucial role in the visual navigation system of intelligent vehicles. In lane detection, intelligent algorithms are capable of effectively processing images to accurately identify the desired objects. However, due to the complexity of real-world scenarios, objects to be detected in road images face challenges such as shape distortion, uneven illumination, shadow occlusion, and low resolution. Effectively extracting road features has become a research challenge. This study proposes a lane detection method based on generative adversarial networks (GANs) and attention mechanisms to address the severe imbalance between daytime and nighttime images in existing datasets. The method utilizes GANs for style transfer to augment the training dataset and designs a Coordinated Attention Detection Network (CADNet). By incorporating attention mechanisms into the residual modules of the backbone network, the model learns the correlation between feature channels, automatically calibrates attention in the channel dimension, and enhances the model’s robustness and generalization ability. Experimental evaluations on mainstream datasets and comparisons with state-of-the-art algorithms demonstrate that the proposed method effectively mitigates the impact of lighting and shadows on image detection, enabling more accurate lane detection in complex scenarios involving occlusion and low light conditions. The method exhibits higher accuracy, lower false detection rate, and missing detection rate, while significantly improving feature extraction performance.},
  keywords={Training;Visualization;Lane detection;Shape;Roads;Lighting;Feature extraction;feature extraction;lane detection;information fusion;image enhancement;attention mechanism},
  doi={10.1109/IAI59504.2023.10327515},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11042725,
  author={P, Matan and Velvizhy, P and Sherly, P Steffy},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Optimized Story Generation using DeepSeek LLM with Supervised Fine-Tuning}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Automated story generation using Large Language Models (LLMs) has gained significant attention due to its applications in creative writing, education, and interactive systems. Despite advancements, existing LLMs often generate narratives that lack coherence, contextual alignment, and semantic accuracy, limiting their effectiveness in real-world applications. Addressing these challenges requires systematic model adaptation and optimization. This study focuses on enhancing the story generation capabilities of the DeepSeek-7B model through Supervised Fine-Tuning (SFT) and content moderation. The objective is to improve lexical overlap, contextual relevance, and narrative fluency by training the model on a curated dataset of structured narratives. The proposed methodology involves fine-tuning DeepSeek-7B using high-quality training data, optimizing for linguistic coherence and semantic alignment followed by content filtering. The model's performance is evaluated using ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BERT Score to quantify improvements over zero-shot generation. Experimental results demonstrate a 15% improvement in ROUGE-1, a 31% increase in ROUGE-2, and notable gains in METEOR and BERT scores, indicating superior content generation capabilities. These enhancements validate the effectiveness of SFT in refining narrative consistency and linguistic diversity. The findings contribute to advancing AI-driven storytelling by improving the structural and semantic integrity of generated content. This research underscores the potential of supervised adaptation in enhancing LLM performance for real-world generative applications.},
  keywords={Training;Adaptation models;Accuracy;Large language models;Semantics;Coherence;Linguistics;Natural language processing;Meteors;Context modeling;Large Language Models (LLMs);Supervised Fine Tuning;Text Classification;BERT;Generative Models;Deep Learning;Natural Language Processing (NLP)},
  doi={10.1109/RMKMATE64874.2025.11042725},
  ISSN={},
  month={May},}@ARTICLE{9697341,
  author={Ruan, Jiaqi and Liang, Gaoqi and Zhao, Junhua and Qiu, Jing and Dong, Zhao Yang},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={An Inertia-Based Data Recovery Scheme for False Data Injection Attack}, 
  year={2022},
  volume={18},
  number={11},
  pages={7814-7823},
  abstract={Due to vulnerabilities exposed to cyberattacks in the cyber physical power system, increasing concerns have been paid to its cybersecurity, especially on the so-called false data injection attack. Timely recovering true values of measurements and states after encountering cyber-attacks is of paramount importance for ensuring the subsequent controls and operations of the cyber physical power system. This article, for the first time, discovers a measurement data inertia effect, and uses this effect to deduce coarse values of preattack measurements as a preliminary work for data recovery. Then, based on the deduced coarse values and suggested state bounds, an optimization model is proposed to recover the measurements and states contaminated by attacks in-time. Moreover, an error criterion named interval error is proposed to assess the entire performance of the proposed recovery scheme. Extensive and comprehensive experiments are implemented on the IEEE 30-bus test benchmark to verify the feasibility and effectiveness of the proposed recovery scheme. The numerical studies reveal that the proposed method can achieve high accuracy and efficient timeliness for data recovery.},
  keywords={Pollution measurement;Power measurement;Weight measurement;Power grids;Phasor measurement units;Optimization;Data models;Cyber physical power system;cybersecurity;data recovery;measurement data inertia},
  doi={10.1109/TII.2022.3146859},
  ISSN={1941-0050},
  month={Nov},}@ARTICLE{9385401,
  author={Yan, Liang and Fan, Bin and Xiang, Shiming and Pan, Chunhong},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={CMT: Cross Mean Teacher Unsupervised Domain Adaptation for VHR Image Semantic Segmentation}, 
  year={2022},
  volume={19},
  number={},
  pages={1-5},
  abstract={Semantic segmentation of remote sensing images has achieved superior results with the supervised deep learning models. However, their performance to unseen data domains could be very bad due to the domain shift between different domains. Recently, a series of unsupervised domain adaptation (UDA) methods has been developed to solve the domain shift problem in semantic segmentation. Most of them use adversarial learning to achieve global cross-domain alignment and use a self-training (ST) strategy to generate pseudo-labels for classwise alignment. However, these methods ignore the pixels that are not assigned pseudo-labels. Those pixels are mostly at the boundaries, which are vital to the final segmentation results. To solve this problem, this letter proposes a cross mean teacher (CMT) UDA method. The whole framework consists of two parts. On the one hand, the global cross-domain distribution alignment is performed, and then, reliable pseudo-labels are assigned to the target data. On the other hand, a cross teacher–student network (CTSN) is developed to effectively use those pixels with and without pseudo-labels. This network contains two student networks ( $S_{1}$  and  $S_{2}$ ) and two teacher networks ( $T_{1}$  and  $T_{2}$ ) for cross-consistency constraints that supervises  $S_{2}$  (or  $S_{1}$ ) by the prediction results of  $T_{1}$  (or  $T_{2}$ ). The cross supervision by CTSN is helpful to prevent performance bottlenecks caused by the high coupling of teacher–student network in existing methods. Extensive experiments on three different remote sensing adaptation scenes verify the effectiveness and superiority of the proposed method.},
  keywords={Image segmentation;Semantics;Remote sensing;Training;Adaptation models;Automation;Task analysis;Cross mean teacher (CMT);self-training (ST);semantic segmentation;unsupervised domain adaptation (UDA);very-high-resolution (VHR) image},
  doi={10.1109/LGRS.2021.3065982},
  ISSN={1558-0571},
  month={},}@ARTICLE{10413589,
  author={Lu, Yuwu and Huang, Haoyu and Zeng, Biqing and Lai, Zhihui and Li, Xuelong},
  journal={IEEE Transactions on Multimedia}, 
  title={Multi-Source and Multi-Target Domain Adaptation Based on Dynamic Generator with Attention}, 
  year={2024},
  volume={26},
  number={},
  pages={6891-6905},
  abstract={As a branch of domain adaptation (DA), multi-source DA (MSDA) is a challenging issue that aims to transfer knowledge from multiple well-labeled source domains to a target domain for target tasks. However, most existing related works focus on single-target domain adaptation, and multiple target domain adaptation is not accounted for. We believe that multiple target domains provide valuable knowledge. Meanwhile, in multi-source and multi-target adaptation scenarios, feature generators with static parameters have difficulty generating deep features of each individual domain. In this article, we propose a Dynamic Generator With Attention (DGWA) method for multi-source and multi-target domain adaptation to adapt domain-agnostic deep features in multi source and multi target domain scenarios. The feature generator with dynamic parameters can dynamically change its parameters with data input from different domains, which greatly improves the generalization of the feature pools. An attention mechanism is used in our DGWA to learn more transferable information from different domains. To demonstrate the performance of DGWA, we conduct extensive experiments on several popular domain adaptation datasets, including the digits, Office+Caltech10, Office-Home, and ImageCLEF-DA datasets. The experimental results demonstrate that our method performs better than state-of-the-art methods.},
  keywords={Generators;Feature extraction;Adaptation models;Training;Task analysis;Adversarial machine learning;Semantics;Multiple targets;dynamic feature generator;multi-source domain adaptation;attention mechanism},
  doi={10.1109/TMM.2024.3358062},
  ISSN={1941-0077},
  month={},}@ARTICLE{9693299,
  author={Liu, An-An and Guo, Fu-Bin and Zhou, He-Yu and Yan, Cheng-Gang and Gao, Zan and Li, Xuan-Ya and Li, Wen-Hui},
  journal={IEEE Transactions on Cybernetics}, 
  title={Domain-Adversarial-Guided Siamese Network for Unsupervised Cross-Domain 3-D Object Retrieval}, 
  year={2022},
  volume={52},
  number={12},
  pages={13862-13873},
  abstract={Recent advances in 3-D sensors and 3-D modeling have led to the availability of massive amounts of 3-D data. It is too onerous and time consuming to manually label a plentiful of 3-D objects in real applications. In this article, we address this issue by transferring the knowledge from the existing labeled data (e.g., the annotated 2-D images or 3-D objects) to the unlabeled 3-D objects. Specifically, we propose a domain-adversarial guided siamese network (DAGSN) for unsupervised cross-domain 3-D object retrieval (CD3DOR). It is mainly composed of three key modules: 1) siamese network-based visual feature learning; 2) mutual information (MI)-based feature enhancement; and 3) conditional domain classifier-based feature adaptation. First, we design a siamese network to encode both 3-D objects and 2-D images from two domains because of its balanced accuracy and efficiency. Besides, it can guarantee the same transformation applied to both domains, which is crucial for the positive domain shift. The core issue for the retrieval task is to improve the capability of feature abstraction, but the previous CD3DOR approaches merely focus on how to eliminate the domain shift. We solve this problem by maximizing the MI between the input 3-D object or 2-D image data and the high-level feature in the second module. To eliminate the domain shift, we design a conditional domain classifier, which can exploit multiplicative interactions between the features and predictive labels, to enforce the joint alignment in both feature level and category level. Consequently, the network can generate domain-invariant yet discriminative features for both domains, which is essential for CD3DOR. Extensive experiments on two protocols, including the cross-dataset 3-D object retrieval protocol (3-D to 3-D) on PSB/NTU, and the cross-modal 3-D object retrieval protocol (2-D to 3-D) on MI3DOR-2, demonstrate that the proposed DAGSN can significantly outperform state-of-the-art CD3DOR methods.},
  keywords={Feature extraction;Mutual information;Protocols;3-D object retrieval;cross-domain retrieval;multiview},
  doi={10.1109/TCYB.2021.3139927},
  ISSN={2168-2275},
  month={Dec},}@ARTICLE{10287972,
  author={Kim, Changwoo and Choi, Jinho and Yoon, Jongyeon and Yoo, Daehun and Lee, Woojin},
  journal={IEEE Access}, 
  title={Fairness-Aware Multimodal Learning in Automatic Video Interview Assessment}, 
  year={2023},
  volume={11},
  number={},
  pages={122677-122693},
  abstract={With the ever-growing reliance on Artificial Intelligence (AI) across diverse domains, there is an increasing concern surrounding the possibility of biases and unfairness inherent in AI systems. Fairness problems in automatic interview assessment systems, especially video-based automated interview assessments, have less been addressed despite their prevalence in the recruiting field. In this paper, we propose a method that resolves fairness problems in an automated interview assessment system that uses multimodal data as input. This is mainly done by minimizing the Wasserstein distance between two sensitive groups by introducing a regularization term. Subsequently, we employ a hyperparameter that can control the trade-off between fairness and accuracy. To test our method in various data settings, we suggest a preprocessing method that can manually adjust the underlying degree of unfairness in the training data. Experimental results show that our method presents state-of-the-art results in terms of fairness compared to previous methods.},
  keywords={Interviews;Recruitment;Predictive models;Training data;Machine learning algorithms;Data models;Prediction algorithms;Automatic interview assessment;multimodal;fairness;sensitive attribute;unfair assumption;Wasserstein distance;adversarial;representation learning},
  doi={10.1109/ACCESS.2023.3325891},
  ISSN={2169-3536},
  month={},}@ARTICLE{10769051,
  author={Qiao, Yu and Zhang, Chaoning and Adhikary, Apurba and Hong, Choong Seon},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data}, 
  year={2025},
  volume={12},
  number={2},
  pages={636-652},
  abstract={Federated learning (FL) is a privacy-preserving distributed framework for collaborative model training in edge networks. However, challenges such as vulnerability to adversarial examples and non-independent and identically distributed (non-IID) data across devices hinder the deployment of adversarially robust and accurate models at the edge. While adversarial training (AT) is widely recognized as an effective defense strategy against adversarial attacks in centralized training, we shed light on the adverse effects of directly applying AT in FL, which can severely compromise accuracy under non-IID scenarios. To address this limitation, this paper proposes FatCC, which incorporates local logit Calibration and global feature Contrast into the vanilla federated adversarial training (Fat) process from both logit and feature perspectives. This approach effectively enhances the robust accuracy (RA) and clean accuracy (CA) of the federated system. First, we introduce logit calibration, where the logits are calibrated during local adversarial updates, thereby improving adversarial robustness. Second, FatCC incorporates feature contrast, which involves a global alignment term that aligns each local representation with corresponding unbiased global features, thus enhancing robustness and accuracy. Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines.},
  keywords={Training;Data models;Robustness;Computational modeling;Accuracy;Calibration;Contrastive learning;Federated learning;Fats;Distributed databases;Adversarial robustness;feature contrast;federated learning;logit calibration;multi-access edge computing},
  doi={10.1109/TNSE.2024.3507273},
  ISSN={2327-4697},
  month={March},}@ARTICLE{10017425,
  author={Khan, Muhammad Usman Ghani and Tariq, Abdullah and Saba, Tanzila and Rehman, Amjad and Kolivand, Hoshang},
  journal={IT Professional}, 
  title={VCFN: Virtual Cloth Fitting Try-On Network}, 
  year={2022},
  volume={24},
  number={6},
  pages={20-26},
  abstract={Virtual cloth fitting network has an increasing demand with a growing online shopping trend to map target clothes on reference subject. Previous research depicts limitations in the generation of promising deformed clothes on the wearer's body while retaining the design features of cloth-like logo, text, and wrinkles. The proposed model first learns thin-plate spline transformations to warp images according to body shape, followed by a try-on module. The former model combines deformed cloth with a rendered image to generate a composition mask and outputs target body without blurry clothes while preserving critical requirements of the wearer. Experiments are performed on the Zalando dataset and the model produces fine richer details and promised generalized results.},
  keywords={Deformable models;Electronic commerce;Fitting;Market research;Electronic commerce;Splines (mathematics);Smart textiles;Clothing industry},
  doi={10.1109/MITP.2022.3204063},
  ISSN={1941-045X},
  month={Nov},}@INPROCEEDINGS{9869916,
  author={Lee, Tae-Ho and Munasinghe, Viduranga and Li, Yan-Mei and Xu, Jiajie and Lee, Hyuk-Jae and Kim, Jin-Sung},
  booktitle={2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS)}, 
  title={GAN - Based Medical Image Registration for Augmented Reality Applications}, 
  year={2022},
  volume={},
  number={},
  pages={279-282},
  abstract={Recently, with the development of AR/VR technology, boosting virtual information to the real world has been applied to various fields to increase convenience. In particular, in the medical field, different types of image information, such as X-ray, CT, and MRI data are used in surgery with AR devices for medical diagnosis and analysis of the cause of a patient's disease. This paper proposes an approach by which to explore a patient's posture and joints to match X-ray image information in an AR environment using deep learning networks such as GAN along with pose estimation. Thereby, we employ the CP-VTON+ virtual try-on network architecture to map chest X-ray information to the patient's body. Finally, we compare the try-on results of the chest X-ray image of the patient's body using the proposed method and CP-VTON+. The mean SSIM value of the proposed method is 0.0272 higher than that of CP-VTON+, and the mean PSNR value is 5.49 higher than that of CP-VTON+. The proposed method is more appropriate for application to AR devices for medical diagnosis and analysis due to the characteristics of medical images, as even minor misdiagnoses can lead to fatalities.},
  keywords={Training;Deep learning;PSNR;Pose estimation;Surgery;Network architecture;Generative adversarial networks;X-ray image;convolutional neural network;deep learning;virtual try-on;CP-VTON+},
  doi={10.1109/AICAS54282.2022.9869916},
  ISSN={},
  month={June},}@INPROCEEDINGS{10278083,
  author={Gong, Jinhong and Ling, Shiyong},
  booktitle={2023 2nd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)}, 
  title={Research of Black-Box Adversarial Attack Detection Based on GAN}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The abnormal behavior detector based on machine learning is vulnerable to adversarial attacks. Attackers achieve adversarial attacks with constructing counter samples on the detection algorithm, and get the expected output for the target model, by changing the attack behavior pattern and API call sequence. In order to improve the performance of black-box attack, this paper can generate adversarial sample to deceive discriminator by training the generator and discriminator through the black-box adversarial attack network based on GAN. Experiments on real world API attacks and benchmark data sets have proved that the anti-attack anomaly behavior detection system can well detect malware and anomaly behavior.},
  keywords={Training;Closed box;Training data;Machine learning;Benchmark testing;Generative adversarial networks;Malware;Black-box attack;Generate adversarial network;Abnormal behavior detection},
  doi={10.1109/AICIT59054.2023.10278083},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10760848,
  author={Devi, Bali and Shrivastava, Bhavya and Shankar, Venkatesh Gauri},
  booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={A Scalable Recommendation based Approach for Predictive Restaurant Ratings and Personalized Content Filtering using Zomato Data}, 
  year={2024},
  volume={},
  number={},
  pages={1280-1286},
  abstract={In recent years, the exponential growth of online restaurant platforms has revolutionized the dining experience, enabling users to explore various culinary options. Consequently, restaurant rating prediction and personalized recommendation systems have become indispensable tools for users seeking dining suggestions and platform operators aiming to enhance user engagement and satisfaction. The proposed integrated framework for Zomato combines rating prediction with content filtering to optimize user experiences. By integrating machine learning models, the ratings are predicted based on features like location and cuisine type, while content filtering personalizes recommendations. Real-world Zomato data experiments validate our framework’s efficacy in enhancing user satisfaction and engagement, offering tailored suggestions aligned with individual preferences. This work has an accuracy of 0.89 and f1 score of ${0. 8 1}$. Experimental results show significant improvements in rating prediction accuracy and the effectiveness of personalized recommendations, highlighting the potential of our approach to enhance the overall user experience on Zomato.},
  keywords={Accuracy;Machine learning algorithms;Filtering;Generative AI;Machine learning;Predictive models;User experience;Recommender systems;Restaurant Ratings;Predictions;Machine Learning;Recommender System;Zomato Dataset},
  doi={10.1109/ICSSAS64001.2024.10760848},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9696100,
  author={Qiu, Yuanhong and Zhou, Cong and Zhang, Zefeng and Li, Bin},
  booktitle={2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={BIIR: Blind Inpainting based Image Reconstructon for Texture Defect Detection}, 
  year={2021},
  volume={},
  number={},
  pages={552-555},
  abstract={Image reconstruction is an important method for texture defect detection, and the existing image reconstruction algorithms based on Autoencoder and GAN cannot suppress the reconstruction of defect information, which affects the detection accuracy. To solve this problem, this paper proposes a novel image reconstruction algorithm based on image inpainting, which includes two modules of defect estimation network and defect inpainting network. Firstly, the defect estimation network used the pre-training model to extract the deep features of the defect image and applied the Gaussian distance to estimate the background area. and then the image inpainting network applied the contextual attention mechanism to repair the non-background area of the defect image. Through the experimental analysis which compared with other state-of-the-art image reconstruction algorithms on the public Mvtec texture dataset, the superiority of the proposed algorithm is effectively verified.},
  keywords={Software algorithms;Estimation;Maintenance engineering;Feature extraction;Generative adversarial networks;Image reconstruction;Software engineering;defect detection;image inpainting;contextual attention mechanism},
  doi={10.1109/ICBASE53849.2021.00108},
  ISSN={},
  month={Sep.},}@INBOOK{10785997,
  author={Tardy, Jean},
  booktitle={The Creation of a Conscious Machine: The Quest for Artificial Intelligence}, 
  title={Chapter 1: Consciousness as Cognitive Capability}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={},
  keywords={Generative AI;External stimuli;Ear;Computer crashes;Boats;Autonomous agents;Auditory system;Urban areas;Sea measurements;Problem-solving},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9781501518331},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10785997},}@INPROCEEDINGS{10947841,
  author={Luhach, Suman},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={An Exploratory Study of AI Assisted Feedback and Formative Assessment of Writing Skills}, 
  year={2024},
  volume={},
  number={},
  pages={1277-1281},
  abstract={The present study integrates ChatGPT to facilitate formative assessment of paragraph writing of undergraduate students pursuing a law degree. The study explores the pattern and efficacy of AI assisted feedback along with students' perception about the feedback received on four prompts related to grammar, coherence, organization, and content. The data collection was done from 15 students through the transcripts of ChatGPT generated feedback, and questionnaire. The study employs mixed-method research to analyze the efficacy of feedback by studying the feedback pattern, and students' perception by analysis of their responses on a survey questionnaire. Results indicate that ChatGPT's explanatory and suggestive comments give a quick overview of gaps in students' writing and can contribute to achieve the desired learning outcomes in Outcome Based Education.},
  keywords={Training;Surveys;Generative AI;Refining;Organizations;Coherence;Writing;Data collection;Chatbots;Grammar;AI assisted feedback;writing skills;student perception;feedback pattern;ChatGPT},
  doi={10.1109/GlobalAISummit62156.2024.10947841},
  ISSN={},
  month={Sep.},}@ARTICLE{11068133,
  author={Xie, Zongwu and Xie, Guanghu and Liu, Yang and Zhang, Yonglong and Cao, Baoshi and Ji, Yiming and Wang, Zhengpu and Liu, Hong},
  journal={IEEE Robotics and Automation Letters}, 
  title={DexMGNet: Multi-Mode Dexterous Grasping in Cluttered Scenes With Generative Models}, 
  year={2025},
  volume={10},
  number={8},
  pages={8483-8490},
  abstract={Dexterous grasping is a crucial technique in humanoid robot manipulation. However, existing methods still fall short in effectively detecting dexterous grasps in cluttered environments. In this work, we propose DexMGNet, a novel multi-mode dexterous grasping framework designed for such challenging scenarios. We introduce the concept of pre-grasping and redefine dexterous grasping to enhance adaptability. We propose an effective pre-grasp and grasp data sampling strategy and develop a conditional generative model for grasp and pre-grasp generation. Additionally, we integrate pre-grasp collision detection within the hand's workspace, significantly improving grasping performance in cluttered environments. Our method supports multi-mode grasping, including two-finger, three-finger, and four-finger grasps, enabling greater flexibility across diverse grasping tasks. In real-world desktop grasping experiments, our approach achieves a 93.3% success rate in single-object scenes and a 78.3% success rate in multi-object scenes, demonstrating its effectiveness and superiority.},
  keywords={Grasping;Hands;Humanoid robots;Training;Shape;Point cloud compression;Adaptation models;Collision avoidance;Artificial intelligence;Faces;Dexterous grasping;cluttered scenes;generative models},
  doi={10.1109/LRA.2025.3585761},
  ISSN={2377-3766},
  month={Aug},}@ARTICLE{9344652,
  author={Aldegheishem, Abdulaziz and Anwar, Mubbashra and Javaid, Nadeem and Alrajeh, Nabil and Shafiq, Muhammad and Ahmed, Hasan},
  journal={IEEE Access}, 
  title={Towards Sustainable Energy Efficiency With Intelligent Electricity Theft Detection in Smart Grids Emphasising Enhanced Neural Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={25036-25061},
  abstract={In smart grids, electricity theft is the most significant challenge. It cannot be identified easily since existing methods are dependent on specific devices. Also, the methods lack in extracting meaningful information from high-dimensional electricity consumption data and increase the false positive rate that limit their performance. Moreover, imbalanced data is a hurdle in accurate electricity theft detection (ETD) using data driven methods. To address this problem, sampling techniques are used in the literature. However, the traditional sampling techniques generate insufficient and unrealistic data that degrade the ETD rate. In this work, two novel ETD models are developed. A hybrid sampling approach, i.e., synthetic minority oversampling technique with edited nearest neighbor, is introduced in the first model. Furthermore, AlexNet is used for dimensionality reduction and extracting useful information from electricity consumption data. Finally, a light gradient boosting model is used for classification purpose. In the second model, conditional wasserstein generative adversarial network with gradient penalty is used to capture the real distribution of the electricity consumption data. It is constructed by adding auxiliary provisional information to generate more realistic data for the minority class. Moreover, GoogLeNet architecture is employed to reduce the dataset's dimensionality. Finally, adaptive boosting is used for classification of honest and suspicious consumers. Both models are trained and tested using real power consumption data provided by state grid corporation of China. The proposed models' performance is evaluated using different performance metrics like precision, recall, accuracy, F1-score, etc. The simulation results prove that the proposed models outperform the existing techniques, such as support vector machine, extreme gradient boosting, convolution neural network, etc., in terms of efficient ETD.},
  keywords={Boosting;Meters;Generative adversarial networks;Gallium nitride;Adaptation models;Data models;Biological system modeling;Electricity theft detection;generative adversarial network;GoogLeNet;imbalanced data;Urban planning;SMOTEENN},
  doi={10.1109/ACCESS.2021.3056566},
  ISSN={2169-3536},
  month={},}@ARTICLE{9163376,
  author={Tingfei, Huang and Guangquan, Cheng and Kuihua, Huang},
  journal={IEEE Access}, 
  title={Using Variational Auto Encoding in Credit Card Fraud Detection}, 
  year={2020},
  volume={8},
  number={},
  pages={149841-149853},
  abstract={Machine learning approaches are widely used to analyze and detect the increasingly serious problem of credit card fraud. However, typical credit card datasets present imbalanced classification situations because of severely skewed class distributions. Although researchers have proposed some strategies to deal with these imbalances, disadvantages remain. We propose an oversampling method based on variational automatic coding (VAE), combined with classic deep learning techniques, to solve this problem. The VAE method is used to generate a large amount of diverse cases from minority groups in an imbalanced dataset, which are then used to train the classification network. The proposed method is tested on an open credit card fraud dataset, which contains transactions conducted by European cardholders over two days in September 2013. Experimental results show that the VAE method performs better than synthetic minority oversampling techniques and traditional deep neural network methods. In addition, it outperforms recent oversampling methods based on generative adversarial network (GAN) models. After submitting the extended dataset to the baseline for training, the test of the VAE model performs well on indicators including precision, F-measure, accuracy and specificity. These experimental results suggest that the VAE-based oversampling method can be effectively applied to imbalanced classification problems.},
  keywords={Credit cards;Machine learning;Training;Neural networks;Gallium nitride;Generative adversarial networks;Encoding;Credit card fraud;variational automatic coding;oversampling;generative adversarial network;deep learning},
  doi={10.1109/ACCESS.2020.3015600},
  ISSN={2169-3536},
  month={},}@ARTICLE{9171240,
  author={Kim, Doyeon and Joo, Donggyu and Kim, Junmo},
  journal={IEEE Access}, 
  title={TiVGAN: Text to Image to Video Generation With Step-by-Step Evolutionary Generator}, 
  year={2020},
  volume={8},
  number={},
  pages={153113-153122},
  abstract={Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames. This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.},
  keywords={Training;Generators;Gallium nitride;Generative adversarial networks;Image synthesis;Task analysis;Streaming media;Computer vision;deep learning;generative adversarial networks;video generation;text-to-video generation},
  doi={10.1109/ACCESS.2020.3017881},
  ISSN={2169-3536},
  month={},}@ARTICLE{8759893,
  author={Lu, Yang and Wang, Shigang and Zhao, Wenting and Zhao, Yan},
  journal={IEEE Access}, 
  title={WGAN-Based Robust Occluded Facial Expression Recognition}, 
  year={2019},
  volume={7},
  number={},
  pages={93594-93610},
  abstract={Research on facial expression recognition (FER) technology can promote the development of theoretical and practical applications for our daily life. Currently, most of the related works on this technology are focused on un-occluded FER. However, in real life, facial expression images often have partial occlusion; therefore, the accurate recognition of occluded facial expression images is a topic that should be explored. In this paper, we proposed a novel Wasserstein generative adversarial network-based method to perform occluded FER. After complementing the face occlusion image with complex facial expression information, the recognition is achieved by learning the facial expression features of the images. This method consists of a generator G and two discriminators D1 and D2. The generator naturally complements occlusion in the expression image under the triple constraints of weighted reconstruction loss lwr, triplet loss lt, and adversarial loss la. We optimize the discriminator D1 to distinguish between real and fake by constructing an adversarial loss la between the generated complementing images, original un-occluded images, and small-scale-occluded images based on the Wasserstein distance. Finally, the FER is completed by introducing classification loss lc into D2. To verify the effectiveness of the proposed method, an experimental analysis was performed on the AffectNet and RAF-DB datasets. The visual occlusion complementing results, comparison of recognition rates of facial expression images with and without de-occlusion processing, and T-distributed stochastic neighbor embedding visual analysis of facial expression features all prove the effectiveness of the proposed method. The experimental results show that the proposed method is better than the existing state-of-the-art methods.},
  keywords={Feature extraction;Generative adversarial networks;Face recognition;Generators;Image recognition;Face;Filling;Facial expression recognition;partial occlusion;image complementation;Wasserstein generative adversarial network},
  doi={10.1109/ACCESS.2019.2928125},
  ISSN={2169-3536},
  month={},}@ARTICLE{10185559,
  author={Adib, Edmonmd and Fernandez, Amanda S. and Afghah, Fatemeh and Prevost, John J.},
  journal={IEEE Access}, 
  title={Synthetic ECG Signal Generation Using Probabilistic Diffusion Models}, 
  year={2023},
  volume={11},
  number={},
  pages={75818-75828},
  abstract={Deep learning image processing models have had remarkable success in recent years in generating high quality images. Particularly, the Improved Denoising Diffusion Probabilistic Models (DDPM) have shown superiority in image quality to the state-of-the-art generative models, which motivated us to investigate their capability in the generation of the synthetic electrocardiogram (ECG) signals. In this work, synthetic ECG signals are generated by the Improved DDPM and by the Wasserstein GAN with Gradient Penalty (WGAN-GP) models and then compared. To this end, we devise a pipeline to utilize DDPM in its original  $2D$  form. First, the  $1D$  ECG time series data are embedded into the  $2D$  space, for which we employed the Gramian Angular Summation/Difference Fields (GASF/GADF) as well as Markov Transition Fields (MTF) to generate three  $2D$  matrices from each ECG time series, which when put together, form a 3-channel  $2D$  datum. Then  $2D$  DDPM is used to generate  $2D~3$ -channel synthetic ECG images. The  $1D$  ECG signals are created by de-embedding the  $2D$  generated image files back into the  $1D$  space. This work focuses on unconditional models and the generation of Normal Sinus Beat ECG signals exclusively, where the Normal Sinus Beat class from the MIT-BIH Arrhythmia dataset is used in the training phase. The quality, distribution, and the authenticity of the generated ECG signals by each model are quantitatively evaluated and compared. Our results show that in the proposed pipeline and in the particular setting of this paper, the WGAN-GP model is consistently superior to DDPM in all the considered metrics.},
  keywords={Electrocardiography;Generative adversarial networks;Training;Probabilistic logic;Arrhythmia;Time series analysis;Task analysis;Deep learning;Noise reduction;Markov processes;Image quality;Image processing;Generative adversarial networks;wasserstein GAN;synthetic ECG generation;probabilistic diffusion model;improved denoising diffusion probabilistic models (DDPM)},
  doi={10.1109/ACCESS.2023.3296542},
  ISSN={2169-3536},
  month={},}@ARTICLE{9416431,
  author={Hossain, Md. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid and Bennamoun, Mohammed},
  journal={IEEE Access}, 
  title={Text to Image Synthesis for Improved Image Captioning}, 
  year={2021},
  volume={9},
  number={},
  pages={64918-64928},
  abstract={Generating textual descriptions of images has been an important topic in computer vision and natural language processing. A number of techniques based on deep learning have been proposed on this topic. These techniques use human-annotated images for training and testing the models. These models require a large number of training data to perform at their full potential. Collecting human generated images with associative captions is expensive and time-consuming. In this paper, we propose an image captioning method that uses both real and synthetic data for training and testing the model. We use a Generative Adversarial Network (GAN) based text to image generator to generate synthetic images. We use an attention-based image captioning method trained on both real and synthetic images to generate the captions. We demonstrate the results of our models using both qualitative and quantitative analysis on popularly used evaluation metrics. We show that our experimental results achieve two fold benefits of our proposed work: i) it demonstrates the effectiveness of image captioning for synthetic images, and ii) it further improves the quality of the generated captions for real images, understandably because we use additional images for training.},
  keywords={Training;Visualization;Generative adversarial networks;Decoding;Feature extraction;Computer architecture;Generators;Image captioning;synthetic images;attention;generative adversarial network},
  doi={10.1109/ACCESS.2021.3075579},
  ISSN={2169-3536},
  month={},}@ARTICLE{9120029,
  author={Wang, Jing and Li, Ping and Deng, Jianhua and Du, Yongzhao and Zhuang, Jiafu and Liang, Peidong and Liu, Peizhong},
  journal={IEEE Access}, 
  title={CA-GAN: Class-Condition Attention GAN for Underwater Image Enhancement}, 
  year={2020},
  volume={8},
  number={},
  pages={130719-130728},
  abstract={Underwater images suffer from serious color distortion and detail loss because of the wavelength-dependent light absorption and scattering, which seriously influences the subsequent underwater object detection and recognition. The latest methods for underwater image enhancement are based on deep models, which focus on finding a mapping function from the underwater image subspace to a ground-truth image subspace. They neglect the diversity of underwater conditions which leads to different background colors of underwater images. In this paper, we propose a Class-condition Attention Generative Adversarial Network (CA-GAN) to enhance an underwater image. We build an underwater image dataset which contains ten categories generated by the simulator with different water attenuation coefficient and depth. Relying on the underwater image classes, CA-GAN creates a many-to-one mapping function for an underwater image. Moreover, in order to generate the realistic image, attention mechanism is utilized. In the channel attention block, the feature maps in the front-end layers and the back-end layers are fused along channels, and in the spatial attention block, feature maps are pixel-wise fused. Extensive experiments are conducted on synthetic and real underwater images. The experimental results demonstrate that CA-GAN can effectively recover color and detail of various scenes of underwater images and is superior to the state-of-the-art methods.},
  keywords={Gallium nitride;Image enhancement;Image color analysis;Generative adversarial networks;Attenuation;Sea measurements;Distortion;Underwater image;generative adversarial network},
  doi={10.1109/ACCESS.2020.3003351},
  ISSN={2169-3536},
  month={},}@ARTICLE{9539183,
  author={Singh, Akhil and Jaiswal, Vaibhav and Joshi, Gaurav and Sanjeeve, Adith and Gite, Shilpa and Kotecha, Ketan},
  journal={IEEE Access}, 
  title={Neural Style Transfer: A Critical Review}, 
  year={2021},
  volume={9},
  number={},
  pages={131583-131613},
  abstract={Neural Style Transfer (NST) is a class of software algorithms that allows us to transform scenes, change/edit the environment of a media with the help of a Neural Network. NST finds use in image and video editing software allowing image stylization based on a general model, unlike traditional methods. This made NST a trending topic in the entertainment industry as professional editors/media producers create media faster and offer the general public recreational use. In this paper, the current progress in Neural Style Transfer with all related aspects such as still images and videos is presented critically. The authors looked at the different architectures used and compared their advantages and limitations. Multiple literature reviews focus on either the Neural Style Transfer (of images) or cover Generative Adversarial Networks (GANs) that generate video. As per the authors’ knowledge, this is the only research article that looks at image and video style transfer, particularly mobile devices with high potential usage. This article also reviewed the challenges faced in applyingvideo neural style transfer in real-time on mobile devices and presents research gaps with future research directions. NST, a fascinating deep learning application, has considerable research and application potential in the coming years.},
  keywords={Streaming media;Symbiosis;Generators;Generative adversarial networks;Training;Mobile handsets;Real-time systems;Style transfer;video style transfer;mobile;convolutional neural networks;generative adversarial networks},
  doi={10.1109/ACCESS.2021.3112996},
  ISSN={2169-3536},
  month={},}@ARTICLE{9245471,
  author={Chang, Ching-Chun},
  journal={IEEE Access}, 
  title={Adversarial Learning for Invertible Steganography}, 
  year={2020},
  volume={8},
  number={},
  pages={198425-198435},
  abstract={Deep neural networks have revolutionised the research landscape of steganography. However, their potential has not been explored in invertible steganography, a special class of methods that permits the recovery of distorted objects due to steganographic perturbations to their pristine condition. In this paper, we revisit the regular-singular (RS) method and show that this elegant but obsolete invertible steganographic method can be reinvigorated and brought forwards to modern generation via neuralisation. Towards developing a renewed RS method, we introduce adversarial learning to capture the regularity of natural images automatically in contrast to handcrafted discrimination functions based on heuristic image prior. Specifically, we train generative adversarial networks (GANs) to predict bit-planes that have been used to carry hidden information. We then form a synthetic image and use it as a reference to provide guidance on data embedding and image recovery. Experimental results showed a significant improvement over the prior implementation of the RS method based on large-scale statistical evaluations.},
  keywords={Gallium nitride;Nonlinear distortion;Generative adversarial networks;Training;Neural networks;Image coding;Generators;Convolutional neural networks;generative adversarial networks;invertible steganography},
  doi={10.1109/ACCESS.2020.3034936},
  ISSN={2169-3536},
  month={},}@ARTICLE{9333671,
  author={Sanabria, Andrea Rosales and Zambonelli, Franco and Ye, Juan},
  journal={IEEE Access}, 
  title={Unsupervised Domain Adaptation in Activity Recognition: A GAN-Based Approach}, 
  year={2021},
  volume={9},
  number={},
  pages={19421-19438},
  abstract={Sensor-based human activity recognition (HAR) is having a significant impact in a wide range of applications in smart city, smart home, and personal healthcare. Such wide deployment of HAR systems often faces the annotation-scarcity challenge; that is, most of the HAR techniques, especially the deep learning techniques, require a large number of training data while annotating sensor data is very time- and effort-consuming. Unsupervised domain adaptation has been successfully applied to tackle this challenge, where the activity knowledge from a well-annotated domain can be transferred to a new, unlabelled domain. However, these existing techniques do not perform well on highly heterogeneous domains. This article proposes shift-GAN that integrate bidirectional generative adversarial networks (Bi-GAN) and kernel mean matching (KMM) in an innovative way to learn intrinsic, robust feature transfer between two heterogeneous domains. Bi-GAN consists of two GANs that are bound by a cyclic constraint, which enables more effective feature transfer than a classic, single GAN model. KMM is a powerful non-parametric technique to correct covariate shift, which further improves feature space alignment. Through a series of comprehensive, empirical evaluations, shift-GAN has not only achieved its superior performance over 10 state-of-the-art domain adaptation techniques but also demonstrated its effectiveness in learning activity-independent, intrinsic feature mappings between two domains, robustness to sensor noise, and less sensitivity to training data.},
  keywords={Robot sensing systems;Accelerometers;Adaptation models;Kernel;Generative adversarial networks;Training data;Task analysis;Human activity recognition;domain adaptation;ensemble learning;generative adversarial networks;covariate shift;kernel mean matching},
  doi={10.1109/ACCESS.2021.3053704},
  ISSN={2169-3536},
  month={},}@ARTICLE{9866776,
  author={Gonwirat, Sarayut and Surinta, Olarik},
  journal={IEEE Access}, 
  title={DeblurGAN-CNN: Effective Image Denoising and Recognition for Noisy Handwritten Characters}, 
  year={2022},
  volume={10},
  number={},
  pages={90133-90148},
  abstract={Many problems can reduce handwritten character recognition performance, such as image degradation, light conditions, low-resolution images, and even the quality of the capture devices. However, in this research, we have focused on the noise in the character images that could decrease the accuracy of handwritten character recognition. Many types of noise penalties influence the recognition performance, for example, low resolution, Gaussian noise, low contrast, and blur. First, this research proposes a method that learns from the noisy handwritten character images and synthesizes clean character images using the robust deblur generative adversarial network (DeblurGAN). Second, we combine the DeblurGAN architecture with a convolutional neural network (CNN), called DeblurGAN-CNN. Subsequently, two state-of-the-art CNN architectures are combined with DeblurGAN, namely DeblurGAN-DenseNet121 and DeblurGAN-MobileNetV2, to address many noise problems and enhance the recognition performance of the handwritten character images. Finally, the DeblurGAN-CNN could transform the noisy characters to the new clean characters and recognize clean characters simultaneously. We have evaluated and compared the experimental results of the proposed DeblurGAN-CNN architectures with the existing methods on four handwritten character datasets: n-THI-C68, n-MNIST, THI-C68, and THCC-67. For the n-THI-C68 dataset, the DeblurGAN-CNN achieved above 98% and outperformed the other existing methods. For the n-MNIST, the proposed DeblurGAN-CNN achieved an accuracy of 97.59% when the AWGN+Contrast noise method was applied to the handwritten digits. We have evaluated the DeblurGAN-CNN on the THCC-67 dataset. The result showed that the proposed DeblurGAN-CNN achieved an accuracy of 80.68%, which is significantly higher than the existing method, approximately 10%.},
  keywords={Character recognition;Feature extraction;Noise measurement;Generative adversarial networks;Image recognition;Handwriting recognition;Convolutional neural networks;Handwritten character recognition;denoising image;generative adversarial network;DeblurGAN;convolutional neural network},
  doi={10.1109/ACCESS.2022.3201560},
  ISSN={2169-3536},
  month={},}@ARTICLE{9091008,
  author={Hashmi, Mohammad Farukh and Ashish, B. Kiran Kumar and Keskar, Avinash G. and Bokde, Neeraj Dhanraj and Geem, Zong Woo},
  journal={IEEE Access}, 
  title={FashionFit: Analysis of Mapping 3D Pose and Neural Body Fit for Custom Virtual Try-On}, 
  year={2020},
  volume={8},
  number={},
  pages={91603-91615},
  abstract={Visual compatibility and virtual feel are critical metrics for fashion analysis yet are missing in existing fashion designs and platforms. An explicit model is much needed for implanting visual compatibility through fashion image inpainting and virtual try-on. With rapid advancements in the Computer Vision realm, the increase in creating customer experience which leads to the great potential of interest to retailers and customers. The public datasets available are very much fit for generating outfits from Generative Adversarial Networks (GANs) but the custom outfits of the users themselves lead to low accuracy levels. This work is the first step in analyzing and experimenting with the fit of custom outfits and visualizing it to the users on them which creates the great customer experience. The work analyses the need for providing visualization of custom outfits on users in the large corpora of AI in Fashion. The authors propose a novel architecture which facilitates the combining outfits provided by the retailers and visualize it on the users themselves using Neural Body Fit. This work creates a benchmark in disentangling the custom generation of cloth outfits using GANs and virtually trying it on the users to ensure a virtual-photorealistic appearance and results to create a great customer experience by using AI. Extensive experiments show the high accuracy levels on custom outfits generated by GANs but not in customized levels. This experiment creates new state-of-art results by plotting users pose for calculating the lengths of each body-part segment (hand, leg, and so forth), segmentation + NBF for accurate fitting of the cloth outfit. This paper is different from all other competitors in terms of approach for the virtual try-on for creating a new customer experience.},
  keywords={Gallium nitride;Visualization;Clothing;Computer vision;Generative adversarial networks;Computer architecture;Image segmentation;Neural body fit;generative adversarial networks (GANs);pose;customer experience;segmentation},
  doi={10.1109/ACCESS.2020.2993574},
  ISSN={2169-3536},
  month={},}@ARTICLE{8868089,
  author={Abiko, Ryo and Ikehara, Masaaki},
  journal={IEEE Access}, 
  title={Single Image Reflection Removal Based on GAN With Gradient Constraint}, 
  year={2019},
  volume={7},
  number={},
  pages={148790-148799},
  abstract={When we take a picture through glass windows, the photographs are often degraded by undesired reflections. To separate reflection layer and background layer is an important problem for enhancing image quality. However, single-image reflection removal is a challenging process because of the ill-posed nature of the problem. In this paper, we propose a single-image reflection removal method based on generative adversarial networks. Our network is an end-to-end trained network with four types of losses. It includes pixel loss, feature loss, adversarial loss, and gradient constraint loss. We propose a novel gradient constraint loss in order to separate the background layer and the reflection layer clearly. Gradient constraint loss is applied in a gradient domain and it minimizes the correlation between the background and reflection layer. Owing to the novel loss and our new synthetic dataset, our reflection removal method outperforms state-of-the-art methods in PSNR and SSIM, especially in real world images.},
  keywords={Generative adversarial networks;Training;Generators;Feature extraction;Correlation;Glass;Task analysis;Image restoration;deep learning;reflection removal;image separation;generative adversarial network},
  doi={10.1109/ACCESS.2019.2947266},
  ISSN={2169-3536},
  month={},}@ARTICLE{8863362,
  author={Zhang, Weijia},
  journal={IEEE Access}, 
  title={Generating Adversarial Examples in One Shot With Image-to-Image Translation GAN}, 
  year={2019},
  volume={7},
  number={},
  pages={151103-151119},
  abstract={Deep Neural Networks (DNNs) provide state-of-the-art results for most machine learning and computer vision tasks. However, they have been found susceptible to adversarial examples. In the recent literature, many ways of generating adversarial examples have been discovered. In this work, we propose a novel method to generate adversarial examples with generative adversarial networks (GANs). Compared to traditional optimisation-based methods, our method provides a fast yet powerful alternative for adversary generation. Unlike other GAN-based approaches in the literature which learn to generate an intermediate perturbation vector, our method generates adversarial examples from benign input images in a straightforward manner. By directly generating adversarial examples from given input images, our method produces perturbations that better align with the underlying edge and shape contained in the inputs, hence more natural-looking and imperceptible to human eyes. We evaluate our method on the MNIST and the CIFAR-10 dataset and demonstrate that it outperforms the state-of-the-art GAN-based attack AdvGAN with similar attack capability in terms of distortion. We show that our method produces competitive results to notable optimisation-based attacks in the literature including the strongest Carlini & Wagner (CW) attack.},
  keywords={Measurement;Perturbation methods;Generative adversarial networks;Neural networks;Task analysis;Distortion;Gallium nitride;Adversarial examples;DNN attacks;generative adversarial networks (GANs)},
  doi={10.1109/ACCESS.2019.2946461},
  ISSN={2169-3536},
  month={},}@ARTICLE{9627928,
  author={Wu, Danlan and Hur, Kyeon and Xiao, Zhifeng},
  journal={IEEE Access}, 
  title={A GAN-Enhanced Ensemble Model for Energy Consumption Forecasting in Large Commercial Buildings}, 
  year={2021},
  volume={9},
  number={},
  pages={158820-158830},
  abstract={Building an accurate and robust energy consumption prediction model is a core mission for the energy management and operation system of a smart building. Prior efforts have explored numerous predictive models in various load prediction scenarios. However, the joint effect of data augmentation and ensemble learning in energy forecasting has not been fully explored. In this paper, we propose a generative adversarial network (GAN)-enhanced ensemble model for energy consumption forecasting in large commercial buildings. The ensemble model aggregates five constituent models with a stacking ensemble method. In addition, we employ a GAN to learn the sample distribution from the original dataset and generate high-quality samples to enhance the training set. The augmented dataset allows a model to be trained with more diverse samples to increase its robustness. A series of experiments are conducted to validate the proposed method with three GAN variants using three performance metrics, including mean absolute error (MAE), root mean square error (RMSE), and coefficient of variation of root mean square error (CVRMSE). Results demonstrate the GAN-enhanced ensemble models are more robust with consistent improvement in reducing the prediction errors. The best ensemble model, enhanced by the Information Maximizing GAN (InfoGAN), outperforms the model without augmentation by decreasing the average error by 1.71, 1.63, and 4.72%, in MAE, RMSE, and CVRMSE respectively, validating its piratical value for building an energy consumption forecasting model in a real-world system.},
  keywords={Load modeling;Predictive models;Buildings;Generative adversarial networks;Energy consumption;Training;Forecasting;Energy consumption forecasting;machine learning;ensemble model;generative adversarial network;data augmentation},
  doi={10.1109/ACCESS.2021.3131185},
  ISSN={2169-3536},
  month={},}@ARTICLE{9352788,
  author={Zhang, Han and Zhu, Hongqing and Yang, Suyi and Li, Wenhao},
  journal={IEEE Access}, 
  title={DGattGAN: Cooperative Up-Sampling Based Dual Generator Attentional GAN on Text-to-Image Synthesis}, 
  year={2021},
  volume={9},
  number={},
  pages={29584-29598},
  abstract={Text-to-image synthesis task aims at generating images consistent with input text descriptions and is well developed by the Generative Adversarial Network (GAN). Although GAN based image generation approaches have achieved promising results, synthesizing quality is sometimes unsatisfied due to discursive generation of background and object. In this article, we propose a cooperative up-sampling based Dual Generator attentional GAN (DGattGAN) to generate high-quality images from text description. To achieve this, two generators with individual generation purpose are established to decouple object and background generation. In particular, we introduce a cooperative up-sampling mechanism to build cooperation between object and background generators during training. This strategy is potentially very useful as any dual generator architecture in GAN models can benefit from this mechanism. Furthermore, we propose an asymmetric information feeding scheme to distinguish two synthesis tasks, such that each generator only synthesizes based on semantic information they accept. Taking advantage of effective dual generator, the attention mechanism we incorporated on object generator could devote to fine-grained details generation on actual targeted objects. Experiments on Caltech-UCSD Bird (CUB) and Oxford-102 datasets suggest that generated images by the proposed model are more realistic and consistent with input text, and DGattGAN is competent compared to state-of-the-art methods according to Inception Score (IS) and R-precision metrics. Our codes are available at: https://github.com/ecfish/DGattGAN.},
  keywords={Generators;Image resolution;Generative adversarial networks;Gallium nitride;Task analysis;Image synthesis;Visualization;Asymmetric information feeding;cooperative up-sampling;dual generator;generative adversarial networks;text-to-image synthesis},
  doi={10.1109/ACCESS.2021.3058674},
  ISSN={2169-3536},
  month={},}@ARTICLE{10553223,
  author={Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Xing, Zhenchang and Whittle, Jon},
  journal={IEEE Software}, 
  title={Toward Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model-Based Systems}, 
  year={2024},
  volume={41},
  number={6},
  pages={91-100},
  abstract={To address AI architecture design challenges, we present an architecture evolution of AI systems in the era of foundation models, transitioning from “foundation-model-as-a-connector” to “foundation-model-as-a-monolithic architecture.” We then identify key design decisions and propose a pattern-oriented reference architecture for designing responsible foundation-model-based systems.},
  keywords={Frequency modulation;Artificial intelligence;Software development management;Data models;Generative AI;Supply chain management;Chatbots;Software engineering;Software architecture;Large language models},
  doi={10.1109/MS.2024.3406333},
  ISSN={1937-4194},
  month={Nov},}@ARTICLE{9146826,
  author={Li, Mingchun and Chen, Dali and Liu, Shixin},
  journal={IEEE Access}, 
  title={Grain Boundary Detection Based on Multi-Level Loss From Feature and Adversarial Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={135640-135651},
  abstract={The shape, size and distribution of grains play an important role in the quality of alloy materials. However, in the actual production process, the grain boundary is not always visible in the metallographic image observed by microscope. In this paper, we propose an end-to-end deep network named GF-RCF based on adversary network and feature learning to detect grain boundaries effectively. The network is guided by a novel multi-level loss through its three parts: base network, adversary network and metric module, which are used for boundary detection, boundary inpainting and feature learning respectively. Specifically, for base network, its architecture is based on richer convolution feature to achieve basic boundary detection by pixel-level loss. For adversary network, it employs the image-level adversary loss and feature matching loss to repair the invisible boundary. For metric module, it involves a feature learning strategy by comparing the features of unlabeled data with the labeled data to improve the generalization of the network. The multi-level loss refers to the total loss of pixel-level, image-level and feature-level obtained from the three subnetwork, respectively. From the perspective of machine learning, GF-RCF involves semi-supervised learning, transfer learning and generative adversarial network, which makes our model have more powerful learning ability. In experiments, GF-RCF achieves 0.8792 F1 score, surpassing other methods and reaching the state of the art.},
  keywords={Feature extraction;Grain boundaries;Generative adversarial networks;Task analysis;Machine learning;Metals;Measurement;Grain detection;boundary detection;generative adversarial network;semi-supervised learning;transfer learning},
  doi={10.1109/ACCESS.2020.3011703},
  ISSN={2169-3536},
  month={},}@ARTICLE{10443392,
  author={Junoh, Suhardi Azliy and Pyun, Jae-Young},
  journal={IEEE Access}, 
  title={Augmentation of Fingerprints for Indoor BLE Localization Using Conditional GANs}, 
  year={2024},
  volume={12},
  number={},
  pages={30176-30190},
  abstract={Location estimation in indoor environments using radiofrequency (RF) has garnered considerable attention in recent years owing to the widespread adoption of mobile devices. RF-based fingerprinting—a direct approach that allows location estimation based on observed signals—relies on manual surveys during the offline phase to create a radio map with coordinates and RF measurements at multiple locations. The accuracy of RF fingerprint-based localization is proportional to the number of reference points. However, conventional site survey procedures incur substantial expenses. To alleviate the workload of site surveys and address the challenge of incomplete fingerprint databases, we propose a data-augmentation method to complement existing fingerprint data. Our approach leverages a conditional generative adversarial network with long short-term memory (CGAN-LSTM) prediction model to effectively learn the intricate patterns inherent in the initial training data and generate high-quality synthetic data that align with the underlying data distribution. In an experimental evaluation conducted on a real testbed, our data augmentation framework increased the average localization accuracy by 15.74% compared with fingerprinting without data augmentation. Compared with linear interpolation, inverse distance weighting, and Gaussian process regression, the proposed approach demonstrates an average accuracy improvement ranging from 1.84% to 14.04%, achieving average accuracies of 1.065 and 1.956 m in both scenarios. In experiments conducted in two typical indoor environments using sparse data, the proposed approach substantially reduced localization error and proved comparable to state-of-the-art data-augmentation methods.},
  keywords={Fingerprint recognition;Location awareness;Radio frequency;Wireless fidelity;Costs;Databases;Surveys;Bluetooth Low Energy;Data augmentation;Generative adversarial networks;Indoor environment;Bluetooth low energy (BLE);fingerprint;data augmentation;generative adversarial network (GAN);location estimation},
  doi={10.1109/ACCESS.2024.3368449},
  ISSN={2169-3536},
  month={},}@ARTICLE{9862988,
  author={Zhao, Duan and Li, Tao and Zou, Xiangyu and He, Yaoyi and Zhao, Lichang and Chen, Hui and Zhuo, Minmin},
  journal={IEEE Access}, 
  title={Multimodal Pedestrian Trajectory Prediction Based on Relative Interactive Spatial-Temporal Graph}, 
  year={2022},
  volume={10},
  number={},
  pages={88707-88718},
  abstract={Predicting and understanding pedestrian intentions is crucial for autonomous vehicles and mobile robots to navigate in a crowd. However, the movement of pedestrian is random. Pedestrian trajectory modeling needs to consider not only the past movement of pedestrians, the interaction between different pedestrians, the constraints of static obstacles in the scene, but also multi-modal of the human trajectory, which brings challenges to pedestrian trajectory prediction. Most of the existing trajectory prediction methods only consider the interaction between pedestrians in the scene, ignoring the static obstacles in the scene can also have impacts on the trajectory of pedestrian. In this paper, a scalable relative interactive spatial-temporal graph generation adversarial network architecture (RISTG-GAN) is proposed to generate a reasonable multi-modal prediction trajectory by considering the interaction effects of all agents in the scene. Our method extends recent work on trajectory prediction. First, LSTM nodes are flexibly used to model the spatial-temporal graph of human-environment interactions, and the spatial-temporal graph is converted into feed-forward differentiable feature coding, and the time attention module is proposed to capture the trajectory information in time domain and learn the time dependence in long time range. Then, we capture the relative importance of the interaction of all agents in the scene on the pedestrian trajectory through the improved relative scaled dot product attention and use the generative adversarial network architecture for training to generate reasonable pedestrian future trajectory distribution. Experiments on five commonly used real public datasets show that RISTG-GAN is better than previous work in terms of reasoning speed, accuracy and the rationality of trajectory prediction.},
  keywords={Trajectory;Predictive models;Hidden Markov models;Logic gates;Generative adversarial networks;Legged locomotion;Visualization;Pedestrian trajectory prediction;spatial-temporal graph;time attention;relative scaled dot product attention;generative adversarial network},
  doi={10.1109/ACCESS.2022.3200066},
  ISSN={2169-3536},
  month={},}@ARTICLE{9086494,
  author={Zou, Zi-Zhuang and Xie, Kai and Zhao, Yi-Fei and Wan, Jing and Lan, Lan and Wen, Chang},
  journal={IEEE Access}, 
  title={Intelligent Assessment of Percutaneous Coronary Intervention Based on GAN and LSTM Models}, 
  year={2020},
  volume={8},
  number={},
  pages={90640-90651},
  abstract={Coronary artery calcification affects the arteries that supply the heart with blood, and percutaneous coronary intervention (PCI) is a direct and effective surgery to alleviate this symptom. In this paper, we propose a framework to judge if a patient requires surgery, based on cardiac computerized tomography scans. We adopt generative adversarial network to segment the calcified areas from slices. This architecture provides an environment for the generator to perform joint learning from ground truth images and the high-resolution discriminator. We use images reconstructed using two types of filters to test our method. An F1 score of 96.1% and 85.0% was achieved for the soft and sharp filters. In addition, we explored different recurrent neural networks for making the final decision. Including long short-term memory, which was ultimately used to deal with the calcium score normalized by the age and score threshold. Using the soft reconstruction image as the input, the whole framework achieved an accuracy of 76.6%. These results certify that our method can precisely locate lesion in artery, and make a reasonable risk assessment for PCI.},
  keywords={Computed tomography;Gallium nitride;Generative adversarial networks;Heart;Image segmentation;Arteries;Solid modeling;Generative adversarial network;low-dose cardiac CT;recurrent neural network;percutaneous coronary intervention;coronary calcium scoring},
  doi={10.1109/ACCESS.2020.2992578},
  ISSN={2169-3536},
  month={},}@ARTICLE{8723371,
  author={Weng, Yu and Zhou, Haiwen and Wan, Jian},
  journal={IEEE Access}, 
  title={Image Inpainting Technique Based on Smart Terminal: A Case Study in CPS Ancient Image Data}, 
  year={2019},
  volume={7},
  number={},
  pages={69837-69847},
  abstract={Cyber-physical system (CPS) can intelligently feel the interactive information between data and terminal. The combination of CPS data and intelligent terminal can make terminal devices communicate with each other, coordinate operation, and share information in a complex environment. At present, the demand for smart terminal technology in university libraries and national cultural museums is also increasing. However, the data for smart terminal display and development of shared services face the problem that the original data is difficult to collect and the incomplete images are difficult to repair. In order to display the ancient books in a smart terminal, we have established a model for image inpainting through deep learning. In this paper, we set up a smart terminal display solution to display the digital protection effect of incomplete character images. Then, we propose a model based on Conditional Generative Adversarial Network (CGAN) to solve the problem of repairing incomplete character images. We put this model into the computing module and deployed it to the cloud computing platform of the smart terminal. Moreover, we construct a handwritten Yi character image data set in the cloud. To verify the CGAN calculation model, we design and construct two quality detection tests. According to the experimental results, the image inpainting quality of the CGAN is better than the traditional image restoration technology. Based on the experimental analysis and evaluation criteria, we find that the CGAN calculation model is of great significance for ancient book inpainting.},
  keywords={Maintenance engineering;Image restoration;Computational modeling;Data models;Gallium nitride;Generative adversarial networks;Libraries;CPS data;smart terminal;character image;conditional generative adversarial network},
  doi={10.1109/ACCESS.2019.2919326},
  ISSN={2169-3536},
  month={},}@ARTICLE{10363188,
  author={Araujo, Gustavo F. and Machado, Renato and Pettersson, Mats I.},
  journal={IEEE Access}, 
  title={Synthetic SAR Data Generator Using Pix2pix cGAN Architecture for Automatic Target Recognition}, 
  year={2023},
  volume={11},
  number={},
  pages={143369-143386},
  abstract={Synthetic Aperture Radar (SAR) technology has unique advantages but faces challenges in obtaining enough data for noncooperative target classes. We propose a method to generate synthetic SAR data using a modified pix2pix Conditional Generative Adversarial Networks (cGAN) architecture. The cGAN is trained to create synthetic SAR images with specific azimuth and elevation angles, demonstrating its capability to closely mimic authentic SAR imagery through convergence and collapsing analyses. The study uses a model-based algorithm to assess the practicality of the generated synthetic data for Automatic Target Recognition (ATR). The results reveal that the classification accuracy achieved with synthetic data is comparable to that attained with original data, highlighting the effectiveness of the proposed method in mitigating the limitations imposed by noncooperative SAR data scarcity for ATR. This innovative approach offers a promising solution to craft customized synthetic SAR data, ultimately enhancing ATR performance in remote sensing.},
  keywords={Generative adversarial networks;Radar polarimetry;Training;Optical imaging;Generators;Neurons;Classification algorithms;Target recognition;Classification algorithms;Data augmentation;Synthetic aperture radar;Automatic target recognition;classification;conditional generative adversarial networks;data augmentation;Pix2Pix;synthetic aperture radar;synthetic data},
  doi={10.1109/ACCESS.2023.3343910},
  ISSN={2169-3536},
  month={},}@ARTICLE{10630805,
  author={Patel, Harsh and Boucher, Dominique and Fallahzadeh, Emad and Hassan, Ahmed E. and Adams, Bram},
  journal={IEEE Software}, 
  title={A State-of-the-Practice Release-Readiness Checklist for Generative AI-Based Software Products: A Gray Literature Survey}, 
  year={2025},
  volume={42},
  number={1},
  pages={74-83},
  abstract={We investigate the complexities of integrating large language models (LLMs) into software products, focusing on challenges encountered for determining their readiness for release. Our review of gray literature identifies common challenges in deploying LLMs, from pretraining and fine-tuning to user experience considerations.},
  keywords={Software engineering;Data models;Artificial intelligence;Production systems;Ethics;Companies;Large language models;Generative AI;Systematic literature review},
  doi={10.1109/MS.2024.3440190},
  ISSN={1937-4194},
  month={Jan},}@ARTICLE{10629164,
  author={Ebert, Christof and Arockiasamy, John Pravin and Hettich, Lennard and Weyrich, Michael},
  journal={IEEE Software}, 
  title={Hints for Generative AI Software Development}, 
  year={2024},
  volume={41},
  number={5},
  pages={24-33},
  abstract={Developers benefit from enhanced productivity with GAI. Yet, often they question how to approach GAI development and how to integrate GAI to their systems. This article provides guidance for developing GAI software and developing software with GAI. Practical hints are shared from industrial settings.},
  keywords={Productivity;Generative AI;Software development management;Productivity;Artificial intelligence},
  doi={10.1109/MS.2024.3410641},
  ISSN={1937-4194},
  month={Sep.},}@ARTICLE{10332173,
  author={Kafunah, Jefkine and Verma, Priyanka and Ali, Muhammad Intizar and Breslin, John G.},
  journal={IEEE Access}, 
  title={Out-of-Distribution Data Generation for Fault Detection and Diagnosis in Industrial Systems}, 
  year={2023},
  volume={11},
  number={},
  pages={135061-135073},
  abstract={The emergence of Industry 4.0 has transformed modern-day factories into high-tech industrial sites through rapid automation and increased access to real-time data. Deep learning approaches possessing superior capabilities for intelligent, data-driven fault diagnosis have become critical in ensuring process safety and reliability in these industrial sites. However, such applications trained exclusively on in-distribution process data face challenges in the wake of previously unseen out-of-distribution (OOD) data in the real world. This paper addresses the challenge of out-of-distribution data detection for deep learning-based fault diagnosis models by generating synthetic data to simulate real-world anomalies not present in the training set. We propose Manifold Guided Sampling (MGS), a data-driven method for generating synthetic OOD samples from the in-distribution data-supporting manifold estimated through a deep generative model. Synthetic data from MGS enhances the model capacity for prediction uncertainty quantification, resulting in safe and reliable models for real-world industrial process monitoring. Furthermore, the MGS algorithm maintains the in-distribution data feature space as a reference point during data generation to ensure the resulting synthetic OOD data is realistic. We analyze the effectiveness of MGS through experiments conducted on the steel plates faults dataset and demonstrate that augmenting training data with synthetic data from MGS enhances the model performance in OOD detection tasks and provides robustness against dataset distributional shifts. The findings underscore the effectiveness of utilizing synthetic MGS-generated OOD data in scenarios where real-world OOD data is limited, enabling better generalization and more reliable fault detection in practical applications.},
  keywords={Training;Manifolds;Data models;Fault diagnosis;Vibrations;Feature extraction;Fault detection;Generative adversarial networks;Process monitoring;Deep generative models;fault diagnosis;process monitoring;safety-critical;out-of-distribution data;variational autoencoder;uncertainty estimation},
  doi={10.1109/ACCESS.2023.3337658},
  ISSN={2169-3536},
  month={},}@ARTICLE{10852315,
  author={Li, Ji and Liao, Chenyi},
  journal={IEEE Access}, 
  title={Tea Disease Recognition Based on Image Segmentation and Data Augmentation}, 
  year={2025},
  volume={13},
  number={},
  pages={19664-19677},
  abstract={Accurate identification of tea leaf diseases is crucial for intelligent tea cultivation and monitoring. However, the complex environment of tea plantations—affected by weather variations and uneven lighting—poses significant challenges for building effective disease recognition models using raw field-captured images. To address this, we propose a method that combines two-stage image segmentation with an improved conditional generative adversarial network (IC-GAN). The two-stage segmentation approach, integrating graph cuts and support vector machines (SVM), effectively isolates disease regions from complex backgrounds. The IC-GAN augments the dataset by generating high-quality synthetic disease images for model training. Finally, an Inception Embedded Pooling Convolutional Neural Network (IDCNN) is developed for disease recognition. Experimental results demonstrate that the segmentation method improves recognition accuracy from 53.36% to 75.63%, while the IC-GAN increases the dataset size. The IDCNN achieves 97.66% accuracy, 97.36% recall, and a 96.98% F1 score across three types of tea diseases. Comparative evaluations on two additional datasets further confirm the method’s robustness and accuracy, offering a practical solution to reduce tea production losses and improve quality.},
  keywords={Diseases;Image segmentation;Accuracy;Feature extraction;Support vector machines;Data augmentation;Lesions;Plant diseases;Object recognition;Generative adversarial networks;Conditional generative adversarial network;disease recognition;deep learning;image generation},
  doi={10.1109/ACCESS.2025.3534024},
  ISSN={2169-3536},
  month={},}@ARTICLE{10535108,
  author={Kurian, Asha and Tripathi, Shikha},
  journal={IEEE Access}, 
  title={m_AutNet–A Framework for Personalized Multimodal Emotion Recognition in Autistic Children}, 
  year={2025},
  volume={13},
  number={},
  pages={1651-1662},
  abstract={Challenges associated with autism spectrum disorder (ASD) include deficits in interpersonal communication, social interaction skills, and behavior. Autistic children experience difficulties in recognizing emotions and expressing emotions, along with intense emotional upheavals called meltdowns. These outbreaks lead to immense physical and emotional distress in children with autism. Generalized emotion recognition classifiers cannot handle the variations in the prototypical display of affect experienced by ASD children. This paper looks at developing a personalized multimodal neural framework, m_AutNet, that can effectively identify the emotions of autistic children by combining data from their facial and vocal expression modalities. The proposed network includes a personalized facial feature extraction module (that incorporates a distance metric to cluster embeddings with similar labels together and marginalizes dissimilar embeddings), and an audio modality CNN feature extractor that works on speech expression samples of autistic children. Domain adaptation of the multimodal features is achieved through a generative adversarial network tuned with the Wasserstein metric to form a domain-invariant distribution alignment of the feature vectors. A classifier performs emotion classification on this domain space following adaptation. The proposed algorithm shows higher performance than state-of-the-art affect recognition classifiers for autistic children, with an accuracy of 88.25%.},
  keywords={Emotion recognition;Generative adversarial networks;Measurement;Neural networks;Feature extraction;Training;Pediatrics;Autism;Affective computing;Data integration;Multisensory integration;Autism;affective computing;domain adaptation;data fusion;generative adversarial network;multimodal neural network;personalized neural network;Wasserstein GAN},
  doi={10.1109/ACCESS.2024.3403087},
  ISSN={2169-3536},
  month={},}@ARTICLE{10643076,
  author={Du, Yongqiang and Liu, Haoran and He, Shengjie and Chen, Songnan},
  journal={IEEE Access}, 
  title={InViT: GAN Inversion-Based Vision Transformer for Blind Image Inpainting}, 
  year={2024},
  volume={12},
  number={},
  pages={129956-129965},
  abstract={Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.},
  keywords={Transformers;Image restoration;Generative adversarial networks;Computer vision;Image reconstruction;Codes;Blind image inpainting;GAN inversion;vision transformer;generative model},
  doi={10.1109/ACCESS.2024.3447055},
  ISSN={2169-3536},
  month={},}@ARTICLE{10697179,
  author={Karahoda, Bertan},
  journal={IEEE Access}, 
  title={Generating Time Series Data With Real-Valued DC-GAN From Complex Time-Frequency Domain: Application to ECG Synthesis}, 
  year={2024},
  volume={12},
  number={},
  pages={143215-143225},
  abstract={A large amount of data is needed to increase performance in learning algorithms. In many fields such as health, dataset size is limited due to the difficulty of data collection and ethical restrictions. In these cases, synthetic data is needed to increase the performance of learning algorithms. Deep Convolutional Generative Adversarial Networks (DC-GAN) produce very successful results in obtaining synthetic images. In time series synthesis, additional signal-specific information is often needed. With Time-Frequency Transform, the change in frequency and characteristics of time series over time is obtained. In this study, time frequency domain coefficients of time series are generated with 2D real-valued DC-GAN. By embedding the real and imaginary parts of the complex time-frequency domain coefficients into the RGB channels of the real-valued images, the time-frequency domain characteristics of the time series are generated with the real-valued DC-GAN. By extracting the real and imaginary parts from the generated images and converting them into complex coefficients, time series are obtained by inverse transformation. The proposed method is applied to the synthesis of ECG beats and the results obtained are tested with evaluation metrics. The proposed method is also applied to the synthesis of Atrial Fibrillation (AF) and Ventricular Tachycardia (VT) ECG signals. The results obtained show the effectiveness of the proposed method in the synthesis of time series data.},
  keywords={Generative adversarial networks;Electrocardiography;Time series analysis;Convolution;Time-frequency analysis;Generators;Heart beat;Filtering theory;Batch normalization;Training;Generative adversarial networks;time series data synthesis;ECG beat synthesis;data augmentation},
  doi={10.1109/ACCESS.2024.3469541},
  ISSN={2169-3536},
  month={},}@ARTICLE{10328572,
  author={Yoon, Dongsik and Kim, Jineui and Lorant, Vincent and Kang, Sungku},
  journal={IEEE Access}, 
  title={Manipulation of Age Variation Using StyleGAN Inversion and Fine-Tuning}, 
  year={2023},
  volume={11},
  number={},
  pages={131475-131486},
  abstract={Recent advancements in deep learning have yielded significant developments in age manipulation techniques in the field of computer vision. To handle this task, recent approaches using generative adversarial networks latent space transformation or image-to-image based techniques have been developed. However, such methods are limited in terms of preserving the facial identity of the subject and recovering background details during lifelong age variation. To address these limitations, this paper presents a novel framework to manipulate the age of subjects in photos. The proposed framework involves two main steps, i.e., age manipulation and StyleGAN fine-tuning. In the first step, the iterative ReStyle StyleGAN inversion technique discovers a latent vector that is most similar to the input image, and which is used to train an age manipulation encoder. In the second step, a StyleGAN fine-tuning process is used to reconstruct the details lost in the images synthesized using the StyleGAN generator during the age manipulation. To preserve substructures, e.g., backgrounds, we optimize the loss function using facial masks generated from the original and age-manipulated images. The proposed framework is compatible with various StyleGAN-based techniques, e.g., stylization and view synthesis. Compared with state-of-the-art methods, the proposed framework achieves reasonable manipulation and variation of the target age for real-world input images. The results demonstrate the effectiveness of the proposed method in preserving the facial identity and background details during lifelong age variation.},
  keywords={Faces;Generative adversarial networks;Task analysis;Transforms;Image reconstruction;Shape;Iterative methods;Generative adversarial networks;image editing;age manipulation},
  doi={10.1109/ACCESS.2023.3336401},
  ISSN={2169-3536},
  month={},}@ARTICLE{10772115,
  author={Liu, Junjie and Xie, Jun and Tao, Qing and Zhang, Huanqing and Wang, Hu and Hu, Bo},
  journal={IEEE Access}, 
  title={AAC-WGAN: A Novel Attention-Enhanced GAN Framework for SSVEP Augmentation and Classification}, 
  year={2024},
  volume={12},
  number={},
  pages={182627-182639},
  abstract={The rapid advancement of brain-computer interface (BCI) technology has created interest in steady-state visual evoked potential (SSVEP)-based BCIs, which are valued for their high information transfer rates and ability to manage multiple targets. Nonetheless, the efficacy of SSVEP decoding is often constrained by the volume and duration of user calibration data, limiting its practical application. Generative adversarial networks (GANs) have shown promise in synthesizing SSVEP electroencephalogram (EEG) data. However, they face challenges, such as low signal-to-noise ratio and capturing both temporal and spatial features. To address the need for high-quality data generation, the Attention-Aided Classifier Wasserstein GAN (AAC-WGAN) is proposed, which is a novel GAN model that combines an Attention Mechanism and an Auxiliary Classifier to improve data quality and classification performance. Our experiments on the Direction and Dial datasets reveal that our model performs obviously better, particularly with a training set ratio of 25% synthetic data and 75% real data. It achieves a classification accuracy of 91.65% on the Direction dataset, which is a significant improvement over the baseline accuracy of 84.32% (p =0.008) and outperforms other comparable models (p <0.05 for all comparisons). On the Dial dataset, our model achieves a classification accuracy of 83.48%, outperforming both the baseline of 80.86% and other models. Additional analyses, such as t-SNE visualization and FFT, confirm our model’s effectiveness in feature representation and frequency domain characteristics, with significant improvements in both data quality and classification stability. This work advances the state of SSVEP generation and classification, resulting in significant improvements in data quality and accuracy. It also represents a significant step forward in the development of BCI technologies.},
  keywords={Electroencephalography;Brain modeling;Feature extraction;Data models;Accuracy;Generators;Attention mechanisms;Noise;Generative adversarial networks;Visualization;Brain-computer interface (BCI);generative adversarial network (GAN);data augmentation;global attention mechanism;auxiliary classifier;steady-state visual evoked potential (SSVEP)},
  doi={10.1109/ACCESS.2024.3509519},
  ISSN={2169-3536},
  month={},}@ARTICLE{10918718,
  author={Lee, Jangho and Park, Sun Young},
  journal={IEEE Access}, 
  title={WGAN-GP-Based Conditional GAN (cGAN) With Extreme Critic for Precipitation Downscaling in a Key Agricultural Region of the Northeastern U.S.}, 
  year={2025},
  volume={13},
  number={},
  pages={46030-46041},
  abstract={This study develops a conditional Generative Adversarial Network with a multi-head Critic (cGAN_ext), under a Wasserstein GAN with gradient penalty (WGAN-GP) framework, to downscale coarse-resolution meteorological data into high-resolution precipitation fields. The model’s U-Net Generator combines large-scale atmospheric inputs—2 m temperature, total column water vapor, mean sea-level pressure, and downsampled precipitation—with a noise tensor, while the Critic enforces adversarial constraints and explicitly classifies extreme events. By focusing on rare high-intensity rainfall within the adversarial training loop, cGAN_ext captures crucial tail behavior that can be overlooked by conventional approaches. Experimental results reveal that cGAN_ext not only preserves fine-grained spatial details but also better represents heavy precipitation episodes, thereby improving essential metrics such as mean squared error, fractions skill scores for extremes, and temporal correlation. Visual analysis further confirms the model’s ability to reproduce sharp precipitation fronts and narrow bands, underscoring the benefits of integrating an extreme-classification objective into a WGAN-GP cGAN pipeline. This enhanced downscaling method offers more accurate and coherent high-resolution precipitation maps, supporting informed decision-making in agricultural planning and water resource management.},
  keywords={Meteorology;Atmospheric modeling;Spatial resolution;Generative adversarial networks;Rain;Data models;Water resources;Predictive models;Agriculture;Accuracy;Precipitation;radar;multi-radar multi-sensor;generative adversarial network (GAN);conditional GAN;Wasserstein GAN;deep learning;machine learning;ERA-5;downscaling;climate informatics},
  doi={10.1109/ACCESS.2025.3549443},
  ISSN={2169-3536},
  month={},}@ARTICLE{9138393,
  author={Fan, Zijia and Wang, Zhongyang and Xin, Junchang and Wang, Zhiqiong and Liu, Lu and Zhang, Xia and Liu, Jiren},
  journal={IEEE Access}, 
  title={Dual-Enhanced Registration for Field of View Ultrasound Sonography}, 
  year={2020},
  volume={8},
  number={},
  pages={128602-128612},
  abstract={Extended Field of View Ultrasound Sonography (EFOV-US) uses the existing ultrasound images for image stitching, so as to display the shape and scope of organ occupation and the relationship with surrounding tissues comprehensively. However, there are still some problems in Extended Field of View Ultrasound Sonography, such as matching error and unstable quality of image stitching. In view of these problems, we propose Dual-enhanced EFOV-US method that overcomes the limitation and produces higher quality results. Firstly, the gray enhancement method is used to improve the image contrast and reduce the noise interference. Then the super-resolution method based on the generative adversarial network is used to improve the resolution of the ultrasonic image further and increase the number of feature point matching between stitching images. The high quality ultrasound wide-range image is gotten by stitching and fusing the double enhanced image. The experimental results show that the proposed method is effective and practical.},
  keywords={Ultrasonic imaging;Generative adversarial networks;Feature extraction;Training;Gallium nitride;Extended field of view ultrasound sonography;gray enhancement;generative adversarial network;super-resolution;image registration},
  doi={10.1109/ACCESS.2020.3008525},
  ISSN={2169-3536},
  month={},}@ARTICLE{10485426,
  author={Hernandez, Ivan and Yu, Wen and Li, Xiaoou},
  journal={IEEE Access}, 
  title={Optimal PD Control Using Conditional GAN and Bayesian Inference}, 
  year={2024},
  volume={12},
  number={},
  pages={48255-48265},
  abstract={PD control is a widely used model-free method; however, it often falls short of guaranteeing optimal performance. Optimal model-based control, such as the Linear Quadratic Regulator (LQR), can indeed achieve the desired control performance, but only for known linear systems. In this paper, we present a novel approach for designing optimal PD control for unknown mechanical systems. We utilize a conditional Generative Adversarial Network (GAN) and a Long Short-Term Memory (LSTM) neural network to approximate an optimal PD control. We employ Bayesian inference to generate PD control that can be applied at different operating points. This design mechanism ensures both stability and optimal performance. Finally, we apply this control methodology to lower limb prostheses, and the results demonstrate that the optimal PD control, using GAN and Bayesian inference, outperforms other classical controllers.},
  keywords={PD control;Generative adversarial networks;Aerospace electronics;Robots;Nonlinear systems;Generators;Adaptation models;Optimal control;Optimal PID;Bayesian inference;generative adversarial network;deep learning},
  doi={10.1109/ACCESS.2024.3382993},
  ISSN={2169-3536},
  month={},}@ARTICLE{11062578,
  author={Zhao, Zhiguo and Li, Ke and Dai, Yibo and Chen, Biao and Wang, Yeqin and Zhao, Qian},
  journal={IEEE Access}, 
  title={RUL Prediction Based on MBGD-WGAN-GRU for Lithium-Ion Batteries}, 
  year={2025},
  volume={13},
  number={},
  pages={114834-114844},
  abstract={To address the challenges associated with acquiring complete charge-discharge cycle data and extracting health indicator factors (IHFs) from fragmented datasets in current automotive lithium-ion batteries (LIBs), this study proposes a novel online remaining useful life (RUL) prediction method. First, the IHF, which captures battery aging characteristics, is extracted from raw LIBs data, and the dataset is partitioned into training (70%) and testing (30%) subsets. Subsequently, the mini-batch stochastic gradient descent algorithm is employed to optimize a Wasserstein generative adversarial network, thereby augmenting the training set and enhancing data diversity. The extracted IHF and expanded training set are then used to train a gated recurrent unit (GRU) model, leveraging GRU’s strengths in sequential data processing to improve the characterization of battery aging trends. Finally, the proposed GRU-based model is validated for RUL prediction using an open-source test dataset. Experimental results demonstrate that the mean absolute error of the proposed method on B0005, B0006, and B0007 batteries is 0.0023 Ah, 0.0030 Ah, and 0.0014 Ah, respectively, confirming its effectiveness and practical applicability. This approach provides robust technical support for LIBs health management.},
  keywords={Logic gates;Training;Generators;Interpolation;Generative adversarial networks;Aging;Predictive models;Degradation;Optimization;Noise;Lithium-ion battery;remaining useful life;mini-batch stochastic gradient descent;generative adversarial network;gated recurrent unit},
  doi={10.1109/ACCESS.2025.3585036},
  ISSN={2169-3536},
  month={},}@ARTICLE{10945309,
  author={Venugopal, Viji and Tanna, Paresh and Karnati, Ramesh},
  journal={IEEE Access}, 
  title={Handling Sensor Faults in Hydroponics: A Deep Learning Imputation Technique for Accurate Tomato Yield Prediction}, 
  year={2025},
  volume={13},
  number={},
  pages={65776-65796},
  abstract={Sensor faults in hydroponic systems pose significant challenges for precision agriculture by compromising the nutrient monitoring accuracy and yield prediction reliability. Current imputation methods lack domain-specific agricultural pattern-preservation capabilities. This paper presents a novel Deep Learning Precision Imputation Model (DLPIM) based on a generative adversarial network (GAN) architecture to recover missing agricultural data in IoT-based hydroponic monitoring systems. DLPIM introduces a feature encoder-based generator with a 4-headed self-attention mechanism and a Crop Growth Rate (CGR) guided temporal processor to capture complex time-series patterns while maintaining physiological consistency. The discriminator implements dual validity and feature matching pathways, enhanced by minibatch discrimination and spectral normalization. A dataset comprising 1092 data points related to tomato growth and nutrient analysis was utilized to develop and evaluate the proposed model, demonstrating the superior performance of DLPIM compared to nine state-of-the-art imputation methods across different missing data scenarios (10-60%). The proposed model achieved optimal accuracy (MSE =2.367-2.664, MAE =0.795-0.851, R ${}^{2} = 0.990$ -0.992), while maintaining consistent performance across all missing rates. Ablation studies confirmed the effectiveness of architectural innovations, with CGR removal resulting in the most substantial performance decline (R ${}^{2} = 0.851$  at a 60% missing rate). The DLPIM framework establishes a new benchmark for agricultural time series imputation, enabling robust decision-making in precision agriculture.},
  keywords={Imputation;Hydroponics;Time series analysis;Deep learning;Accuracy;Generative adversarial networks;Nutrients;Monitoring;Farming;Data models;Deep learning;generative AI;hydroponics;precision agriculture;sensor system},
  doi={10.1109/ACCESS.2025.3555875},
  ISSN={2169-3536},
  month={},}@ARTICLE{11129647,
  author={Nandakumar, Hari Prasad and Nethravathi, K. A. and Vadada, Srikanth},
  journal={IEEE Access}, 
  title={ISAR Image Generation of Ships by Inpainting Using SinGAN}, 
  year={2025},
  volume={13},
  number={},
  pages={146371-146379},
  abstract={Inverse Synthetic Aperture Radar (ISAR) images are important in civilian and military applications. This is particularly true in military applications, where they are used for surveillance. One of the applications that we focus on is the Automatic Target Recognition (ATR) system for military ships that processes ISAR data. Extensive ISAR datasets are required to develop robust target recognition systems for ships. In this paper, we propose the use of SinGAN, a deep learning model which is architecturally a Generative Adversarial Network (GAN), to generate ISAR images of ships using an inpainting technique. The methodology begins with a single ISAR image of a ship being trained with the SinGAN model. Then, clip-arts, outlines, or even photographs of ships are preprocessed to ensure optimal inpainting results. This paper also evaluates noise reproducibility and demonstrates dataset creation using the generated ISAR images. Finally, this dataset is used to train a custom Convolutional Neural Network (CNN) model.},
  keywords={Training;Marine vehicles;Noise;Generative adversarial networks;Radar imaging;Image resolution;Generators;Mathematical models;Deep learning;Image synthesis;Inverse synthetic aperture radar (ISAR);sin-generative adversarial network (SinGAN)},
  doi={10.1109/ACCESS.2025.3600459},
  ISSN={2169-3536},
  month={},}@ARTICLE{11146712,
  author={Onodera, Yukito and Takeshita, Erina and Kosugi, Tomoya and Nakanishi, Takashi and Shimada, Tatsuya},
  journal={IEEE Access}, 
  title={Synthetic Network Packet Data Generator With Protocol State and Arrival Timing Awareness}, 
  year={2025},
  volume={13},
  number={},
  pages={153398-153405},
  abstract={Recent advances in network traffic analysis use machine learning (ML) on packet-level data, but obtaining large amount of real data for training ML is becoming a challenge. ML-based generators like Generative Adversarial Networks and Variational Autoencoders are promising candidates for generation the packet-level data but often fail to preserve TCP protocol states and packet arrival time. Without accurately reproducing these characteristics, the synthetic data may fail to reflect real network behavior, potentially leading to erroneous conclusions in the training and evaluation of machine learning models. This paper proposes a new method to synthesize realistic packet-level data with accurate protocol transitions and timing. By separating IP address and port number generation and aligning timing with handshake completion, the model avoids overfitting. Using the CIC-IDS 2017 dataset, our method better replicates headers and timing, producing synthetic data that closely resembles real data features distribution. Visual and statistical comparisons reveal that the synthetic data closely resembles real packet traces, providing a reliable alternative for training and evaluating network analysis.},
  keywords={IP networks;Synthetic data;Timing;Vectors;Protocols;Training;Generators;Generative adversarial networks;Telecommunication traffic;Servers;Synthetic data;generative adversarial model;data generation;network simulation;PCA;TCP protocol;arrival timing;network traffic analyses},
  doi={10.1109/ACCESS.2025.3604773},
  ISSN={2169-3536},
  month={},}@ARTICLE{11062634,
  author={Udu, Amadi G. and Salman, Marwah T. and Ghalati, Maryam K. and Lecchini-Visintini, Andrea and Siddle, David R. and Dong, Hongbiao},
  journal={IEEE Access}, 
  title={Emerging SMOTE and GAN Variants for Data Augmentation in Imbalance Machine Learning Tasks: A Review}, 
  year={2025},
  volume={13},
  number={},
  pages={113838-113853},
  abstract={Class imbalance is a pervasive challenge in real-world machine learning (ML) applications, where the minority class, often the class of interest, is significantly underrepresented. This imbalance can degrade model performance, result in misleading evaluation metrics, and complicate validation processes. Two prominent data-augmentation techniques to address class imbalance are the Synthetic Minority Oversampling Technique (SMOTE) and Generative Adversarial Networks (GAN). However, both techniques have inherent limitations, motivating the emergence of novel variants designed to overcome these challenges. While previous reviews have typically focused on specific domains, conventional methodologies, or broad strategy overviews, this review presents a unified taxonomy that outlines the causes, types, and implications of class imbalance across diverse ML tasks. It further examines emerging trends in the application of SMOTE and GAN techniques, their limitations, and hybrid adaptations. By categorising imbalance types and analysing models, metrics, datasets, and comparative approaches, this review provides actionable insights and identifies future research directions for practitioners and researchers working to address class imbalance in real-world ML tasks.},
  keywords={Reviews;Generative adversarial networks;Machine learning;Data augmentation;Measurement;Diseases;Data models;Taxonomy;Predictive models;Fraud;Class imbalance;data-augmentation;generative adversarial networks;machine learning;SMOTE},
  doi={10.1109/ACCESS.2025.3584532},
  ISSN={2169-3536},
  month={},}@INBOOK{10953167,
  author={Diamond, Stephanie and Allan, Jeffrey},
  booktitle={Writing AI Prompts For Dummies}, 
  title={Grasping the Basics of Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={7-26},
  abstract={Summary <p>This chapter looks at AI systems that need guidance, like students in a class and those who learn on their own. It discusses AI that makes entirely new content instead of just organizing data. The chapter explores the diverse world of AI. It also look at different types of AI to understand what they're like and how they work. The chapter starts with two main types: AI that learns from data, which we call machine learning (ML) and AI that follows specific rules. ML can perform tasks like creating personal recommendations, organizing our phone's photo albums, or helping self&#x2010;driving cars make decisions. The ML can further break down into two specific types: supervised learning and unsupervised learning. There are two types of specialized AI, each with a unique role. The first is GenAI, which creates new content. The second is discriminative AI, which sorts and categorizes existing information.</p>},
  keywords={Artificial intelligence;Unsupervised learning;Supervised learning;Grasping;Face recognition;Social networking (online);Magnetic resonance imaging;Generative AI;Business;X-rays},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394244683},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953167},}@ARTICLE{11123435,
  author={Zheng, Laixi},
  journal={IEEE Access}, 
  title={ArtStroke-GAN: A Unified Framework for Interpreting Watercolor Aesthetics via Personalized Stroke-Based Graphic Language}, 
  year={2025},
  volume={13},
  number={},
  pages={144954-144974},
  abstract={Painting is an expressive art form traditionally associated with considerable technical skill and prolonged training, making it largely inaccessible to the general public. Computational stroke-based rendering (SBR) approaches have emerged to simulate human-like artistic processes digitally; however, current methods encounter notable limitations, such as spatial inconsistencies, stylistic discontinuities, and insufficient control over fine-grained stroke details. To address these challenges, this paper introduces ArtStroke-GAN, a novel generative adversarial network framework designed to synthesize realistic and personalized watercolor paintings. The proposed framework employs a modular three-part GAN architecture consisting of a stroke-generation network, a dedicated color-enhancement module, and an adversarial discriminator. Adaptive spline-based stroke modeling and iterative, semantic-aware stroke optimization strategies are integrated, enabling progressive refinement from coarse to detailed artistic representations. Experimental evaluations conducted on diverse image datasets (CelebA and ImageNet) demonstrate that ArtStroke-GAN achieves superior performance compared to state-of-the-art methods, including Stroke-GAN, Paint Transformer, and Neural-Paint, across quantitative metrics such as LPIPS, FID, style loss, and content fidelity. Qualitative assessments further confirm its ability to produce aesthetically coherent, structurally consistent, and stylistically diverse watercolor renderings, significantly advancing the capability of computational art generation frameworks.},
  keywords={Painting;Rendering (computer graphics);Visualization;Semantics;Generative adversarial networks;Paints;Training;Computational modeling;Transformers;Adaptation models;Deep learning;generative adversarial networks;visual aesthetics;digital painting;artistic image generation},
  doi={10.1109/ACCESS.2025.3598183},
  ISSN={2169-3536},
  month={},}@ARTICLE{9653662,
  author={Liu, Jinxin and Nogueira, Michele and Fernandes, Johan and Kantarci, Burak},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={Adversarial Machine Learning: A Multilayer Review of the State-of-the-Art and Challenges for Wireless and Mobile Systems}, 
  year={2022},
  volume={24},
  number={1},
  pages={123-159},
  abstract={Machine Learning (ML) models are susceptible to adversarial samples that appear as normal samples but have some imperceptible noise added to them with the intention of misleading a trained classifier and misclassifying the input. Adversarial Machine Learning (AML) was initially coined following upon researchers pointing out certain blind spots in image classifiers in computer vision field which were exploited by these adversarial samples to deceive the model. Although this has been investigated remarkably in computer vision, the impact of AML in wireless and mobile systems has recently attracted attention. Wireless and mobile networks have intensely benefited from the application of ML classifiers to detect network traffic anomalies and malware detection. However, ML detectors themselves can be exfiltrated/evaded by the samples carefully designed by attackers, raising security concerns for ML-based network applications. Thus, it is crucial to detect such samples to safeguard the network. This survey article presents a systematic mapping and a comprehensive literature review on AML to wireless and mobile systems from physical layer to network and application layers. The article reviews the state-of-the-art AML approaches in the generation and detection of adversarial samples. The samples can be generated by adversarial models such as Generative Adversarial Networks (GANS) and techniques such as Fast Gradient Sign Method (FGSM). The samples can be detected by adversarial models acting as classifiers or ML classifiers reinforced with knowledge on how to detect such samples. For each approach, a high-level overview is provided alongside its impact on solving the problems in wireless and mobile settings. Furthermore, this article provides detailed discussions to highlight the open issues and challenges faced by these approaches, as well as research opportunities which can be of interest to the researchers and developers in Artificial Intelligence (AI)-driven wireless and mobile networking.},
  keywords={Wireless communication;Communication system security;Malware;Feature extraction;Wireless sensor networks;Intrusion detection;Generative adversarial networks;Wireless networks;mobile networks;adversarial machine learning;artificial intelligence;intrusion detection},
  doi={10.1109/COMST.2021.3136132},
  ISSN={1553-877X},
  month={Firstquarter},}@INPROCEEDINGS{9506282,
  author={Kim, Junho and Kim, Minsu and Ro, Yong Man},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Interpretation of Lesional Detection via Counterfactual Generation}, 
  year={2021},
  volume={},
  number={},
  pages={96-100},
  abstract={To interpret the decision of Deep Neural Networks (DNNs), explainable artificial intelligence research has been widely investigated. Especially, visualizing the attribution maps is known as one of the efficient ways to provide explanations for the trained networks. Applying existing visualization methods on medical images has significant issues in that the medical images commonly have inherent imbalanced data poses and scarcity. To tackle such issues and provide more accurate explanations in medical images, in this paper, we propose a new explainable framework, Counterfactual Generative Network (CGN). We embed counterfactual lesion prediction of DNNs to our explainable framework as prior conditions and guide to generate various counterfactual lesional images from normal input sources, or vice versa. By doing so, CGN can represent detailed attribution maps and generate corresponding normal images from leisonal inputs. Extensive experiments are conducted on the two chest X-ray datasets to verify the effectiveness of our method.},
  keywords={Deep learning;Visualization;Image processing;Conferences;Data visualization;Medical diagnosis;Lesions;Deep learning;Explainable AI;Counterfactual generation;Medical image},
  doi={10.1109/ICIP42928.2021.9506282},
  ISSN={2381-8549},
  month={Sep.},}@ARTICLE{9000600,
  author={Xia, Xuan and Yu, Fengqi and Li, Nan and Qu, Yansong and Zhang, Jiajia and Zhu, Chengguang},
  journal={IEEE Access}, 
  title={Self-Attention-Masking Semantic Decomposition and Segmentation for Facial Attribute Manipulation}, 
  year={2020},
  volume={8},
  number={},
  pages={36154-36165},
  abstract={Many face attribute manipulation methods can only provide global attribute manipulation according to the attribute labels. In this paper, we propose a self-attention-masking semantic decomposition method which is able to learn an attribute attention mask for each attribute. User can adjust the strength and color of each attribute smoothly and more freely. We decouple the attention of different attributes and overcome the disadvantage of overlap between different attribute attention masks by an attention weighting module. Thanks to the attribute attention masks, our method allows manipulate facial attribute without generator after only once generation. Moreover, we can perform facial semantic segmentation without pixel level semantic labels. Experiments show that our method simultaneously improves the freedom of attribute manipulation and the authenticity of synthetic face. The mean intersection over union of semantic segmentation is over 65% for hair and skin. Our code is available at github.com/flyfeatherok/SAMSD.},
  keywords={Semantics;Generators;Image color analysis;Face;Facial features;Image segmentation;Integrated circuits;Generative adversarial networks;semantic decomposition;semantic segmentation;facial attribute manipulation},
  doi={10.1109/ACCESS.2020.2974239},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10534381,
  author={Siddik, Abu Bakar and Biswas, Sree Suvro Kumar and Islam, Afroza and Saziduzzaman, Md},
  booktitle={2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT)}, 
  title={Unraveling the Enigmatic Frontier: Deciphering the Distinction Between AI-Generated and Real Images}, 
  year={2024},
  volume={},
  number={},
  pages={517-522},
  abstract={The exponential advancement of artificial intelligence (AI) techniques like generative adversarial networks has fueled the proliferation of synthesized media challenging reliable discrimination. As AI frameworks now conjure stunningly realistic imagery, developing enhanced systems to authenticate media provenance and expose forgeries is vital for sectors across the board. This research proposes a new convolutional neural network architecture to reliably distinguish genuine photographic images from AI-fabricated fakes. By analyzing underlying spatial correlations, noise patterns, and scene coherence contradictions among images, we compute intrinsic fingerprints exposing synthetic imagery provenance. The proposed architecture achieves 94.44% binary accuracy, 0.9863 AUC, 94.53% precision, and 95.61 % recall in discriminating real pictures from AI fakes, significantly outperforming the previous approaches. The proposed methodology hence delivers a breakthrough solution directly combating growing threats around misinformation and deception enabled by synthetic media. However, this research also exposes the deeper imperative for trustworthy AI design that perpetually outpaces the exponential curve of generative progress to enact techno-social guardrails before threats violate ethics, safety, or justice.},
  keywords={Image communication;Noise;Computer architecture;Fingerprint recognition;Generative adversarial networks;Forgery;Safety;AI-generated image Deep Learning Classification},
  doi={10.1109/ICEEICT62016.2024.10534381},
  ISSN={2769-5700},
  month={May},}@ARTICLE{10988663,
  author={Wang, Zhaowen and Zhou, Yong and Zhang, YangSiyu and Cong, Fengyu and Zhou, Dongdong and An, Zhijian},
  journal={IEEE Internet of Things Journal}, 
  title={PA-Rank: A GAN and Reinforcement Learning Powered Framework for Multimetric Anomaly Detection and Causal Diagnosis}, 
  year={2025},
  volume={12},
  number={14},
  pages={28889-28898},
  abstract={The increasing scale and complexity of modern IT systems necessitate advanced solutions for monitoring and managing performance anomalies. Artificial intelligence for IT operations (AIOps) has emerged as a promising approach to enhance the efficiency and effectiveness of IT operations. However, existing methods struggle with effectively detecting anomalies in multidimensional performance data and accurately identifying their root causes in complex interdependent systems. This article proposes a novel framework, PA-Rank, that combines generative adversarial networks (GANs), reinforcement learning, and graph-based methods to address these challenges comprehensively. For anomaly detection, an unsupervised GAN-based model is developed to identify anomalous time periods and assign weighted scores to metrics, facilitating precise anomaly identification. For root cause localization, a causal graph construction model (CGCM) has been developed, utilizing a reinforcement learning-based causal discovery method that is integrated with graph attention networks (GAT) to construct a causal graph representing the relationships between metrics. A random walk algorithm further ranks metric importance during anomalies, enabling effective root cause localization. Extensive experiments on real-world datasets, including server machine dataset (SMD), ASD, and DAMADICS, demonstrate the superiority of PA-Rank over traditional statistical and state-of-the-art machine learning methods. On the SMD dataset, the proposed framework achieved an F1 score of 0.9542 for anomaly detection and consistently identified root causes among top-ranked candidates on the Pymicro and RMS datasets with the highest PR@Avg scores. These results underscore PA-Rank’s efficacy in diagnosing performance anomalies and supporting efficient system maintenance.},
  keywords={Anomaly detection;Measurement;Location awareness;Data models;Training;Vectors;Time series analysis;Accuracy;Root cause analysis;Internet of Things;Anomaly detection;artificial intelligence for IT operations (AIOps);reinforcement learning;root cause localization},
  doi={10.1109/JIOT.2025.3567091},
  ISSN={2327-4662},
  month={July},}@ARTICLE{11141393,
  author={Bohara, Ritushree and Bairwa, Amit Kumar},
  journal={IEEE Access}, 
  title={Detecting Deepfake Audio Using Spectrogram-Based Machine Learning Approaches}, 
  year={2025},
  volume={13},
  number={},
  pages={149478-149489},
  abstract={The increasing ability of deep learning models to produce realistic-sounding synthetic speech poses serious problems for privacy, public trust, and digital security. To counter this danger, we offer a methodology for identifying deepfake audio that is based on machine learning. We use three ensemble learning models (Random Forest, Gradient Boosting, and XGBoost) for classification after transforming speech samples into mel-spectrograms to extract time-frequency information. These models, which were trained on the Deep Voice dataset, which included a variety of actual and synthetic samples, were assessed using common metrics such as F1-score, accuracy, precision, and recall. At 99.32% accuracy, XGBoost performed better than the others. These findings show the promise of lightweight, interpretable machine learning methods for identifying fake audio and protecting media authentication, digital forensics, and cybersecurity applications.},
  keywords={Deepfakes;Computational modeling;Real-time systems;Accuracy;Feature extraction;Solid modeling;Machine learning;Deep learning;Media;Benchmark testing;Auto encoders;music generation;generative AI;deep learning},
  doi={10.1109/ACCESS.2025.3602531},
  ISSN={2169-3536},
  month={},}@ARTICLE{10756695,
  author={Subhani Khan, Osama and Iltaf, Naima and Zia, Usman and Latif, Rabia and Shahida Mohd Jamail, Nor},
  journal={IEEE Access}, 
  title={Efficient Text Style Transfer Through Robust Masked Language Model and Iterative Inference}, 
  year={2024},
  volume={12},
  number={},
  pages={182353-182373},
  abstract={Emergence of Large Language Models (LLMs) have prompted researchers to exploit the abilities of such models for text style transfer (TST). However, these models are prone to hallucinations and suffer from problems of manually crafting prompts and high computation requirements. The purpose of TST is to edit text sequences so that their style is changed without hindering the meaning of their content. Owing to the scarcity of parallel data, existing approaches rely on various strategies to identify and replace style attributes or to edit a given sequence as a whole. Successful style transfer should be fluent and reflect original content. To address these challenges, we propose a novel technique leveraging explanations of prompt-free few-shot contrastive learning based lightweight classifier. First, we create a style-independent corpus of target style sequences by masking out style attributes and train a generator with masked language modeling objective that learns to predict target style tokens. Then, we apply an iterative mechanism to mask source style sequences and predict target style attributes until style is transferred gauged by a pre-trained evaluator model. We conduct experiments on two real world widely used product reviews sentiment datasets on both polarities, i.e., positive to negative and negative to positive. Comparison with various prompt-based as well as unsupervised learning based methods demonstrate state-of-the-art performance of our approach.},
  keywords={Iterative methods;Accuracy;Unsupervised learning;Transformers;Computational modeling;Semantics;Training;Predictive models;Generators;Encoding;Text style transfer;generative AI;contrastive learning;masked language modeling;explainable AI},
  doi={10.1109/ACCESS.2024.3501320},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10445948,
  author={Guo, Yiwei and Du, Chenpeng and Ma, Ziyang and Chen, Xie and Yu, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={VoiceFlow: Efficient Text-To-Speech with Rectified Flow Matching}, 
  year={2024},
  volume={},
  number={},
  pages={11121-11125},
  abstract={Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.},
  keywords={Signal processing algorithms;Signal processing;Acoustics;Mathematical models;Vectors;Trajectory;Speech processing;Text-to-speech;flow matching;rectified flow;efficiency;speed-quality tradeoff},
  doi={10.1109/ICASSP48485.2024.10445948},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10205110,
  author={Chowdhury, Pinaki Nath and Bhunia, Ayan Kumar and Sain, Aneeshan and Koley, Subhadeep and Xiang, Tao and Song, Yi-Zhe},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text}, 
  year={2023},
  volume={},
  number={},
  pages={10972-10983},
  abstract={In this paper, we extend scene understanding to include that of human sketch. The result is a complete trilogy of scene representation from three diverse and complementary modalities – sketch, photo, and text. Instead of learning a rigid three-way embedding and be done with it, wefocus on learning a flexible joint embedding that fully supports the “optionality” that this complementarity brings. Our embedding supports optionality on two axes: (i) optionality across modalities – use any combination of modalities as query for downstream tasks like retrieval, (ii) optionality across tasks – simultaneously utilising the embedding for either discriminative (e.g., retrieval) or generative tasks (e.g., captioning). This provides flexibility to end-users by exploiting the best of each modality, therefore serving the very purpose behind our proposal of a trilogy in the first place. First, a combination of information-bottleneck and conditional invertible neural networks disentangle the modality-specific component from modality-agnostic in sketch, photo, and text. Second, the modality-agnostic instances from sketch, photo, and text are synergised using a modified cross-attention. Once learned, we show our embedding can accommodate a multi-facet of scene-related tasks, including those enabled for the first time by the inclusion of sketch, all without any task-specific modifications. Project Page: https://pinakinathc.github.io/scenetrilogy},
  keywords={Computer vision;Image synthesis;Image retrieval;Neural networks;Pattern recognition;Proposals;Task analysis;Multi-modal learning},
  doi={10.1109/CVPR52729.2023.01056},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{7078635,
  author={Korpusik, Mandy and Schmidt, Nicole and Drexler, Jennifer and Cyphers, Scott and Glass, James},
  booktitle={2014 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Data collection and language understanding of food descriptions}, 
  year={2014},
  volume={},
  number={},
  pages={560-565},
  abstract={This paper presents initial data collection and language understanding experiments conducted as part of a larger effort to create a nutrition dialogue system that automatically extracts food concepts from a user's spoken meal description. We first summarize the data collection and annotation of food descriptions performed via Amazon Mechanical Turk. We then present semantic labeling experiments using a semi-Markov conditional random field (CRF) that obtains an F1 test score of 85.1. Finally, we report food segmentation experiments that explored three methods for associating foods with their corresponding attributes: a generative Markov model, transformation-based learning, and a CRF classifier. The CRF performed best, achieving an F1 test score of 87.1.},
  keywords={Markov processes;Labeling;Data collection;Semantics;Dairy products;Tagging;Data models;Data collection;Semantic tagging;CRF;Markov model;Transformation-based learning},
  doi={10.1109/SLT.2014.7078635},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10651227,
  author={Moser, Brian B. and Frolov, Stanislav and Raue, Federico and Palacio, Sebastian and Dengel, Andreas},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Image Super-Resolution (SR) remains challenging, particularly in achieving high-quality details without extensive computational cost. Existing methods often struggle to balance the trade-off between image quality, especially in high-frequency details, and computational efficiency. In this paper, we present a novel Diffusion-Wavelet (DiWa) approach for bridging this gap. It leverages the strengths of diffusion models and discrete wavelet transformation. By enabling the diffusion model to operate in the frequency domain, our models effectively hallucinate highfrequency information for SR images on the wavelet spectrum, resulting in high-quality and detailed reconstructions in image space. Quantitatively, our method outperforms other state-ofthe-art diffusion-based SR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using the frequency domain allows us to use fewer parameters than the compared models: 92M parameters instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff. Additionally, DiWa outperforms other state-of-the-art generative methods on general SR datasets while saving inference time (ca. 250 %).},
  keywords={Wavelet domain;Frequency-domain analysis;Superresolution;Memory management;Diffusion models;Throughput;Computational efficiency;Diffusion;Wavelets;Image Super-Resolution},
  doi={10.1109/IJCNN60899.2024.10651227},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10617473,
  author={Chan, S.},
  booktitle={2024 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={AI-Facilitated Dynamic Threshold-Tuning for a Maritime Domain Awareness Module}, 
  year={2024},
  volume={},
  number={},
  pages={192-198},
  abstract={This paper presents a Decision Support Engine (DSE), which utilizes a unique Lower Ambiguity, Higher Uncertainty (LAHU) and Higher Ambiguity, Lower Uncertainty (HALU) Module (LHM) to facilitate decision engineering pathways under compressed decision cycles. Central to this is the identification of isomorphic paradigms, via the discernment of relevant sparse and “Very Small/Non-Obvious” (VSNO) High Dimensional Data (HDD). Accordingly, it is found that an amalgam of a particular sequence of transformations, a Robust Convex Relaxations (RCR)-centric neural network construct, and a Type II Fuzzy Set/Three-Way Soft Clustering approach (and other components buttressing the LHM) can be of value-added proposition in the challenge of employing fuzzy logic and deep learning to adaptive systems for intelligent control within this proxy as well as other domains.},
  keywords={Fuzzy logic;Uncertainty;Correlation;Neural networks;Robustness;Extensibility;Fourth Industrial Revolution;Decision Support System;High Dimensional Data;Artificial Intelligence;Machine Learning;Multi-Criteria Decision-Making;Adaptive Criteria Weighting System},
  doi={10.1109/IAICT62357.2024.10617473},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{10000153,
  author={Kenghagho, Franklin K. and Neumann, Michael and Mania, Patrick and Tan, Toni and Siddiky, Feroz A. and Weller, René and Zachmann, Gabriel and Beetz, Michael},
  booktitle={2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)}, 
  title={NaivPhys4RP - Towards Human-like Robot Perception “Physical Reasoning based on Embodied Probabilistic Simulation”}, 
  year={2022},
  volume={},
  number={},
  pages={815-822},
  abstract={Perception in complex environments especially dynamic and human-centered ones goes beyond classical tasks such as classification usually known as the what- and where-object-questions from sensor data, and poses at least three challenges that are missed by most and not properly addressed by some actual robot perception systems. Note that sensors are extrinsically (e.g., clutter, embodiedness-due noise, delayed processing) and intrinsically (e.g., depth of transparent objects) very limited, resulting in a lack of or high-entropy data, that can only be difficultly compressed during learning, difficultly explained or intensively processed during interpretation. (a) Therefore, the perception system should rather reason about the causes that produce such effects (how/why-happen-questions). (b) It should reason about the consequences (effects) of agent-object and object-object interactions in order to anticipate (what-happen-questions) the (e.g., undesired) world state and then enable successful action on time. (c) Finally, it should explain its outputs for safety (meta why/how-happen-questions). This paper introduces a novel white-box and causal generative model of robot perception (NaivPhys4RP) that emulates human perception by capturing the Big Five aspects (FPCIU)11Functionality, Physics, Causality, Intention, Utility of human commonsense, recently established, that invisibly (dark) drive our observational data and allow us to overcome the above problems. However, NaivPhys4RP particularly focuses on the aspect of physics, which ultimately and constructively determines the world state.},
  keywords={Systematics;Dynamics;Humanoid robots;Ontologies;Robot sensing systems;Probabilistic logic;Safety},
  doi={10.1109/Humanoids53995.2022.10000153},
  ISSN={2164-0580},
  month={Nov},}@INPROCEEDINGS{10678598,
  author={Li, Chenghua and Yang, Bo and Wu, Zhiqi and Chen, Gao and Yu, Yihan and Zhou, Shengxiao},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Shadow Removal based on Diffusion, Segmentation and Super-resolution Models}, 
  year={2024},
  volume={},
  number={},
  pages={6045-6054},
  abstract={Shadow removal is one of essential tasks among image restoration tasks which aims to eliminate the visual semantic information hidden or obscured by the shadow in the image to the largest extent. Variations in lighting and the diverse complexity of shadow depth and color resulting from random background factors are common in the shadow removal task. To address these challenges, this paper proposes a novel interactive shadow removal architecture based on the diffusion model, semantic segmentation and multimodal large language model. Our method utilizes a powerful diffusion model to generate shadow-free images with fewer artifacts and super-resolution models to enhance image details. A universal semantic segmentation model is also involved to reduce percpetual dissonance caused by slicing inference. Furthermore, we integrate the capabilities of multimodal large language models to realize prior rule-based optimization. Leveraging the exceptional generative capability of diffusion model and elaborate cooperation among all the modules, our method achieves outstanding perceptual performance on WSRD dataset. We conduct comprehensive experiments to demonstrate the effectiveness of our approach and share insights gained during the participation in the NTIRE 2024 Image Shadow Removal Challenge.},
  keywords={Visualization;Semantic segmentation;Large language models;Superresolution;Semantics;Lighting;Diffusion models;Shadow Removal;Diffusion;Segmentation;Super-Resolution},
  doi={10.1109/CVPRW63382.2024.00611},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{10528423,
  author={Yuan, Sheng and Qiu, Zhao and Yuan, Lin and Chen, Huajing},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Security Processing of Street View Image Information}, 
  year={2023},
  volume={},
  number={},
  pages={1304-1310},
  abstract={In the era of cyber-physical-social service convergence, the applications of image scenes have become more and more complex, leading to serious image security problems. In order to realize the security processing of street scene image information, this paper designs a single-level face and license plate detection model based on YOLO v5x to realize the automatic recognition and detection of faces and license plates in street scenes, and then erases the recognition region to realize the desensitization of street scene image information at the pixel level. On this basis, an image restoration model based on atrous deformable convolution is proposed to accomplish the restoration of street scene images. In this paper, we use the self-made Hong Kong street view dataset for model training. Compared with the YOLO series of target detection models and the classical image complementation model, better performance is achieved, and the secure processing of the information in the actual scene map of Hainan is also realized.},
  keywords={YOLO;Deformable models;Adaptation models;Image recognition;Convolution;Face recognition;Image restoration;artificial intelligence and applications;image processing;attention module;atrous deformable convolution},
  doi={10.1109/ACAIT60137.2023.10528423},
  ISSN={},
  month={Nov},}@ARTICLE{10184959,
  author={Yang, Yachao and Sun, Yanfeng and Wang, Shaofan and Gao, Junbin and Ju, Fujiao and Yin, Baocai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Dual-Masked Deep Structural Clustering Network With Adaptive Bidirectional Information Delivery}, 
  year={2024},
  volume={35},
  number={10},
  pages={14783-14796},
  abstract={Structured clustering networks, which alleviate the oversmoothing issue by delivering hidden features from autoencoder (AE) to graph convolutional networks (GCNs), involve two shortcomings for the clustering task. For one thing, they used vanilla structure to learn clustering representations without considering feature and structure corruption; for another thing, they exhibit network degradation and vanishing gradient issues after stacking multilayer GCNs. In this article, we propose a clustering method called dual-masked deep structural clustering network (DMDSC) with adaptive bidirectional information delivery (ABID). Specifically, DMDSC enables generative self-supervised learning to mine deeper interstructure and interfeature correlations by simultaneously reconstructing corrupted structures and features. Furthermore, DMDSC develops an ABID module to establish an information transfer channel between each pairwise layer of AE and GCNs to alleviate the oversmoothing and vanishing gradient problems. Numerous experiments on six benchmark datasets have shown that the proposed DMDSC outperforms the most advanced deep clustering algorithms.},
  keywords={Learning systems;Clustering methods;Adaptive systems;Self-supervised learning;Graph convolutional networks;Representation learning;Deep clustering;graph convolutional networks (GCNs);network representation learning;self-supervised learning},
  doi={10.1109/TNNLS.2023.3281570},
  ISSN={2162-2388},
  month={Oct},}@INPROCEEDINGS{10342406,
  author={Xing, Dengpeng and Yang, Yiming and Wang, Zechang and Li, Jiale and Xu, Bo},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Generalized Robot Dynamics Learning and Gen2Real Transfer}, 
  year={2023},
  volume={},
  number={},
  pages={4279-4284},
  abstract={Acquiring dynamics is critical for robot learning and is fundamental to planning and control. This paper concerns two fundamental questions: How can we learn a model that covers massive, diverse robot dynamics? Can we construct a model that lifts the data-collection pain and domain expertise required for building specific robot models? We learn the dynamics involved in a dataset containing a large number of serial articulated robots and propose a new concept, “Gen2Real”, to transfer simulated, generalized models to physical, specific robots. We generate a large-scale dataset by randomizing dynamics parameters, topology configurations, and model dimensions, which, in sequence, correspond to different properties, connections, and numbers of robot links. A structure modified from the generative pre-trained transformer is applied to approximate the dynamics of massive heterogeneous robots. In Gen2Real, we transfer the pre-trained model to a target robot using distillation, for the sake of real-time computation. The results demonstrate the superiority of the proposed method in terms of its accuracy in learning a tremendous amount of robot dynamics and its generality to transfer to different robots.},
  keywords={Parallel robots;Parameter estimation;Pain;Computational modeling;Transformers;Robot learning;Real-time systems},
  doi={10.1109/IROS55552.2023.10342406},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{10822029,
  author={Li, Youhao and Huang, Yongzhi and Gao, Qingchen and Chen, Pindong and Liu, Yong and Tu, Liyun},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Learning Interpretable and Robust Spatiotemporal Dynamics from fMRI for Precise Identification of Neurological Disorders}, 
  year={2024},
  volume={},
  number={},
  pages={968-975},
  abstract={Resting-state functional magnetic resonance imaging (rs-fMRI) has significantly advanced the diagnosis of brain diseases. However, existing methods are generally limited to small, disease-specific datasets with less convincing outcomes or lack the interpretability needed to identify reliable disease-associated biomarkers. In this paper, we introduce a novel generative inference model that integrates a Variational Autoencoder (VAE) with Non-negative Matrix Factorization (NMF). Our model comprises three key components: an encoder for learning spatiotemporal dynamic feature embeddings within fMRI data, a decoder to reconstruct the input data from the encoded latent space, and a classifier to distinguish between neurological disorders and normal controls. The three components are simultaneously optimized to perform inference by estimating the posterior distribution of the latent variables from the input fMRI, yielding predictive and interpretable biomarkers for the diagnosis of neurological disorders. We extensively evaluated our method for identifying Autism Spectrum Disorder (ASD) and Alzheimer’s Disease (AD) using two public datasets, ABIDE and ADNI. Experimental results show that our method achieves state-of-the-art performance across various metrics.},
  keywords={Neurological diseases;Measurement;Biological system modeling;Autoencoders;Functional magnetic resonance imaging;Biomarkers;Brain modeling;Spatiotemporal phenomena;Reliability;Image reconstruction;Brain disorders;classification;resting-state fMRI;supervised variational autoencoders;non-negative matrix factorization},
  doi={10.1109/BIBM62325.2024.10822029},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{11093736,
  author={Ren, Xingyu and Deng, Jiankang and Cheng, Yuhao and Zhu, Wenhan and Yan, Yichao and Yang, Xiaokang and Zafeiriou, Stefanos and Ma, Chao},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={S3-Face: SSS-Compliant Facial Reflectance Estimation via Diffusion Priors}, 
  year={2025},
  volume={},
  number={},
  pages={16051-16060},
  abstract={Recent 3D face reconstruction methods have made remarkable advancements, yet achieving high-quality facial reflectance from monocular input remains challenging. Existing methods rely on the light-stage captured data to learn facial reflectance models. However, limited subject diversity in these datasets poses challenges in achieving good generalization and broad applicability. This motivates us to explore whether the extensive priors captured in recent generative diffusion models (e.g., Stable Diffusion) can enable more generalizable facial reflectance estimation as these models have been pre-trained on large-scale internet image collections containing rich visual patterns. In this paper, we introduce the use of Stable Diffusion as a prior for facial reflectance estimation, achieving robust results with minimal captured data for fine-tuning. We present S3-Face, a comprehensive framework capable of producing SSS-compliant skin reflectance from in-the-wild images. Our method adopts a two-stage training approach: in the first stage, DSN-Net is trained to predict diffuse albedo, specular albedo, and normal maps from in-the-wild images using a novel joint reflectance attention module. In the second stage, HM-Net is trained to generate hemoglobin and melanin maps based on the diffuse albedo predicted in the first stage, yielding SSS-compliant and detailed reflectance maps. Extensive experiments demonstrate that our method achieves strong generalization and produces high-fidelity, SSS-compliant facial reflectance estimation.},
  keywords={Reflectivity;Training;Visualization;Three-dimensional displays;Face recognition;Estimation;Reconstruction algorithms;Diffusion models;Skin;Data models;3d face;reflectance;skin;rendering},
  doi={10.1109/CVPR52734.2025.01496},
  ISSN={2575-7075},
  month={June},}@ARTICLE{11143564,
  author={Chi, Kaichen and Li, Junjie and Chu, Jun and Li, Qiang and Wang, Qi},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={A Diffusion Model With Physically Plausible Gradient for Remote Sensing Shadow Removal}, 
  year={2025},
  volume={63},
  number={},
  pages={1-11},
  abstract={Remote sensing imagery is important for geographical object exploration, but shadow contamination consistently challenges the image formation quality and subsequent applications. Although the diffusion model significantly advances the shadow removal field, current paradigms ignore the physical property of shadow images and thus lose the desired interpretability. To bridge this gap, we propose SR-Diffusion, a shadow removal diffusion model that collaborates with infrared thermal distribution, chromaticity, and illumination intensity regulations. The core insight of SR-Diffusion is to inject nearly all available physical priors into the noise during the reverse process, thus providing desirable generative paths in noisy environments. Specifically, we leverage a modal translation (visible  $\mapsto $  infrared) scheme to explore the cross-domain mapping, thus providing the thermal spectrum. Simultaneously, we introduce a novel horizontal/vertical-intensity (HVI) space to decouple the visible modality into chromaticity and illumination. Coupled with a gradient guidance, the above physical constraints are embedded into the noise, which contributes to generating stable shadow-free images. Comprehensive experiments demonstrate that SR-Diffusion outperforms state-of-the-art shadow removal methods.},
  keywords={Lighting;Diffusion models;Noise;Image color analysis;Noise reduction;Degradation;Colored noise;Visualization;Learning systems;Diffusion processes;Denoising diffusion model;physical property;remote sensing;shadow removal},
  doi={10.1109/TGRS.2025.3603651},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{11094722,
  author={Lai, Yingxin and Xu, Cuijie and Shi, Haitian and Yang, Guoqing and Li, Xiaoning and Luo, Zhiming and Li, Shaozi},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Font-Agent: Enhancing Font Understanding with Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={19670-19680},
  abstract={The rapid development of generative models has significantly advanced font generation. However, limited exploration has been devoted to the evaluation and interpretability of graphical fonts. Existing quality assessment models can only provide basic visual analyses, such as recognizing clarity and brightness, without in-depth explanations. To address these limitations, we first constructed a large-scale multimodal dataset named the Diversity Font Dataset (DFD), comprising 135,000 font-text pairs. This dataset encompasses a wide range of generated font types and annotations, including language descriptions and quality assessments, thus providing a robust foundation for training and evaluating font analysis models. Based on this dataset, we developed a font agent built upon a Vision-Language Model (VLM) aiming to enhance font quality assessment and offer interpretable question-answering capabilities. Alongside the original visual encoder in VLM, we integrated an Edge-Aware Traces (EAT) module to capture detailed edge information of font strokes and components. Furthermore, we introduced a Dynamic Direct Preference Optimization (D-DPO) strategy to facilitate efficient model fine-tuning. Experimental results demonstrate that Font-Agent achieves state-of-the-art performance on the established dataset. To further evaluate the generalization ability of our algorithm, we conducted additional experiments on several public datasets. The results highlight the notable advantage of Font-Agent in both assessing the quality of generated fonts and comprehending their content.},
  keywords={Training;Visualization;Analytical models;Protocols;Large language models;Heuristic algorithms;Quality assessment;Pattern recognition;Reliability;Optimization},
  doi={10.1109/CVPR52734.2025.01832},
  ISSN={2575-7075},
  month={June},}@ARTICLE{11072471,
  author={Wang, Lizhi and Zhou, Feng and Yu, Bo and Cao, Pu and Yin, Jianqin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian Segmentation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation. Specifically, we propose a novel 3D target segmentation technique based on 2D Gaussian Splatting, which segments 3D consistent target masks in multi-view scene images and generates a preliminary target model. Moreover, to reconstruct the unseen portions of the target, we propose a novel target replenishment technique driven by large-scale generative diffusion priors. We demonstrate that our method can accurately reconstruct specific targets from large scenes, both quantitatively and qualitatively. Our experiments show that OMEGAS significantly outperforms existing reconstruction methods across various scenarios.},
  keywords={Three-dimensional displays;Image segmentation;Image reconstruction;Solid modeling;Neural radiance field;Rendering (computer graphics);Object oriented modeling;Vectors;Training;Reviews;3D reconstruction;Object segmentation;Diffusion models;Mesh generation},
  doi={10.1109/TCSVT.2025.3586755},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{10896143,
  author={Chen, Jia and Liu, Fangze and Wang, Yingying},
  booktitle={2025 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)}, 
  title={MDD: Masked Deconstructed Diffusion for 3D Human Motion Generation from Text}, 
  year={2025},
  volume={},
  number={},
  pages={61-72},
  abstract={We present MDD (Masked Deconstructed Diffusion), a novel framework for generating high-fidelity 3D human motions from textual descriptions. Our MDD framework employs a multi-stage Kinematic Chain Quantization (KCQ) that effectively encodes motion sequences into a compact yet expressive codebook by capturing both local and global human kinematic features. This codebook is then leveraged by a Masked Deconstructed Diffusion Transformer (MDDT), which takes text inputs and iteratively refines the output motion sequence through masked index prediction in a deconstructed diffusion process. By aligning the prediction with the denoising process, our method strikes an optimal balance between generation quality and computational efficiency. Extensive evaluations on multiple established benchmarks demonstrate that MDD consistently outperforms state-of-the-art methods in terms of precision and semantic accuracy, while achieving superior inference speed. Our generated motions are further validated in multiple virtual reality (VR) scenes, showcasing the effectiveness of our framework in VR applications.},
  keywords={Solid modeling;Three-dimensional displays;Quantization (signal);Semantics;Noise reduction;Diffusion processes;Kinematics;Virtual reality;Transformers;Computational efficiency;3D Human Motion Generation;Artificial Intelligence;Virtual Reality;Character Animation;Deep Learning},
  doi={10.1109/AIxVR63409.2025.00017},
  ISSN={2771-7453},
  month={Jan},}@INPROCEEDINGS{10936678,
  author={Zhao, Changhua and Yang, Yimei and Yang, Yujun and Tang, Bo and Mi, Chunqiao},
  booktitle={2024 International Conference on Information Technology, Comunication Ecosystem and Management (ITCEM)}, 
  title={Personalized Learning Path Recommendation Driven by Multi-Modal Knowledge Graphs and Large Models Collaboration}, 
  year={2024},
  volume={},
  number={},
  pages={227-232},
  abstract={This study examines ways to create personalized learning pathways that are influenced by large collaborative models and multi-modal knowledge graphs. By integrating multi-modal data to construct a comprehensive knowledge network, this study utilizes the generative and comprehension capabilities of large models to achieve personalized customization of learning resources. The focus of the research is on accurately depicting learner characteristics through multi-modal profiling techniques, ensuring the transparency and credibility of the content generated by large models. The personalized learning route suggestion model proposed in this paper includes two parts: recommendations for learning paths within courses and recommendations for learning paths within learning tasks, aiming to enhance learning efficiency and outcomes through the collaborative effects of multi-modal knowledge graphs and large models. Experimental results indicate that the learning path recommendation model driven by multi-modal knowledge graphs and large collaborative models performs excellently in improving learning outcomes and recommendation performance.},
  keywords={Knowledge engineering;Analytical models;Accuracy;Biological system modeling;Education;Ecosystems;Collaboration;Knowledge graphs;Data models;Information technology;Multi-Modal Knowledge Graph;Large Model Collaborative Driving;Personalized Learning;Learning Path Recommendation},
  doi={10.1109/ITCEM65710.2024.00049},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11127647,
  author={Ma, Hao and Pu, Zhiqiang and Wang, Shijie and Liu, Boyin and Wang, Huimu and Liang, Yanyan and Yi, Jianqiang},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Stochastic Trajectory Prediction Under Unstructured Constraints}, 
  year={2025},
  volume={},
  number={},
  pages={11750-11757},
  abstract={Trajectory prediction facilitates effective planning and decision-making, while constrained trajectory prediction integrates regulation into prediction. Recent advances in constrained trajectory prediction focus on structured constraints by constructing optimization objectives. However, handling unstructured constraints is challenging due to the lack of differentiable formal definitions. To address this, we propose a novel method for constrained trajectory prediction using a conditional generative paradigm, named Controllable Trajectory Diffusion (CTD). The key idea is that any trajectory corresponds to a degree of conformity to a constraint. By quantifying this degree and treating it as a condition, a model can implicitly learn to predict trajectories under unstructured constraints. CTD employs a pre-trained scoring model to predict the degree of conformity (i.e., a score), and uses this score as a condition for a conditional diffusion model to generate trajectories. Experimental results demonstrate that CTD achieves high accuracy on the ETH/UCY and SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to unstructured constraints and can predict trajectories that satisfy combinatorial constraints.},
  keywords={Measurement;Stochastic processes;Predictive models;Diffusion models;Turning;Regulation;Trajectory;Planning;Robotics and automation;Optimization},
  doi={10.1109/ICRA55743.2025.11127647},
  ISSN={},
  month={May},}@ARTICLE{11165205,
  author={Geng, Sheng and Jiang, Shu and Hou, Tao and Yao, Hongcheng and Huang, Jiashuang and Ding, Weiping},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={FEU-Diff: A Diffusion Model With Fuzzy Evidence-Driven Dynamic Uncertainty Fusion for Medical Image Segmentation}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Diffusion models, as a class of generative frameworks based on step-wise denoising, have recently attracted significant attention in the field of medical image segmentation. However, existing diffusion-based methods typically rely on static fusion strategies to integrate conditional priors with denoised features, making them difficult to adaptively balance their respective contributions at different denoising stages. Moreover, these methods often lack explicit modeling of pixel-level uncertainty in ambiguous regions, which may lead to the loss of structural details during the iterative denoising process, ultimately compromising the accuracy (Acc) and completeness of the final segmentation results. To this end, we propose FEU-Diff, a diffusion-based segmentation framework that integrates fuzzy evidence modeling and uncertainty fusion (UF) mechanisms. Specifically, a fuzzy semantic enhancement (FSE) module is designed to model pixel-level uncertainty through Gaussian membership functions and fuzzy logic rules, enhancing the model’s ability to identify and represent ambiguous boundaries. An evidence dynamic fusion (EDF) module estimates feature confidence via a Dirichlet-based distribution and adaptively guides the fusion of conditional information and denoised features across different denoising stages. Furthermore, the UF module quantifies discrepancies among multisource predictions to compensate for structural detail loss during the iterative denoising process. Extensive experiments on four public datasets show that FEU-Diff consistently outperforms state-of-the-art (SOTA) methods, achieving an average gain of 1.42% in the Dice similarity coefficient (DSC), 1.47% in intersection over union (IoU), and a 2.26 mm reduction in the 95th percentile Hausdorff distance (HD95). In addition, our method generates uncertainty maps that enhance clinical interpretability.},
  keywords={Image segmentation;Diffusion models;Uncertainty;Noise reduction;Transformers;Accuracy;Medical diagnostic imaging;Adaptation models;Semantics;Fuzzy logic;Diffusion model;evidence dynamic fusion (EDF);fuzzy logic;interpretability;medical image segmentation},
  doi={10.1109/TNNLS.2025.3609085},
  ISSN={2162-2388},
  month={},}@ARTICLE{11021303,
  author={Ren, Hao Pan and Duan, Wei and Li, Wan Yu and Liu, Yi and Guo, Yu Dong and Huang, Shi-Sheng and Zhang, Ju Yong and Huang, Hua},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={EGAvatar: Efficient GAN Inversion for Generalizable Head Avatar From Few-Shot Images}, 
  year={2025},
  volume={31},
  number={10},
  pages={8654-8667},
  abstract={Controllable head avatar reconstruction via the inversion of few-shot images using 3D generative models has demonstrated significant potential for efficient avatar creation. However, under limited input conditions, existing one-shot inversion methods often fail to produce high-fidelity results, frequently leading to shape distortions, expression deviations, and identity inconsistencies. To address these limitations, we propose EGAvatar, a novel and efficient 3DGAN inversion framework designed to generate high-fidelity, generalizable head avatars from few-shot images. The core principle of EGAvatar is a decoupling-by-inverting strategy, built upon an animatable 3DGAN prior. Specifically, we introduce an effective animatable 3DGAN model that synthesizes high-quality 3D avatars by integrating a coarse 3D triplane representation (derived from a latent 3DGAN) with an offset 3D triplane (learned via a triplane 3DGAN). Leveraging this architecture, we design a 3DGAN-based inversion approach to reconstruct 3D avatars efficiently. Additionally, we incorporate an expression-view disentanglement mechanism to maintain consistent appearance across varying expressions and viewpoints, thereby enhancing the generalizability of avatar reconstruction from limited input images. Extensive experiments conducted on two publicly available benchmarks and a private dataset demonstrate that EGAvatar outperforms existing state-of-the-art methods in both qualitative and quantitative evaluations. Notably, EGAvatar achieves superior performance while requiring significantly fewer input images and offering more efficient training and inference.},
  keywords={Avatars;Three-dimensional displays;Head;Image reconstruction;Solid modeling;Computational modeling;Computational efficiency;Training;Animation;Codes;High-fidelity 3D avatar;3DGAN inversion;expression–view disentanglement;few-shot images},
  doi={10.1109/TVCG.2025.3575782},
  ISSN={1941-0506},
  month={Oct},}@INPROCEEDINGS{10893341,
  author={Li, Xiaomei and Xia, Lei and He, Ziming and Wu, Pengfei and Chen, Danyang and Fan, Ling},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={Enhancing Interior Design Education Through the Integration of AIGC Tools: A Novel “Creator-Thon” Approach}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This innovative practice full paper proposes Creator-Thon, a novel teaching project designed to introduce Generative AI into traditional interior design workflows. The project adopts an interdisciplinary approach including application scenario investigation, workflow construction, collaborative innovation, and iterative optimization. The underlying research is grounded in a six-level goal hierarchy, comprising sub-goals of integrating AI capabilities and design knowledge, embedding AI skills into workflows, enhancing the learning engagement, empowering personal career development, fostering (human-AI and human-human) collaboration, and transforming the design paradigms. The teaching framework is progressively refined across multiple iterations, enhancing learning engagement by simplifying tool operations, refining design language, and re-engineering the design process. The findings underscore that integration of authentic application scenarios markedly boosts engagement and motivation among learners, modular instruction also enhances teaching quality and practical skills, and the adaptive optimization of the teaching framework informed by participants' feedback, is key to a successful Generative AI involved interior design pedagogy. This work provides insights that contribute to the application of Generative AI technology in interior design education and practice, facilitating transformations and progress in AI literacy in the field.},
  keywords={Technological innovation;Career development;Generative AI;Conferences;Education;Refining;Collaboration;Iterative methods;Optimization;generative AI;interior design education;interdisciplinary collaboration;iterative optimization},
  doi={10.1109/FIE61694.2024.10893341},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10690418,
  author={Lingesh, B. and Monesh, M. S. and Kannaiah, Sathish Kumar and S, Senthil Pandi},
  booktitle={2024 Second International Conference on Advances in Information Technology (ICAIT)}, 
  title={Detecting AI Face Fraud Detection Using CNN Based Deep Learning Algorithm}, 
  year={2024},
  volume={1},
  number={},
  pages={1-7},
  abstract={The advancement of deep learning and generative models has led to the creation of highly realistic artificial images, including human faces. However, this progress has also raised concerns about potential misuse, such as using AI-generated faces for fraudulent activities. Addressing this issue, AI Face Fraud Detection has become a crucial area of focus for research and development. The primary objective is to develop an Artificial Intelligence system sophisticated enough to effectively discern true human images from AI-generated ones. The research's Background is the increased face-based fraud threat. Human animators can misuse instances of AI-generated faces to act as individuals with that face or create false identities. The AI Face Fraud Detection system is based on CNN, GAN advanced deep learning techniques to analyze and differentiate between true human faces and AI generated faces. By subjecting it to extensive training with a wide range of human images and AI-generated faces, the system is resistant and highly flexible to discern illegal activity. AI Face Fraud Detection involves key components like facial recognition algorithms, feature extraction techniques (CNN, LBP, HOG), and fraud detection models. Performance metrics are used to evaluate model effectiveness in distinguishing real human images from AI-generated ones. The proposed model achieves more than 90% as in all performance metrics terms. The impact of this research spans across multiple domains, including cyber security, digital forensics, and identity verification. By effectively identifying AI-generated faces, organizations can bolster their fraud prevention strategies, safeguard user identities, and foster trust in digital interactions. In summary, this study contributes to leveraging AI for enhancing security and authenticity in today's digital landscape.},
  keywords={Training;Measurement;Deep learning;Feature extraction;Fraud;Security;Artificial intelligence;Faces;Videos;Testing;Real Human Images;AI-Generated Faces;Fraud Detection;Identity Verification},
  doi={10.1109/ICAIT61638.2024.10690418},
  ISSN={},
  month={July},}@INPROCEEDINGS{11147198,
  author={Zhang, Elaine and Liu, Hillary and Lin, Shixuan and Zhou, Qisong and Kostanyan, Siranush and Fortino, Andres},
  booktitle={2025 IEEE Integrated STEM Education Conference (ISEC)}, 
  title={Navigating Ethical Challenges in Media: AI Tools for Detecting Harmful Narratives}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The prevalence of xenophobic language and misinformation in media reporting poses a significant challenge to ethical journalism and informed public discourse. This project proposes an AI-driven media analytics tool that utilizes advanced natural language processing techniques (including fine-tuned GPT-4 and BERT models) to detect harmful language, verify facts, and provide thematic analysis in real-time. The proof of concept empowers media professionals by providing actionable insights to promote balanced and inclusive reporting.The tool was developed using a modular design that integrates exclusionary language detection, fact-checking, and narrative analysis into a user-friendly web-based platform. Rigorous testing (including accuracy assessments and user feedback) proved the effectiveness of the tool, with a detection accuracy rate of over 85% and a guaranteed error rate of 10%. The results confirm the tool’s ability to identify explicit and implicit bias while reducing false positives and omissions, thereby improving the quality of media coverage. These findings highlight the tool’s potential to transform journalistic practices and promote more ethical and empathetic coverage of migration and displacement topics. Future work will focus on expanding multilingual support, increasing contextual sensitivity to subtle biases, taking into account cultural and linguistic differences across regions, and integrating the tool into larger media management systems. Broader applications include extending its use to fields requiring real-time content analysis, such as education, policy development, and advocacy. The project highlights the transformative potential of AI to address societal challenges, providing a scalable, efficient, and impactful solution for promoting accountability and inclusion in digital media environments.},
  keywords={Ethics;Accuracy;Transforms;Media;Journalism;Real-time systems;Natural language processing;Cultural differences;Artificial intelligence;Fake news;Artificial Intelligence (AI);Natural Language Processing (NLP);Harmful Narratives;Xenophobic Language Detection;Ethical Journalism;Generative AI Models},
  doi={10.1109/ISEC64801.2025.11147198},
  ISSN={2473-7623},
  month={March},}@INPROCEEDINGS{9397226,
  author={Gupta, Megha and Rama Kishore, R.},
  booktitle={2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)}, 
  title={A Survey of Watermarking Technique using Deep Neural Network Architecture}, 
  year={2021},
  volume={},
  number={},
  pages={630-635},
  abstract={A computerized watermark is a sort of marker secretly introduced in a noise accepting signals, let us say, video, audio or image. It is normally accustomed to recognize proprietorship for the copyright of such signals. "Watermarking" is the way toward covering computerized bits in a conveyor signal. The shrouded data need not have to contain a connection to the transporter signal. Computerized watermarks might be applied to verify the genuineness or uprightness of the conveyor signal or to demonstrate the uniqueness of its proprietors. This paper explores a new family of transformation dependent on Deep Learning systems. We survey and present a relative investigation of the different profound learning procedures that target installing watermark that can be worked upon to accomplish substantial security level for the data being transmitted, and robustness over a few attacks particularly when transmitted over the loud medium. Our paper centers on how deep learning-based watermarking approaches show their prevalence because of the level of imperceptibility and robustness presented by adjustments in recurrence coefficients. We likewise feature the parts of the current methodologies that can be worked upon to accomplish images that are exceptionally indistinct from one another in the future.},
  keywords={Deep learning;Digital images;Neural networks;Watermarking;Learning (artificial intelligence);Robustness;Intelligent systems;Artificial neural network (ANN);Generative Adversarial neutral network (GAN);Convolutional neural network (CNN);Probabilistic neural network (PNN);Deep learning Deep neural network (DNN);Digital image watermarking;Blind;Robust;Fragile;Semi-fragile Watermarking},
  doi={10.1109/ICCCIS51004.2021.9397226},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10499177,
  author={Kumar, Vijay and Kapoor, Ansh and Chaudhary, Rishi Raj and Gupta, Lakshya and Khokhar, Dev},
  booktitle={2024 11th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Preserving Integrity: A Binary Classification Approach to Unmasking Artificially Generated Voices in the Age of Deepkakes}, 
  year={2024},
  volume={},
  number={},
  pages={1449-1454},
  abstract={Generative AI uses machine learning techniques like semi-supervised or unsupervised learning algorithms for the creation of digital content such as images, audio, and videos. There are ethical concerns arising from generative AI’s impact on speech technology, specifically, voice cloning and real-time voice conversion. To mitigate the associated risks of privacy breaches and misrepresentation, the research utilizes a DEEP-VOICE dataset that comprises audio clips from eight notable figures, converted using Retrieval-based Voice Conversion to detect Deepfake audio files. Framed as a binary classification problem, statistical analysis reveals significantly different distributions of temporal audio features between real and AI-generated speech. The experimental results show that the Random Forest Classifier over the 5-fold Cross-Validation technique results in a classification accuracy of 98.574%.},
  keywords={Ethics;Deepfakes;Statistical analysis;Pressing;Privacy breach;Real-time systems;Security;Artificial Intelligence;Machine Learning;Binary Classification;Deepfake;Audio Detection},
  doi={10.23919/INDIACom61295.2024.10499177},
  ISSN={},
  month={Feb},}@ARTICLE{10422884,
  author={Liu, Yinqiu and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Miao, Chunyan and Shen, Xuemin and Jamalipour, Abbas},
  journal={IEEE Wireless Communications}, 
  title={Blockchain-Empowered Lifecycle Management for AI-Generated Content Products in Edge Networks}, 
  year={2024},
  volume={31},
  number={3},
  pages={286-294},
  abstract={The rapid development of Artificial Intelligence-Generated Content (AIGC) has brought daunting challenges in the areas of service latency, security, and trustworthiness. Recently researchers have presented the edge AIGC paradigm, effectively optimizing the service latency by distributing AIGC services to edge devices. However, AIGC products are still unprotected and vulnerable to tampering and plagiarization. Moreover, as a kind of online non-fungible digital property, the free circulation of AIGC products is hindered by the lack of trustworthiness in open networks. For the first time, in this article, we present a blockchain-empowered framework to manage the lifecycle of edge AIGC products. Specifically, leveraging fraud proofability, we first propose a protocol to protect the ownership and copyright of AIGC, called Proof-of-AIGC. Then, we design an incentive mechanism to guarantee the legitimate and timely execution of the funds-AIGC ownership exchanges among anonymous users. Furthermore, we implement a multi-weight subjective logic-based reputation scheme with which AIGC producers can determine which edge service provider is trustworthy and can reliably handle their services. Through the use of test data, we demonstrate the superiority of the proposed approach. Last but not least, we discuss important open directions for further research.},
  keywords={Internet;Blockchains;Servers;Generative adversarial networks;Training;Task analysis;Security;Artificial intelligence;Content management},
  doi={10.1109/MWC.003.2300053},
  ISSN={1558-0687},
  month={June},}@INPROCEEDINGS{10465570,
  author={Jagan, S. and Pokhariyal, Rajesh and Mahajan, Kirti and Deepika, Ch L N and Sudha, P. Deva and Dutta, Abhradita},
  booktitle={2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)}, 
  title={Machine Learning with Deep Learning Approach for Cyber Security Threats Prevention Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The need for effective cybersecurity solutions has never been greater than in today's world of constant digitalization and interconnection. This study explores the use of deep learning to the creation of a model for preventing cyber security threats using machine learning. The model is meant to successfully identify and prevent cyber-attacks by analysing network traffic and system behaviour using a convolutional neural network (CNN) linked with variationalautoencoders (VAEs). This study's findings illustrate the model's competency in threat recognition with a high degree of accuracy, precision, and recall. Actively monitoring and evaluating network data, providing automatic threat identification and response, further demonstrates its usefulness when used in the real world. This work sets the path for future developments in quantum-resistant encryption, adversarial machine learning studies, deep learning architectures, autonomous cybersecurity systems, explainable AI, and global threat intelligence cooperation. These new approaches have great potential for paving the way towards a more secure and resilient cyber future.},
  keywords={Deep learning;Analytical models;Electric potential;Explainable AI;Neural networks;Telecommunication traffic;Encryption;Deep Learning;Machine Learning;Convolutional Neural Network (CNN);Real-time Monitoring;Variational Autoencoders (VAEs);Quantum-Resistant Cryptography;Threat Prevention;Network Traffic Analysis},
  doi={10.1109/ICSES60034.2023.10465570},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10957105,
  author={Rajpoot, Kuldeep Singh and Saxena, Utkarsh and Shelke, Nitin Arvind and Agrawal, Rohit},
  booktitle={2025 International Conference on Ambient Intelligence in Health Care (ICAIHC)}, 
  title={Revolutionizing Attendance Tracking: A Holistic Automated Facial Recognition System}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This work intends to solve shortcomings of existing systems, particularly in big courses prone to proxy attendance, by developing an automated facial recognition system to monitor student attendance in classrooms. The system aims to provide a contemporary, effective, and expandable approach to monitoring attendance. Important features include interaction with security cameras to lower proxy attendance, continuous observation to improve accuracy, and facial recognition for accurate tracking. Comparing automated attendance tracking to manual techniques, it increases efficiency and dependability while minimizing human labor. It uses a holistic method, encompassing duties such as taking pictures using a camera, identifying people, and matching and identifying them.},
  keywords={Histograms;Accuracy;Face recognition;Medical services;Manuals;Machine learning;Cameras;Ambient intelligence;Security;Monitoring;Artificial Intelligence;Facial attendance;Machine Learning;Histogram of Oriented Gradient},
  doi={10.1109/ICAIHC64101.2025.10957105},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11096470,
  author={Desai, Tanvi and Kumar Pal, Rakesh},
  booktitle={2025 4th International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)}, 
  title={Machine Learning in Cybersecurity: A Comprehensive Review of Threat Detection, Prevention, and Response Strategies}, 
  year={2025},
  volume={},
  number={},
  pages={148-153},
  abstract={The increased complexity and rate of cyberattacks have necessitated a change in the paradigm of cybersecurity strategies. The conventional rule-based systems are increasingly insufficient to address the dynamic and adaptive behaviour of modern threats. Machine learning (ML), with its ability to learn from vast amounts of data and identify complex patterns, has emerged as a powerful ally in combating these threats. This paper offers a comprehensive review of the application of ML in cybersecurity, its applications across the entire threat lifecycle. We present a wide range of ML approaches, including supervised, unsupervised, and deep learning approaches, and their effectiveness in areas of high consequence like intrusion detection, malware analysis, phishing detection, and anomaly detection. The review also provides ML-based prevention techniques, including vulnerability assessment and access control, and response techniques, including automatic incident response and threat intelligence. The paper also provides the challenges and limitations of applying ML in cybersecurity, including adversarial attacks, data quality issues, and explainability.},
  keywords={Technological innovation;Adaptive systems;Reviews;Prevention and mitigation;Data integrity;Phishing;Threat assessment;Computer security;Computer crime;Optimization;cyber security;threat detection;response strategies;machine learning},
  doi={10.1109/ICCMSO67468.2025.00035},
  ISSN={},
  month={June},}@INPROCEEDINGS{10894909,
  author={V, Jananee and N S, Ragava Krishnan and V S, Rakesh},
  booktitle={2024 International Conference on Emerging Research in Computational Science (ICERCS)}, 
  title={AI-Powered Dynamic Images: A New Frontier in Graphical Password Authentication}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Graphical Password Authentication is an emerging and advanced technique in password authentication for hypothesis that provides a stronger and more secure password compared to the traditional text password, which is thus deemed superior. These methods of graphical authentication, however, have benefits that they have designed for current systems that still incorporate vulnerabilities like shoulder surfing, screen recording, brute force, and other attacks. The present research works on the existing graphical encryption systems that indicate the strengths and weaknesses the systems have. Enter a brand-new method-the AI-based dynamic image-which brings in more security for graphical passwords on authentication process as an authentication difference. It is an artificial intelligence back-end, in which with every access attempt, the picture is automatically varying but presents easy visible contents which give a higher level of enhanced security.},
  keywords={Visualization;Scientific computing;Heuristic algorithms;Authentication;Passwords;Generative adversarial networks;User experience;Recording;Security;Artificial intelligence;Visual Password Authentication;Dynamic Image;Security Enhancement;Vulnerabilities;Authentication Process},
  doi={10.1109/ICERCS63125.2024.10894909},
  ISSN={},
  month={Dec},}@ARTICLE{4182377,
  author={Diplaros, Aristeidis and Vlassis, Nikos and Gevers, Theo},
  journal={IEEE Transactions on Neural Networks}, 
  title={A Spatially Constrained Generative Model and an EM Algorithm for Image Segmentation}, 
  year={2007},
  volume={18},
  number={3},
  pages={798-808},
  abstract={In this paper, we present a novel spatially constrained generative model and an expectation-maximization (EM) algorithm for model-based image segmentation. The generative model assumes that the unobserved class labels of neighboring pixels in the image are generated by prior distributions with similar parameters, where similarity is defined by entropic quantities relating to the neighboring priors. In order to estimate model parameters from observations, we derive a spatially constrained EM algorithm that iteratively maximizes a lower bound on the data log-likelihood, where the penalty term is data-dependent. Our algorithm is very easy to implement and is similar to the standard EM algorithm for Gaussian mixtures with the main difference that the labels posteriors are "smoothed" over pixels between each E- and M-step by a standard image filter. Experiments on synthetic and real images show that our algorithm achieves competitive segmentation results compared to other Markov-based methods, and is in general faster},
  keywords={Image segmentation;Iterative algorithms;Pixel;Clustering algorithms;Hidden Markov models;Image color analysis;Image edge detection;Informatics;Intelligent systems;Intelligent sensors;Bound optimization;expectation–maximization (EM) algorithm;hidden Markov random fields (MRFs);image segmentation;spatial clustering},
  doi={10.1109/TNN.2007.891190},
  ISSN={1941-0093},
  month={May},}@ARTICLE{10669822,
  author={Pooryousef, Vahid and Cordeil, Maxime and Besançon, Lonni and Bassed, Richard and Dwyer, Tim},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Collaborative Forensic Autopsy Documentation and Supervised Report Generation Using a Hybrid Mixed-Reality Environment and Generative AI}, 
  year={2024},
  volume={30},
  number={11},
  pages={7452-7462},
  abstract={Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities—including mixed-reality (MR) tools to support 3D visualisation of such data—these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education. A free copy of this paper and all supplemental materials are available at https://osf.io/ygfzx.},
  keywords={Autopsy;Virtual reality;Forensics;Biomedical imaging;Collaboration;Computed tomography;Documentation;Forensic autopsy;report generation;documentation;mixed reality;generative AI},
  doi={10.1109/TVCG.2024.3456212},
  ISSN={1941-0506},
  month={Nov},}@ARTICLE{10196424,
  author={Kazmi, Syed M. Kazam Abbas and Aafaq, Nayyer and Khan, Mansoor Ahmed and Khalil, Mohsin and Saleem, Ammar},
  journal={IEEE Access}, 
  title={From Pixel to Peril: Investigating Adversarial Attacks on Aerial Imagery Through Comprehensive Review and Prospective Trajectories}, 
  year={2023},
  volume={11},
  number={},
  pages={81256-81278},
  abstract={Deep models’ feature learning capabilities have gained traction in recent years, driving significant progress in various Artificial Intelligence (AI) domains. The use of Deep Neural Networks (DNNs) has expanded the scope of Computer Vision (CV) and revealed their vulnerability to deliberate adversarial attacks. These attacks involve the careful introduction of perturbations crafted through complex optimization problems. Exploiting vulnerabilities in advanced deep neural network algorithms present security concerns, particularly in practical applications with high stakes like unmanned aerial vehicles (UAVs) and satellite imagery in computer vision. Adversarial attacks, both in digital and physical dimensions, pose a serious threat in the field. This research provides a comprehensive examination of state-of-the-art adversarial attacks specific to aerial imagery using autonomous platforms such as UAVs and satellites. This review covers fundamental concepts, techniques, and the latest advancements, identifying research gaps and suggesting future directions. It aims to deepen researchers’ understanding of the challenges and threats related to adversarial attacks on aerial imagery, serving as a valuable resource to guide future research and enhance the security of computer vision systems in aerial environments.},
  keywords={Artificial neural networks;Training;Computer vision;Terminology;Perturbation methods;Security;Sensors;Image processing;Generative adversarial networks;Autonomous systems;Remote sensing;Artificial intelligence;Autonomous aerial vehicles;Aerial imagery;adversarial attacks;adversarial perturbations;autonomous systems;remote sensing;AI-applications},
  doi={10.1109/ACCESS.2023.3299878},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10940951,
  author={Patil, Shalmali and Bhat, Aparna and Jain, Nilesh and Javalkar, Vishal},
  booktitle={2025 International Conference on Pervasive Computational Technologies (ICPCT)}, 
  title={Integrating Research on AI-Driven Hyper-Personalization: A Review and Framework for Scalable Social Media Campaigns}, 
  year={2025},
  volume={},
  number={},
  pages={766-771},
  abstract={This paper introduces a novel theoretical framework for AI-driven hyper-personalization in social media marketing, addressing the critical gap in existing research that overlooks the integration of technological, strategic, and ethical dimensions at scale. The framework uniquely combines advanced user profiling, dynamic content personalization, and iterative feedback loops to optimize engagement, loyalty, and ROI. It further tackles scalability challenges through innovations like cloud computing and federated learning, while embedding ethical principles of fairness, transparency, and privacy into its design. Unlike prior studies, this paper provides a holistic roadmap for implementing scalable, cross-platform AI systems and explores the integration of emerging technologies such as AR, VR, and blockchain. By synthesizing actionable insights and proposing future research directions, it redefines the potential of hyper-personalized, user-centric marketing strategies in the digital era.},
  keywords={Ethics;Cloud computing;Technological innovation;Privacy;Feedback loop;Social networking (online);Federated learning;Reviews;Scalability;Blockchains;AI-driven hyper-personalization;content personalization;Scalable AI;Ethical AI;Privacy;Generative AI;Recommender systems;Cloud computing;Big data},
  doi={10.1109/ICPCT64145.2025.10940951},
  ISSN={},
  month={Feb},}@ARTICLE{10520929,
  author={Zheng, Jie and Du, Baoxia and Du, Hongyang and Kang, Jiawen and Niyato, Dusit and Zhang, Haijun},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Energy-Efficient Resource Allocation in Generative AI-Aided Secure Semantic Mobile Networks}, 
  year={2024},
  volume={23},
  number={12},
  pages={11422-11435},
  abstract={The integration of semantic communication with Internet of Things (IoT) technologies has advanced the development of Semantic IoT (SIoT), with edge mobile networks playing an increasingly vital role. This paper presents a framework for SIoT-based image retrieval services, focusing on the application in automotive market analysis. Here, semantic information in the form of textual representations is transmitted to users, such as automotive companies, and stored as knowledge graphs, instead of raw imagery. This approach reduces the amount of data transmitted, thereby lowering communication resource usage, and ensures user privacy. We explore potential adversarial attacks that could disrupt image transmission in SIoT and propose a defense mechanism utilizing Generative Artificial Intelligence (GAI), specifically the Generative Diffusion Models (GDMs). Unlike methods that necessitate adversarial training with specifically crafted adversarial example samples, GDMs adopt a strategy of adding and removing noise to negate adversarial perturbations embedded in images, offering a more universally applicable defense strategy. The GDM-based defense aims to protect image transmission in SIoT. Furthermore, considering mobile devices’ resource constraints, we employ GDM to devise resource allocation strategies, optimizing energy use and balancing between image transmission and defense-related energy consumption. Our numerical analysis reveals the efficacy of GDM in reducing energy consumption during adversarial attacks. For instance, in a scenario, GDM-based defense lowers energy consumption by 5.64%, decreasing the number of image retransmissions from 18 to 6, thus underscoring GDM's role in bolstering network security.},
  keywords={Semantics;Task analysis;Training;Image edge detection;Computational modeling;Data models;Social Internet of Things;Generative AI;resource allocation;semantic communication;energy efficiency},
  doi={10.1109/TMC.2024.3396860},
  ISSN={1558-0660},
  month={Dec},}
