@INPROCEEDINGS{9515089,
  author={Florea, Corneliu and Florea, Laura},
  booktitle={2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, 
  title={Multitask Regularization for Image Aesthetic Evaluation}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={Convolutional neural networks are data hungry and in cases when annotation is costly or difficult, additional information from other sets may be welcomed. In this paper, to improve the performance on the main task, we introduce a secondary one, over unlabeled data to provide better structuring. The solution falls in the theme of multiple task learning and unlabeled data is integrated next to the main regression task by classification via pseudo-labeling. The method is showed to improve the baseline performance for image aesthetic assessment on the AADB benchmark.},
  keywords={Computers;Annotations;Benchmark testing;Convolutional neural networks;Task analysis;Artificial intelligence;multi-task;self-labeling;aesthetic;entropy reg-ularization},
  doi={10.1109/ECAI52376.2021.9515089},
  ISSN={},
  month={July},}@INPROCEEDINGS{10463512,
  author={Yun, Sohyeon and Kim, Han-joon},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Univariate Time Series Data Correction Method using LSTM Autoencoder with Temporal Distance}, 
  year={2024},
  volume={},
  number={},
  pages={648-652},
  abstract={High-quality data is essential to increase the reliability of machine learning-based prediction models. For time series data, anomalies significantly reduce the accuracy of prediction models. In this paper, we propose a novel time series data correction method that converts abnormal values of univariate time series data into normal ones. For anomaly detection and correction, we utilize the LSTM Autoencoder model, where we propose a new weight function that considers temporal distance. Through experiments using the open NAB data, we show that our proposed method is superior to the recent conventional methods.},
  keywords={Correlation;Time series analysis;Deep architecture;Predictive models;Data models;Reliability;Artificial intelligence;time series data;LSTM Autoencoder;anomaly detection;data correction;deep learning;data quality},
  doi={10.1109/ICAIIC60209.2024.10463512},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{11163196,
  author={Xu, Xiaohao and Ma, Xiaolin and Liu, Xue and Kuang, Hailan and Liu, Xinhua},
  booktitle={2025 IEEE 12th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={An Improved Heart Rate Measurement Method based on Contrast-Phys+}, 
  year={2025},
  volume={12},
  number={},
  pages={721-725},
  abstract={Existing research on the development of non-contact heart rate measurement technology mainly focuses on rPPG heart rate measurement technology, which mainly extracts rPPG similar to PPG signals by acquiring periodic changes in facial skin color to calculate heart rate, among which rPPG heart rate measurement methods based on deep learning are developing particularly rapidly. However, these deep learning methods either do not have enough detection accuracy or the network model is not lightweight enough, which limits the wide application of rPPG heart rate measurement techniques. In this paper, we propose a method with less computation based on the Contrast-Phys+ method, which introduces T-Max-Avg Pooling, which is friendly to both detailed information and salient features, and feed-forward network to improve the fusion efficiency, which can effectively balance the contradiction between the heart rate detection accuracy and the lightweight of the network model.},
  keywords={Deep learning;Accuracy;Image color analysis;Computational modeling;Learning (artificial intelligence);Photoplethysmography;Heart rate detection;Skin;Heart rate measurement;Information technology;remote photoplethysmography;heart rate measurement;deep learning;contrast learning},
  doi={10.1109/ITAIC64559.2025.11163196},
  ISSN={2693-2865},
  month={May},}@INPROCEEDINGS{10369098,
  author={K, Vivekrabinson and J, Bharath Singh and K, Ragavan and S, Joshua Kumaresan and Veerappa Dinesh S, Erana and G, Mareeswari},
  booktitle={2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Multi-Stroke Handwriting Character Recognition and Enhancing Proficiency with CNN: A Touch-Based Writing Approach}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Handwriting is an essential skill, and difficulties in this area can significantly impact learning and communication. In recent years, Convolutional Neural Networks (CNNs) have evolved as a powerful tool for image recognition and analysis, making them well-suited for handwriting recognition and improvement tasks. The goal of this study is to create and build a touch-based therapy application to improve hand dexterity for handwriting preparation using the CNN model. The system leveraged a large dataset of handwriting samples and employed CNN to learn and recognize patterns in handwritten characters. The CNN model was trained using supervised learning techniques, optimizing its ability to accurately classify and analyze different handwriting styles. To assess the effectiveness of the proposed system, a sequence of tests was conducted. The trained CNN model showed promising accuracy in recognizing and analyzing multi-user free-style multi-stroke handwriting characters and providing valuable feedback to the users. Participants who utilized the system reported improvements in letter formation, alignment, and overall handwriting legibility.},
  keywords={Handwriting recognition;Analytical models;Supervised learning;Medical treatment;Learning (artificial intelligence);Writing;Knowledge management;Convolutional Neural Networks;handwriting;InceptionNet;multi-stroke;touch-based therapy application},
  doi={10.1109/RMKMATE59243.2023.10369098},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9904248,
  author={Yan, Zhengbin and Xing, Yue and Xiao, Tengfei and Zhang, Xiaobing},
  booktitle={2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={SDAN: Semantic-Driven Dual Attentional Network for Image Generation}, 
  year={2022},
  volume={},
  number={},
  pages={521-525},
  abstract={The objective of pose-guided person image generation is to convert a given person’s pose into the desired pose. Exciting approaches employing the U-net architecture would result in the loss of the majority of fine-grained information, leading to over-smoothed clothing and missing details in the final results. In this paper, we propose a novel semantic-driven dual-attention network to address this issue. The whole framework contains two stages. In the first stage, the spatial misalignment is initially corrected by transferring the semantic parsing map to the target pose. In the second stage, we leverage a novel dual attention mechanism to better incorporate the spatial structure and texture style information into the image generation process. Extensive experiments demonstrate that our proposed method can generate images that are more consistent and photo-realistic than current methods in quantitative and qualitative evaluation.},
  keywords={Image synthesis;Semantics;Clothing;Feature extraction;Pattern recognition;Task analysis;Artificial intelligence;person image generation;semantic parsing;pose transfer},
  doi={10.1109/PRAI55851.2022.9904248},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10947823,
  author={Shukla, Vinay and Kumar, Sanjay and Solanki, Prince and Verma, Ishanvi},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={Robustness of a 3D Watermarking Algorithm Against Vertex Re-Ordering Attack in Forensics}, 
  year={2024},
  volume={},
  number={},
  pages={1336-1341},
  abstract={A 3D watermarking scheme based on a modified Kirchoff matrix and visual cryptography is proposed. It is shown that the proposed scheme is resilient to vertex-reordering attacks. Numerical experiments are performed to strengthen the underlying objectives. The effectiveness of the watermarking method was assessed by subjecting 3D objects (Mushroom, Nefertiti, Pipes, and Sphere), created using MATLAB's 3D Mesh generation tool, to various attacks. The Nefertiti object yielded the most impressive results, with a remarkable 99.89% recovery of the original watermark, even when 5–6 vertices were perturbed in each object. Various stages involved in the owner registration and identification phase, including 3D image processing, ownership share generation, attacked and restored host objects and watermarks, are depicted with the help of pictorial illustrations. It is found that the resilience of the proposed scheme to vertex re-ordering has applications in medical science, especially in forensics.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Forensics;Watermarking;Robustness;Cryptography;Artificial intelligence;Usability;Biomedical imaging;Watermarking;visual cryptograpgy;Vertex reordering;Forensics;3D models},
  doi={10.1109/GlobalAISummit62156.2024.10947823},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10873126,
  author={Deng, Biqin},
  booktitle={2024 International Conference on Artificial Intelligence, Deep Learning and Neural Networks (AIDLNN)}, 
  title={The Application of AI Video Generation Technology in Virtual Reality (VR) and Augmented Reality (AR)}, 
  year={2024},
  volume={},
  number={},
  pages={168-173},
  abstract={AI has become one of the key forces driving social progress, particularly in the fields of VR and AR, where the application of AI video generation technology is spearheading a technological revolution. This article delves into the specific applications, potential impacts, and future development trends of AI video generation technology in VR and AR. VR technology enables users to fully immerse themselves and interact with virtual environments. AR technology, on the other hand, overlays virtual information onto the real world, integrating virtual objects and information into the user's actual surroundings. AI video generation technology plays a crucial role in these two domains. In the VR sector, AI video generation technology, leveraging deep learning techniques, can learn and mimic human creative styles, automatically generating video content. Through image processing and machine learning technologies, this article explores how AI can generate high-quality virtual objects and scenes, making them more realistic and seamlessly integrated with the real world. Additionally, this article analyzes the challenges and opportunities faced by AI video generation technology in VR and AR applications, such as technological bottlenecks, data privacy, and security issues. AI video generation technology is poised to play an increasingly significant role in the VR and AR sectors.},
  keywords={Deep learning;Hands;Image processing;Neural networks;Fitting;Virtual environments;Market research;User experience;Security;Artificial intelligence;Deep Learning;AI;Virtual Reality;Augmented Reality;Video Generation},
  doi={10.1109/AIDLNN65358.2024.00035},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11047855,
  author={Liu, Suyang and Wei, Zhigang},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={Multi-Source Domain Adaptive Bearing Fault Diagnosis Method with Lightweight Attention and Hierarchical Training}, 
  year={2025},
  volume={},
  number={},
  pages={73-77},
  abstract={In order to solve the problem that the sample size was insufficient and multiple source domains could not be used to provide diversified diagnosis information in single-source domain migration fault diagnosis, a multi-source domain adaptive bearing fault diagnosis method with light attention layered training was proposed in this paper. The existing multi-source domain adaptive methods mostly focused on the innovation of feature extraction networks, ignoring the improvement of model generalization ability by hierarchical training methods. In this paper, a private feature extractor was used to extract the features of each pair of source domain and target domain respectively, and the training was carried out in three stages. Finally, according to the KM-MMD (multi-kernel maximum mean discrepancy) recorded in the training process, the weights of different source domains were determined to realize the comprehensive diagnosis of equipment state. Experiments on two publicly available rolling bearing data sets show that the proposed method has higher diagnostic accuracy and generalization ability than the existing methods.},
  keywords={Training;Fault diagnosis;Employee welfare;Adaptation models;Technological innovation;Rolling bearings;Network architecture;Feature extraction;Data mining;Artificial intelligence;Fault diagnosis;Multi-source domain;Feature extraction;Hierarchical alignment;Decision fusion},
  doi={10.1109/AIITA65135.2025.11047855},
  ISSN={},
  month={March},}@INPROCEEDINGS{11022221,
  author={Li, Peng and Zhang, Bo and Yin, Lili},
  booktitle={2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Multi-Head Attention Tacotron: A Chinese Speech Synthesis Model Based on Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={1169-1173},
  abstract={Speech and text are one of the most important ways for human beings to receive and express information, and they play a vital role in people's lives. Speech synthesis is a technology that converts text information into speech signals. In recent years, with the improvement of computer computing power and the development of deep learning, research in the field of speech synthesis has made rapid progress, and many novel and efficient synthesis methods have emerged, among which the method based on end-to-end model has received extensive attention. The quality of speech greatly affects human understanding of text, and the quality of the current end-to-end model synthesis speech needs to be improved, and its attention mechanism and vocoder part need to be further improved. Therefore, to solve the above problems, this paper conducts an in-depth study of the existing speech synthesis systems and proposes a new model, Multi-Head Attention Tacotron. Based on Tacotron2, the model is trained on a Chinese dataset through the improvement of its attention mechanism and vocoder, aiming to improve the quality of Chinese speech synthesis. Eventually, the MOS value of synthesized speech was increased from 3.82 to 4.12, proving that the method we used was effective.},
  keywords={Deep learning;Attention mechanisms;Vocoders;Computational modeling;Neural networks;Switches;Speech synthesis;Artificial intelligence;Chinese speech synthesis;end-to-end;tacotron2;multi-head attention;deep learning},
  doi={10.1109/ACAIT63902.2024.11022221},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10369221,
  author={Kumar, V. Bhuvana and Kathiravan, M.},
  booktitle={2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Automatic music generation using RNN}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={this paper explains the development of music automatically. Deep learning is employed in order to do this. In the shape of an ABC note sequence, melody is created. Using vast amounts of data for music is presently feasible. mostly for the creation of music utilizing deep learning For modeling, it utilize LSTM or GRU. In terms of music creation, it is comparable to sequence creation. Since LSTM creates sequences most effectively, using it is recommended. Subjective evaluation is carried out by people with diverse backgrounds, including classical music interest, performance, composer, and digital composer. The results reveal that the double stacked layer LSTM model outperforms the composer pattern in music, with a 70% recall score. Furthermore, subjective evaluation indicates that the created music is listenable and intriguing, with a score of 6.85 out of 10 on double LSTM stacked layer.},
  keywords={Deep learning;Recurrent neural networks;Shape;Music;Knowledge management;Communications technology;Artificial intelligence;Char-Recurrent neural networks;Long short term memory;ABC Notation;Flask;Deep learning;Gate recurrent unit},
  doi={10.1109/RMKMATE59243.2023.10369221},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10864010,
  author={Chen, Zeyu and Wu, Yiming and Cao, Ronghui},
  booktitle={2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Multi-Modal Emotion Recognition Network with Balanced Audio-Visual Feature Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={675-679},
  abstract={Video and audio are ways for humans to perceive the world beyond language. It is meant to enable robots to imitate and recognize human emotional expressions. However, most current audio-visual emotion analysis models tend to extract deep features from only one modality. In contrast, the other modality plays a supporting role and cannot fully extract deep features from both modalities. This article proposes a balanced audio-visual emotion analysis model. Specifically, EfficientNet and Wav2vec 2.0 are used for visual and auditory modality feature extraction, respectively, ensuring that indepth features can be extracted from both modalities. Secondly, we use Transformer as the decision-level fusion operator, exchanging information between the two modalities. We verified our model on the RAVDESS dataset and achieved a Top1 accuracy of 88.54%, surpassing audio-visual emotion analysis models of the auxiliary type.},
  keywords={Analytical models;Visualization;Sentiment analysis;Emotion recognition;Data integration;Transforms;Learning (artificial intelligence);Feature extraction;Transformers;Robots;deep learning;multimodal;emotion recognition;Transformer},
  doi={10.1109/ICAICE63571.2024.10864010},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11035247,
  author={Zhao, Tong and Qi, Xiaopeng},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Image Style Migration and Art Creation Based on VGG19 Deep Learning Algorithm}, 
  year={2025},
  volume={},
  number={},
  pages={726-730},
  abstract={The origin of image style migration techniques can be traced back to the intersection of computer graphics and art creation. Early research explored stylized representations through mathematical modeling and image processing algorithms, but their effectiveness was limited by the limitations of manually designed features. The introduction of deep learning has revolutionized the implementation path of this technique, especially the convolutional neural network (CNN), which provides a new paradigm for decoupling and reorganizing style and content by virtue of its multilevel feature extraction capability. In this paper, we construct an image style migration model based on VGG19 network, separate the content and style representations by pre-training the deep feature space of the network, and use the joint optimization of the content loss function and the style loss function to realize the efficient fusion of the content of the source image and the target style. Experiments show that the method can accurately capture art style texture features while preserving the structural integrity of the content, and the generation quality is significantly better than the traditional algorithm. More stylized content will be developed subsequently, with important applications in image style examples and references, and graphic creative design, laying the foundation for the innovative and stylized development of graphic image design creativity. Using image style migration means can create more series of style art works, providing more creative inspiration and creative methods for art practitioners. It can be widely used in commercial illustration design, digital photography, digital painting, packaging design and other aspects.},
  keywords={Deep learning;Seminars;Art;Visual communication;Packaging;Convolutional neural networks;Digital photography;Artificial intelligence;Creativity;Painting;Image style migration;Convolutional neural networks;Visual communication},
  doi={10.1109/AINIT65432.2025.11035247},
  ISSN={},
  month={April},}@INPROCEEDINGS{10920875,
  author={Han, Dongyeob},
  booktitle={2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Estimation of Normalized DSM From Single Satellite Imagery Using Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={0599-0601},
  abstract={Height estimation and terrain classification using single optical images have been actively researched. To improve the accuracy of single image-based height estimation, regression models between optical images and normalized digital surface models (nDSM) have shown good performance. Accordingly, in this paper, the performance was improved by testing various augmentation and parameters.},
  keywords={Integrated optics;Deep learning;Accuracy;Estimation;Optical imaging;Satellite images;Artificial intelligence;Testing;Single Image;Normalized Digital Surface Model;Deep Learning},
  doi={10.1109/ICAIIC64266.2025.10920875},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{11048128,
  author={Wang, Zhen and Xin, Liming},
  booktitle={2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)}, 
  title={Group Adversarial Imitation Learning with Shared Cognition for Morphological Adaptation}, 
  year={2025},
  volume={},
  number={},
  pages={1337-1340},
  abstract={Attaining human-like adaptability to morphological variations in robotic systems is a substantial challenge in the field of robotics. While adversarial imitation learning has proven effective in learning behavior policies from expert demonstrations, the policies it produces are often limited by the specific morphology of the agent, resulting in suboptimal adaptation to morphological changes. This limitation restricts their effectiveness in performing complex tasks. To overcome this, we introduce a novel approach named Morphology Adaptive adversarial Imitation Learning with Shared Cognition pattern. Our method acquires morphological knowledge and fosters adaptive behavior by concurrently imitating a diverse group of experts, each with distinct morphologies. Furthermore, we facilitate the sharing of experiences and demonstrations among individuals within this group with a cognition sharing framework. We compare our algorithm with other imitation learning algorithms on a variety of robotic locomotion tasks. The numerical results indicate that our method excels in adapting to a broader spectrum of morphologies compared to other approaches.},
  keywords={Training;Service robots;Imitation learning;Morphology;Reinforcement learning;Cognition;Artificial intelligence;Reinforcement Learning;Adversarial Imitation Learning;Morphological Adaptation;Robotics},
  doi={10.1109/AIITA65135.2025.11048128},
  ISSN={},
  month={March},}@INPROCEEDINGS{9904125,
  author={Cheng, Yongqiang and Lu, Jinzheng and Li, Qiang and Peng, Bo and Liu, Qiyuan and Han, Jiaojiao},
  booktitle={2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={AO-SF: Alignment Optimization and Sample Fusion for Unsupervised Nighttime Segmentation}, 
  year={2022},
  volume={},
  number={},
  pages={776-782},
  abstract={Among the existing nighttime domain adaptation methods, the semantic consistency of the same features is largely ignored in the process of semantic alignment, which leads to negative shift and distribution discrepancy between the source and target domains. To address this problem, we propose an unsupervised nighttime semantic segmentation network based on semantic alignment optimization and sample fusion (AO-SF), which can make a contribution to the segmentation performance of our nighttime model. Specially, the AO-SF constructs a nighttime generator with mutually exclusive classifiers in order to enhance the alignment of each feature. Moreover, we further design an image fusion model to leverage shared information between day-night image pairs, which can boost the quality of relighting network. The experimental results on Dark Zurich and Nighttime Driving show that the segmentation performance of our method is superior to that of other methods in unsupervised nighttime semantic segmentation tasks. The mIoU reaches 45.2% and 46.5%, respectively.},
  keywords={Semantics;Generators;Pattern recognition;Reliability;Task analysis;Artificial intelligence;Optimization;nighttime segmentation;domain adaptation;co-training;sample fusion},
  doi={10.1109/PRAI55851.2022.9904125},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11050670,
  author={Akhter, Farheen and Alzahrani, Nabeel and Dajani, Khalil},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Bias Mitigation in Deep Learning: A Survey of Modern Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={577-578},
  abstract={Ensuring fairness in AI systems is crucial, particularly in medical imaging, natural language processing (NLP), and recommender systems. This survey reviews modern bias mitigation techniques based on a systematic review of 100 studies, narrowing down to fewer than 20 with significant contributions to AI fairness. The techniques are categorized into data-centric, algorithmic, and hybrid approaches, including strategies such as data augmentation, feature selection, fairness-aware metrics, and attention mechanisms. Notable methods include AttEN for dermatological prediction, VERB for debiasing word embeddings, and FairIF for fairness improvements via influence functions. Domain-specific solutions address bias in coronary disease diagnosis and toxic comment detection, while frameworks like EBRank enhance fairness in ranking systems. This paper evaluates these techniques in terms of effectiveness, computational efficiency, and applicability, highlighting the role of hybrid approaches in advancing equitable AI development.},
  keywords={Surveys;Measurement;Prevention and mitigation;Transfer learning;Data augmentation;Inference algorithms;Natural language processing;Artificial intelligence;Recommender systems;Systematic literature review;Bias Mitigation;Fairness-aware Metrics;Data Augmentation;Algorithmic Bias;Transfer Learning},
  doi={10.1109/CAI64502.2025.00105},
  ISSN={},
  month={May},}@INPROCEEDINGS{10904368,
  author={Wenngren, Edward Motoaki and Gonsalves, Tad},
  booktitle={2024 6th International Workshop on Artificial Intelligence and Education (WAIE)}, 
  title={Data Augmentation with Image Diffusion Morphing as an Educational Tool}, 
  year={2024},
  volume={},
  number={},
  pages={379-383},
  abstract={This paper goes into the viability of image diffusion morphing as a method of data augmentation. With the recent surge of AI and machine learning models, getting more data to achieve better results from models is imperative and there can never be too many methods for data augmentation. There is pre-existing research which delves into the viability for data augmentation using pure synthetic images, however, there is no research into the viability of using image diffusion morphing, a technique used to smoothly interpolate two images together using image diffusion as of yet. This paper hopes to introduce and investigate the viability of image diffusion morphing as a data augmentation method for future generations.},
  keywords={Image resolution;Education;Image morphing;Machine learning;Learning (artificial intelligence);Data augmentation;Data models;Surges;Monitoring;Immersive learning;Image Diffusion;Data Augmentation;Image Morphing;Image Classification;Machine Learning},
  doi={10.1109/WAIE63876.2024.00074},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11073701,
  author={Ficzere, Dániel and Hollósi, Gergely and Varga, Pál},
  booktitle={NOMS 2025-2025 IEEE Network Operations and Management Symposium}, 
  title={Beyond Intent Translation: Research Gaps in the Application of Generative AI for Intent-Based Networking}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Intent-Based Networking (IBN) promises to redefine network management by automating operations to align with high-level user intents. The advent of powerful Generative AI (GenAI) models, including Large Language Models (LLMs), could significantly accelerate this transformation. However, cur-rent research remains narrowly focused on LLM-based intent translation, leaving substantial gaps in understanding how GenAI can be applied across the entire IBN life cycle. This paper aims to bridge these gaps by investigating the wider potential of Generative AI (GenAI) in areas like intent orchestration, moni-toring, compliance assessment, and automated actions. Through a systematic categorization of tasks based on GenAI's suitability and the presentation of a practical use case, this work highlights the critical need for more comprehensive research to fully harness the potential of GenAI in advancing IBN.},
  keywords={Bridges;Translation;Systematics;Automation;Generative AI;Large language models;intent-based networking;Generative AI;LLM;network automation},
  doi={10.1109/NOMS57970.2025.11073701},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{11144625,
  author={Yu, Man and Xu, Kai and Xu, Ke and Wang, Zhenyu},
  booktitle={2025 2nd International Conference on Artificial Intelligence and Digital Technology (ICAIDT)}, 
  title={A Deep Q-Network Based Reward Shaping for Task-Oriented Dialogue Agent}, 
  year={2025},
  volume={},
  number={},
  pages={494-497},
  abstract={The dialogue agent plays a core role in the pipeline of a task-oriented dialogue system, selecting dialogue actions for dialogue generation and determining overall dialogue performance. A promising approach for dialogue agent learning is deep reinforcement learning. However, the acquisition of reward signals occurs only after the dialogue is complete, leading to delayed rewards. In this paper, we design a dynamic reward function based on various dialogue trajectories and lengths. This function rewards conversations based on the dialogue state and the position of the turns. We incorporate the designed reward function into the deep Dyna-Q reinforcement learning framework to model dialogue agents. Experiments conducted on a movie-ticket booking dataset demonstrate that our proposed reward shaping methods achieve optimal results compared to baseline dialogue agents. The experiments validate the effectiveness of the proposed reward shaping model.},
  keywords={Training;Pipelines;Oral communication;Multilayer perceptrons;Deep reinforcement learning;Trajectory;Delays;Artificial intelligence;Long short term memory;dialogue agent;reward shaping;deep reinforcement learning},
  doi={10.1109/ICAIDT66272.2025.00103},
  ISSN={},
  month={April},}@INPROCEEDINGS{10195090,
  author={Hsieh, Yu-Hsing and Li, Jia-Da and Lee, Yao-Chih and Chen, Chu-Song and Wu, LiFu and Cheng, Skye H.},
  booktitle={2023 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Improved Contrastive Unpaired Translation for Metal Artifacts Reduction in Nasopharyngeal CT Images}, 
  year={2023},
  volume={},
  number={},
  pages={345-346},
  abstract={Metal artifacts (MA) reduction is crucial for clinical application yet often lacks paired training data. Learning MA reduction from unpaired data and enforcing fidelity seems a trade-off. The study proposed an improved contrastive unpaired translation solution to address the issues and demonstrate its efficacy.},
  keywords={Computed tomography;Metals;Training data;Learning (artificial intelligence);metal artifacts reduction;CT;constrastive learning;negative learning},
  doi={10.1109/CAI54212.2023.00152},
  ISSN={},
  month={June},}@INPROCEEDINGS{10409061,
  author={Wang, Dan and Zhong, Liangjie and Hong, Jie and Lei, Chunguang},
  booktitle={2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Multi-level Feature Fusion Guided Distillation for Industrial Anomaly Detection}, 
  year={2023},
  volume={11},
  number={},
  pages={1721-1725},
  abstract={Knowledge distillation has yielded promising results in the demanding task of unsupervised anomaly detection. The representation divergence of anomalies within the teacher-student (T-S) model plays a crucial role in anomaly detection. To enhance the model's capabilities, we designed a multi-level fusion-guided network for the student network to boost the representation capabilities of the shallow stages. We also introduced a low-level feature path with contextual information, to guide the feature extraction process and enhance the differentiation between the teacher and student models. A feature fusion block was designed for effectively merging features of different levels within the network. Experimental results have demonstrated that our approach outperforms previous methods in the field of anomaly detection.},
  keywords={Merging;Learning (artificial intelligence);Feature extraction;Task analysis;Information technology;Anomaly detection;Context modeling;anomaly detection;knowledge distillation;deep learning},
  doi={10.1109/ITAIC58329.2023.10409061},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10692878,
  author={Zhang, Guichen},
  booktitle={2024 5th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Integrating DenseNet and RDN for Enhanced Image Colorization: Development of a Novel CNN Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={382-386},
  abstract={This paper introduces a novel Convolutional Neural Network (CNN) architecture for image colorization. We review existing research, identifying key progress and persistent challenges in the field. Our approach features a multi-path network architecture, integrating DenseNet and Residual Dense Networks (RDN) for superior feature extraction and colorization. Additionally, we implement advanced data cleaning techniques to improve dataset integrity. Experimental results demonstrate that our model achieves significant improvements in both qualitative and quantitative evaluations, offering a robust solution for high-quality image colorization.},
  keywords={Automation;Reviews;Network architecture;Feature extraction;Generators;Cleaning;Convolutional neural networks;Artificial intelligence;Image Colorization;CNNs;Multi-Path Network;DenseNet;RDN},
  doi={10.1109/AIEA62095.2024.10692878},
  ISSN={},
  month={June},}@INPROCEEDINGS{10957425,
  author={Wang, Xiaofeng and Wang, Liudi and Guo, Shiru},
  booktitle={2025 International Conference on Electrical Automation and Artificial Intelligence (ICEAAI)}, 
  title={Small Sample Maintainability Analysis of CNC Machine Tools}, 
  year={2025},
  volume={},
  number={},
  pages={372-376},
  abstract={With the continuous improvement of the reliability of CNC machine tools, some subsystems of CNC machine tools show the characteristics of long life, less faults and less maintenance. In these long-life subsystems, not only the maintenance data distributed in each subsystem is less, but also the prior information about the maintainability of the subsystem is less. In view of this situation, the lognormal distribution model is transformed into a normal distribution model, and a normal distribution small sample modeling method without prior information is used to establish the maintainability model of each subsystem. It can be found that the MTTR of exchange table is the maximum, and the maintainability level is the lowest through the maintainability evaluation results. The conclusion lays a foundation for the improvement measures of machining center credibility.},
  keywords={Parameter estimation;Automation;Accuracy;Electric variables measurement;Distributed databases;Machining;Gaussian distribution;Machine tools;Maintenance;Artificial intelligence;Small sample;Maintainability modeling;Lognormal distribution;CNC machine tools},
  doi={10.1109/ICEAAI64185.2025.10957425},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10284704,
  author={Hidayaturrahman and Trisetyarso, Agung and Kartowisastro, Iman Herwidiana and Budiharto, Widodo},
  booktitle={2023 4th International Conference on Artificial Intelligence and Data Sciences (AiDAS)}, 
  title={Domain Adaptation in Color-Shifted Datasets Through Adversarial Learning Balancing}, 
  year={2023},
  volume={},
  number={},
  pages={120-125},
  abstract={Due to the disparity in the levels of difficulty presented by the several tasks, doing domain adaptation in an adversarial way may result in an imbalanced learning process. In the MNIST dataset, this phenomenon also manifests itself in the form of domain adaptation for color-shifted distribution. In this particular situation, the domain classifier has a higher tendency to fit more quickly, but the category classifier fits quite poorly in the learning process. In order to address this problem, a new hyper-parameter has been added to the loss function in order to strike a compromise between the learning speed of the domain and the categorical classifier. By using this technique, the categorical classifier may better match the data while still maintaining the same level of performance as the domain classifier. In order to determine whether or not making use of this hyper-parameter is useful, the phenomena in question is examined using three distinct color-shifted settings. Following the evaluations, it was discovered that the newly introduced hyper-parameter is capable of coping with imbalanced learning while simultaneously engaging in domain adaptation.},
  keywords={Training;Adaptation models;Adversarial machine learning;Artificial intelligence;Task analysis;Domain Adaptation;Adversarial Learning;Domain Shifted Problem;Color Shifted Dataset},
  doi={10.1109/AiDAS60501.2023.10284704},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10899495,
  author={Hou, Huashuang and Zhang, Hui},
  booktitle={2024 2nd International Conference on Artificial Intelligence and Automation Control (AIAC)}, 
  title={Boosting Audio-Visual Speech Enhancement with Lip-Movement Speech Synthesis}, 
  year={2024},
  volume={},
  number={},
  pages={431-435},
  abstract={In this study, we revisit the challenge of speech enhancement in environments characterized by low signal-to-noise ratios (SNRs) and complex noise conditions. Conventional methods primarily depend on audio signals alone, yet their effectiveness diminishes under these conditions. Recent research has incorporated lip movements as additional cues to enhance speech quality, but these approaches often lack a clean training target. To address this limitation, we introduce a novel framework that integrates speech enhancement with lip-movement-based speech synthesis. By employing speech directly synthesized from lip movements as a supplementary task to guide the learning process, our approach notably improves speech intelligibility and quality in challenging low SNR environments. In comparison to traditional audio-visual speech enhancement models, our method exhibits significant performance improvement.},
  keywords={Training;Visualization;Automation;Lips;Speech enhancement;Boosting;Speech synthesis;Artificial intelligence;Signal to noise ratio;component;speech enhancement;audio-visual;lip-movement speech synthesis},
  doi={10.1109/AIAC63745.2024.10899495},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10468041,
  author={Corsaro, Miriana and Palazzo, Simone and Spampinato, Concetto and Cannavò, Flavio},
  booktitle={2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Continuous Learning for Anomaly Detection: A Case Study in Volcanic Unrest Monitoring}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the field of volcanology, timely detection of anomalies is essential for disaster mitigation. Traditional methods often fall short in adapting to evolving volcanic behavior. We propose a model that combines continual learning and autoencoders to adaptively detect anomalies. The autoencoder extracts relevant features from sensor data, while continual learning enables the model to adapt to changing volcanic patterns. A case study demonstrates its effectiveness in real-time monitoring, offering a data-driven and efficient solution for volcanic anomaly detection.},
  keywords={Adaptation models;Disasters;Computational modeling;Learning (artificial intelligence);Feature extraction;Real-time systems;Data models;Anomaly Detection;Time series Analysis;Continual Learning;Volcanic Unrest},
  doi={10.1109/ACDSA59508.2024.10468041},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10408842,
  author={Liu, Xiaojing and Fan, Zhen},
  booktitle={2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={A Blind Deblurring Strategy for Images based on Iterative and Guided Filtering}, 
  year={2023},
  volume={11},
  number={},
  pages={1564-1568},
  abstract={Image deblurring is an important branch of image restoration tasks. In order to eliminate blurring in a single image, an image deblurring strategy is proposed. Specifically, first enhance the image, then delimit the special area, calculate the initial iterative Point spread function according to the initial state of the special area and the enhanced state, and finally restore the original image according to the blind image Iterative method method combined with guided filtering. Compared with the Iterative method directly used for image blind deblurring, the strategy in this paper can give an initial Point spread function that is closer to the true value. At the same time, due to the existence of guided filtering, the iterative strategy in this paper can achieve the estimation of the original image faster.},
  keywords={Filtering;Estimation;Image restoration;Iterative methods;Task analysis;Information technology;Artificial intelligence;blind deblurring;Point spread function;Guided filtering},
  doi={10.1109/ITAIC58329.2023.10408842},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10760616,
  author={Kumaran, U and Gurupriya, M and Reddy, Annem Gnaneswara and Nithin, G and Bhanusri, Kodati},
  booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={A Secure Approach for Strengthening Session Management with Custom Session Handlers}, 
  year={2024},
  volume={},
  number={},
  pages={1167-1172},
  abstract={Session management is the underlying security mechanism that is instrumental in protecting the security and integrity of web applications’ user sessions. The research is dedicated to the examination of session administration solutions in web applications to provide secure user sessions and minimize the risks of session hijacking and session fixation. It touches on session management before and after user login, breaking it down into session maintenance. The custom session handler offers passive protection against session hijacking via session management using a MySQL database. This approach provides a safe way of creating and managing sessions, validating them as well as handling session expiration and performance optimization. The implementation intends to strengthen session securities of users, therefore, increasing the safety and reliableness of web applications},
  keywords={Databases;Instruments;Session hijacking;Maintenance;Security;Protection;Artificial intelligence;Optimization;Session management;Web applications;Vulnerabilities;Enhancing security},
  doi={10.1109/ICSSAS64001.2024.10760616},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9892702,
  author={Sun, Fanglei and Tao, Qian and Yan, Jun and Hu, Jianqiao and Yang, Zongyuan},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={MRGAN: Multi-Criteria Relational GAN for Lyrics-Conditional Melody Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Music generation, as a creativity problem, attracts growing attention from artificial intelligence researchers. Among the challenging tasks, lyrics-conditional melody generation aims to leverage natural language processing (NLP) techniques to generate music from texts, for which Generative Adversarial Networks (GAN) has become a promising unsupervised solution. The adversarial training of two agents, i.e., generator and discriminator, allows GAN to achieve a better generation performance and has been proven effective in conditional generation tasks. In this paper, we propose the multi-criteria relational GAN (MRGAN), which includes a relation memory-based generator and two discriminators with a unique discrimination criterion each. The relational memory in the generator is adopted for long-time dependency modeling. Meanwhile, the two discriminators can judge both musical quality and conditional correspondence. Based on the bilingual evaluation understudy (BLEU) score, a new metric, named Music-BLEU, has also be designed to evaluate the lyrics-conditional melody generation. Experimental results verify that MRGAN outperforms existing approaches in related key metrics.},
  keywords={Measurement;Training;Neural networks;Music;Generative adversarial networks;Generators;Natural language processing;Generative adversarial networks;melody generation;relational memory;BLEU},
  doi={10.1109/IJCNN55064.2022.9892702},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9853492,
  author={Du, Wei and Yu, Ya-Nan and Pan, Qi},
  booktitle={2022 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)}, 
  title={Human motion prediction based on bidirectional feature sequence learning}, 
  year={2022},
  volume={},
  number={},
  pages={195-199},
  abstract={Dynamic analysis of human behavior is an important research branch of computer vision and pattern recognition field in recent years. In order to enable the computer to predict the intention of future action posture in advance by monitoring human body's early behavior, a behavior prediction model Bi-Seq2Seq is proposed in this paper, which is based on the integration of Bi-GRU unit and Seq2Seq model. Bidirectional Gated Recurrent Unit (GRU)is introduced into the encoder of traditional Seq2Seq structure to integrate forward and reverse features. It breaks away from the limitation of temporal order and effectively combines context information. The decoder part still adopts one-way GRU unit. The experimental results show that the human behavior prediction model based on bidirectional feature sequence reduces the prediction error of Smoking, Posing, Greeting and Sitting movements, with the maximum reduction up to 0.52. The short-term prediction accuracy of attitude prediction algorithm within 400ms is improved.},
  keywords={Computer vision;Computational modeling;Learning (artificial intelligence);Predictive models;Logic gates;Prediction algorithms;Behavioral sciences;component;behavior prediction;recurrent neural network;bidirectionalgated recurrent unit;Seq2Seq model},
  doi={10.1109/ICCEAI55464.2022.00049},
  ISSN={},
  month={July},}@INPROCEEDINGS{10581809,
  author={Xu, Zhijun and Zhang, Mingkun and Zhang, Dongyu},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Facial Expression-Aware Talking Head Generation with 3D Morphable Model}, 
  year={2024},
  volume={},
  number={},
  pages={1214-1217},
  abstract={Currently GAN-based talking head generation methods either fail to translate facial expressions accurately, or present obvious artifacts, jitter, and irregular head movements. To address this challenge, in this paper, we propose a novel facial expression-aware talking head generation framework based on 3D Morphable Model(3DMM). First, we use 3DMM to predict parametric 3D facial representations from face images, decoupling the structure and expressions of the face to provide more efficient and precise facial features. Furthermore, we introduce a multi-scale motion network to predict facial motion fields, which could efficiently align facial features from coarse to fine, and generate richer expression details. Experimental results demonstrate that our method can generate more accurate facial expressions and higher-quality images.},
  keywords={Seminars;Solid modeling;Three-dimensional displays;Learning (artificial intelligence);Jitter;Task analysis;Information technology;talking head generation;face reenactment;3DMM;deep learning},
  doi={10.1109/AINIT61980.2024.10581809},
  ISSN={},
  month={March},}@INPROCEEDINGS{10065285,
  author={Saraswat, Amar and Sharma, Neeta and Dalal, Anupam},
  booktitle={2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)}, 
  title={Investigation of Brain Tumors Detection using Automatic Segmentation Techniques}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={One of the most crucial steps in the medical imaging is to achieve the accuracy, as relying on the modal’s performance for the patient’s treatment is very risky. The risks go higher in context with the brain tumors, which are also known as intracranial tumors. Apart from the several diagnostic techniques, Magnetic Resonance Images have been used for extraction of the finer details of the brain tumor and have been used till date. In this paper, the analysis of various techniques has been performed in order to conclude for the best technique that can be used for the Content-Based Image Retrieval (CBIR) for brain tumors and its various categories. The analysis will also be useful, in order to remove the semantic gap, between the low-level features and high-level features that are obtained by the model used and the manual diagnostics respectively.},
  keywords={Image segmentation;Analytical models;Semantics;Image retrieval;Magnetic resonance;Brain modeling;Artificial intelligence;Content Based Image Retrieval;Magnetic Resonance Images;Convolution Neural Network;Deep Neural Network},
  doi={10.1109/AIST55798.2022.10065285},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10934666,
  author={Liu, Jinting and Wang, Xiaofeng and Liu, Yang},
  booktitle={2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)}, 
  title={Maintainability evaluation of machining center with small samples}, 
  year={2024},
  volume={},
  number={},
  pages={593-597},
  abstract={In recent years, some subsystems of machining center show the characteristics of long life, less faults and less maintenance. In these long-life subsystems, not only the maintenance data distributed in each subsystem is less, but also the prior information about the maintainability of the subsystem is less. In view of this situation, the lognormal distribution model is transformed into a normal distribution model, and a normal distribution small sample modeling method without prior information is used to establish the maintainability model of each subsystem. Then the maintainability of each subsystem is evaluated. The results show that the proposed method is obviously better than traditional large samples estimation method. The conclusion lays a foundation for the improvement measures of machining center credibility.},
  keywords={Parameter estimation;Accuracy;Estimation;Distributed databases;Machining;Gaussian distribution;Maintenance;Manufacturing;Machine tools;Artificial intelligence;Small sample;Maintainability modeling;Lognormal distribution;machining center},
  doi={10.1109/AIIM64537.2024.10934666},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10441164,
  author={Wang, Wei and Gu, Jintao and Zhang, Siyuan},
  booktitle={2023 3rd International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)}, 
  title={Dual Contrastive Learning for Unpaired Image Dehazing}, 
  year={2023},
  volume={},
  number={},
  pages={51-56},
  abstract={In real hazy conditions, the existing unpaired image dehazing by using cycle-consistency loss constraints and adversarial loss constraints effect is not enough for the network to be more effective for the acquisition of potential relationship between hazy images and clear images. In this paper, we propose an effective framework for unpaired image dehazing. Our approach improves the Cycle-Consistent Adversarial Networks (Cyclegan) by incorporating an improved Deep Residual Shrinkage Network in the generator, an attentional mechanism for the deep features to filter out the efficient subset as well as soft thresholding to remove the noise to further mine the potential feature distributions between the domains, introducing the contrast learning in self-supervised learning, double layer contrastive learning constrains deep feature relationships for better image dehazing and restoration. Extensive experiments show that we demonstrate superiority over existing unpaired dehazing methods on Nyu-depth and Reside datasets, and produce dehazing results comparable to several fully supervised dehazing methods.},
  keywords={Self-supervised learning;Generators;Manufacturing;Image restoration;Artificial intelligence;unpaired image;image dehazing;Cycle-Consistent Adversarial Networks (Cyclegan);Deep Residual Shrinkage Network;contrastive learning},
  doi={10.1109/AIIM60438.2023.10441164},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10963233,
  author={Tang, Songli and Lin, Jianchun},
  booktitle={2024 5th International Conference on Computers and Artificial Intelligence Technology (CAIT)}, 
  title={A joint distribution domain adaptive rolling bearing fault diagnosis model for processing noisy signals}, 
  year={2024},
  volume={},
  number={},
  pages={266-271},
  abstract={Based on deep domain adaptation (DA) methods, which have been widely applied in cross-condition fault diagnosis, traditional DA methods often struggle to extract domaininvariant features under noisy signals and overlook the influence of the quality of pseudo-labels generated in the target domain during class alignment. Therefore, this paper proposes a Joint Distribution-based Robust Cross-domain Adaptation (JACDA) model for fault diagnosis with noisy signals. Firstly, an feature extraction module is proposed to extract transferable features from the source and target domains under noisy signals. Secondly, an adaptive threshold pseudo-label strategy is introduced to address the issue of lacking labels and low label quality in the target domain. Additionally, a discriminative discrepancy module is proposed, utilizing target domain pseudo-labels obtained through an adaptive threshold to compute MK-MMD inter-class distances with source domain data, encouraging tight clustering of features from the same category and separation of features from different categories, thereby enhancing feature discriminability. Experimental results demonstrate the model's strong transferability in cross-domain bearing fault diagnosis.},
  keywords={Fault diagnosis;Deep learning;Computers;Adaptation models;Computational modeling;Transfer learning;Rolling bearings;Feature extraction;Noise measurement;Artificial intelligence;fault diagnosis;transfer learning;rolling bearings;Deep learning},
  doi={10.1109/CAIT64506.2024.10963233},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10920841,
  author={Masuda, Yuya and Ishikawa, Hiroshi},
  booktitle={2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Enhanced Super-Resolution Using Cross Attention: Refining HMA for Local Image Restoration}, 
  year={2025},
  volume={},
  number={},
  pages={0681-0686},
  abstract={In this paper, we propose a novel method that integrates Cross Attention into the existing Hybrid Multi-Axis Aggregation Network for Image Super-Resolution(HMANet) to improve local super-resolution accuracy. While previous HMANet methods primarily focused on enhancing the resolution of the entire image, our approach emphasizes local image regions for more detailed restoration. By leveraging Cross Attention for context interaction, we achieve localized super-resolution with a focus on specific parts of the image. Our experiments demonstrate that the proposed method outperforms existing approaches in terms of accuracy, and shows promising results when evaluated with different loss functions. Our source code is available at https://github.com/msdsmlhmaca-for-Iocal-image-restoration.},
  keywords={Adaptation models;Accuracy;Source coding;Superresolution;Refining;Image restoration;Artificial intelligence;Tuning},
  doi={10.1109/ICAIIC64266.2025.10920841},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10827445,
  author={Yang, He and Ji, Xun and Wang, Xu and Chen, Shijie},
  booktitle={2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Multi-Scale Mixed Pixel Network for Remote Sensing Image Super-Resolution}, 
  year={2024},
  volume={},
  number={},
  pages={303-308},
  abstract={Deep learning-based image super-resolution (SR) technology has gained extensive attention in the remote sensing community, which aims to reconstruct the abundant details of target images. However, the practical application of existing deep learning-based SR methods is often limited by the trade-off between network performance and computational load. This paper presents a novel multi-scale mixed pixel network (MSMP-Net) to explore the precise and cost-effective SR reconstruction of remote sensing images. Specifically, a dual-path feature capture mechanism (DPFCM) architecture is devised that incorporates diverse dilated convolution operations to comprehensively capture hierarchical features across varying receptive fields. In addition, a triple-path high-resolution rebuilder (TPHRR) structure is designed, which uses a set of dense branches, residual branches and pyramid branches to effectively promote the transfer and reuse of features. Extensive experiments reveal that our proposed MSMP-Net outperforms other state-of-the-art SR methods in reconstruction effects.},
  keywords={Convolution;Superresolution;Poles and towers;Learning (artificial intelligence);Computer architecture;Streaming media;Feature extraction;Pattern recognition;Remote sensing;Image reconstruction;Remote sensing images;super-resolution;convolutional neural networks;deep learning},
  doi={10.1109/PRAI62207.2024.10827445},
  ISSN={},
  month={Aug},}@ARTICLE{10558817,
  author={Tariq, Shehbaz and Khalid, Uman and Arfeto, Brian E. and Duong, Trung Q. and Shin, Hyundong},
  journal={IEEE Wireless Communications}, 
  title={Integrating Sustainable Big AI: Quantum Anonymous Semantic Broadcast}, 
  year={2024},
  volume={31},
  number={3},
  pages={86-99},
  abstract={Semantic communication (SC) with native artificial intelligence (AI) is a context-centric framework that intelligently extracts task-specific semantics from source data and efficiently regenerates the intended meaning at the destination. Hence, this computing-intensive methodology enables goal-oriented communication by maintaining a high semantic quality of service with a low requirement for data transfer. Recently, the emergence of big-AI foundation models, such as the generative pre-trained transformer and diffusion models with zero-shot task generalization and native cross-modal learning capabilities, has brought a paradigm shift in designing AI-native frameworks for wireless networks. However, deploying big AI in wireless networks involves inherent challenges, such as large training parameters and computing requirements. To address these challenges, we use sustainability techniques, such as pruning and fine-tuning, to create sustainable (lightweight) models from big AI, which can reduce the resource consumption and environmental impact in computation-heavy SC systems while preserving or enhancing the task performance. Moreover, classical communication networks lack quantum-safe communication security and data privacy. In this article, we prototype a sustainable big AI-native quantum anonymous SC system. In this framework, we leverage big-AI models for semantic retrieval processing, that is, semantic extraction and recovery, and employ quantum anonymous communication protocols to broadcast semantics. We detail the underlying functionalities, sustainable practices, and potential challenges of integrating big AI into a quantum anonymous semantic broadcast system. We also formulate case studies demonstrating the sustainability and reliability of the envisioned framework. This work provides a sustainable and quantum-safe semantic communication framework by integrating big AI and quantum anonymous communication.},
  keywords={Training;Adaptation models;Protocols;Computational modeling;Wireless networks;Semantics;Security;Context awareness;Broadcasting;Quantum communication;Artificial intelligence;Information retrieval},
  doi={10.1109/MWC.007.2300503},
  ISSN={1558-0687},
  month={June},}@ARTICLE{10529952,
  author={Hou, Xueyu and Guan, Yongjie and Han, Tao and Wu, Pingfan and Lu, Ning and Dajer, Miguel and Peng, Liang},
  journal={IEEE Wireless Communications}, 
  title={A Digitized You in My Eye: A Perceptually Driven Spatial Communication Prototype for XR}, 
  year={2024},
  volume={31},
  number={4},
  pages={252-259},
  abstract={Amid the proliferation of immersive extended reality (XR), three key challenges are identified: the transmission and synthesis of intensive volumetric streams, the need for personalized XR experiences, and the availability of a comprehensive 3D data capturing system. To overcome these challenges, we propose a “Digitized You in My Eye” (DYME) spatial communication prototype, which leverages 5G, distributed computing, and artificial intelligence (AI) to address challenges in XR. DYME proposes a 3D live volumetric chat procedure that eliminates real-time image capture by utilizing pre-existing digitized 3D models stored in the cloud. It further embraces a distributed methodology for volumetric streaming, encompassing the utilization of cloud, edge, and local devices, capitalizing on the proximity of user infrastructure, and employing expeditious transmission facilitated by AI generative models. Additionally, a user-specific management module (DYME manager) is integrated to personalize XR experiences based on user behaviors and preferences. By combining these technologies, DYME aims to provide efficient, tailored, and immersive XR experiences. The prototype demonstrates the potential of AI and 5G-driven solutions in addressing challenges and enhancing user experiences in XR environments.},
  keywords={Solid modeling;Three-dimensional displays;Computational modeling;Artificial intelligence;X reality;Real-time systems;Data models},
  doi={10.1109/MWC.011.2300344},
  ISSN={1558-0687},
  month={August},}@INPROCEEDINGS{11165880,
  author={Anastasiou, Theodora and Pastellas, Ioannis and Karagiorgou, Sophia and Konidi, Mariza},
  booktitle={2025 6th International Conference in Electronic Engineering & Information Technology (EEITE)}, 
  title={Explanation-Driven Adversarial Attacks against Multimedia Edge Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Generative Artificial Intelligence (GenAI) and automated attacks on AI models raise concerns about the credibility and reliability of AI systems. Specifically, for civilian applications, there is an increasing need for AI systems to be robust, transparent, and interpretable to earn the trust of decision-makers. To effectively address the challenge of learning with enhanced explanations in constrained edge applications under conditions of massive adversarial attacks, we benchmark and present an Explanation-Driven Adversarial approach. By harvesting multi-media data collected from drones at the edge and augmenting it with diverse and massive adversarial examples, we transparently train explainers and robustify Neural Networks (NNs) to improve the AI model’s fidelity. The new and robust AI model performs exceptionally well against novel and unseen attack types and concept drifts. Finally, through benchmarking across diverse adversarial attacks, we extend research in sensitive application domains and promote the adoption of more responsible and informed AI integration.},
  keywords={YOLO;Performance evaluation;Location awareness;Image edge detection;Benchmark testing;Reliability;Security;Artificial intelligence;Drones;Resilience;adversarial explanations;adversarial attack benchmarking;robust and trustworthy explainability;AI model interpretability with confidence scores},
  doi={10.1109/EEITE65381.2025.11165880},
  ISSN={},
  month={June},}@INPROCEEDINGS{9308512,
  author={Bashar, Md Abul and Nayak, Richi},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={TAnoGAN: Time Series Anomaly Detection with Generative Adversarial Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1778-1785},
  abstract={Anomaly detection in time series data is a significant problem faced in many application areas such as manufacturing, medical imaging and cyber-security. Recently, Generative Adversarial Networks (GAN) have gained attention for generation and anomaly detection in image domain. In this paper, we propose a novel GAN-based unsupervised method called TAnoGan for detecting anomalies in time series when a small number of data points are available. We evaluate TAnoGan with 46 real-world time series datasets that cover a variety of domains. Extensive experimental results show that TAnoGan performs better than traditional and neural network models.},
  keywords={Time series analysis;Generative adversarial networks;Data models;Generators;Training;Anomaly detection;Real-time systems},
  doi={10.1109/SSCI47803.2020.9308512},
  ISSN={},
  month={Dec},}@ARTICLE{10328721,
  author={Haider, Majumder and Ahmed, Imtiaz and Rubaai, Ahmed and Pu, Cong and Rawat, Danda B.},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={GAN-Based Channel Estimation for IRS-Aided Communication Systems}, 
  year={2024},
  volume={73},
  number={4},
  pages={6012-6017},
  abstract={This paper proposes a generative adversarial network (GAN) based channel estimation scheme for intelligent reflecting surface (IRS)-aided single-input multiple-output (SIMO) communication systems. The proposed novel GAN-based deep learning technique is efficient to estimate channels in IRS-aided wireless communication systems with high accuracy. The generator of GAN can reproduce data whose distributions are similar to the actual underlying channel. Consequently, the proposed approach does not require the statistical distribution of the underlying channel to be known in advance. Simulation results prove that the proposed GAN-based channel estimation approach outperforms the conventional least square estimation (LSE) approach significantly in terms of estimation accuracy as well as provides better performance than a fully connected deep neural network (DNN) and convolutional neural network (CNN)-based methods.},
  keywords={Channel estimation;Generators;Estimation;Generative adversarial networks;Communication systems;Training;Optimized production technology;6G;artificial intelligence;channel estimation;generative adversarial network;intelligent reflecting surface},
  doi={10.1109/TVT.2023.3336601},
  ISSN={1939-9359},
  month={April},}@INPROCEEDINGS{10135875,
  author={Shi, Wenlong and Han, Xiao and Wang, XinYing and Li, Jian},
  booktitle={2023 8th Asia Conference on Power and Electrical Engineering (ACPEE)}, 
  title={Optimization Scheduling Strategy with Multi-Agent Training Data Rolling Enhancement for Regional Power Grid Considering Operation Risk and Reserve Availability}, 
  year={2023},
  volume={},
  number={},
  pages={1774-1781},
  abstract={This study proposes an optimal dispatching method for regional grids, considering reserve availability and operational risks, while incorporating renewable energy and load uncertainties during day-ahead dispatch. The approach uses rolling augmentation of multi-intelligence training data, ensuring effectiveness and economy in intra-day scheduling, while improving model convergence speed and accuracy. First, a novel approach is proposed to measure system scheduling risk using Conditional Value-at Risk (CVaR). The proposed approach employs the Conditional Generative Adversarial Network (CGAN) to generate novel sets of load and energy output scenarios along with admissible error bounds. By utilizing these sets and intervals, the proposed approach can accurately and efficiently estimate the scheduling risk of the system. A day- ahead optimization model is proposed to minimize system operation cost, including risk cost, while optimizing active scheduling and backup plans to ensure system economy and robustness based on limit scenarios. To improve the effectiveness of the training data for the Multi-agent Proximal Policy Optimization (MAPPO) intra-day scheduling model, the dataset is enhanced using CGAN and updated daily on a rolling basis, optimizing the model's training effect. During the intra-day phase, the intra-day dispatch model utilizes ultra-short-term forecast data as input to generate real-time dispatch plans for standby units. The proposed approach is validated for its feasibility and effectiveness through experiments conducted on the IEEE39 node system.},
  keywords={Training;Renewable energy sources;Costs;Uncertainty;Systems operation;Training data;Data models;operational risk;reserve availability;admissibility error interval;Conditional Generative Adversarial Network;Multi-agent Proximal Policy Optimization},
  doi={10.1109/ACPEE56931.2023.10135875},
  ISSN={},
  month={April},}@ARTICLE{9146557,
  author={Zhang, Fan and Bai, Jing and Zhang, Jingsen and Xiao, Zhu and Pei, Changxing},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={An Optimized Training Method for GAN-Based Hyperspectral Image Classification}, 
  year={2021},
  volume={18},
  number={10},
  pages={1791-1795},
  abstract={This letter explores how to apply a generative adversarial network (GAN) to the classification of hyperspectral images (HSIs) to obtain a smooth training process and better classification results. To this end, the ideas of the progressive growing GAN (PG-GAN) and Wasserstein generative adversarial network gradient penalty (WGAN-GP) are combined to propose a new method for HSI classification. PG-GAN is optimized from the training process of generating adversarial networks. It gradually increases the depth of the network and the size of the input image, making the training smoother. WGAN-GP is optimized in terms of the loss function. The gradient penalty method is used to solve the problems of vanishing gradient and exploding gradient, making the training more stable. Based on the combination of the two methods, a classifier is added to the model so that it can complete the HSI classification task. The proposed method is evaluated over two publicly available hyperspectral data sets, the Indian Pines and University of Pavia data sets. The results show that the proposed method can achieve good training results with only a small amount of labeled training data.},
  keywords={Training;Gallium nitride;Generative adversarial networks;Hyperspectral imaging;Task analysis;Generators;Image resolution;Generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised learning},
  doi={10.1109/LGRS.2020.3009017},
  ISSN={1558-0571},
  month={Oct},}@ARTICLE{9447890,
  author={Wang, Jinghua and Cheng, Ming-Ming and Jiang, Jianmin},
  journal={IEEE Transactions on Image Processing}, 
  title={Domain Shift Preservation for Zero-Shot Domain Adaptation}, 
  year={2021},
  volume={30},
  number={},
  pages={5505-5517},
  abstract={In learning-based image processing a model that is learned in one domain often performs poorly in another since the image samples originate from different sources and thus have different distributions. Domain adaptation techniques alleviate the problem of domain shift by learning transferable knowledge from the source domain to the target domain. Zero-shot domain adaptation (ZSDA) refers to a category of challenging tasks in which no target-domain sample for the task of interest is accessible for training. To address this challenge, we propose a simple but effective method that is based on the strategy of domain shift preservation across tasks. First, we learn the shift between the source domain and the target domain from an irrelevant task for which sufficient data samples from both domains are available. Then, we transfer the domain shift to the task of interest under the hypothesis that different tasks may share the domain shift for a specified pair of domains. Via this strategy, we can learn a model for the unseen target domain of the task of interest. Our method uses two coupled generative adversarial networks (CoGANs) to capture the joint distribution of data samples in dual-domains and another generative adversarial network (GAN) to explicitly model the domain shift. The experimental results on image classification and semantic segmentation demonstrate the satisfactory performance of our method in transferring various kinds of domain shifts across tasks.},
  keywords={Task analysis;Data models;Generative adversarial networks;Training;Feature extraction;Adaptation models;Semantics;Domain adaptation;zero-shot domain adaptation;zero-shot learning;coupled generative adversarial networks;adversarial learning},
  doi={10.1109/TIP.2021.3084354},
  ISSN={1941-0042},
  month={},}@ARTICLE{10589572,
  author={Li, Dongbo and Liu, Xiangyu and Yin, Zhisheng and Cheng, Nan and Liu, Jie},
  journal={IEEE Internet of Things Journal}, 
  title={CWGAN-Based Channel Modeling of Convolutional Autoencoder-Aided SCMA for Satellite-Terrestrial Communication}, 
  year={2024},
  volume={11},
  number={22},
  pages={36775-36785},
  abstract={Sparse code multiple access (SCMA) has excellent application prospects in satellite-terrestrial links because of its high spectral efficiency and access capacity. In the end-to-end SCMA systems, channel modeling is a fundamental task for the communication algorithm design and performance optimization, which however is very challenging as it requires in-depth domain knowledge and technical expertise in radio signal propagations, especially for modeling satellite-terrestrial fading channels. In this article, a convolutional autoencoder-aided SCMA paradigm based on the stochastic channel modeling and autoencoder structure is developed. We are the first to exploit generative adversarial network to represent the satellite-terrestrial fading channel effects for the convolutional autoencoder-aided SCMA. Specifically, convolutional neural networks (CNNs) are employed to jointly construct the encoder and decoder for SCMA to alleviate the curse of dimensionality. Furthermore, we propose a conditional Wasserstein generative adversarial network with the gradient penalty (CWGAN-GP)-based channel modeling approach to achieve approximately accurate conditional channel distribution. Particularly, the received signal corresponding to the pilot symbol is used as a part of the condition information, and the Wasserstein distance is used as a measure of the distance between the distributions. Gradient penalty is adopted to solve the problem of weight pruning forcing Lipschitz constraints, which leads to some data being unable to converge. The numerical results demonstrate the effectiveness of the proposed approach in terms of the bit error rate (BER), block error rate (BLER), and complexity in satellite-terrestrial fading channels.},
  keywords={Space-air-ground integrated networks;Generative adversarial networks;Wireless communication;Decoding;Internet of Things;Fading channels;Stochastic processes;Channel modeling;generative adversarial network;satellite-terrestrial communication;sparse code multiple access (SCMA)},
  doi={10.1109/JIOT.2024.3425470},
  ISSN={2327-4662},
  month={Nov},}@INPROCEEDINGS{10674191,
  author={Brutzman, Kathryn and Burns, Quinn and Cammisa, Vincent and Peracchio, Joe and Phillips, Christian and Sarpong, Kwaku and Liu, Liyuan},
  booktitle={2024 IEEE 4th International Conference on Software Engineering and Artificial Intelligence (SEAI)}, 
  title={Integrating AI and ChatGPT in Wearable Devices for Enhanced Abnormal Activity Reporting: A Mixture of Experts Approach}, 
  year={2024},
  volume={},
  number={},
  pages={231-235},
  abstract={The rapid advancement of Artificial Intelligence (AI) and Generative AI (GAI) has greatly enhanced the capabilities of wearable devices, extending their use beyond just senior independent living to a broader user base. These technologies excel at detecting abnormal activities, crucial for the timely identification of potential health emergencies. This functionality enables users and healthcare providers to monitor abnormalities effortlessly, though it also presents substantial challenges such as high computational demands and sustainability concerns in AI data centers. A significant challenge arises from the variability of data across different channels, which can lead to suboptimal predictions when relying on one-dimensional data. To address these issues, we have implemented a Mixture of Experts (MoE) approach, operating multiple AI models simultaneously, each tailored to the specific characteristics of the data they process. Our system includes ChatGPT for generating timely reports, a deep neural network for analyzing sensor data, a Convolutional Neural Network (CNN) for identifying patterns in image data, and a Recurrent Neural Network (RNN) for processing time-series data, capturing the dynamics inherent in physical activities. This integrated approach not only improves the accuracy of abnormal activity detection but also efficiently manages multichannel data, significantly reducing computational and electrical loads compared to traditional methods. This makes the wearable devices more effective and user-friendly, enhancing overall system performance and sustainability.},
  keywords={Training;Recurrent neural networks;Generative AI;Time series analysis;Chatbots;Data models;Convolutional neural networks;artificial intelligence;ChatGPT;abnormal activ-ity detection;mixture of experts},
  doi={10.1109/SEAI62072.2024.10674191},
  ISSN={},
  month={June},}@INPROCEEDINGS{10868872,
  author={Lu, Yu and He, Shuai and Zhang, Riyue},
  booktitle={2024 4th International Conference on Educational Technology (ICET)}, 
  title={A Study of Student Behavioral Pathways in Gen AI-Enabled Economics and Management Courses Teaching and Learning}, 
  year={2024},
  volume={},
  number={},
  pages={563-567},
  abstract={1 In the digital intelligence era, generative artificial intelligence, particularly ChatGPT, has seen rapid development. Numerous studies have confirmed that generative AI can provide efficient and personalized intelligent services and technical support for education Yet, few researchers have delved into the underlying mechanisms by which ChatGPT enhances student performance. This study employs Economics and Management Courses instruction as a case study, utilizing lag series analysis from a student behavior perspective. It describes the effects of exam answering on 24 college students in the experimental and control groups under different learning conditions of ChatGPT-assisted instruction versus traditional methods. By contrasting their behavioral networks, the study seeks to uncover the fundamental principle through which ChatGPT boosts student performance. The results of the study showed that the integration of ChatGPT into the teaching of economics and management courses significantly increased the level of student achievement . This enhancement is primarily attributed to its ability to spark students' curiosity and foster an exploratory spirit, thereby elevating their cognitive capabilities and broadening their knowledge base, leading to marked improvements in academic performance},
  keywords={Economics;Generative AI;Knowledge based systems;Educational technology;Chatbots;Sparks;Digital intelligence;Economics and Management Courses teaching;Gen AI;Learning Behavior},
  doi={10.1109/ICET62460.2024.10868872},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11042512,
  author={Manchanda, Nandita and Singla, Sanjay},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Hybrid Attention Conditional Generative Adversarial Network Based Framework for Latent Fingerprint Enhancement}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a novel Hybrid Attention-Guided Conditional Generative Adversarial Network Framework (Hybrid cGAN) for latent fingerprint enhancement, which provides a solution for improving low-quality latent fingerprints widely encountered in latent fingerprint analysis, i.e., noise, blur and partial ridge. The proposed methodology utilizes the data from the IIITD Latent Fingerprint Dataset to carry out image preprocessing of the latent fingerprint, using contrast-limited adaptive Histogram Equalization (CLAHE) and data augmentation via synthetic degradations as a simulation of system and problems occurring in a real-world scenario. A multi-scale CNN backbone effectively extracts the hierarchical fingerprint features and is refined using a dual attention mechanism with channel and spatial attention modules. As a result, incorporating a dual attention mechanism distinctly improves the enhancement, concentrating on crucial ridge channels and spatial areas where enhancing fingerprint is paramount. Besides, a conditional Generative Adversarial Network (GCN) is incorporated to restore latent fingerprint images conditioned on attention-refined features. Further, steps are used to enhance the generative restoration with orientation, frequency, and perceptual losses that specialize to the fingerprint domain to improve structural integrity and visual realism.},
  keywords={Visualization;Image matching;Noise;Fingerprint recognition;Feature extraction;Generative adversarial networks;Hybrid power systems;Image restoration;Convolutional neural networks;Image preprocessing;latent fingerprints;enhancement;deep learning},
  doi={10.1109/RMKMATE64874.2025.11042512},
  ISSN={},
  month={May},}@INPROCEEDINGS{9728408,
  author={Meng, Jiao and Niu, Qingran and Huo, Xin and Zhao, Hui and Zhang, Liming and Wang, Xun and Wang, Yang},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={A Detection Method for Parkinson’s Hand Tremor Based on Machine Learning}, 
  year={2021},
  volume={},
  number={},
  pages={4105-4109},
  abstract={Parkinson’s disease (PD) is a common neurode-generative disease, which threatens people’s health seriously. Advanced artificial intelligence methods such as machine learning provide a new way for the diagnosis of PD, so it has become a hot topic of concern. This paper proposes a method to expand the hand tremor data set by Generative Adversarial Networks (GAN), extracts the characteristics of tremor data by Discrete Wavelet Transform-Singular Value Decomposition (DWT-SVD), and finally utilizes multiple classification by Support Vector Machine (SVM) to realize the detection of Parkinson’s disease.},
  keywords={Training;Support vector machines;Time-frequency analysis;Computational modeling;Machine learning;Generative adversarial networks;Feature extraction;Parkinson’s Diagnosis;Machine Learning;Generative Adversarial Networks;Feature Extraction},
  doi={10.1109/CAC53003.2021.9728408},
  ISSN={2688-0938},
  month={Oct},}@INPROCEEDINGS{10987560,
  author={Chang, Huixin},
  booktitle={2025 IEEE International Conference on Electronics, Energy Systems and Power Engineering (EESPE)}, 
  title={Research on the Style Transfer Fusion Model of Piano Improvisation Accompaniment Driven by Artificial Intelligence}, 
  year={2025},
  volume={},
  number={},
  pages={173-178},
  abstract={This paper proposes a style transfer fusion model for piano improvisation accompaniment driven by artificial intelligence. The model combines CNN and RNN, extracts audio features in piano improvisation accompaniment through CNN, and uses RNN to process the time dependency in audio data, and finally realizes the fusion of style transfer and improvisation accompaniment generation. Different from the traditional piano improvisation accompaniment generation method, the model in this paper can adaptively learn the music features of different styles through deep learning technology to generate piano improvisation accompaniment that meets user needs. Experimental results show that the model performs well in multiple style transfer tasks, and can generate expressive improvisation accompaniment while ensuring style consistency, with an accuracy rate of 92%, which is 15% higher than the traditional style transfer method. The innovation of this model lies in the combination of style transfer and improvisation accompaniment generation, which improves the diversity and creative freedom of generated accompaniment. In addition, the model has strong real-time performance and can be applied to real-time music creation and performance.},
  keywords={Adaptation models;Technological innovation;Power engineering;Accuracy;Computational modeling;Reinforcement learning;Feature extraction;Generative adversarial networks;Real-time systems;Artificial intelligence;Piano improvisation;style transfer;CNN;RNN;experimental simulation},
  doi={10.1109/EESPE63401.2025.10987560},
  ISSN={},
  month={March},}@ARTICLE{9502093,
  author={Sayyad, Sameer and Kumar, Satish and Bongale, Arunkumar and Kamat, Pooja and Patil, Shruti and Kotecha, Ketan},
  journal={IEEE Access}, 
  title={Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions}, 
  year={2021},
  volume={9},
  number={},
  pages={110255-110286},
  abstract={An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.},
  keywords={Estimation;Tools;Milling;Cutting tools;Software;Monitoring;Inspection;Artificial intelligence;milling process;predictive maintenance;remaining useful life;sensors;tool wear},
  doi={10.1109/ACCESS.2021.3101284},
  ISSN={2169-3536},
  month={},}@ARTICLE{10102546,
  author={Wu, Dan and Han, Mina and Yang, Yang and Zhao, Shan and Rao, Yujing and Li, Hao and Lin, Xing and Zhou, Chengjiang and Bai, Haicheng},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={DCFusion: A Dual-Frequency Cross-Enhanced Fusion Network for Infrared and Visible Image Fusion}, 
  year={2023},
  volume={72},
  number={},
  pages={1-15},
  abstract={The visible image contains many high-frequency components that provide texture details with high spatial resolution and definition consistent with human visual perception, but it is easily affected by external factors such as light, weather, and obstructions. On the other hand, the infrared image is a radiation image whose contrast is determined by the temperature difference between the target and the background and is not easily affected by external conditions. Integrating complementary information from both image types into one image is therefore very useful. In our article, we propose a dual-frequency cross-enhanced fusion network called DCFusion for infrared and visible image fusion. We design a frequency decomposition module and a frequency enhancement module based on Laplacian of Gaussian (LoG) for feature decomposition and enhancement, respectively. We then build a dual-frequency cross-enhanced fusion generator network based on these two modules to achieve enhanced fusion. We also use the sum of visible and infrared discriminator and the visible discriminator to balance our fusion results, replacing the traditional single visible discriminator. Our method is an end-to-end model, avoiding the manual design of complex fusion rules like traditional methods. Compared with existing advanced fusion algorithms, our method outperforms most of them in qualitative comparison, quantitative comparison, and target detection accuracy. Finally, the experiment proves that our method can effectively enhance the fusion of the target scene even in harsh environments such as complex lighting, low illumination, and smoke scenes.},
  keywords={Feature extraction;Image fusion;Low-pass filters;Generative adversarial networks;Generators;Transformers;Deep learning;Filter;frequency decomposition;frequency enhancement;image fusion;infrared image;visible image},
  doi={10.1109/TIM.2023.3267380},
  ISSN={1557-9662},
  month={},}@ARTICLE{10288477,
  author={Li, Hui and Xiao, Yongbiao and Cheng, Chunyang and Shen, Zhongwei and Song, Xiaoning},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={DePF: A Novel Fusion Approach Based on Decomposition Pooling for Infrared and Visible Images}, 
  year={2023},
  volume={72},
  number={},
  pages={1-14},
  abstract={Infrared and visible image fusion is a crucial technique in the field of computer vision, aiming to create synthetic images that simultaneously capture salient features and rich texture details. These fused images play a pivotal role in enhancing various downstream tasks. However, existing fusion methods often encounter challenges such as texture loss and deficiencies in edge information, resulting in less than optimal fusion outcomes. Additionally, straightforward up-sampling techniques struggle to preserve source information from multiscale features. To tackle these issues, a novel fusion network based on the decomposition pooling (de-pooling) manner is proposed, termed as Decomposition Pooling-based Fusion (DePF). DePF features a de-pooling-based encoder designed to extract multiscale image and detail features from source images concurrently. Furthermore, a spatial attention model aggregates these salient features, and the decoder employs a de-pooling reversed operation instead of the typical up-sampling operator to reconstruct the fused features. Unlike conventional max-pooling techniques, the de-pooling layer preserves abundant detail information, facilitating a richer texture and multiscale information retention during the reconstruction phase. Significantly, our approach exhibits remarkable efficiency, requiring merely 23 ms to integrate a pair of infrared and visible images, each with dimensions of  $640\,\, {}\times {}480$ . Furthermore, empirical findings corroborate the exceptional fusion efficacy of our methodology in the domains of object detection and noise-related assessments, surpassing the performance of contemporary techniques within numerous image fusion benchmarks.},
  keywords={Feature extraction;Image fusion;Task analysis;Decoding;Image reconstruction;Data mining;Generative adversarial networks;Decomposition pooling (de-pooling);deep learning;detail features;image fusion;multiscale features},
  doi={10.1109/TIM.2023.3326252},
  ISSN={1557-9662},
  month={},}@ARTICLE{10745725,
  author={Rajaei, Flora and Minoccheri, Cristian and Wittrup, Emily and Wilson, Richard C. and Athey, Brian D. and Omenn, Gilbert S. and Najarian, Kayvan},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={AI-Based Computational Methods in Early Drug Discovery and Post Market Drug Assessment: A Survey}, 
  year={2025},
  volume={22},
  number={1},
  pages={97-115},
  abstract={Over the past few years, artificial intelligence (AI) has emerged as a transformative force in drug discovery and development (DDD), revolutionizing many aspects of the process. This survey provides a comprehensive review of recent advancements in AI applications within early drug discovery and post-market drug assessment. It addresses the identification and prioritization of new therapeutic targets, prediction of drug-target interaction (DTI), design of novel drug-like molecules, and assessment of the clinical efficacy of new medications. By integrating AI technologies, pharmaceutical companies can accelerate the discovery of new treatments, enhance the precision of drug development, and bring more effective therapies to market. This shift represents a significant move towards more efficient and cost-effective methodologies in the DDD landscape.},
  keywords={Drugs;Diseases;Biology;Artificial intelligence;Reviews;Drug discovery;Proteins;Diffusion tensor imaging;Surveys;Network topology;Artificial intelligence;drug discovery and development;target identification;drug-target interaction;de novo drug design;and post-market drug assessment},
  doi={10.1109/TCBB.2024.3492708},
  ISSN={2998-4165},
  month={Jan},}@INPROCEEDINGS{10737579,
  author={Mudabbiruddin, Mohammed and Imre, Felde and Amir, Mosavi and Perez, Husein},
  booktitle={2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY)}, 
  title={Deep Learning for Detecting Building Defects}, 
  year={2024},
  volume={},
  number={},
  pages={000289-000296},
  abstract={This review article identifies the deep learning methods for building defects detection, adapting an updated PRISMA guideline to ensure a rigorous and transparent review process. The review reveals that convolutional neural networks (CNNs) and their variations are the most popular in this field. By systematically identifying and categorizing the most relevant articles, we present a detailed taxonomy of the methods and applications. Additionally, the article explores current trends and discusses future directions, including advancements in real-time defect detection and the utilization of more diverse and comprehensive datasets.},
  keywords={Deep learning;YOLO;Reviews;Buildings;Taxonomy;Real-time systems;Structural engineering;Convolutional neural networks;Monitoring;Defect detection;building defects;structural health monitoring;artificial intelligence;machine learning;literature review;mathematics;survey;generative artificial intelligence;deep learning;data science;big data;generative AI;data mining;applied artificial intelligence;XAI;applied mathematics;applied informatics;information systems;soft computing},
  doi={10.1109/SISY62279.2024.10737579},
  ISSN={1949-0488},
  month={Sep.},}@ARTICLE{10795474,
  author={Liu, Zhenhong and Wang, Xingce and Wu, Zhongke and Ju, Xiaodong and Zhu, YiCheng and Frangi, Alejandro F.},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={MRI Joint Superresolution and Denoising Based on Conditional Stochastic Normalizing Flow}, 
  year={2025},
  volume={6},
  number={6},
  pages={1472-1487},
  abstract={Magnetic resonance imaging (MRI) is often limited by noise and low-resolution (LR), which can impact the precision of the diagnosis and treatment of patients. LR images and mixed noise (e.g., Gaussian noise, Rician noise, and Impulse noise) are inherent in MR images, and current approaches typically address image superresolution (SR) reconstruction and denoising separately, resulting in a discrepancy between the actual MRI data distribution and the reconstructed images. This research introduces a new algorithm SRDSNF, the stochastic normalizing flow-based MR image SR and denoising model, which tackles SR and denoising simultaneously through a stochastic normalizing flow. Our method integrates the encoded information of the input image as a conditional variable in each reverse step of the stochastic normalizing flow, ensuring a consistent representation of the spatial distribution between the reconstructed image and the original data. Additionally, we incorporate range-null space decomposition and subsequence sampling techniques to increase the consistency between the original and constructed data and accelerate model generation. To assess the efficacy of our approach, we conducted experiments on the BrainWeb and NFBS datasets, which include simultaneous SR and denoising, standalone denoising, and standalone SR tasks. The results of the experiments illustrate that our method achieves superior SR and denoising performance with fewer sampling steps, closely approximating the ground truths. Furthermore, our results surpass those of existing methods in various tasks, showing improvement of up to 4.64 dB in PSNR and 13.8% in SSIM achieved by our SRDSNF model was contextualized against state-of-the-art approaches such as AMIR, SwinIR, and InstructIR. These methods typically report PSNR improvements ranging from 0.5 to 2 dB and SSIM increases of 3%–6%, underscoring the potential clinical value of our methodology.},
  keywords={Noise reduction;Noise;Computational modeling;Magnetic resonance imaging;Noise measurement;Data models;Stochastic processes;Image reconstruction;Superresolution;Generative adversarial networks;Denoising;diffusion model;magnetic resonance image (MRI);stochastic normalizing flow (SNF);superresolution},
  doi={10.1109/TAI.2024.3515936},
  ISSN={2691-4581},
  month={June},}@INPROCEEDINGS{10654427,
  author={Nikolakopoulos, Anastasios and Evangelatos, Spyridon and Veroni, Eleni and Chasapas, Konstantinos and Gousetis, Nikolaos and Apostolaras, Apostolos and Nikolopoulos, Christos D. and Korakis, Thanasis},
  booktitle={2024 5th International Conference in Electronic Engineering, Information Technology & Education (EEITE)}, 
  title={Large Language Models in Modern Forensic Investigations: Harnessing the Power of Generative Artificial Intelligence in Crime Resolution and Suspect Identification}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Large Language Models (LLMs) have recently captured the attention of the scientific c ommunity. S ince t he global launch of LLM-based chatbots in late 2022, the field h as witnessed a rapid increase in interest from researchers, technology providers and citizens alike. With its wide-ranging applicability, Generative Artificial I ntelligence (GenAI) h as t he p otential to impact various aspects of society, from improving communication and accessibility to transforming industries such as healthcare, education and security. More specifically, in the field of Forensic Science, LLMs could offer significant b enefits as sisting Law Enforcement Agencies (LEAs) and Forensic Practitioners in crime investigations. This paper proposes the implementation of a Retrieval Augmented Generation (RAG) LLM, trained with criminology data, to provide swift and actionable insights into specific incidents, thereby enhancing Forensic Data Analysis and facilitating the daily operations of LEAs.},
  keywords={Law enforcement;Forensics;Large language models;Education;Natural languages;Medical services;Performance analysis;Generative Artificial Intelligence;Large Language Models;Security;Forensics;Law Enforcement Agencies;Forensic Data Analysis;Biometric Data},
  doi={10.1109/EEITE61750.2024.10654427},
  ISSN={},
  month={May},}@INBOOK{10951624,
  author={Razmi, Ronald M.},
  booktitle={AI Doctor: The Rise of Artificial Intelligence in Healthcare - A Guide for Users, Buyers, Builders, and Investors}, 
  title={Which Health AI Applications Are Ready for Their Moment?}, 
  year={2024},
  volume={},
  number={},
  pages={275-294},
  abstract={Summary <p>In clinical care, the authors have single&#x2010;file diagnostic use cases such as reading radiology, pathology, dermatology, and ophthalmology images. Computer vision, a form of deep learning methodology, performs very well on digital images and can potentially surpass human capabilities. As with the clinical applications of Artificial intelligence (AI), the administrative and operational use cases can be categorized depending upon whether they use ML to recognize patterns in data and whether they require the comprehension of notes and voice records. In life sciences, the authors can expect the same patterns of adoption based on the level of performance of the underlying AI methodology and the data used in each potential application. AI is already making an impact in public and private sector research in use cases like learning how proteins fold or learning to recognize enhancer regions in genetic sequences.</p>},
  keywords={Artificial intelligence;Medical services;Natural language processing;Large language models;Clinical trials;Neural networks;Generative AI;Computer vision;Computational modeling;Training},
  doi={10.1002/9781394240197.ch12},
  ISSN={},
  publisher={Wiley},
  isbn={9781394240180},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951624},}@INPROCEEDINGS{10453867,
  author={Kazmi, Syed Hussain Ali and Qamar, Faizan and Hassan, Rosilah and Nisar, Kashif and Dahnil, Dahlila Putri Binti and Al-Betar, Mohammed Azmi},
  booktitle={2023 24th International Arab Conference on Information Technology (ACIT)}, 
  title={Threat Intelligence with Non-IID Data in Federated Learning enabled Intrusion Detection for SDN: An Experimental Study}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In the realm of cybersecurity, the ever-evolving threat landscape necessitates innovative approaches to design Intrusion Detection Systems (IDS). Software-Defined Networking (SDN) integrated with Deep Learning (DL) has emerged as a transformative paradigm of threat intelligence in IDS. However, centralized data processing in DL based IDS causes privacy issues. Within this context, Federated Learning (FL) has gained significant attention for its potential to enhance intrusion detection while maintaining privacy. This study presents an experimental investigation into the efficacy of FL-enabled intrusion detection in SDN environments, specifically addressing the challenging aspect of threat specific features selection in Non-IID (Non-Independently and Identically Distributed) data. We used the InSDN intrusion dataset containing different attacks including Denial-of-Service (DoS), Distributed-DoS (DDoS), brute force, probe, web and botnet attacks. After data pre-processing, Principal Component Analysis (PCA) is applied to analyze the impact of Non-IID data on features importance. The detailed results of simulations show large variations in features importance for Non-IID data in terms of quantity and threat type distribution. Furthermore, we discuss the implications of our results for future research directions.},
  keywords={Data privacy;Federated learning;Intrusion detection;Feature extraction;Data processing;Data models;Principal component analysis;Federated Learning;Machine Learning;SDN;Privacy;IDS},
  doi={10.1109/ACIT58888.2023.10453867},
  ISSN={2831-4948},
  month={Dec},}@ARTICLE{10640130,
  author={Rajcic, Nina and Campbell, Bruce D. and Samsel, Francesca},
  journal={IEEE Computer Graphics and Applications}, 
  title={Nina Rajcic: Navigating Artificial Intelligence for a Meaningful Artistic Practice}, 
  year={2024},
  volume={44},
  number={4},
  pages={133-139},
  abstract={As a self-professed AI artist, Nina Rajcic presented an opportunity for us to explore a curiosity regarding how AI artists have been developing a process during an AI boon brought on by transformer and generative AI tools. Although her journey has been one of pursuing text as a creative output, the nature of transformers and diffusion suggested relevance to graphical outputs. The following interview did not disappoint in that pursuit.},
  keywords={Art;Artificial intelligence;Graphical models;Text processing},
  doi={10.1109/MCG.2024.3396617},
  ISSN={1558-1756},
  month={July},}@INPROCEEDINGS{10980728,
  author={Chou, Yu-Cheng and Li, Gary Y. and Chen, Li and Zahiri, Mohsen and Balaraju, Naveen and Patil, Shubham and Hicks, Bryson and Schnittke, Nikolai and Parker, Maria and Kessler, David O. and Shupp, Jeffrey and Baloescu, Cristiana and Moore, Christopher and Gregory, Cynthia and Gregory, Kenton and Raju, Balasundar and Kruecker, Jochen and Chen, Alvin},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Ultrasound Image Synthesis Using Generative AI for Lung Consolidation Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Developing reliable healthcare AI models requires training with representative and diverse data. In imbalanced datasets, model performance tends to plateau on the more prevalent classes while remaining low on less common cases. To overcome this limitation, we propose DiffUltra, the first generative AI technique capable of synthesizing realistic Lung Ultrasound (LUS) images with extensive lesion variability. Specifically, we condition the generative AI by the introduced Lesion-anatomy Bank, which captures the lesion's structural and positional properties from real patient data to guide the image synthesis. We demonstrate that DiffUltra improves consolidation detection by 5.6% in AP compared to the models trained solely on real patient data. More importantly, DiffUltra increases data diversity and prevalence of rare cases, leading to a 25% AP improvement in detecting rare instances such as large lung consolidations, which make up only 10% of the dataset.},
  keywords={Training;Ultrasonic imaging;Generative AI;Image synthesis;Lungs;Object detection;Data models;Lesions;Reliability;Synthetic data;Synthetic data training;conditional diffusion model;lung consolidation;Video object detection},
  doi={10.1109/ISBI60581.2025.10980728},
  ISSN={1945-8452},
  month={April},}@ARTICLE{9739773,
  author={Peng, Fei and Chen, Guanfu and Long, Min},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={A Robust Coverless Steganography Based on Generative Adversarial Networks and Gradient Descent Approximation}, 
  year={2022},
  volume={32},
  number={9},
  pages={5817-5829},
  abstract={Aiming at resolving the problem of the irreversibility in some common neural networks for secret data extraction, a novel image steganography framework is proposed based on the generator of GAN (Generative Adversarial Networks) and gradient descent approximation. During data embedding, the secret data is first mapped into a stego noise vector by a specific mapping rule, and it is input into the generator of a GAN to produce a stego image. The data extraction is accomplished by iteratively updating the noise vector using the gradient descent with the generator. When the error is declined within the allowable error, the output image of the generator is approximate to the stego image, and the updated noise vector will also approach to the stego noise vector. Finally, the secret data is extracted from the updated noise vector. Experiments and analysis with WGAN-GP (Wasserstein GAN-Gradient Penalty) show that it can achieve good performance in extraction accuracy, capacity and robustness. Furthermore, the discussions also illustrate its good generalization with different GAN models and image datasets.},
  keywords={Steganography;Generative adversarial networks;Generators;Data mining;Neural networks;Image segmentation;Distortion;Steganography;generative adversarial networks;gradient descent},
  doi={10.1109/TCSVT.2022.3161419},
  ISSN={1558-2205},
  month={Sep.},}@INPROCEEDINGS{8516271,
  author={Bischke, Benjamin and Helber, Patrick and Koenig, Florian and Borth, Damian and Dengel, Andreas},
  booktitle={2018 International Conference on Content-Based Multimedia Indexing (CBMI)}, 
  title={Overcoming Missing and Incomplete Modalities with Generative Adversarial Networks for Building Footprint Segmentation}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.},
  keywords={Generators;Image segmentation;Gallium nitride;Generative adversarial networks;Training;Buildings;Feature extraction;Generative Adversarial Networks;Semantic Segmentation;Missing Modalities},
  doi={10.1109/CBMI.2018.8516271},
  ISSN={},
  month={Sep.},}@ARTICLE{10171388,
  author={Chen, Donghua and Zhang, Runtong},
  journal={IEEE Transactions on Multimedia}, 
  title={Building Multimodal Knowledge Bases With Multimodal Computational Sequences and Generative Adversarial Networks}, 
  year={2024},
  volume={26},
  number={},
  pages={2027-2040},
  abstract={Conventional knowledge graphs (KGs) are composed solely of entities, attributes, and relationships, which poses challenges for enhancing multimodal knowledge representation and reasoning. To address the issue, this article proposes a multimodal deep learning-based approach to build a multimodal knowledge base (MMKB) for better multimodal feature (MMF) utilization. First, we construct a multimodal computation sequence (MCS) model for structured multimodal data storage. Then, we propose multimodal node, relationship, and dictionary models to enhance multimodal knowledge representation. Various feature extractors are used to extract MMFs from text, audio, image, and video data. Finally, we leverage generative adversarial networks (GANs) to facilitate MMF representation and update the MMKB dynamically. We examine the performance of the proposed method by using three multimodal datasets. BOW-, LBP-, Volume-, and VGGish-based feature extractors outperform the other methods by reducing at least 1.13%, 22.14%, 39.87, and 5.65% of the time cost, respectively. The average time costs of creating multimodal indexes improve by approximately 55.07% and 68.60% exact matching rates compared with the baseline method, respectively. The deep learning-based autoencoder method reduces the search time cost by 98.90% after using the trained model, outperforming the state-of-the-art methods. In terms of multimodal data representation, the GAN-CNN models achieve an average correct rate of 82.70%. Our open-source work highlights the importance of flexible MMF utilization in multimodal KGs, leading to more powerful and diverse applications that can leverage different types of data.},
  keywords={Cognition;Generative adversarial networks;Data models;Visualization;Feature extraction;Databases;Computational modeling;Decision support systems;deep learning;generative adversarial networks;knowledge representation;multimodal data},
  doi={10.1109/TMM.2023.3291503},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{8848119,
  author={Dingli, Alexiei and Bondin, Luca},
  booktitle={2019 IEEE Conference on Games (CoG)}, 
  title={Realtime Adaptive Virtual Reality for Pain Reduction}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={Recent years have seen digital game mediums taking conventional amusement, entertainment and leisure industries by storm. They have revolutionized the system to the extent that the industry cannot now even dream to do without this overwhelming reality. The same game mediums that have capitalized on intrinsic leisure aspects have simultaneously focused with equal vigor on other equally, if not more, important collateral objectives. This paper builds on this concept and discusses a work in progress currently being carried out at the University of Malta. It proposes the use of games as a means of distraction therapy for individuals undergoing painful clinical treatment procedures. The creation of an adaptive Virtual Reality (VR) game within an Artificial Intelligence framework will without doubt be of a significantly greater benefit to the community than mere entertainment applications.},
  keywords={Games;Virtual reality;Task analysis;Software;Medical treatment;Affective computing;Artificial Intelligence;Affective Computing;Adaptive Games;Serious Games;Virtual Reality;Deep Learning},
  doi={10.1109/CIG.2019.8848119},
  ISSN={2325-4289},
  month={Aug},}@ARTICLE{9940986,
  author={Jin, Taisong and Yang, Xixi and Yu, Zhengtao and Luo, Han and Zhang, Yongmei and Jie, Feiran and Zeng, Xiangxiang and Jiang, Min},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={WalkGAN: Network Representation Learning With Sequence-Based Generative Adversarial Networks}, 
  year={2024},
  volume={35},
  number={4},
  pages={5684-5694},
  abstract={Network representation learning, also known as network embedding, aims to learn the low-dimensional representations of vertices while capturing and preserving the network structure. For real-world networks, the edges that represent some important relationships between the vertices of a network may be missed and may result in degenerated performance. The existing methods usually treat missing edges as negative samples, thereby ignoring the true connections between two vertices in a network. To capture the true network structure effectively, we propose a novel network representation learning method called WalkGAN, where random walk scheme and generative adversarial networks (GAN) are incorporated into a network embedding framework. Specifically, WalkGAN leverages GAN to generate the synthetic sequences of the vertices that sufficiently simulate random walk on a network and further learn vertex representations from these vertex sequences. Thus, the unobserved links between the vertices are inferred with high probability instead of treating them as nonexistence. Experimental results on the benchmark network datasets demonstrate that WalkGAN achieves significant performance improvements for vertex classification, link prediction, and visualization tasks.},
  keywords={Generative adversarial networks;Representation learning;Neural networks;Generators;Feature extraction;Task analysis;Learning systems;Generative adversarial networks (GANs);network representation;random walk;sequence},
  doi={10.1109/TNNLS.2022.3208914},
  ISSN={2162-2388},
  month={April},}@ARTICLE{10504291,
  author={Li, Xiaohui and Han, Xinhai and Yang, Jingsong and Wang, Jiuke and Han, Guoqi},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Transfer Learning-Based Generative Adversarial Network Model for Tropical Cyclone Wind Speed Reconstruction From SAR Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-16},
  abstract={Synthetic aperture radar (SAR) plays a crucial role in monitoring the fine structure of tropical cyclones, but its effectiveness is constrained by limitations such as signal degradation and saturation. To address this challenge, we proposed a transfer learning-based generative adversarial network (GAN) framework with a dilated convolution and attention mechanism for reconstructing inner-core high winds from SAR images. We have employed the principles of transfer learning to adapt pretrained models developed by the Hurricane Weather Research and Forecasting (HWRF) model winds to SAR images during tropical cyclone events for reconstruction. The proposed model can effectively capture the relationship between features in the low-precision areas and global features from SAR images, facilitating tropical cyclone wind speed reconstruction. The utilization of Global Precipitation Measurement (GPM) Level 3 rainfall data facilitates the identification of rainfall regions in 89 SAR images obtained from Radarsat-2 and Sentinel-1A/B missions. Comparison with stepped frequency microwave radiometer (SFMR) data reveals that the model exhibits a bias of −0.69 m/s, a root-mean-square error (RMSE) of 4.08 m/s, and an R value of 0.91 under heavy rainfall conditions (>7.62 mm/h). Remarkably, the GAN model exhibits excellent performance compared with measurements from the soil moisture active passive (SMAP) L-band radiometer, achieving an RMSE of 3.78 m/s. Our findings indicate that deep learning (DL) technology holds significant promise for the reconstruction and monitoring of tropical cyclones through the utilization of SAR imagery.},
  keywords={Tropical cyclones;Rain;Radar polarimetry;Image reconstruction;Generative adversarial networks;Spaceborne radar;Wind speed;Generative adversarial network (GAN);machine learning;synthetic aperture radar (SAR);transfer learning;tropical cyclone;winds reconstruction},
  doi={10.1109/TGRS.2024.3390392},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{11070644,
  author={Padthe, Adithya and Sallaah, Mohhamied Husaein and Buvaneswari, PR. and Mazumder, Debarshi and Sincija, C.},
  booktitle={2025 3rd International Conference on Data Science and Information System (ICDSIS)}, 
  title={Pattern Recognition for Bone Age Assessment based on Generative Adversarial Network and Swin Transformer}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In this era, Bone Age Assessment (BAA) is a critical task in pediatric endocrinology which is performed using hand X-ray images to estimate skeletal maturity. Even though, previous researchers have suggested advanced models which faced the challenges as the model are less adaptive to varying bone structures and growth stages, inefficient multi-scale representation and limited generalization. Therefore, to resolve these issues, an effective framework is proposed namely Generative Adversarial Network and Swin Transformer (GAN-ST) model. The framework initially applies deep active learning approach to segment specific Regions of Interests (RoIs) from raw medical image by using annotated data. Then, GAN is incorporated to enhance contrast, sharpness, and brightness of segmented RoIs. Finally, ST is employed for effective pattern recognition; additionally, Class Active Map (CAM) is utilized to understand the necessity of deep-learning-based medical image processing tasks. Furthermore, to evaluate the performance of the proposed framework BAA task, Radiological Society of North America (RSNA) is considered.},
  keywords={Hands;Adaptation models;Image segmentation;Generative adversarial networks;Bones;Transformers;Pattern recognition;X-ray imaging;Endocrinology;Biomedical imaging;bone age assessment;class active map;generative adversarial network;pattern recognition and swin transformer},
  doi={10.1109/ICDSIS65355.2025.11070644},
  ISSN={},
  month={May},}@INPROCEEDINGS{10915507,
  author={Naik, Mahendra Shridhar and S. N, Chaitra and Shashikala, K. S.},
  booktitle={2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)}, 
  title={A Novel Generative AI-Powered Approach for Sentiment Analysis: Enhancing Deep Learning Models with Contextual Understanding}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Sentiment analysis, a pivotal aspect of natural language processing (NLP), has seen notable advancements with the advent of deep learning models. Despite these advancements, challenges persist in accurately capturing nuanced emotions and understanding context-dependent sentiments. This paper introduces an innovative method that integrates generative adversarial networks (GANs) into deep learning architectures to enhance sentiment analysis. By leveraging GAN’s ability to generate contextually relevant text, the proposed approach enriches the training data, thereby improving the model's ability to discern subtle sentiment variations. Our extensive experiments on benchmark datasets reveal that our method surpasses traditional deep learning models, achieving a 5% improvement in accuracy, a 7% higher F1-score, and demonstrating superior robustness to sarcasm and irony.},
  keywords={Deep learning;Sentiment analysis;Analytical models;Accuracy;Training data;Benchmark testing;Generative adversarial networks;Robustness;Data models;Context modeling;Sentiment Analysis;Generative Adversarial Networks (GANs);Deep Learning;Sarcasm Detection;NLP},
  doi={10.1109/IITCEE64140.2025.10915507},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10581423,
  author={Xue, Tianbao and Lan, Quanxiang},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={An Improved Image Style Transfer Algorithm Based on CycleGAN and Multi-Path CNN}, 
  year={2024},
  volume={},
  number={},
  pages={1262-1267},
  abstract={Image style transfer plays an important role in digital media processing technology and is widely used in aerospace, satellite remote sensing technology and other fields. This paper conducts an in-depth study of the traditional color transfer algorithm and the image style transfer algorithm based on VGG19, and analyzes their principles and existing problems. On this basis, an image style transfer algorithm combining cyclic adversarial neural network (CycleGAN) and multi-path convolutional neural network (CNN) is proposed. Through comparative experiments, the algorithm in this paper was compared with VGG19 and dense connection-based generative adversarial network (DenseNetGAN) algorithms. The results show that under the same data set conditions, the algorithm proposed in this paper is at least 0.3 % higher in terms of standard deviation, at least 1.95% higher in terms of peak signal-to-noise ratio (PSNR), and at least 1.95% higher in terms of structural similarity index (SSIM). 1.23% higher.},
  keywords={Seminars;PSNR;Satellites;Image color analysis;Neural networks;Statistical learning;Media;image style transfer;multi-path convolutional neural network;recurrent generative adversarial network;algorithm optimization},
  doi={10.1109/AINIT61980.2024.10581423},
  ISSN={},
  month={March},}@ARTICLE{10787121,
  author={Raouf, Hussien Abdel and Fouda, Mostafa M. and Ibrahem, Mohamed I.},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Revolutionizing User Authentication Exploiting Explainable AI and CTGAN-Based Keystroke Dynamics}, 
  year={2025},
  volume={6},
  number={},
  pages={97-108},
  abstract={Due to the reliability and efficiency of keystroke dynamics, enterprises have adopted it widely in multi-factor authentication systems, effectively strengthening user authentication and thereby boosting the security of online and offline services. The existing works that detect imposter users suffer from performance and robustness degradation. Therefore, this article introduces a novel methodology to enhance user authentication and identify imposter users who attempt to have unauthorized access. We first use quantile transformation (QT) to mitigate outliers in the user's typing behavior that affects the authentication process and then employ conditional tabular generative adversarial networks (CTGAN) for data augmentation to learn the users' typing patterns better. Next, five accurate transfer learning models (VGG19, EfficientNetB0, Resnet50, MobileNetV2, and DenseNet121) are utilized for extracting effective features within the typing patterns, so our methodology can detect imposter users accurately and hence make precise decisions to enhance the user authentication process. Finally, we ensure transparency and trust in our user authentication methodology by incorporating explainable artificial intelligence (XAI), utilizing local interpretable model-agnostic explanations (LIME). Extensive experiments using a publicly available keystroke dynamics benchmark dataset from Carnegie Mellon University (CMU) showcase superior security performance and robustness using the proposed methodology compared to the state-of-the-art approaches.},
  keywords={Authentication;Accuracy;Feature extraction;Support vector machines;Keystroke dynamics;Hidden Markov models;Biometrics;Passwords;Transfer learning;Robustness;Conditional tabular generative adversarial networks (CTGAN);explainable artificial intelligence (XAI);keystroke dynamics;user authentication},
  doi={10.1109/OJCS.2024.3513895},
  ISSN={2644-1268},
  month={},}@ARTICLE{10137399,
  author={Wang, Chendan and Chen, Bowen and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Remote Sensing Image Synthesis via Semantic Embedding Generative Adversarial Networks}, 
  year={2023},
  volume={61},
  number={},
  pages={1-11},
  abstract={Generating photo-realistic remote sensing images conditioned on semantic masks has many practical applications like image editing, detecting deep fake geography, and data augmentation. Although previous methods achieved high-quality synthesis results for natural images like faces and everyday objects, they still underperform in remote sensing scenarios in terms of both visual fidelity and diversity. The high data imbalance and high semantic similarity of remote-sensing object categories make the semantic synthesis of remote sensing images more challenging than natural images. To tackle these challenges, we propose a novel method named conducted semantic embedding GAN (CSEBGAN) for semantic-controllable remote sensing image synthesis. The proposed method decouples different semantic classes into independent semantic embeddings, which explores the regularities between classes to improve visual fidelity and naturally supports semantic-level. We further introduce a novel tripartite cooperation adversarial training scheme that involves a conductor network to provide fine-grained semantic feedback for the generator. We also show that the proposed semantic image synthesis method can be utilized as an effective data augmentation approach on improving the performance of the downstream remote sensing image segmentation tasks. Extensive experiments show the superiority of our method compared with the state-of-the-art image synthesis methods.},
  keywords={Semantics;Remote sensing;Image synthesis;Generators;Training;Image segmentation;Visualization;Generative adversarial networks;image segmentation;remote sensing images;semantic image synthesis},
  doi={10.1109/TGRS.2023.3279663},
  ISSN={1558-0644},
  month={},}@ARTICLE{10183834,
  author={Hyun, Sangeek and Lew, Jaihyun and Chung, Jiwoo and Kim, Euiyeon and Heo, Jae-Pil},
  journal={IEEE Transactions on Image Processing}, 
  title={Frequency-Based Motion Representation for Video Generative Adversarial Networks}, 
  year={2023},
  volume={32},
  number={},
  pages={3949-3963},
  abstract={Videos contain motions of various speeds. For example, the motions of one’s head and mouth differ in terms of speed — the head being relatively stable and the mouth moving rapidly as one speaks. Despite its diverse nature, previous video GANs generate video based on a single unified motion representation without considering the aspect of speed. In this paper, we propose a frequency-based motion representation for video GANs to realize the concept of speed in video generation process. In detail, we represent motions as continuous sinusoidal signals of various frequencies by introducing a coordinate-based motion generator. We show, in that case, frequency is highly related to the speed of motion. Based on this observation, we present frequency-aware weight modulation that enables manipulation of motions within a specific range of speed, which could not be achieved with the previous techniques. Extensive experiments validate that the proposed method outperforms state-of-the-art video GANs in terms of generation quality by its capability to model various speed of motions. Furthermore, we also show that our temporally continuous representation enables to further synthesize intermediate and future frames of generated videos.},
  keywords={Generators;Frequency modulation;Codes;Time-frequency analysis;Encoding;Three-dimensional displays;Interpolation;Generative adversarial networks;video generation;sinusoidal motion representation;speed-level motion manipulation},
  doi={10.1109/TIP.2023.3293767},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10497293,
  author={Gorowara, Naaz and Prakash, Anshika and Correa, Frederick Sidney and Malik, Varun and Mittal, Ruchi},
  booktitle={2024 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={AI Personalizing Training and Reskilling Employees for the Digital Age}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Organizations are investing in reskilling and individualized training programmers to stay up with the digital era's fast-paced technical progress. This paper provides a novel strategy that leverages AI principles to make these algorithms even more effective. We focus on two important components: decision tree imputation preprocessing and better generative adversarial neural network (GAN) classification/prediction. During the preprocessing stage, Decision Tree Imputation is utilised to fill in missing data in training datasets. When confronted with little data, it is usual for conventional methodologies to offer biassed results and less-than-ideal models. Nonetheless, Decision Tree Imputation appears as a viable choice due to its creative utilisation of pre-existing data to fill in dataset gaps. This ensures a more thorough and representative training dataset, which results in more accurate AI models. For individualised training recommendations, we propose utilising an Improved Generative Adversarial Neural Network (GAN), which goes beyond traditional classification and prediction models. Enhanced GANs, which are well-known for their ability to generate synthetic data, are used to create personalised learning pathways for each employee. The improved GAN takes into account the user's current skill set, as well as their learning style, future work aspirations, and the ever-changing needs of the digital world. This research proposes an integrated system that can handle bad data and personalise training recommendations for each employee by using powerful AI models and preprocessing approaches.},
  keywords={Training;Neural networks;Organizations;Predictive models;Generative adversarial networks;Data models;Decision trees;Artificial Intelligence;Digital Age;Decision Tree Imputation;Generative Adversarial Neural Network},
  doi={10.1109/ESCI59607.2024.10497293},
  ISSN={},
  month={March},}@INPROCEEDINGS{11058881,
  author={Martín Gómez del Moral Herranz, Rodrigo and Barba Beltrán, Adrián and Rujas, Miguel and Merino-Barbancho, Beatriz and Arredondo, Maria Teresa and Fernanda Cabrera-Umpierrez, Maria and Fico, Giuseppe},
  booktitle={2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={Synthetic Data Generation for Physical Activity in Wearable Devices: A Multivariate Time Series Approach}, 
  year={2025},
  volume={},
  number={},
  pages={9-14},
  abstract={Synthetic data generation is an emerging solution to address current data privacy and availability challenges, especially in healthcare applications and wearable devices. This study aims to generate synthetic multivariate time-series data that simulates physical activities, preserving the variability and dependencies observed in real-world data (RWD). An innovative approach that includes data organization, preprocessing, and model training on a dataset of 100 users with several datetime, numeric, and categorical variables is conducted. By using two artificial intelligence (AI) models — Periodic Autoregressive (PAR) and Conditional Tabular Generative Adversarial Network (CTGAN)—a total of 100 new synthetic users with temporal and feature-specific activity patterns were generated. Evaluation results showed strong alignment between synthetic and original data, with mean values of 0.014 for Kolmogorov-Smirnov (KS) Test, 0.057 for Jensen-Shannon (JS) Distance, and 0.011 for Pairwise Correlation, indicating realistic data relationships and feature distributions. Despite these preliminary results, future work includes enhancing computational efficiency and scalability, expanding its generalizability across diverse datasets, and further validation.},
  keywords={Training;Pairwise error probability;Scalability;Time series analysis;Pipelines;Organizations;Data models;Numerical models;Wearable devices;Synthetic data;synthetic data generation;wearable devices;multivariate time series;artificial intelligence;generative models},
  doi={10.1109/CBMS65348.2025.00012},
  ISSN={2372-9198},
  month={June},}@ARTICLE{9954279,
  author={Dong, Chen and Liang, Haotai and Xu, Xiaodong and Han, Shujun and Wang, Bizhu and Zhang, Ping},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Semantic Communication System Based on Semantic Slice Models Propagation}, 
  year={2023},
  volume={41},
  number={1},
  pages={202-213},
  abstract={Traditional communication systems treat messages’ semantic aspects and meaning as irrelevant to communication, revealing its limitations in the era of artificial intelligence (AI), such as communication efficiency and intent-sharing among different entities. Through broadening the scope of the traditional communication system and the AI-based encoding techniques, in this manuscript, we present a novel semantic communication system, which involves the essential semantic information exploration, transmission and recovery for more efficient communications. Compared to other state-of-the-art semantic communication-related works, our proposed semantic communication system is characterized by the “flow of the intelligence” via the propagation of the model. Besides, the concept of semantic slice-models (SeSM) is proposed to enable flexible model-resembling under the different requirements of the model performance, channel situation and transmission goals. Specifically, a layer-based semantic communication system for images (LSCI) is built on the simulation platform to demonstrate the feasibility of the proposed system and a novel semantic metric called semantic service quality (SS) is proposed to evaluate the semantic communication systems. We evaluate the proposed system on Cityscapes and Open Images datasets, resulting in averaged 10% and 2% bit rate reduction over JPEG and JPEG2000, respectively. In comparison to LDPC, the proposed channel coding scheme can averagely save 2dB and 5dB in AWGN channel and Rayleigh fading channel, respectively.},
  keywords={Semantics;Artificial intelligence;Communication systems;Channel coding;Task analysis;Image coding;Transform coding;Semantic communication;models propagation;LSCI;SeSM;SS},
  doi={10.1109/JSAC.2022.3221948},
  ISSN={1558-0008},
  month={Jan},}@ARTICLE{10579546,
  author={Chen, Zirui and Zhang, Zhaoyang and Yang, Zhaohui},
  journal={IEEE Wireless Communications}, 
  title={Big AI Models for 6G Wireless Networks: Opportunities, Challenges, and Research Directions}, 
  year={2024},
  volume={31},
  number={5},
  pages={164-172},
  abstract={Recently, big artificial intelligence models (BAIMs) represented by chatGPT have brought an incredible revolution. With the pre-trained BAIMs in certain fields, numerous downstream tasks can be accomplished with only few-shot, or even zero-shot, learning, and exhibit state-of-the-art performances. As widely envisioned, the big AI models can rapidly penetrate into major intelligent services and applications, and are able to run at low unit cost with high flexibility. In 6G wireless networks, to fully enable intelligent communication, sensing, and computing, apart from providing other intelligent wireless services and applications, it is of vital importance to design and deploy certain wireless BAIMs (wBAIMs). However, investigation into architecture design and system evaluation for wBAIM is still lacking. In this article, we provide a comprehensive discussion as well as some in-depth prospects on the demand, design, and deployment aspects of the wBAIM. We opine that wBAIM will be a recipe for the 6G wireless networks to build high-efficient, sustainable, versatile, and extensible wireless intelligence for numerous promising visions. Then, we provide the core characteristics, principles, and pilot studies to guide the design of wBAIMs, and discuss the key aspects of developing wBAIMs through identifying the differences between the existing BAIMs and the emerging wBAIMs. Finally, related research directions and potential solutions are outlined.},
  keywords={Wireless sensor networks;Artificial intelligence;Task analysis;6G mobile communication;Wireless networks;Data models;Adaptation models},
  doi={10.1109/MWC.015.2300404},
  ISSN={1558-0687},
  month={October},}@INPROCEEDINGS{10774538,
  author={Zhao, Yan and Li, Zhongyun and Pan, Yushan and Wang, Jiaxing and Zhang, Zhiman and Wang, Yihong},
  booktitle={2024 IEEE 22nd International Conference on Industrial Informatics (INDIN)}, 
  title={LB-KBQA:Large-language-model and BERT based Knowledge-Based Question and Answering System}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect newly appeared intent and acquire new knowledge. In experiments on financial domain question answering, our model has demonstrated superior effectiveness.},
  keywords={Adaptation models;Adaptive learning;Accuracy;Generative AI;Intent recognition;Large language models;Knowledge based systems;Semantics;Linguistics;Vectors;Generative AI;KBQA;LLM},
  doi={10.1109/INDIN58382.2024.10774538},
  ISSN={2378-363X},
  month={Aug},}@INPROCEEDINGS{10823328,
  author={Julaiha, AG. Noorul and Hemamalini, S. and Preiya, V. Sathya and Sathyamoorthy, K. and Priyanka, V.},
  booktitle={2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)}, 
  title={Artificial Intelligence Support for the Prompt Identification and Understanding of the Broader Spectrum of Autism in Children}, 
  year={2024},
  volume={},
  number={},
  pages={1275-1281},
  abstract={This research work presents a novel language intervention system for Tamil-speaking children with autism spectrum disorder (ASD). The system satisfies the considerable requirement for tools aimed at one more section of population that has actually been forgotten by mainstream economics. Traditional language learning applications often lack specificity for diverse linguistic contexts, focusing on general learning rather than the unique challenges faced by ASD children. The proposed solution leverages Deep Learning (DL) modules to transform partial spoken Tamil words into complete, child-friendly vocabulary. This system goes beyond simple word completion; it incorporates interactive features to engage children with ASD and promote active learning. Preliminary evaluations yielded promising results, suggesting the system's potential in enhancing communication skills among Tamil-speaking children with ASD. This paper research study reviews the limitations of existing language learning tools and the rationale behind employing a DL-based approach. We discuss the specific DL architecture (e.g., Recurrent Neural Networks) envisioned for the system and how it will be trained on a corpus of Tamil speech data. This study also investigate the potential advantages beyond vocabulary learning, such as improved sentence formulation and the development of practical language abilities. Future ambitions include broadening the system's scope to include more languages, refining the DL model through ongoing training with enriched datasets, and conducting extensive user research with varied ASD populations. Validation across demographics is critical to ensuring the system's efficiency and generality. This research has the potential to bridge the language acquisition gap for Tamil-speaking children with ASD, empowering them to develop crucial communication skills and fostering their social inclusion.},
  keywords={Deep learning;Training;Autism;Vocabulary;Translation;Recurrent neural networks;Reviews;Refining;Transforms;Linguistics;Autism Spectrum Disorder (ASD);Language Acquisition;Speech Processing;Deep Learning;Voice-enabled Learning;Partial Word Completion},
  doi={10.1109/ICICNIS64247.2024.10823328},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11165937,
  author={Konidi, Mariza and Karagiorgou, Sophia and Veroni, Eleni and Nikolopoulos, Christos},
  booktitle={2025 6th International Conference in Electronic Engineering & Information Technology (EEITE)}, 
  title={Addressing Disinformation and Deep Fakes Spread through AI-fuelled International Collaboration}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In the current era of digital transformation, the nature of criminal activities has undergone significant evolution, with cybercrime and terrorism presenting intricate challenges to Law Enforcement Authorities (LEAs). These challenges are compounded by the utilization of digital tools by criminals to accomplish their objectives. The widespread use of social networks, the vastness of the deep and shallow web, and the substantial amount of data accessible online have become means for illicit activities such as disinformation, hate speech, deepfakes, and the cultivation of mistrust. Concurrently, advancements in Big Data (BD) and Artificial Intelligence (AI) technologies provide opportunities to augment the capabilities of LEAs in identifying and countering such activities. This paper presents a conceptual framework aimed at enhancing evidence collection and facilitating global security information exchange and cooperation. The objective is to create a safer world by mitigating hate speech, disinformation, deepfakes, and related practices, thereby fostering resilient societies and promoting justice and the rule of law. The framework is grounded in technological solutions, including mining and intelligence operations on both the dark and surface web, decision support systems, sentiment analysis for LEAs investigations and polarity assessment, as well as behavioral analysis for correlating actors, events, patterns, and motifs in the dissemination of disinformation, deepfakes, and hate speech narratives.},
  keywords={Deepfakes;Sentiment analysis;Social networking (online);Law enforcement;Terrorism;Hate speech;Threat assessment;International collaboration;Artificial intelligence;Information technology;disinformation;hate speech;deepfakes;distrust;AI models;threat detection;decision making},
  doi={10.1109/EEITE65381.2025.11165937},
  ISSN={},
  month={June},}@INPROCEEDINGS{10956320,
  author={Meshram, Revat and Krishna, Sachith and Kulkarni, Omkaresh and Patil, Rutuja Rajendra and Kaur, Gagandeep and Maheshwari, Shruti},
  booktitle={2025 International Conference on Automation and Computation (AUTOCOM)}, 
  title={NPC Behavior in Games Using Unity ML-Agents: A Reinforcement Learning Approach}, 
  year={2025},
  volume={},
  number={},
  pages={1519-1523},
  abstract={Machine learning is having a significant impact on video games, from speeding up the development of new games to the development of new methods in gaming. This study discusses the impact of using ML models in the development and improvement of video games. Examples cited in this paper relate to the ways in which NPCs behave. With the aid of Unity's ML-Agents toolkit, developers can create NPCs that respond to different strategies in real time. This makes the games more exciting and complex. The paper also outlines the possibilities of single-player games, such as first-person shooters and a role-playing game, to notice NPC behavior during the gameplay. In these gaming styles, RL frameworks can be applied to NPCs to enable them to learn and adapt to the environment to create an interactive experience for the player. The conclusions put an emphasis on the evolution of video games with the advent of ML tools; as predicted, NPC behavior becomes more intuitive and less expected.},
  keywords={Industries;Procedural generation;Automation;Role playing games;Games;Reinforcement learning;Real-time systems;Unity;ML-Agents;NPC;Video Game;Reinforcement Learning},
  doi={10.1109/AUTOCOM64127.2025.10956320},
  ISSN={},
  month={March},}@ARTICLE{9091179,
  author={Yang, Yang and Dan, Xiaodong and Qiu, Xuesong and Gao, Zhipeng},
  journal={IEEE Access}, 
  title={FGGAN: Feature-Guiding Generative Adversarial Networks for Text Generation}, 
  year={2020},
  volume={8},
  number={},
  pages={105217-105225},
  abstract={Text generation is a basic work of natural language processing, which plays an important role in dialogue system and intelligent translation. As a kind of deep learning framework, Generative Adversarial Networks (GAN) has been widely used in text generation. In combination with reinforcement learning, GAN uses the output of discriminator as reward signal of reinforcement learning to guide generator training, but the reward signal is a scalar and the guidance is weak. This paper proposes a text generation model named Feature-Guiding Generative Adversarial Networks (FGGAN). To solve the problem of insufficient feedback guidance from the discriminator network, FGGAN uses a feature guidance module to extract text features from the discriminator network, convert them into feature guidance vectors and feed them into the generator network for guidance. In addition, sampling is required to complete the sequence before feeding it into the discriminator to get feedback signal in text generation. However, the randomness and insufficiency of the sampling method lead to poor quality of generated text. This paper formulates text semantic rules to restrict the token of the next time step in the sequence generation process and remove semantically unreasonable tokens to improve the quality of generated text. Finally, text generation experiments are performed on different datasets and the results verify the effectiveness and superiority of FGGAN.},
  keywords={Generators;Gallium nitride;Generative adversarial networks;Feature extraction;Learning (artificial intelligence);Training;Semantics;Generative adversarial networks;text generation;deep learning;reinforcement learning},
  doi={10.1109/ACCESS.2020.2993928},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11082779,
  author={Almasre, Miada A. and Al-Malki, Norah},
  booktitle={2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT)}, 
  title={Expert and LLM Evaluation of LearnShield: A Generative AI Recomnadation Application in E-Learning Environments}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={With the increasing demand for e-learning systems in educational contexts, and the constant interactions with them in day-to-day learning activities, there is a high demand for dynamic approaches that will facilitate the provision of cybersecurity recommendations to e-learning stakeholders. Thus, the objective of this researcher is to develop and evaluate LearnShield, a GenAI recommender tool that can used by educators and students to generate context-specific user-oriented recommendations. The study evaluates this application using a mixed method approach where human and LLM-based evaluators assess the application considering multiple metrics. The results indicate the overall positive assessment of LearnShield as well as its potential in the field of cybersecurity awareness.},
  keywords={Measurement;Technological innovation;Electronic learning;Generative AI;Large language models;Stakeholders;Internet of Things;Computer security;Artificial Intelligence;Large Language Model;LLM;Generative AI;Cybersecurity;E-Learning},
  doi={10.1109/AIIT63112.2025.11082779},
  ISSN={},
  month={May},}@INPROCEEDINGS{10975878,
  author={Huang, Shu and Wang, Wenpu},
  booktitle={2025 14th International Conference on Educational and Information Technology (ICEIT)}, 
  title={Foreign Language Learners' Perceptions of Chatbot-Assisted Speech-Giving Tasks}, 
  year={2025},
  volume={},
  number={},
  pages={79-83},
  abstract={With the development of generative artificial intelligence (GenAI) technology, many studies advocate for the incorporation of AI technology into traditional teaching. Effective AI integration requires a comprehensive understanding of students' perceptions of innovative learning experiences in specific educational contexts. The study, therefore, examines students' perceptions of a novel task design, the chatbot-assisted speech-giving tasks (CaST), in an English as a Foreign Language (EFL) course among Chinese undergraduates. Data of the study include questionnaire responses from 124 students and 6 semi-structured interviews. Analysis of the quantitate data found that students held very positive views about CaST. The qualitative data triangulated the quantitative data and further revealed specific benefits students perceived in CaST. The contribution of this study is two-fold. First, it advances an in-depth understanding of students' perceptions related to the integration of AI chatbots in authentic classroom contexts, offering insights into the effective integration of generative artificial intelligence technology in foreign language teaching. Second, it suggests potential problems of AI integration in educational contexts that require further attention.},
  keywords={Generative AI;Large language models;Education;Collaboration;Linguistics;Chatbots;Logic;Artificial intelligence;Interviews;Information technology;GenAI;English as a Foreign language (EFL);AI Integration;student perception;Chatbot-assisted speech-giving tasks (CaST)},
  doi={10.1109/ICEIT64364.2025.10975878},
  ISSN={},
  month={March},}@ARTICLE{9039669,
  author={Li, Huan and Tang, Jinglei},
  journal={IEEE Access}, 
  title={Dairy Goat Image Generation Based on Improved-Self-Attention Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={62448-62457},
  abstract={The lack of long-range dependence in convolutional neural networks causes weaker performance in generative adversarial networks(GANs) with regard to generating image details. The self-attention generative adversarial network(SAGAN) use the self-attention mechanism to calculate the correlation coefficient between feature vectors, which improves the global coherence of the network. In this paper, we put forward an improved-self-attention GANs(Improved-SAGAN) to improve the method for calculating correlation in the SAGAN. We can better measure the correlation between features by normalizing the feature vectors to eliminate as many errors caused by noise as possible. As the network learns the global information by calculating the correlation coefficient between all features, it can make up for the defects of local receptive field in the convolution network. We replace the conventional one-hot label with multi-label to obtain more supervised information for generative adversarial networks. We generate dairy goat images based on auxiliary condition generative adversarial network(ACGAN) incorporating the normalized self-attention mechanism and prove that images generated under multi-label are of higher quality than images generated under one-hot label. The generative results of different networks on the public dataset are compared by the inception score and FID evaluation algorithms, and we propose a new evaluation algorithm called SSIM-Mean to measure the quality of generated dairy goat images to further verify the effectiveness of the improved-self-attention GANs.},
  keywords={Convolution;Correlation;Gallium nitride;Training;Image synthesis;Generative adversarial networks;Coherence;Generative adversarial network;deeping learning;image generation;self-attention mechanism},
  doi={10.1109/ACCESS.2020.2981496},
  ISSN={2169-3536},
  month={},}@ARTICLE{9495812,
  author={Zhang, Zhenfeng},
  journal={IEEE Access}, 
  title={PAMSGAN: Pyramid Attention Mechanism-Oriented Symmetry Generative Adversarial Network for Motion Image Deblurring}, 
  year={2021},
  volume={9},
  number={},
  pages={105131-105143},
  abstract={Motion blur is a common problem in optical imaging, which is caused by the relative displacement between the subject and the camera in the exposure process of the camera. This can result in motion blur of the acquired image, reduce the image resolution and affect the imaging quality. Motion blur image restoration technology uses the existing motion blur image to restore the clear image through the modeling of imaging physical process and mathematical solution without re-photographing the target scene. It has an important application value in the civil and military fields. Solving the problem of motion blur caused by camera jitter and object motion during camera imaging is a very challenging problem. When the popular generative adversarial network model is directly applied to the image blur blind removal task, serious pattern collapse phenomenon will occur. In this paper, we propose a novel motion image deblurring model based on pyramid attention mechanism-oriented symmetry generative adversarial network. This new method does not need to predict the fuzzy kernel of the blurred images, and can directly realize the blind removal of image motion blur. Based on the original CycleGan, the network structure and loss function of the symmetry generative adversarial network are improved. The accuracy of blind removal of motion images is improved, and the stability of the network is greatly enhanced in the case of limited samples. The generative network adopts the encoding and decoding structure, and introduces the feature pyramid attention mechanism. The combination of multi-scale pyramid features and attention mechanism can capture more rich advanced features to improve the model performance. In the experiment, the RMSProp algorithm is used to optimize the network training. Finally, a clear image is obtained through network adversarial training between generative and discriminant network. Experimental results on the related image blur benchmark datasets show that the restoration quality of the proposed method is higher in terms of subjective and objective evaluation. Meanwhile, the restoration results can achieve better results in subsequent object detection tasks.},
  keywords={Image restoration;Generative adversarial networks;Kernel;Feature extraction;Neural networks;Convolutional neural networks;Motion image deblurring;pyramid attention mechanism;symmetry generative adversarial network;RMSProp;CycleGAN},
  doi={10.1109/ACCESS.2021.3099803},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10607225,
  author={Noor, Nur Qamarina Mohd and Zabidi, Azlee and Jaya, Mohd Izham Bin Mohd and Ler, Tan Jia},
  booktitle={2024 IEEE Symposium on Industrial Electronics & Applications (ISIEA)}, 
  title={Performance Comparison between Generative Adversarial Networks (GAN) Variants in Generating Anime/Comic Character Images - A Preliminary Result}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In recent years, the popularity and demand for digital animation, specifically anime characters, has brought significant challenges and opportunities for the world of computer graphics and artificial intelligence. This research dives deep into the comprehensive exploration of two well-known Generative Adversarial Networks (GANs)— Deep Convolutional Generative Adversarial Network (DCGAN) and CycleGAN, — with a specific focus on anime character generation. GANs, consisting of a generator and discriminator, operate in a feedback loop to create and evaluate synthetic data. This research will identify challenges within each GAN model and develop objectives to address these challenges. Both of GAN models were mainly executed in the Google Colab environment to optimize the GPU-accelerated runtime. The dataset is sourced from publicly accessible anime image repositories and both GAN models will be evaluated using Fréchet Inception Distance (FID) and Inception Score (IS). FID compares the distribution of generated images with the distribution of a set of real images (“ground truth”) while IS only evaluates the distribution of generated images. Together, they provide a comprehensive evaluation of a GAN's performance. In the realm of anime character generation, achieving authenticity and diversity is crucial.},
  keywords={Feedback loop;Runtime;Character generation;Computer graphics;Generative adversarial networks;Generators;Internet;GAN;FID;IS;DCGAN;CycleGAN},
  doi={10.1109/ISIEA61920.2024.10607225},
  ISSN={2472-7660},
  month={July},}@ARTICLE{11169748,
  author={Gong, Yifan and Shi, Kaiting and Niu, Xiaolong and Yang, Lijun and Yang, Xiaohui and Zheng, Chen},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Multi-source Discriminant Dynamic Domain Adaptation for Cross-subject Motor Imagery EEG Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Electroencephalography (EEG) has emerged as a widely utilized signal in motor imagery (MI) brain-computer interfaces(BCI) due to its convenience and safety. Recently, deep learning methods have rapidly developed in the field of brain computer interfaces. However, traditional EEG classification methods often face challenges related to limited generalization capability across subjects. To address this issue, this paper proposes a multi-source discriminant dynamic domain adaptation model(MSD-DDA) aimed at fully leveraging domain adaptation to enhance the accuracy of motor imagery classification. The model adeptly handles global and local disparities in motor imagery classification by dynamically minimizing differences between global domain and local subdomain. Furthermore, to ensure discriminability and diversity in the target domain, we introduce batch kernel norm maximization of the difference, thereby enhancing the model's discriminability in the target domain while preserving prediction diversity. To tackle variations in similarity between different source domains and the target domain, we devise a weighted joint prediction mechanism. This mechanism automatically adjusts the contribution weight of each source domain based on its similarity to the target domain, facilitating more precise discriminant prediction and improved adaptability to scenarios with multiple source domains. To evaluate our approach, we conducted a large number of experiments on datasets 1 and 2a of the Fourth BCI Competition and on the openBMI dataset, with average classification accuracy of 92.43%, 79.24% and 71.96%, respectively.Finally, we compare the proposed method with several classical and recent algorithms, and prove that its performance is better than the existing methods.},
  keywords={Electroencephalography;Adaptation models;Brain modeling;Motors;Decoding;Training;Predictive models;Noise;Feature extraction;Accuracy;Electroencephalography;brain-computer interfaces;dynamic domain adaptation;cross-subject;transfer learning},
  doi={10.1109/JBHI.2025.3610446},
  ISSN={2168-2208},
  month={},}@INPROCEEDINGS{10760812,
  author={Wu, Chia-Huan and Zhao, Yu-Xiang and Chou, Cheng-Hou and Hsieh, Yi-Zeng},
  booktitle={2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)}, 
  title={Kinmen Wind Lion Face Generation Based on the Deep Convolutional Generative Adversarial Network}, 
  year={2024},
  volume={},
  number={},
  pages={508-509},
  abstract={Under the advancements of science and technology at present, artificial intelligence has become widely applied in daily life. Hence, deep learning has attracted much attention in recent years and has been widely used in recognition, classification and generation. The results generated by generative adversarial networks have become increasingly realistic and are now being applied to human face generation and reconstruction. Among the various technologies, the deep convolutional generative adversarial network (DCGAN) is more widely used. Based on the trained DCGAN of human and cat faces, this paper collected Kinmen wind lion data by YOLOv3 and crawler technology and transferred the data to the DCGAN of wind lions through network-based transfer learning to generate complete wind lion faces. The experimental process consisted of four parts. The wind lion dataset was directly used to train the DCGAN in the first two parts, and the DCGAN of the human and cat faces was used for transfer learning in the second two parts. According to the results, the training effect of human face-based transfer learning was the best and was the closest to Kinmen’s wind lions.},
  keywords={YOLO;Training;Image resolution;Face recognition;Transfer learning;Training data;Generative adversarial networks;Image restoration;Sun;Image reconstruction;component;formatting;style;styling;insert (key words)},
  doi={10.1109/GCCE62371.2024.10760812},
  ISSN={2693-0854},
  month={Oct},}@ARTICLE{8565937,
  author={Yan, Yichao and Ni, Bingbing and Zhang, Wendong and Xu, Jingwei and Yang, Xiaokang},
  journal={IEEE Transactions on Multimedia}, 
  title={Structure-Constrained Motion Sequence Generation}, 
  year={2019},
  volume={21},
  number={7},
  pages={1799-1812},
  abstract={Video generation is a challenging task due to the extremely high-dimensional distribution of the solution space. Good constraints in the solution domain would thus reduce the difficulty of approximating optimal solutions. In this paper, instead of directly generating high-dimensional video data, we propose using object landmarks as explicit structure constraints to address this issue. Specifically, we propose a two-stage framework for an action-conditioned video generation task. In our framework, the first stage aims to generate landmark sequences according to predefined motion types, and a recurrent model (RNN/LSTM) is adopted for this purpose. The landmark sequence can be regarded as a low-dimensional structure embedding of high-dimensional video data, and generating landmark sequences is much easier than generating videos. The second stage is inspired by a conditional generative adversarial network (CGAN), and we take the generated landmark sequence as a structure condition to learn a landmark-to-image translation network. Such a one-to-one translation framework avoids the difficulty of generating videos and instead transfers the video generation task to image generation, which is resolvable due to the maturity of current GAN-based models. The experimental results demonstrate that our model not only achieves promising results on rigid/nonrigid motion generation tasks but also can be extended to multiobject motion situations.},
  keywords={Task analysis;Gallium nitride;Image generation;Biological system modeling;Computational modeling;Strain;Adaptation models;Motion generation;structure condition;video analysis},
  doi={10.1109/TMM.2018.2885235},
  ISSN={1941-0077},
  month={July},}@ARTICLE{10090450,
  author={Kong, De-Hua and Zhang, Wen-Wei and He, Xiao-Yang and Xia, Ming-Yao},
  journal={IEEE Transactions on Antennas and Propagation}, 
  title={Intelligent Prediction for Scattering Properties Based on Multihead Attention and Target Inherent Feature Parameter}, 
  year={2023},
  volume={71},
  number={6},
  pages={5504-5509},
  abstract={In this communication, an artificial intelligent method based on the prevailing multihead attention mechanism for prediction of scattering properties of 2-D targets is presented. To make the predicting approach independent of the incident direction of an excitation plane wave, a kind of inherent feature parameters (IFPs) for a specific target is defined and applied as the output of the artificial neural network (ANN). Two types of targets are experimented, one of which is composed of polygons and the other of smooth shapes. Numerical results show that the proposed method has satisfactory prediction accuracy and computing speed, as well as good generalization ability.},
  keywords={Scattering;Artificial neural networks;Shape;Feature extraction;Training;Time-domain analysis;Method of moments;Artificial intelligence (AI);machine learning (ML);multihead attention;scattering property prediction;target inherent feature parameters (IFPs)},
  doi={10.1109/TAP.2023.3262341},
  ISSN={1558-2221},
  month={June},}@INPROCEEDINGS{10493489,
  author={Deshmukh, Afif and Kallivalappil, Neave and D'souza, Kyle and Kadam, Chinmay},
  booktitle={2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)}, 
  title={AL-XAI-MERS: Unveiling Alzheimer's Mysteries with Explainable AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Alzheimer's disease poses an escalating global health challenge, necessitating accurate and timely diagnosis for effective intervention. This study presents a novel approach to Alzheimer's detection utilising advanced machine learning techniques applied to brain MRI scans. Leveraging Explainable Artificial Intelligence (XAI) methods, the developed model not only detects Alzheimer's disease but also offers transparent insights into the intricate patterns within the MRI data. In an era where Alzheimer's prevalence is rising, our methodology provides a valuable tool for clinicians and patients. By employing XAI, individuals can gain a comprehensive understanding of their MRI results, enabling them to seek second opinions and fostering a deeper comprehension of their condition. This research marks a significant step towards democratising medical diagnostics, empowering individuals with knowledge and promoting informed decision-making in Alzheimer's diagnosis and management.},
  keywords={Explainable AI;Magnetic resonance imaging;Decision making;Market research;Brain modeling;Data models;Medical diagnosis;Alzheimer's disease;brain MRI scans;machine learning;Explainable Artificial Intelligence (XAI);medical diagnostics;second opinion;transparency;healthcare;disease detection;pattern recognition},
  doi={10.1109/ic-ETITE58242.2024.10493489},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9943879,
  author={Serra, J. and Quezada, R. and Fortes, S. and Tellez, N. and Allaico, A. and Landaverde, E. and Kumar, Y. and Li, J. J. and Morreale, P.},
  booktitle={2022 5th International Conference on Data Science and Information Technology (DSIT)}, 
  title={Validation of AI models for ITCZ Detection from Climate Data}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={The Climate Change topic itself and associated life adjustments became a top global problem of the XXI century. It requires immediate attention and long-term solutions. Physical tracing of weather changes, performed without Artificial Intelligence (AI), is not capable of detection in a timely manner and correctly classifying dangerous situations, which is crucial for making rapid decisions and taking immediate actions on the ground. AI and Machine Learning (ML) systems are currently dominating solutions in Computer Science research fields and the issues of their testing and validation remain a critical open problem due to their uncertain outcomes. We use test automation to validate and assure the quality of various AI systems for one kind of weather prediction. The subject of our study is Inter-Tropical Convergence Zones (ITCZs). ITCZs play an important role in the global circulation system and even small changes in their patterns can cause severe droughts or flooding as well as other disasters like hurricanes. Global warming is causing more ITCZ scenarios as shown in weather data, which makes physical detection infeasible. Our research initially discovered that ITCZ detection based on a physical model alone could misclassify some unexpected situations when the double bands occur at different places with similar intensity or at the same places with different intensities. We then designed experiments to train AI models to detect ITCZs with test automation to collect results. We further trained several AI models with focus on VGG-16, VGG-19, Xception and MobileNETV2 models, collected and compared their results through test automation. Our exhaustive trials eventually achieved a 96.8% accuracy, which might be the best AI model to detect ITCZs with test automation and without human intervention. These results show that test automation can contribute to the selection of optimum AI models.},
  keywords={Automation;Weather forecasting;Machine learning;Predictive models;Hurricanes;Global warming;Floods;Climate change;ITCZ Detection;Artificial Intelligence (AI);Deep Learning (DL);Weather Prediction;Climate Change;Test Automation},
  doi={10.1109/DSIT55514.2022.9943879},
  ISSN={},
  month={July},}@INPROCEEDINGS{10521090,
  author={Labrèche, Georges and Guzman, Cesar and Bammens, Sam},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={Generative AI... in Space! Adversarial Networks to Denoise Images Onboard the OPS-SAT-1 Spacecraft}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  abstract={The camera onboard the European Space Agency’s OPS-SAT-1 spacecraft has been operating past its design life. As the payload ages, there is an increasing risk of sensor degradation leading to corrupted images. This paper evaluates Generative AI models with Wasserstein GANs (WGANs)—an enhanced type of Generative Adversarial Networks—as a noise reduction solution to reconstruct noisy images directly onboard the spacecraft. Autoencoder neural networks are also trained and evaluated for comparative purposes given their common use in noise reduction. Images downlinked from the spacecraft serve as training data to which artificial fixed-pattern noise is applied to simulate sensor degradation. The trained neural networks are uplinked to the spacecraft’s edge computer payload where they are processed by the onboard TensorFlow Lite interpreter to output the reconstructed images. On September 29, 2023, the OPS-SAT-1 mission achieved a significant milestone when it successfully captured, noised, and subsequently denoised two images using WGANs, marking the pioneering first application of Generative AI in space. The restored images have remarkably high structural similarity indices of 0.894 and 0.922—where 1 would indicate that they are identical to their original images. Interestingly, some reconstructed images are more confidently labeled by the onboard convolutional neural network image classifier than their original counterparts. The counterintuitive observation challenges the conventional understanding that higher resolution always yields better results. This suggests that simplifying or modifying certain data features enhances the ability of some models to accurately interpret given inputs.},
  keywords={Space vehicles;Earth;Degradation;Generative AI;Space missions;Noise reduction;Neural networks},
  doi={10.1109/AERO58975.2024.10521090},
  ISSN={1095-323X},
  month={March},}@ARTICLE{10076457,
  author={Hou, Liang},
  journal={IEEE Access}, 
  title={Regularizing Label-Augmented Generative Adversarial Networks Under Limited Data}, 
  year={2023},
  volume={11},
  number={},
  pages={28966-28976},
  abstract={Training generative adversarial networks (GANs) using limited training data is challenging since the original discriminator is prone to overfitting. The recently proposed label augmentation technique complements categorical data augmentation approaches for discriminator, showing improved data efficiency in training GANs but lacks a theoretical basis. In this paper, we propose a novel regularization approach for the label-augmented discriminator to further improve the data efficiency of training GANs with a theoretical basis. Specifically, the proposed regularization adaptively constrains the predictions of the label-augmented discriminator on generated data to be close to the moving averages of its historical predictions on real data, and vice versa. We theoretically establish a connection between the objective function with the proposed regularization and a  $f$ -divergence that is more robust than the previous reversed Kullback-Leibler divergence. Experimental results on various datasets and diverse architectures show the significantly improved data efficiency of our proposed method compared to state-of-the-art data-efficient GAN training approaches for training GANs under limited training data regimes.},
  keywords={Generative adversarial networks;Generators;Data augmentation;Training data;Task analysis;Self-supervised learning;Linear programming;Image generation;Generative adversarial networks;limited data;adaptive regularization;label augmentation;data augmentation;self-supervised learning;image generation},
  doi={10.1109/ACCESS.2023.3259066},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10656747,
  author={Shou, Jiateng and Xiao, Zeyu and Deng, Shiyu and Huang, Wei and Shi, Peiyao and Zhang, Ruobing and Xiong, Zhiwei and Wu, Feng},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Large-Factor EM Image Super-Resolution with Generative Priors}, 
  year={2024},
  volume={},
  number={},
  pages={11313-11322},
  abstract={As the mainstream technique for capturing images of biological specimens at nanometer resolution, electron microscopy (EM) is extremely time-consuming for scanning wide field-of-view (FOV) specimens. In this paper, we investigate a challenging task of large-factor EM image super-resolution (EMSR), which holds great promise for reducing scanning time, relaxing acquisition conditions, and expanding imaging FOV. By exploiting the repetitive structures and volumetric coherence of EM images, we propose the first generative learning-based framework for large-factor EMSR. Specifically, motivated by the predictability ofrepetitive structures and textures in EM images, we first learn a discrete codebook in the latent space to represent highresolution (HR) cell-specific priors and a latent vector indexer to map low-resolution (LR) EM images to their corresponding latent vectors in a generative manner. By incorporating the generative cell-specific priors from HR EM images through a multi-scale prior fusion module, we then deploy multi-image feature alignment and fusion to further exploit the inter-section coherence in the volumetric EM data. Extensive experiments demonstrate that our proposed framework outperforms advanced single-image and video super-resolution methods for 8× and 16× EMSR (i.e., with 64 times and 256 times less data acquired, respectively), achieving superior visual reconstruction quality and down-stream segmentation accuracy on benchmark EM datasets. Code is available at https://github.com/jtshou/GPEMSR.},
  keywords={Image segmentation;Visualization;Scanning electron microscopy;Accuracy;Superresolution;Coherence;Vectors;Super-resolution;EM images;Generative priors},
  doi={10.1109/CVPR52733.2024.01075},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9645575,
  author={Li, Jiahao and Sun, Bin and Li, Shutao and Kang, Xudong},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Semisupervised Semantic Segmentation of Remote Sensing Images With Consistency Self-Training}, 
  year={2022},
  volume={60},
  number={},
  pages={1-11},
  abstract={Semisupervised semantic segmentation is an effective way to reduce the expensive manual annotation cost and take advantage of the unlabeled data for remote sensing (RS) image interpretation. Recent related research has mainly adopted two strategies: self-training and consistency regularization. Self-training tries to acquire accurate pseudo-labels to explicitly expand the train set. However, the existing methods cannot accurately identify false pseudo-labels, suffering from their negative impact on model optimization. The consistency regularization constrains the model by producing consistent predictions robust to the perturbations introduced in the sample or feature domain but requires a sufficient number of training data. Therefore, we propose a strategy for the semisupervised semantic segmentation of the RS images. The proposed model in the generative adversarial network (GAN) framework is optimized by consistency self-training, learning the distributions of both labeled and unlabeled data. The discriminator is optimized by accurate pixel-level training labels instead of the image-level ones, thereby assessing the confidence for the prediction of each pixel, which is then used to reweight the loss of the unlabeled data in self-training. The generator is optimized with the consistency constraint with respect to all random perturbations on the unlabeled data, which increases the sample diversity and prompts the model to learn the underlying distribution of the unlabeled data. Experimental results on the the large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) datasets and the International Society for Photogrammetry and Remote Sensing (ISPRS) datasets show that our framework outperforms several state-of-the-art semisupervised semantic segmentation methods.},
  keywords={Semantics;Predictive models;Image segmentation;Generative adversarial networks;Perturbation methods;Training;Semisupervised learning;Consistency self-training;generative adversarial network (GAN);remote sensing (RS) image;semantic segmentation;semisupervised learning},
  doi={10.1109/TGRS.2021.3134277},
  ISSN={1558-0644},
  month={},}@ARTICLE{9363513,
  author={Gao, Peng and Tian, Tian and Li, Linfeng and Ma, Jiayi and Tian, Jinwen},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={DE-CycleGAN: An Object Enhancement Network for Weak Vehicle Detection in Satellite Images}, 
  year={2021},
  volume={14},
  number={},
  pages={3403-3414},
  abstract={Vehicle detection is a very important application of remote sensing. However, suffering from the low acutance and insufficient color information, the detection of weak vehicles in satellite imagery still remains a challenge. Image enhancement can improve the visual effects of remote sensing images. Nevertheless, most existing image enhancement methods aim to improve the quality of the entire image without target guidance, which have ambiguous contributions to the detection performance. Methods based on generative adversarial networks (GANs) have realized image enhancement with target guidance by the addition of target-guided branches, but paired training data is not available in some scenarios. In this article, a novel model of detection-guided CycleGAN (DE-CycleGAN) is proposed to enhance the weak targets for the purpose of accurate vehicle detection, where a backbone GAN with a target-guided branch is learned in the absence of paired images. Specifically, enhancements of two levels are mutually executed. At the image level, the color information of the entire satellite image is enriched by refined CycleGAN, and its sharpness is enhanced by the gradient enhancement model. At the object level, the target-guided branch for detection is added to enhance features of the target. The experimental results validate that the detection performance has been significantly improved on the images enhanced by the proposed DE-CycleGAN model, which shows a positive effect on weak target detection.},
  keywords={Task analysis;Generative adversarial networks;Image enhancement;Image color analysis;Vehicle detection;Satellites;Object detection;CycleGAN;generative adversarial networks (GANs);object enhancement;target-guided branch;weak vehicle detection},
  doi={10.1109/JSTARS.2021.3062057},
  ISSN={2151-1535},
  month={},}@ARTICLE{10505157,
  author={Wang, Jianing and Guo, Siying and Hua, Zheng and Huang, Runhu and Hu, Jinyu and Gong, Maoguo},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={CL-CaGAN: Capsule Differential Adversarial Continual Learning for Cross-Domain Hyperspectral Anomaly Detection}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={Anomaly detection (AD) has attracted remarkable attention in hyperspectral image (HSI) processing fields, and most existing deep learning (DL)-based algorithms indicate dramatic potential for detecting anomaly samples through specific training process under current scenario. However, the limited prior information and the catastrophic forgetting problem indicate crucial challenges for existing DL structure in open scenarios cross-domain detection. In order to improve the detection performance, a novel continual learning-based capsule differential generative adversarial network (CL-CaGAN) is proposed to elevate the cross-scenario learning performance for facilitating the real application of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule structure with adversarial learning network is constructed to estimate the background distribution for surmounting the deficiency of prior information. To mitigate the catastrophic forgetting phenomenon, clustering-based sample replay strategy and a designed extra self-distillation regularization are integrated for merging the history and future knowledge in continual AD task, while the discriminative learning ability from previous detection scenario to current scenario is retained by the elaborately designed structure with continual learning (CL) strategy. In addition, the differentiable enhancement is enforced to augment the generation performance of the training data. This further stabilizes the training process with better convergence and efficiently consolidates the reconstruction ability of background samples. To verify the effectiveness of our proposed CL-CaGAN, we conduct experiments on several real HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher detection performance and continuous learning capacity for mitigating the catastrophic forgetting under cross-domain scenarios.},
  keywords={Task analysis;Feature extraction;Generative adversarial networks;Vectors;Training;Hyperspectral imaging;Anomaly detection;Continual learning (CL);cross-scene;generative adversarial network (GAN);hyperspectral anomaly detection (HAD);knowledge distillation},
  doi={10.1109/TGRS.2024.3388426},
  ISSN={1558-0644},
  month={},}
