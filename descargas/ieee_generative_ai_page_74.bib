@ARTICLE{8949510,
  author={White, Gary and Clarke, Siobhán},
  journal={IEEE Access}, 
  title={Urban Intelligence With Deep Edges}, 
  year={2020},
  volume={8},
  number={},
  pages={7518-7530},
  abstract={With the increased accuracy available from state of the art deep learning models and new embedded devices at the edge of the network capable of running and updating these models there is potential for urban intelligence at the edge of the network. The physical proximity of these edge devices will allow for intelligent reasoning one hop away from data generation. This will allow a range of modern urban reasoning applications that require reduced latency and jitter such as remote surgery, vehicle collision detection and augmented reality. The traffic flow from IoT devices to the cloud will also be reduced as with the increased accuracy from deep learning models only a subset of the data will need to be reported after a first pass analysis. However, the training time of deep learning models can be long, taking weeks on multiple desktop GPUs for large datasets. In this paper we show how transfer learning can be used to update the last layers of pre-trained models at the edge of the network, dramatically reducing the training time and allowing the model to perform new tasks without data ever having to be sent to the cloud. This will also improve the users' privacy, which is a key requirement for urban intelligence applications with the introduction of GDPR. We compare our approach to alternative IoT urban intelligence architectures such as cloud-based architectures and deep learning algorithms trained only on local data.},
  keywords={Cloud computing;Deep learning;Image edge detection;Augmented reality;Delays;Data models;Computer architecture;Edge computing;transfer learning;deep learning;urban intelligence;IoT;QoS},
  doi={10.1109/ACCESS.2020.2963912},
  ISSN={2169-3536},
  month={},}@ARTICLE{8751969,
  author={Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  journal={IEEE Access}, 
  title={Synthetic Gastritis Image Generation via Loss Function-Based Conditional PGGAN}, 
  year={2019},
  volume={7},
  number={},
  pages={87448-87457},
  abstract={In this paper, a novel synthetic gastritis image generation method based on a generative adversarial network (GAN) model is presented. Sharing medical image data is a crucial issue for realizing diagnostic supporting systems. However, it is still difficult for researchers to obtain medical image data since the data include individual information. Recently proposed GAN models can learn the distribution of training images without seeing real image data, and individual information can be completely anonymized by generated images. If generated images can be used as training images in medical image classification, promoting medical image analysis will become feasible. In this paper, we targeted gastritis, which is a risk factor for gastric cancer and can be diagnosed by gastric X-ray images. Instead of collecting a large amount of gastric X-ray image data, an image generation approach was adopted in our method. We newly propose loss function-based conditional progressive growing generative adversarial network (LC-PGGAN), a gastritis image generation method that can be used for a gastritis classification problem. The LC-PGGAN gradually learns the characteristics of gastritis in gastric X-ray images by adding new layers during the training step. Moreover, the LC-PGGAN employs loss function-based conditional adversarial learning so that generated images can be used as the gastritis classification task. We show that images generated by the LC-PGGAN are effective for gastritis classification using gastric X-ray images and have clinical characteristics of the target symptom.},
  keywords={Image generation;X-ray imaging;Training;Inspection;Gallium nitride;Biomedical imaging;Generative adversarial networks;Generative adversarial network;anonymization;deep learning;data sharing;medical image analysis},
  doi={10.1109/ACCESS.2019.2925863},
  ISSN={2169-3536},
  month={},}@ARTICLE{9098952,
  author={Luan, Xiao and Geng, Hongmin and Liu, Linghui and Li, Weisheng and Zhao, Yuanyuan and Ren, Min},
  journal={IEEE Access}, 
  title={Geometry Structure Preserving Based GAN for Multi-Pose Face Frontalization and Recognition}, 
  year={2020},
  volume={8},
  number={},
  pages={104676-104687},
  abstract={Face frontalization is the process of converting a face image under arbitrary pose to an image with frontal pose. Benefited from significant improvement of generative adversarial networks (GAN), generative models can use face frontalization to overcome the problem of model degradation owing to the variation of head pose in face recognition. Existing GAN based models can generate a synthesis face image with the same identity as the input, while those models are hard to capture the geometry structure or facial patterns via pixel-wise constraint, e.g. face contour. In this paper, we propose a Geometry Structure Preserving based GAN, i.e. GSP-GAN, for multi-pose face frontalization and recognition. The generator of our model takes the form of a typical auto-encoder, where the encoder extracts identity feature and the decoder synthesizes the corresponding frontal face image. In this process, the perception loss constrains the generator to synthesize a face image with the same identity as the input image. Meanwhile, we adopt real frontal face images as extra input data during training process, where a L1 norm loss is utilized to construct a pixel-wise mapping from arbitrary pose image to frontal image. More importantly, for discriminator of our model, we use the self-attention block to preserve the geometry structure of a face. The discriminator consists of a series of parallel sub-discriminators that carry the global and local attention information. Compared with the state-of-the-art models on datasets of Multi-PIE, LFW and CFP, the proposed GSP-GAN can generate high-quality frontal images under arbitrary pose, and get satisfactory recognition performance.},
  keywords={Face;Gallium nitride;Face recognition;Generative adversarial networks;Geometry;Generators;Feature extraction;Face frontalization;pose;generative adversarial networks;self-attention;geometry structure preserving},
  doi={10.1109/ACCESS.2020.2996637},
  ISSN={2169-3536},
  month={},}@ARTICLE{9323059,
  author={Salem Hussin, Saleh Hussin and Yildirim, Remzi},
  journal={IEEE Access}, 
  title={StyleGAN-LSRO Method for Person Re-Identification}, 
  year={2021},
  volume={9},
  number={},
  pages={13857-13869},
  abstract={In this study, the StyleGAN-LSRO method has been developed for person re-identification (re-ID) tasks. This method applies the style-based generative adversarial network (StyleGAN) to generate new synthetic images from existing person re-ID datasets and the label smoothing regularization for outliers (LSRO) algorithm to process those newly produced unlabeled images by assigning them a uniform label distribution along with the definition of a loss function for the training process. A baseline model based on a convolutional neural network (CNN) was developed to learn the discriminative features to recognize a person's identity. The developed method has been tested on three datasets. These datasets are Market-1501, DukeMTMC-reID, and MSMT17. The experimental results show that the StyleGAN model achieved a Fréchet inception distance score of 12.67 and structural similarity score of 0.387, outperforming all the previous generative methods and demonstrating that the images generated by StyleGAN are of superior quality. Adding these StyleGAN-generated data significantly improves the person re-ID accuracy. The StyleGAN-LSRO person re-ID method achieved 98.5% rank-1 accuracy and 91.8% mean average precision (mAP) on Market-1501, 87.0% rank-1 accuracy and 83.8% mAP on DukeMTMC-reID, and 81.5% rank-1 accuracy and 60.9% mAP on MSMT17, respectively. These results show that the StyleGAN-LSRO method significantly outperforms most of the state-of-the-art person re-ID methods. The success rate for person re-ID increases when the images used are of high resolution and square matrix form. In other cases, the success rate decreases.},
  keywords={Generative adversarial networks;Training;Image resolution;Gallium nitride;Task analysis;Generators;Cameras;Convolutional neural networks;deep learning;generative adversarial networks;person re-identification;StyleGAN;label smoothing regularization},
  doi={10.1109/ACCESS.2021.3051723},
  ISSN={2169-3536},
  month={},}@ARTICLE{9893101,
  author={Osahor, Uche and Nasrabadi, Nasser M.},
  journal={IEEE Access}, 
  title={Text-Guided Sketch-to-Photo Image Synthesis}, 
  year={2022},
  volume={10},
  number={},
  pages={98278-98289},
  abstract={We propose a text-guided sketch-to-image synthesis model that semantically mixes style and content features from the latent space of an inverted Generative Adversarial Network (GAN). Our goal is to synthesize plausible images from human facial sketches and their respective text descriptions. In our approach, we adapted a generative model termed Contextual GAN (CT-GAN) that efficiently encodes visual-linguistic semantic features pre-trained on over 400 million text-image pairs at different resolutions along the model. Also, we introduced an intermediate mapping network called c-Map that combines textual and visual-based features to a disentangled latent space  $\mathcal {W{+}}$  for better feature matching. Furthermore to maximise the computational performance of our model, we implemented a linear-based attention scheme along the pipeline of our model to eliminate the drawbacks of inefficient attention modules that are quadratic in complexity. Finally, the hierarchical setting of our model ensures that textual, style and content features are synthesised based on their unique fine grained details, which result in visually appealing images.},
  keywords={Adaptation models;Computational modeling;Semantics;Generative adversarial networks;Generators;Image synthesis;Context modeling;Text processing;Contextual GAN (CT-GAN);generative adversarial network (GAN);text-guided sketch-to-image synthesis},
  doi={10.1109/ACCESS.2022.3206771},
  ISSN={2169-3536},
  month={},}@ARTICLE{10026811,
  author={Jin, Xiaohang and Wang, Hao and Kong, Ziqian and Xu, Zhengguo and Qiao, Wei},
  journal={IEEE Access}, 
  title={Condition Monitoring of Wind Turbine Generators Based on SCADA Data and Feature Transfer Learning}, 
  year={2023},
  volume={11},
  number={},
  pages={9441-9450},
  abstract={In order to build an effective condition monitoring (CM) model for the target wind turbines (WTs) with few operational data, an approach based on the feature transfer learning and a modified generative adversarial network is proposed. First, a large amount of labelled data from WTs are analyzed to construct a CM model with the aid of an autoencoder. This forms the knowledge of CM for WTs in the source domain. Second, a generative adversarial network is trained to build a mapping relationship between the features of different WTs. Third, the health status of the target WT is determined by analyzing the data collected from it online based on the proposed approach. Two case studies are conducted to verify that the proposed method can transfer the CM knowledge from source WT to target WT and achieve good performance in the CM of target WT.},
  keywords={Wind turbines;Generative adversarial networks;Condition monitoring;Training;Generators;Transfer learning;Data models;Encoding;Autoencoder;condition monitoring (CM);feature transfer learning;generative adversarial network (GAN);wind turbine (WT)},
  doi={10.1109/ACCESS.2023.3240306},
  ISSN={2169-3536},
  month={},}@ARTICLE{9969619,
  author={Khan, Zakir and Umar, Arif Iqbal and Shirazi, Syed Hamad and Shahzad, Muhammad and Assam, Muhammad and El-Wakad, Muhammad Tarek I. M. and Attia, El-Awady},
  journal={IEEE Access}, 
  title={Face Recognition via Multi-Level 3D-GAN Colorization}, 
  year={2022},
  volume={10},
  number={},
  pages={133078-133094},
  abstract={Rapid development in sketch-to-image translation methods boosts the investigation procedure in law enforcement agencies. But, the large modality gap between manually generated sketches makes this task challenging. Generative adversarial network (GAN) and encoder-decoder approach are usually incorporated to accomplish sketch-to-image generation with promising results. This paper targets the sketch-to-image translation with heterogeneous face angles and lighting effects using a multi-level conditional generative adversarial network. The proposed multi-level cGAN work in four different phases. Three independent cGANs’ networks are incorporated separately into each stage, followed by a CNN classifier. The Adam stochastic gradient descent mechanism was used for training with a learning rate of 0.0002 and momentum estimates  $\beta $  and  $\beta $  as 0.5 and 0.999, respectively. The multi-level 3D-convolutional architecture help to preserve spatial facial attributes and pixel-level details. The 3D convolution and deconvolution guide the G1, G2 and G3 to use additional features and attributes for encoding and decoding. This helps to preserve the direction, postures of targeted image attributes and special relationships among the whole image’s features. The proposed framework process the 3D-Convolution and 3D-Deconvolution using vectorization. This process takes the same time as 2D convolution but extracts more features and facial attributes. We used pre-trained ResNet-50, ResNet-101, and Mobile-Net to classify generated high-resolution images from sketches. We have also developed, and state-of-the-art Pakistani Politicians Face-sketch Dataset (PPFD) for experimental purposes. Result reveals that the proposed cGAN model’s framework outperforms with respect to Accuracy, Structural similarity index measure (SSIM), Signal to noise ratio (SNR), and Peak signal-to-noise ratio (PSNR).},
  keywords={Face recognition;Facial features;Generative adversarial networks;Image color analysis;Feature extraction;Generators;Databases;Convolutional neural network;generative adversarial network;sketch-to-Image translation;machine learning},
  doi={10.1109/ACCESS.2022.3226453},
  ISSN={2169-3536},
  month={},}@ARTICLE{9856669,
  author={Shi, Yong and Shang, Mengyu and Qi, Zhiquan},
  journal={IEEE Access}, 
  title={A Conditional Deep Framework for Automatic Layout Generation}, 
  year={2022},
  volume={10},
  number={},
  pages={86092-86100},
  abstract={Automatic layout generation, which means making computers enjoy creativity, is difficult yet exciting work. Up to now, how to generate reasonable and visually appealing layouts remains a complex challenge. In this paper, we propose a novel layout generation model based on Conditional Generative Adversarial Networks (L-CGAN), which can generate layouts simply and efficiently by positioning, scaling, and flipping the given primitives. To break the bottleneck of limitation of the fixed input size of Generative Adversarial Networks, we develop a pre-processing algorithm to enable the model to generate layouts with an unrestricted number of input elements. Moreover, a graph-constraint module is proposed to guide layout optimization. We demonstrate the competitive performance of our designs in diverse data domains such as handwriting digit layout generation (MNIST Layouts), scene layout generation (AbstractScene-Layouts), and document layout generation (PubLayNet).},
  keywords={Layout;Image analysis;Generative adversarial networks;Deep learning;Data models;Image synthesis;Transformers;Deep learning;layout generation;conditional generative adversarial networks;abstract scene layout},
  doi={10.1109/ACCESS.2022.3198686},
  ISSN={2169-3536},
  month={},}@ARTICLE{9410544,
  author={Adnan, Risman and Saputra, Muchlisin Adi and Fadlil, Junaidillah and Ezerman, Martianus Frederic and Iqbal, Muhamad and Basaruddin, Tjan},
  journal={IEEE Access}, 
  title={Convergence of Non-Convex Non-Concave GANs Using Sinkhorn Divergence}, 
  year={2021},
  volume={9},
  number={},
  pages={67595-67609},
  abstract={Sinkhorn divergence is a symmetric normalization of entropic regularized optimal transport. It is a smooth and continuous metrized weak-convergence with excellent geometric properties. We use it as an alternative for the minimax objective function in formulating generative adversarial networks. The optimization is defined with Sinkhorn divergence as the objective, under the non-convex and non-concave condition. This work focuses on the optimization's convergence and stability. We propose a first order sequential stochastic gradient descent ascent (SeqSGDA) algorithm. Under some mild approximations, the learning converges to local minimax points. Using the structural similarity index measure (SSIM), we supply a non-asymptotic analysis of the algorithm's convergence rate. Empirical evidences show a convergence rate, which is inversely proportional to the number of iterations, when tested on tiny colour datasets Cats and CelebA on the deep convolutional generative adversarial networks and ResNet neural architectures. The entropy regularization parameter ε is approximated to the SSIM tolerance ϵ . We determine that the iteration complexity to return to an ϵ-stationary point to be O(κ log(ϵ-1)), where κ is a value that depends on the Sinkhorn divergence's convexity and the minimax step ratio in the SeqSGDA algorithm.},
  keywords={Optimization;Gallium nitride;Convergence;Approximation algorithms;Generators;Generative adversarial networks;Stability criteria;Convergence;generative adversarial networks;optimal transport;Sinkhorn divergence},
  doi={10.1109/ACCESS.2021.3074943},
  ISSN={2169-3536},
  month={},}@ARTICLE{10568093,
  author={Hong, Tzung-Pei and Wu, Jin-Hang and Su, Ja-Hwung and Yin, Tang-Kai},
  journal={IEEE Access}, 
  title={Conditional-GAN-Based Face Inpainting Approaches With Symmetry and View-Degree Utilization}, 
  year={2024},
  volume={12},
  number={},
  pages={87467-87478},
  abstract={Recently, image inpainting has been proposed as a solution for restoring the polluted image in the field of computer vision. Further, face inpainting is a subfield of image inpainting, which refers to a set of image editing algorithms re-conducting the missing regions in face smoothly. Actually, face inpainting is more challenging than general image inpainting because it needs more face structure information. Although a number of past studies were proposed for face inpainting by using face segmentation, face edge and face topology, there is some important information ignored, such as geometric and symmetric properties. Based on such concepts, in this paper, we propose a two-stage face inpainting method called CGAN (Conditional Generative Adversarial Network) which integrates face landmarks and Generative Adversarial Network (called GAN). In the first stage, the face landmark is predicted as the condition, providing GAN with important information of geometry and symmetry. The main idea in this stage is to dynamically adjust the loss by the proposed view degree. Accordingly, the masked face image and the corresponding face landmark are used as conditions input to the GAN in the second stage. Finally, the missing-regions are inpainted by the proposed CGAN. To reveal the effectiveness of proposed method, a number of evaluations were conducted on real datasets. The experimental results show that, the proposed method predicts a better face landmark by information of geometric structures and symmetric outlooks, and thereupon the proposed CGAN reconstructs the missing regions superior to the compared methods.},
  keywords={Generative adversarial networks;Image restoration;Image reconstruction;Facial features;Vectors;Generators;Face recognition;Encoding;Face inpainting;face-landmark;generative adversarial networks;deep learning;autoencoder},
  doi={10.1109/ACCESS.2024.3417442},
  ISSN={2169-3536},
  month={},}@ARTICLE{11082149,
  author={Jabbar, Sohail and Raza, Umar and Asif Habib, Muhammad and Farhan, Muhammad and Saeed, Saqib},
  journal={IEEE Access}, 
  title={Automated Cardiac Disease Prediction Using Composite GAN and DeepLab Model}, 
  year={2025},
  volume={13},
  number={},
  pages={138313-138327},
  abstract={Cardiovascular diseases remain the leading global cause of mortality, resulting in over 17 million deaths annually. Manual cardiac image interpretation is often subjective and varies significantly among clinicians. However, constraints like limited annotation and model generalization persist. We introduce GenDeep, a novel framework integrating an unsupervised Generative Adversarial Network (GAN) and DeepLab model for robust cardiac pathology classification from cine-MRI scans. The GAN component performs data augmentation to synthesize realistic pathological imagery, overcoming dataset constraints. Meanwhile, the DeepLab segmentation network exploits inter-slice spatial contexts for precise anatomical quantification. GenDeep is trained on over 4000 expert-annotated scans from the ACDC dataset, leveraging Apache Spark and Hadoop for efficient parallel data loading and preprocessing. The Generator maps noise vectors to synthetic MRIs while the Discriminator predicts disease labels and classifies images as real/fake. Weights are updated through backpropagation to refine image realism and classification accuracy. Once trained, the Generator produces additional pathological data to boost model generalization. The Discriminator then serves as the diagnostic classifier based on ventricular morphology from DeepLab segmentation. Extensive comparative testing on a held-out test set achieves 97% accuracy and 93% F1 Score, significantly exceeding benchmarks. Smooth convergence is verified with a low 2.21 MSE. These results highlight the effective integration of generative learning and segmentation for automated and reliable cardiac diagnosis.},
  keywords={Diseases;Heart;Accuracy;Machine learning;Image segmentation;Generative adversarial networks;Medical diagnostic imaging;Deep learning;Data models;Magnetic resonance imaging;Cardiovascular diseases;generative adversarial networks;DeepLab;semantic segmentation;deep learning;cardiac MRI analysis;disease detection;heart disease},
  doi={10.1109/ACCESS.2025.3589529},
  ISSN={2169-3536},
  month={},}@ARTICLE{10937172,
  author={Casas, Llogari and Hannah, Samantha and Mitchell, Kenny},
  journal={IEEE Computer Graphics and Applications}, 
  title={HoloJig: Interactive Spoken Prompt Specified Generative AI Environments}, 
  year={2025},
  volume={45},
  number={2},
  pages={69-77},
  abstract={HoloJig offers an interactive, speech-to-virtual reality (VR), VR experience that generates diverse environments in real time based on live spoken descriptions. Unlike traditional VR systems that rely on prebuilt assets, HoloJig dynamically creates personalized and immersive virtual spaces with depth-based parallax 3-D rendering, allowing users to define the characteristics of their immersive environment through verbal prompts. This generative approach opens up new possibilities for interactive experiences, including simulations, training, collaborative workspaces, and entertainment. In addition to speech-to-VR environment generation, a key innovation of HoloJig is its progressive visual transition mechanism, which smoothly dissolves between previously generated and newly requested environments, mitigating the delay caused by neural computations. This feature ensures a seamless and continuous user experience, even as new scenes are being rendered on remote servers.},
  keywords={Generative AI;Virtual environments;Rendering (computer graphics);Real-time systems;Visualization;Training;Speech to text;Artificial intelligence;Three-dimensional displays;Speech processing;Virtual reality},
  doi={10.1109/MCG.2025.3553780},
  ISSN={1558-1756},
  month={March},}@ARTICLE{10884717,
  author={Maiden, Neil and Zachos, Konstantinos and Petrianakis, Kostas and Lockerbie, James and Chanpalangsri, Chantakan and Ernst, Holger and Kara, Seyfettin},
  journal={IEEE Software}, 
  title={Beyond Just Generative AI for Discovering Software Opportunities}, 
  year={2025},
  volume={42},
  number={3},
  pages={41-49},
  abstract={INSIGHTS invents creative requirements from large volumes of project-related information. Two workshops demonstrated its potential to go beyond generative AI chatbots by combining technologies that operationalize different characteristics of an integrated model of creative processes and outcomes.},
  keywords={Artificial intelligence;Training;Customer services;Data mining;Stakeholders;Smart devices;Chatbots;Transforms;Creativity;Project management;Generative AI},
  doi={10.1109/MS.2025.3540672},
  ISSN={1937-4194},
  month={May},}@INPROCEEDINGS{10425480,
  author={R, Mahesh T and Krishna, Meena},
  booktitle={2023 2nd International Conference on Futuristic Technologies (INCOFT)}, 
  title={Generative Adversarial Networks for Self-Supervised Transfer Learning in Medical Image Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Self-supervised transfer mastering for clinical picture analysis is a method that uses deep getting-to-know procedures to research large units of medical imaging facts without using guide labels. By way of using switch getting to know, the version can come across diffused patterns from the facts that are not easily located with the aid of the medical doctors or researchers. This method can diffuse clinical imaging packages consisting of type, segmentation, and item detection. The self-supervised transfer studying technique involves educating an artificial intelligence (AI) model on a set of scientific picas with available labels. The version is then tested on a set of clinical pix that have not been visible before, and the AI version then adjusts its parameters to come across styles and functions that the labels may have neglected. The advantage of the use of self-supervised transfer getting to know for medical image analysis is that it gets rid of the need to spend time creating labels for the photos and, as a substitute, allows the model to study themselves. Further, the model can detect small capabilities and patterns that may be ignored through manual labeling, leading to more excellent correct outcomes…},
  keywords={Transfer learning;Switches;Transforms;Medical services;Generators;Artificial intelligence;Biomedical imaging;Performance;medical;classification;present;supervised},
  doi={10.1109/INCOFT60753.2023.10425480},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10737603,
  author={Ghanbari, Saba Salmani and Mousavi, Milad and Pouria, Maleki and Amir, Mosavi and Choubin, Bahram},
  booktitle={2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY)}, 
  title={Flood Risk Analysis with Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={000341-000346},
  abstract={Predicting flood hazard risk is crucial for reducing potential damage to infrastructures. This study uses machine learning algorithms to improve the accuracy of flood hazard risk predictions. We compare the performance of Long Short-Term Memory (LSTM) networks, Random Forest (RF), and Support Vector Machine (SVM) algorithms using a dataset to assess their effectiveness in predicting flood risk. The findings reveal that RF along with LSTM are the most accurate methods. These findings highlight the potential of machine learning algorithms, particularly RF and LSTM, in enhancing flood hazard risk analysis which offers valuable insights for risk mitigation strategies and infrastructure planning.},
  keywords={Radio frequency;Machine learning algorithms;Accuracy;Computational modeling;Predictive models;Data models;Mathematical models;Floods;Long short term memory;Context modeling;Risk analysis;flood hazard;flood risk assessment;hydrology;hydrological model;artificial intelligence;machine learning;geophysics;mathematics;deep learning;data science;big data;generative AI;data mining;applied artificial intelligence;applied mathematics;applied informatics;information systems;soft computing;geoscience;earth science;earth systems;extreme events;climate change;natural hazards},
  doi={10.1109/SISY62279.2024.10737603},
  ISSN={1949-0488},
  month={Sep.},}@INPROCEEDINGS{10565703,
  author={Cotino Arbelo, Andrea E. and González-González, Carina S. and Molina Gil, Jezabel M.},
  booktitle={2023 XIII International Conference on Virtual Campus (JICV)}, 
  title={Embracing the Future: Unveiling the Revolution of Human-AI Interaction in the Digital Education Era}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={The recent and emerging introduction of generative AI in all areas of society poses new challenges to be overcome, particularly in the educational setting. The intimate and affective relationship that may develop between students and generative AI devices raises critical questions about the psychological and emotional impact of such interactions. This issue becomes particularly significant when the interactions with AI-based systems begin at an early age, and these systems assume roles as digital tutors, digital secretaries, motivator agents, and/or mentors. The aim of this work in progress was to address a recent challenge within the digital educational context: the psychological and emotional impact that the introduction of AI-based systems in digital education may have on students. However, we have faced limitations in terms of data availability and resources, particularly when addressing the issue from early ages, where there is a notable lack of studies. Although the results may not have been significant, the research has provided a general overview and a solid theoretical foundation for future studies in this field. Therefore, we suggest conducting more rigorous studies with representative samples and complete data, alongside developing reliable and validated methodologies and assessment tools to address the issues involved.},
  keywords={Generative AI;Education;Knowledge based systems;Psychology;Medical services;Reliability theory;Solids;Human-AI Interaction;Artificial Intelligence;Human-Computer Interaction;Affective Computing;Digital Education},
  doi={10.1109/JICV59748.2023.10565703},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9796455,
  author={de Santiago Júnior, Valdivino Alexandre},
  booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={A Method and Experiment to evaluate Deep Neural Networks as Test Oracles for Scientific Software}, 
  year={2022},
  volume={},
  number={},
  pages={40-51},
  abstract={Testing scientific software is challenging because usually such type of systems have non-deterministic behaviours and, in addition, they generate non-trivial outputs such as images. Artificial intelligence (AI) is now a reality which is also helping in the development of the software testing activity. In this article, we evaluate seven deep neural networks (DNNs), precisely deep convolutional neural networks (CNNs) with up to 161layers, playing the role of test oracle procedures for testing scientific models. Firstly, we propose a method, TOrC, which starts by generating training, validation, and test image datasets via combinatorial interaction testing applied to the original codes and second-order mutants. Within TOrC we also have classical steps such as transfer learning, a technique recommended for DNNs. Then, we verified the performance of the oracles (CNNs). The main conclusions of this research are: i) not necessarily a greater number of layers means that a CNN will present better performance; ii) transfer learning is a valuable technique but eventually we may need extended solutions to get better performances; iii) data-centric AI is an interesting path to follow; and iv) there is not a clear correlation between the software bugs, in the scientific models, and the errors (image misclassifications) presented by the CNNs. CCS CONCEPTS • Software and its engineering → Software testing and debugging;. Computing methodologies → Neural networks; Supervised learning by classification; Computer vision.},
  keywords={Software testing;Deep learning;Training;Correlation;Computational modeling;Transfer learning;Computer bugs;Test Oracles;Deep Convolutional Neural Networks;Transfer Learning;Explainable Artificial Intelligence;Data-Centric Artificial Intelligence},
  doi={10.1145/3524481.3527232},
  ISSN={},
  month={May},}@INPROCEEDINGS{10973516,
  author={Raganato, Alessandro and Peñaloza, Rafael and Viviani, Marco and Pasi, Gabriella},
  booktitle={2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={Reasoning Capabilities and Invariability of Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={125-132},
  abstract={Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.},
  keywords={Knowledge engineering;Generative AI;Large language models;Psychology;Focusing;Benchmark testing;Cognition;Natural language processing;Intelligent agents;Standards;Natural Language Processing;Knowledge Rep-resentation and Reasoning;LLM benchmark;Generative AI},
  doi={10.1109/WI-IAT62293.2024.00025},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10205207,
  author={Huang, Siyuan and Wang, Zan and Li, Puhao and Jia, Baoxiong and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Zhu, Song-Chun},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Diffusion-based Generation, Optimization, and Planning in 3D Scenes}, 
  year={2023},
  volume={},
  number={},
  pages={16750-16761},
  abstract={We introduce the SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate the SceneDiffuser on various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding.},
  keywords={Solid modeling;Three-dimensional displays;Navigation;Noise reduction;Path planning;Planning;Pattern recognition;Scene analysis and understanding},
  doi={10.1109/CVPR52729.2023.01607},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9211716,
  author={Yang, Xinyu and Zhang, Yuan and Lo, Benny and Wu, Dongrui and Liao, Hongen and Zhang, Yuan-Ting},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={DBAN: Adversarial Network With Multi-Scale Features for Cardiac MRI Segmentation}, 
  year={2021},
  volume={25},
  number={6},
  pages={2018-2028},
  abstract={With the development of medical artificial intelligence, automatic magnetic resonance image (MRI) segmentation method is quite desirable. Inspired by the power of deep neural networks, a novel deep adversarial network, dilated block adversarial network (DBAN), is proposed to perform left ventricle, right ventricle, and myocardium segmentation in short-axis cardiac MRI. DBAN contains a segmentor along with a discriminator. In the segmentor, the dilated block (DB) is proposed to capture, and aggregate multi-scale features. The segmentor can produce segmentation probability maps while the discriminator can differentiate the segmentation probability map, and the ground truth at the pixel level. In addition, confidence probability maps generated by the discriminator can guide the segmentor to modify segmentation probability maps. Extensive experiments demonstrate that DBAN has achieved the state-of-the-art performance on the ACDC dataset. Quantitative analyses indicate that cardiac function indices from DBAN are similar to those from clinical experts. Therefore, DBAN can be a potential candidate for short-axis cardiac MRI segmentation in clinical applications.},
  keywords={Image segmentation;Magnetic resonance imaging;Convolution;Training;Informatics;Kernel;Biomedical imaging;Cardiac MRI;Medical Image Processing;Automatic Segmentation Method;Adversarial Network},
  doi={10.1109/JBHI.2020.3028463},
  ISSN={2168-2208},
  month={June},}@ARTICLE{9388890,
  author={Nie, Lihai and Zhao, Laiping and Li, Keqiu},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Robust Anomaly Detection Using Reconstructive Adversarial Network}, 
  year={2021},
  volume={18},
  number={2},
  pages={1899-1912},
  abstract={Detecting abnormal service performance is significant for Internet-based service management and operation. Recent advances in anomaly detection methods prefer unsupervised learning algorithms since they can work without manually labelled data. However, existing unsupervised methods converge into suboptimal solutions due to their heuristic-based objectives. Moreover, they frequently rely on the strong assumption that noise follows a Gaussian distribution, and their detection accuracy is also highly sensitive to threshold settings. To detect anomalies precisely and robustly, we present Adran, an unsupervised anomaly detection model that introduces adversarial learning into a reconstructive model, generating a reconstructive adversarial network with an anomaly detection-based training objective. It tolerates non-Gaussian noise by activating the discriminator with a non-smooth function. Our experimental results demonstrate that Adran achieves an improvement of  $\geq 32\%$  over the state-of-the-art methods in terms of F-score. Moreover, the robustness analysis demonstrates that it is reasonably easy and straightforward to set an appropriate threshold using Adran.},
  keywords={Training;Data models;Anomaly detection;Generative adversarial networks;Gallium nitride;Key performance indicator;Gaussian distribution;Anomaly detection;reconstructive adversarial network;performance diagnose},
  doi={10.1109/TNSM.2021.3069225},
  ISSN={1932-4537},
  month={June},}@INPROCEEDINGS{10403147,
  author={Kalantar, Reza and Lin, Gigin and Winfield, Jessica M and Messiou, Christina and Koh, Dow-Mu and Blackledge, Matthew D},
  booktitle={2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={MED-INPAINT: Medical Image Synthesis Using Multi-Level Conditional Inpainting with a Denoising Diffusion Probabilistic Model and Adaptive Contrast Priors}, 
  year={2023},
  volume={},
  number={},
  pages={403-413},
  abstract={Denoising diffusion probabilistic models (DDPMs) have shown promise for generating high-resolution synthetic images. In medical imaging, there is a growing demand for both realistic image synthesis and deterministic outcomes that can guide downstream applications effectively. In this study, we propose MED-INPAINT, an adaptable multi-level conditional DDPM framework. MED-INPAINT incorporates contrast priors for accelerated sampling and performs inpainting of pelvic magnetic resonance imaging (MRI) scans, enabling high-quality image synthesis with reasonably low uncertainty. Our results highlight the effectiveness of MED-INPAINT in generating realistic and detailed pelvic MRI images, assessing its uncertainty using various denoising steps at inference. MED-INPAINT outperformed baseline U-Net and cycle-consistent generative adversarial network (Cycle-GAN) models, demonstrating its potential for various medical imaging applications.},
  keywords={Adaptation models;Uncertainty;Image synthesis;Magnetic resonance imaging;Noise reduction;Probabilistic logic;Biomedical imaging;Artificial Intelligence;Deep Learning;Denoising Diffusion Probabilistic Model;Medical Image Synthesis;Inpainting;Magnetic Resonance Imaging},
  doi={10.1109/MedAI59581.2023.00061},
  ISSN={},
  month={Nov},}@ARTICLE{11002477,
  author={Zhang, Jun and Zhang, Yunhua},
  journal={IEEE Access}, 
  title={Color Consistency Anime Style Transfer Based on Hybrid Structural Decomposition Network}, 
  year={2025},
  volume={13},
  number={},
  pages={83335-83347},
  abstract={In the anime industry, character design frequently encounters challenges such as oversimplification and homogenization, which severely restrict the diversity and creativity of character appearances. To tackle these issues, we introduce a Hybrid Structural Decomposition Network (HSDN) that aims to generate anime characters with a realistic style by learning the expressive techniques of traditional anime, thereby providing valuable inspiration and references for designers. The proposed HSDN utilizes an encoder to separately extract structural and color features from input images. The structural features are then processed by our style transfer sub-network, which is based on diffusion models. Subsequently, the structural and color features are fused at multiple scales, and a specially designed decoder is employed to generate the final style-transferred image. Additionally, we have incorporated specific skip connections to mitigate local detail loss during image generation and to enhance the stability of the diffusion process. Experimental results demonstrate that our proposed model not only visually generates anime characters with realistic styles but also achieves superior performance in terms of Style FID and Semantic FID compared with state-of-the-art methods.},
  keywords={Noise;Noise reduction;Image color analysis;Feature extraction;Diffusion models;Diffusion processes;Neural networks;Image synthesis;Generative adversarial networks;Visualization;Style transfer;structural decomposition;low-rank adaptation algorithm;feature fusion;skip connection},
  doi={10.1109/ACCESS.2025.3569326},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10451267,
  author={Choudhary, Sunila},
  booktitle={2023 International Conference on Power Energy, Environment & Intelligent Control (PEEIC)}, 
  title={An Inclusive Review on Recent Trends, Advances and Emerging Areas within Deep Learning Community}, 
  year={2023},
  volume={},
  number={},
  pages={39-45},
  abstract={Deep learning, an innovative discipline, has effectively tackled previously considered impossible obstacles, particularly in the automated identification of data patterns, frequently surpassing human precision. It surpasses conventional machine learning, enticing specialists who are struggling with the immense amount of data. With the increase in public awareness, there is also a corresponding increase in the desire to utilize deep learning. Nevertheless, manoeuvring over this complex terrain might be intimidating. This overview provides a comprehensive exploration of fundamental multilayer artificial neural networks in the field of deep learning. The study examines the use of multi-agent techniques to optimize architecture automatically. The review examines the increasing significance of system uptime and provides analysis on the utilization of neural networks for identifying and resolving faults. Furthermore, it examines innovative uses in anomaly detection, “financial time-series forecasting, predictive analytics, medical image analysis, and power systems research.”},
  keywords={Deep learning;Image resolution;Image analysis;Reviews;Artificial neural networks;Nonhomogeneous media;Market research;Deep Neural Network Architectures;Evolutionary Computation;Applications of Deep Learning;Supervised and Unsupervised Learning;Testing Neural Networks},
  doi={10.1109/PEEIC59336.2023.10451267},
  ISSN={},
  month={Dec},}@ARTICLE{11016950,
  author={Fu, Meijun and Wang, Xiaomin and Wang, Jun and Yi, Zhang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Synthetic Gradient Optimization-Based Implicit Amortized Bayesian Meta-Learning for Few-Shot Pumi Spectrographic Image Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Meta-learning provides a promising solution to the issue of insufficient training samples in Pumi spectrogram recognition. However, capturing model uncertainty remains a critical challenge, particularly for tasks influenced by lexical ambiguities. To overcome this problem, we propose a novel method, Synthetic Gradient Optimization-Based Implicit Amortized Bayesian Meta-Learning (SGO-IABML), which captures model uncertainty by evaluating posterior distributions within a hierarchical Bayesian framework, thereby facilitating few-shot Pumi spectrogram recognition. Specifically, SGO-IABML reformulates meta-learning as a bi-level variational inference problem, leveraging information bottleneck principles. At the lower level, a generative inference module is developed to implicitly model task-specific variational posteriors, thereby enhancing the model’s expressiveness. Given the lack of analytical forms for implicit distributions, we derive the Fenchel-Bayesian Bound Theorem to measure the divergence between arbitrary distributions. For the meta-learning of variational parameters, SGO-IABML constructs a synthetic gradient optimizer, integrating prior gradient information to facilitate rapid adaptation to new tasks. At the upper level, the model is calibrated by estimating the local geometry of the posterior distribution, utilizing the Generalized Gauss-Newton Matrix to capture the directional sensitivity of the loss function. Comprehensive experimental results on Pumi spectrograms demonstrate that SGO-IABML achieves state-of-the-art performance in generalization, calibration, expressiveness, versatility, and cross-domain adaptability. Furthermore, ablation studies confirm the contribution of each component to the overall performance improvement.},
  keywords={Metalearning;Bayes methods;Adaptation models;Uncertainty;Computational modeling;Spectrogram;Matrix decomposition;Training;Optimization;Integrated circuit modeling;Meta-Learning;Information Bottleneck;Implicit Modeling;Fenchel-Bayesian Bound Theorem;Synthetic Gradient optimizer;Generalized Gauss-Newton Matrix},
  doi={10.1109/TCSVT.2025.3574567},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{11035637,
  author={Guo, Zhehao and Lang, Jiedong and Huang, Shuyu and Gao, Yunfei and Ding, Xintong},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={A Comprehensive Review on Noise Control of Diffusion Model}, 
  year={2025},
  volume={},
  number={},
  pages={01-07},
  abstract={Diffusion models have recently emerged as powerful generative frameworks for producing high-quality images. A pivotal component of these models is the noise schedule, which governs the rate of noise injection during the diffusion process. Since the noise schedule substantially influences sampling quality and training quality, understanding its design and implications is crucial. In this discussion, various noise schedules are examined, and their distinguishing features and performance characteristics are highlighted.},
  keywords={Training;Seminars;Schedules;Reviews;Noise;Diffusion processes;Diffusion models;Stability analysis;Information technology;Diffusion Model;Machine Learning;Artificial Intelligence},
  doi={10.1109/AINIT65432.2025.11035637},
  ISSN={},
  month={April},}@ARTICLE{9794684,
  author={Veres, Csaba},
  journal={IEEE Access}, 
  title={Large Language Models are Not Models of Natural Language: They are Corpus Models}, 
  year={2022},
  volume={10},
  number={},
  pages={61970-61979},
  abstract={Natural Language Processing (NLP) has become one of the leading application areas in the current Artificial Intelligence boom. Transfer learning has enabled large deep learning neural networks trained on the language modeling task to vastly improve performance in almost all downstream language tasks. Interestingly, when the language models are trained with data that includes software code, they demonstrate remarkable abilities in generating functioning computer code from natural language specifications. We argue that this creates a conundrum for the claim that eliminative neural models are a radical restructuring in our understanding of cognition in that they eliminate the need for symbolic abstractions like generative phrase structure grammars. Because the syntax of programming languages is by design determined by phrase structure grammars, neural models that produce syntactic code are apparently uninformative about the theoretical foundations of programming languages. The demonstration that neural models perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an argument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that the term language model is misleading and propose the adoption of the working term corpus model instead, which better reflects the genesis and contents of the model.},
  keywords={Grammar;Linguistics;Deep learning;Computational modeling;Syntactics;Production;Task analysis;Natural language processing;deep learning;syntax;linguistics;language model;automatic programming;neural networks},
  doi={10.1109/ACCESS.2022.3182505},
  ISSN={2169-3536},
  month={},}@ARTICLE{11023216,
  author={Li, He and Iwamoto, Yutaro and Han, Xianhua and Lin, Lanfen and Tong, Ruofeng and Hu, Hongjie and Kanasaki, Shuzo and Chen, Yen-Wei},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Weakly-Supervised Liver Lesion Detection in CT Images Using Adversarial Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Liver lesion detection by using deep learning techniques is a challenging research topic. Several existing methods apply supervised learning, wherein predictive models are constructed by learning from annotated lesion training samples. Acquiring professional guidance information is difficult due to the expensive nature of annotating data. Meanwhile, most unsupervised learning schemes that only use unlabeled data (without annotated lesions) during training exhibit low accuracy in detecting lesions. Therefore, we present a weakly-supervised learning framework for liver lesion detection and segmentation, which improves accuracy compared to traditional unsupervised methods. Our framework uses generative adversarial networks to translate abnormal data corresponding to lesions into normal data throughout training. Lesions are detected by calculating the reconstruction error between test samples and corresponding predictions. The proposed framework efficiently detects liver lesions without the pixel- or patch-level annotation of lesions in training. Besides, we introduce a U-Net-like discriminator that can provide more informative feedback to the generator than the existing encoder-like discriminator. We employ a new measure of reconstruction error for liver lesion detection based on the multi-scale gradient magnitude similarity deviation (GMSD), which is more accurate than the commonly used mean square error (MSE) index. A higher reconstruction error indicates a lower similarity between the test sample and its prediction, which indicates the detection of lesions. We evaluated our approach by training and testing on two datasets and found it outperforming existing state-of-the-art methods.},
  keywords={Lesions;Liver;Image reconstruction;Training;Accuracy;Unsupervised learning;Annotations;Generators;Supervised learning;Indexes;Generative adversarial network;liver lesion detection;weakly-supervised learning},
  doi={10.1109/TETCI.2025.3573247},
  ISSN={2471-285X},
  month={},}@ARTICLE{9743914,
  author={Ko, Wonjun and Jung, Wonsik and Jeon, Eunjin and Suk, Heung-Il},
  journal={IEEE Transactions on Medical Imaging}, 
  title={A Deep Generative–Discriminative Learning for Multimodal Representation in Imaging Genetics}, 
  year={2022},
  volume={41},
  number={9},
  pages={2348-2359},
  abstract={Imaging genetics, one of the foremost emerging topics in the medical imaging field, analyzes the inherent relations between neuroimaging and genetic data. As deep learning has gained widespread acceptance in many applications, pioneering studies employed deep learning frameworks for imaging genetics. However, existing approaches suffer from some limitations. First, they often adopt a simple strategy for joint learning of phenotypic and genotypic features. Second, their findings have not been extended to biomedical applications, e.g., degenerative brain disease diagnosis and cognitive score prediction. Finally, existing studies perform insufficient and inappropriate analyses from the perspective of data science and neuroscience. In this work, we propose a novel deep learning framework to simultaneously tackle the aforementioned issues. Our proposed framework learns to effectively represent the neuroimaging and the genetic data jointly, and achieves state-of-the-art performance when used for Alzheimer’s disease and mild cognitive impairment identification. Furthermore, unlike the existing methods, the framework enables learning the relation between imaging phenotypes and genotypes in a nonlinear way without any prior neuroscientific knowledge. To demonstrate the validity of our proposed framework, we conducted experiments on a publicly available dataset and analyzed the results from diverse perspectives. Based on our experimental results, we believe that the proposed framework has immense potential to provide new insights and perspectives in deep learning-based imaging genetics studies.},
  keywords={Genetics;Neuroimaging;Deep learning;Magnetic resonance imaging;Biomedical imaging;Diseases;Kernel;Imaging genetics;deep learning;magnetic resonance imaging;single nucleotide polymorphism},
  doi={10.1109/TMI.2022.3162870},
  ISSN={1558-254X},
  month={Sep.},}@ARTICLE{10547639,
  author={Yi, Myung-Kyu and Hwang, Seong Oun},
  journal={IEEE Sensors Journal}, 
  title={A Data-Driven Feature Extraction Method Based on Data Supplement for Human Activity Recognition}, 
  year={2024},
  volume={24},
  number={14},
  pages={23311-23323},
  abstract={Human activity recognition (HAR) has garnered attention as a significant technology that can enhance the quality of human life. However, existing HAR works still face great challenges such as a shortage of labeled data and the difficulty of rebuilding a deep-learning (DL) model whenever the application environment (e.g., user or sensor position) changes. To address these challenges, we propose a new data-centric approach for HAR by using a semi-supervised generative adversarial network (SGAN). To improve the accuracy of HAR, we propose a data supplement strategy that systematically improves data quality, rather than the model, by using data refinement and data-driven feature extraction techniques. The proposed HAR method applies simple SGAN to achieve considerably high accuracy with only a small fraction of the labeled data. Therefore, the proposed HAR method can reduce overhead from data labeling, which is a labor-intensive and time-consuming process for many HAR tasks. Moreover, the data-centric HAR method is robust even in scenarios when there is a change in person/sensor location. Experimental results show that our method improves accuracy by as much as 3% over state-of-the-art semi-supervised HAR methods with only 3% of the data being labeled, leading to comparable accuracy to state-of-the-art HAR methods based on supervised learning.},
  keywords={Human activity recognition;Data models;Sensors;Feature extraction;Analytical models;Artificial intelligence;Convolutional neural networks;Data-centric artificial intelligence (DC-AI);generative AI;human activity recognition (HAR);semi-supervised learning;wearable devices},
  doi={10.1109/JSEN.2024.3406727},
  ISSN={1558-1748},
  month={July},}@ARTICLE{10716799,
  author={Chen, Jianqi and Chen, Hao and Chen, Keyan and Zhang, Yilan and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Diffusion Models for Imperceptible and Transferable Adversarial Attack}, 
  year={2025},
  volume={47},
  number={2},
  pages={961-977},
  abstract={Many existing adversarial attacks generate $L_{p}$Lp-norm perturbations on image RGB space. Despite some achievements in transferability and attack success rate, the crafted adversarial examples are easily perceived by human eyes. Towards visual imperceptibility, some recent works explore unrestricted attacks without $L_{p}$Lp-norm constraints, yet lacking transferability of attacking black-box models. In this work, we propose a novel imperceptible and transferable attack by leveraging both the generative and discriminative power of diffusion models. Specifically, instead of direct manipulation in pixel space, we craft perturbations in the latent space of diffusion models. Combined with well-designed content-preserving structures, we can generate human-insensitive perturbations embedded with semantic clues. For better transferability, we further “deceive” the diffusion model which can be viewed as an implicit recognition surrogate, by distracting its attention away from the target regions. To our knowledge, our proposed method, DiffAttack, is the first that introduces diffusion models into the adversarial attack field. Extensive experiments conducted across diverse model architectures (CNNs, Transformers, and MLPs), datasets (ImageNet, CUB-200, and Standford Cars), and defense mechanisms underscore the superiority of our attack over existing methods such as iterative attacks, GAN-based attacks, and ensemble attacks. Furthermore, we provide a comprehensive discussion on future research avenues in diffusion-based adversarial attacks, aiming to chart a course for this burgeoning field.},
  keywords={Diffusion models;Perturbation methods;Closed box;Noise reduction;Solid modeling;Image color analysis;Glass box;Semantics;Gaussian noise;Purification;Adversarial attack;diffusion model;imperceptible attack;transferable attack},
  doi={10.1109/TPAMI.2024.3480519},
  ISSN={1939-3539},
  month={Feb},}@ARTICLE{9996549,
  author={Zhang, Hongyuan and Shi, Jiankun and Zhang, Rui and Li, Xuelong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Non-Graph Data Clustering via $\mathcal {O}(n)$O(n) Bipartite Graph Convolution}, 
  year={2023},
  volume={45},
  number={7},
  pages={8729-8742},
  abstract={Since the representative capacity of graph-based clustering methods is usually limited by the graph constructed on the original features, it is attractive to find whether graph neural networks (GNNs), a strong extension of neural networks to graphs, can be applied to augment the capacity of graph-based clustering methods. The core problems mainly come from two aspects. On the one hand, the graph is unavailable in the most general clustering scenes so that how to construct graph on the non-graph data and the quality of graph is usually the most important part. On the other hand, given $n$n samples, the graph-based clustering methods usually consume at least $\mathcal {O}(n^{2})$O(n2) time to build graphs and the graph convolution requires nearly $\mathcal {O}(n^{2})$O(n2) for a dense graph and $\mathcal {O}(|\mathcal {E}|)$O(|E|) for a sparse one with $|\mathcal {E}|$|E| edges. Accordingly, both graph-based clustering and GNNs suffer from the severe inefficiency problem. To tackle these problems, we propose a novel clustering method, AnchorGAE, with the self-supervised estimation of graph and efficient graph convolution. We first show how to convert a non-graph dataset into a graph dataset, by introducing the generative graph model and anchors. A bipartite graph is built via generating anchors and estimating the connectivity distributions of original points and anchors. We then show that the constructed bipartite graph can reduce the computational complexity of graph convolution from $\mathcal {O}(n^{2})$O(n2) and $\mathcal {O}(|\mathcal {E}|)$O(|E|) to $\mathcal {O}(n)$O(n). The succeeding steps for clustering can be easily designed as $\mathcal {O}(n)$O(n) operations. Interestingly, the anchors naturally lead to siamese architecture with the help of the Markov process. Furthermore, the estimated bipartite graph is updated dynamically according to the features extracted by GNN modules, to promote the quality of the graph by exploiting the high-level information by GNNs. However, we theoretically prove that the self-supervised paradigm frequently results in a collapse that often occurs after 2-3 update iterations in experiments, especially when the model is well-trained. A specific strategy is accordingly designed to prevent the collapse. The experiments support the theoretical analysis and show the superiority of AnchorGAE.},
  keywords={Convolution;Clustering methods;Bipartite graph;Feature extraction;Data mining;Computational modeling;Training;Anchors;efficient clustering;graph convolution network;self-supervised learning;siamese network},
  doi={10.1109/TPAMI.2022.3231470},
  ISSN={1939-3539},
  month={July},}@INPROCEEDINGS{10342917,
  author={Banavar, Mahesh K. and Shri, Lavanya and Sparks, Nicholas and Cohen, Alexander},
  booktitle={2023 IEEE Frontiers in Education Conference (FIE)}, 
  title={Being Brave in a New World: Leveraging ChatGPT in Signal Processing Classes}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In this innovative practice work-in-progress paper, we hypothesize that in engineering areas such as signal processing, tools such as ChatGPT do not threaten academic integrity in the classroom. We believe that if questions and problems are suitably posed, ChatGPT can assist, but cannot provide solutions. To test this hypothesis, we ask two questions: (a) How can ChatGPT be used to assist students in a signal processing class? and (b) How can the class itself be designed to leverage what ChatGPT has to offer? To answer these questions, we deploy ChatGPT in three different scenarios: (1) In a graduate level course to explore the use cases and limitations of the tool; (2) In summer REU cohorts to study the attitudes of students before and after one-hour workshops; and (3) in undergraduate signal processing courses where students will be exposed to generative AI tools over an entire semester. Surveys and discussions with the students will be analyzed and results will be presented at the conference. With the three separate activities across different time scales and student levels, our results can be used to generate guidelines for instructors to incorporate generative AI tools in their classes.},
  keywords={Conferences;Signal processing;Chatbots;Artificial intelligence;Guidelines;ChatGPT;AI;generative AI;STEM;machine learning;signal processing;projects;hands-on activities},
  doi={10.1109/FIE58773.2023.10342917},
  ISSN={2377-634X},
  month={Oct},}@ARTICLE{10877772,
  author={Zhao, Yanpeng and Hao, Yiwei and Gao, Siyu and Wang, Yunbo and Yang, Xiaokang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dynamic Scene Understanding Through Object-Centric Voxelization and Neural Rendering}, 
  year={2025},
  volume={47},
  number={5},
  pages={4215-4231},
  abstract={Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.},
  keywords={Three-dimensional displays;Semantics;Rendering (computer graphics);Neural radiance field;Videos;Geometry;Cameras;Training;Solid modeling;Deformation;Inverse rendering;neural radiance field;object-centric representation learning},
  doi={10.1109/TPAMI.2025.3539866},
  ISSN={1939-3539},
  month={May},}@INPROCEEDINGS{11003347,
  author={Sinnappan, Glaret Shirley and Shauki, Nur Baiti Ismail and Cheng, Wei Fong and Teh, Lee Wah},
  booktitle={2025 17th International Conference on Knowledge and Smart Technology (KST)}, 
  title={MetaC Framework: Metacognitive Prompting for AI-Driven Coursework Assessment in Tertiary Education}, 
  year={2025},
  volume={},
  number={},
  pages={145-150},
  abstract={This study develops the Metacognitive Prompting Framework to integrate AI generative tools into tertiary-level education. It focuses on enhancing metacognitive skills-specifically planning, monitoring, and evaluation-in the context of using AI tools for written assessments. The research employs Interactive Structural Modeling (ISM) to identify key strategies for improving these metacognitive skills with AI tools. A total of 21 experts from seven different faculties of a private Malaysian institution provided consensus on strategies for metacognitive thinking in written assessments. The findings highlight crucial strategies for prompting and refining AI tools to support students, promote responsible AI use, and foster critical thinking. This framework is designed to help students effectively leverage AI generative tools to improve their academic performance, critical thinking abilities, and structured feedback in diverse fields of study at the tertiary level.},
  keywords={Education;Refining;Metacognition;Planning;Artificial intelligence;Monitoring;Metacognition;AI Generative Prompting;Coursework assessment;Tertiary Education},
  doi={10.1109/KST65016.2025.11003347},
  ISSN={2473-764X},
  month={Feb},}@INPROCEEDINGS{10773271,
  author={Park, Jeongho and Kwon, Obin and Oh, Songhwai},
  booktitle={2024 24th International Conference on Control, Automation and Systems (ICCAS)}, 
  title={Spatially-Conditional 3D Furniture Generation Model for Indoor Scene Synthesis}, 
  year={2024},
  volume={},
  number={},
  pages={79-83},
  abstract={Recent advances in generative models have significantly enhanced the capabilities of 3D indoor scene synthesis, a field that is rapidly gaining interest due to its implications for applications such as embodied AI. These applications often require diverse, large-scale indoor datasets that include realistically modeled 3D furniture. Traditional text-to-3D models, while capable of producing realistic assets, often lack precise control over spatial dimensions, which is critical for ensuring that furniture fits appropriately within the scenes. This paper introduces SC-Shap·E, a spatially conditional 3D generative model that not only enhances the realism of 3D furniture but also ensures it adheres to specified spatial dimensions of height, width, and depth. Built on the pretrained Shap·E model, SC-Shap·E incorporates an additional network that utilizes spatial information alongside textual prompts, offering improved control over furniture sizing within generated scenes. By comparing our model with the original Shap·E, we demonstrate its superior ability to reflect accurate spatial conditions. Additionally, we present a novel three-stage system for 3D indoor scene synthesis that includes floor-plan creation, furniture layout, and 3D furniture mesh production, showing its effective application in creating diverse and realistic 3D indoor scenes.},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Automation;Accuracy;Layout;Production;Control systems;Artificial intelligence;3D generative models;3D indoor scene synthesis;diffusion model},
  doi={10.23919/ICCAS63016.2024.10773271},
  ISSN={2642-3901},
  month={Oct},}@INPROCEEDINGS{9551132,
  author={Regazzoni, Carlo S.},
  booktitle={2021 IEEE International Conference on Autonomous Systems (ICAS)}, 
  title={Bayesian emergent self awareness}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  abstract={Multisensor signal Data Fusion and Perception, including processing of signals are important cognitive functionalities that can be included in artificial systems to increase their level of autonomy. However, the techniques they rely on have been developed incrementally along time with the underlying assumption that they should have been used mainly to provide a support to decision tasks driving the actions of those systems. Cognitive functionalities like self-awareness have been so far considered as not primary part of embodied knowledge of an autonomous or semi autonomous systems. One of the reason for this choice was the lack of understanding the principles that could allow an agent, even a human one, to organize successive sensorial experiences into a coherent framework of emergent knowledge, by means of integrating signal processing, machine learning and data fusion aspects. However, the developments of this last decade in many fields carried to the possibility to provide integrated solutions capable to sketch how emergent self awareness can be obtained by capturing experiences of autonomous agents like for example vehicles and intelligent radios. In this presentation, a hierarchical Bayesian representation is proposed based on generalized random states and including in a coherent inference framework anomaly detection and incremental learning. Described models are provided of generative (temporally and hierarchically) predictive as well as of discriminative capabilities and can be used as bricks of emergent self awareness in intelligent agents. Discussion of the advantages of including emergent self awareness in intelligent agents will be also provided with respect to different aspects, e.g. explainability of agent’s actions and capability of imitation learning.},
  keywords={Autonomous systems;Conferences;Data integration;Machine learning;Signal processing;Predictive models;Autonomous agents;Self awareness;Dynamic Bayesian Networks;anomaly detection;incremental learning;imitation learning;generalized coordinates;generative models;discriminative models},
  doi={10.1109/ICAS49788.2021.9551132},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9441519,
  author={Masood, Momina and Nawaz, Marriam and Javed, Ali and Nazir, Tahira and Mehmood, Awais and Mahum, Rabbia},
  booktitle={2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2)}, 
  title={Classification of Deepfake Videos Using Pre-trained Convolutional Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The advancement of Artificial Intelligence (AI) has brought a revolution in the field of information technology. Furthermore, AI has empowered the new applications to run with minimum resources and computational cost. One of such applications is Deepfakes, which produces extensively altered and modified multimedia content. However, such manipulated visual data imposed a severe threat to the security and privacy of people and can cause massive sect, religious, political, and communal stress around the globe. Now, the face-swapped base visual content is difficult to recognizable by humans through their naked eyes due to the advancement of Generative adversarial networks (GANs). Therefore, identifying such forgeries is a challenging task for the research community. In this paper, we have introduced a pipeline for identifying and detecting person faces from input visual samples. In the second step, several deep learning (DL) based approaches are employed to compute the deep features from extracted faces. Lastly, a classifier namely SVM is trained over these features to classify the data as real or manipulated. We have performed the performance comparison of various feature extractors and confirmed from reported results that DenseNet-169 along with SVM classifier outperforms the rest of the methods.},
  keywords={Support vector machines;Visualization;Data privacy;Feature extraction;Generative adversarial networks;Security;Artificial intelligence;deepfakes;deep-learning;visual manipulations;convolutional neural networks},
  doi={10.1109/ICoDT252288.2021.9441519},
  ISSN={},
  month={May},}@INPROCEEDINGS{10387306,
  author={Kumar, Roshan and Ayyasamy, Ramesh Kumar and Sangodiah, Anbuselvan and Krishnan, Kesavan and Jebna, Abdulkarim Kanaan and Theam, Lim Jit},
  booktitle={2023 15th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)}, 
  title={Sentiment Analysis of ChatGPT Healthcare Discourse: Insights from Twitter Data}, 
  year={2023},
  volume={},
  number={},
  pages={220-225},
  abstract={This paper explores the application of Chat Generative Pretrained Transformer (ChatGPT) in the healthcare domain, introducing a sentiment analysis model to evaluate ChatGPT-related tweets in healthcare contexts. The study aims to uncover predominant sentiments, thematic content, and diverse perspectives concerning ChatGPT's integration into healthcare, utilizing an extensive dataset from Twitter comprising 10,330 healthcare-related tweets. Leveraging advanced Natural Language Processing (NLP) techniques, we systematically categorized topics and emotional content within these tweets. Additionally, we conducted a comprehensive analysis of frequently occurring words in tweets expressing positive and negative sentiments. The findings reveal that the majority of healthcare-related ChatGPT tweets express either positive or negative sentiments, with a minor proportion conveying neutral viewpoints. Furthermore, to enhance our comprehension of sentiment dynamics in healthcare discussions involving ChatGPT, we applied four machine learning classifiers Support Vector Machine, K-Nearest Neighbors, Naive Bayes and Random Forest. Remarkably, the SVM classifier demonstrated the highest accuracy at 85.6%, affirming its efficacy in healthcare sentiment analysis. In summary, this research sheds light on prevailing sentiments and perspectives regarding ChatGPT in the healthcare sector, highlighting its predominantly positive and neutral reception on platforms like Twitter. Additionally, the success of SVM as a sentiment analysis tool underscores its potential for discerning sentiments in healthcare-related ChatGPT discussions, contributing to ongoing debates on AI integration in healthcare and guiding future endeavors in this evolving field.},
  keywords={Support vector machines;Sentiment analysis;Social networking (online);Blogs;Medical services;Chatbots;Transformers;Generative AI;ChatGPT;Sentiment Analysis;Twitter;Healthcare},
  doi={10.1109/SKIMA59232.2023.10387306},
  ISSN={2573-3214},
  month={Dec},}@INPROCEEDINGS{9383959,
  author={Mahdian, Saied and Blanchet, Jose H. and Glynn, Peter W.},
  booktitle={2020 Winter Simulation Conference (WSC)}, 
  title={A Class of Optimal Transport Regularized Formulations with Applications to Wasserstein GANs}, 
  year={2020},
  volume={},
  number={},
  pages={433-444},
  abstract={Optimal transport costs (e.g. Wasserstein distances) are used for fitting high-dimensional distributions. For example, popular artificial intelligence algorithms such as Wasserstein Generative Adversarial Networks (WGANs) can be interpreted as fitting a black-box simulator of structured data with certain features (e.g. images) using the Wasserstein distance. We propose a regularization of optimal transport costs and study its computational and duality properties. We obtain running time improvements for fitting WGANs with no deterioration in testing performance, relative to current benchmarks. We also derive finite sample bounds for the empirical Wasserstein distance from our regularization.},
  keywords={Computational modeling;Fitting;Benchmark testing;Generative adversarial networks;Artificial intelligence},
  doi={10.1109/WSC48552.2020.9383959},
  ISSN={1558-4305},
  month={Dec},}@ARTICLE{10896941,
  author={Mekki, Yosra Magdi and Simon, Leslie V. and Freeman, William D. and Qadir, Junaid},
  journal={Computer}, 
  title={Medical Education Metaverses (MedEd Metaverses): Opportunities, Use Case, and Guidelines}, 
  year={2025},
  volume={58},
  number={3},
  pages={60-70},
  abstract={This article explores how artificial intelligence (AI), particularly generative AI (GenAI), can enhance extended reality (XR) applications in medical education (MedEd) metaverses. We compare traditional augmented reality/virtual reality methods with AI-enabled XR metaverses, highlighting improvements in immersive learning, adaptive feedback, personalized performance tracking, remote training, and resource efficiency.},
  keywords={Training;Generative AI;Extended reality;Immersive learning;Guidelines;Artificial intelligence;Biomedical engineering education;Education;Educational programs;Medical services;Metaverse;Curriculum development},
  doi={10.1109/MC.2024.3474033},
  ISSN={1558-0814},
  month={March},}@INPROCEEDINGS{11081179,
  author={Nair, Karan Rajesh and Anand, Sruthy and Arjun, U and Prasanth, Niranjan and Kumar, Sidharth S and Rao, Sethuraman N},
  booktitle={2025 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE)}, 
  title={Taxonomy Based Question Generation Using Prompt Engineering for Student Assessment}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The recent wide popularity of ChatGPT has resulted in immense research interest in Large Language Models in many application areas such as education, business, healthcare, and tourism. Its popularity is due to its human-like conversation and comprehension capability. In this paper, we focus on the usage of generative AI in the education system to address the challenges of generating questions to address the different levels of learning taxonomy and help educators design questions based on the different categories of students. In this work, we are trying to build an AI system that is capable of generating questions based on Bloom’s Taxonomy using prompting techniques. The system is also capable of generating personalised quizzes for students who face difficulty answering learning taxonomy-based questions. We have used the LLM API and four types of prompting techniques to generate questions in our personalised evaluation system. The paper presents the system architecture of the personalised evaluation system. The paper also presents a comparison of various prompting techniques, and the result obtained.},
  keywords={Large language models;Taxonomy;Education;Systems architecture;Oral communication;Medical services;Chatbots;Question generation;Prompt engineering;Faces;LLM;Generative AI;Prompt Engineering;Education;Learning Taxonomy;Evaluation System},
  doi={10.1109/AMATHE65477.2025.11081179},
  ISSN={},
  month={April},}@ARTICLE{6272355,
  author={Wu, Jinjian and Lin, Weisi and Shi, Guangming and Liu, Anmin},
  journal={IEEE Transactions on Image Processing}, 
  title={Perceptual Quality Metric With Internal Generative Mechanism}, 
  year={2013},
  volume={22},
  number={1},
  pages={43-54},
  abstract={Objective image quality assessment (IQA) aims to evaluate image quality consistently with human perception. Most of the existing perceptual IQA metrics cannot accurately represent the degradations from different types of distortion, e.g., existing structural similarity metrics perform well on content-dependent distortions while not as well as peak signal-to-noise ratio (PSNR) on content-independent distortions. In this paper, we integrate the merits of the existing IQA metrics with the guide of the recently revealed internal generative mechanism (IGM). The IGM indicates that the human visual system actively predicts sensory information and tries to avoid residual uncertainty for image perception and understanding. Inspired by the IGM theory, we adopt an autoregressive prediction algorithm to decompose an input scene into two portions, the predicted portion with the predicted visual content and the disorderly portion with the residual content. Distortions on the predicted portion degrade the primary visual information, and structural similarity procedures are employed to measure its degradation; distortions on the disorderly portion mainly change the uncertain information and the PNSR is employed for it. Finally, according to the noise energy deployment on the two portions, we combine the two evaluation results to acquire the overall quality score. Experimental results on six publicly available databases demonstrate that the proposed metric is comparable with the state-of-the-art quality metrics.},
  keywords={Visualization;Degradation;Measurement;Image edge detection;PSNR;Bayesian methods;Uncertainty;Human visual system;image decomposition;image quality assessment (IQA);internal generative mechanism (IGM)},
  doi={10.1109/TIP.2012.2214048},
  ISSN={1941-0042},
  month={Jan},}@ARTICLE{5961630,
  author={Batmanghelich, Nematollah K. and Taskar, Ben and Davatzikos, Christos},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Generative-Discriminative Basis Learning for Medical Imaging}, 
  year={2012},
  volume={31},
  number={1},
  pages={51-69},
  abstract={This paper presents a novel dimensionality reduction method for classification in medical imaging. The goal is to transform very high-dimensional input (typically, millions of voxels) to a low-dimensional representation (small number of constructed features) that preserves discriminative signal and is clinically interpretable. We formulate the task as a constrained optimization problem that combines generative and discriminative objectives and show how to extend it to the semi-supervised learning (SSL) setting. We propose a novel large-scale algorithm to solve the resulting optimization problem. In the fully supervised case, we demonstrate accuracy rates that are better than or comparable to state-of-the-art algorithms on several datasets while producing a representation of the group difference that is consistent with prior clinical reports. Effectiveness of the proposed algorithm for SSL is evaluated with both benchmark and medical imaging datasets. In the benchmark datasets, the results are better than or comparable to the state-of-the-art methods for SSL. For evaluation of the SSL setting in medical datasets, we use images of subjects with mild cognitive impairment (MCI), which is believed to be a precursor to Alzheimer's disease (AD), as unlabeled data. AD subjects and normal control (NC) subjects are used as labeled data, and we try to predict conversion from MCI to AD on follow-up. The semi-supervised extension of this method not only improves the generalization accuracy for the labeled data (AD/NC) slightly but is also able to predict subjects which are likely to converge to AD.},
  keywords={Matrix decomposition;Jacobian matrices;Biomedical imaging;Optimization;Machine learning;Image reconstruction;Loading;Basis learning;classification;feature construction;generative-discriminative learning;machine learning;matrix factorization;morphological pattern analysis;optimization;semi-supervised learning;sparsity},
  doi={10.1109/TMI.2011.2162961},
  ISSN={1558-254X},
  month={Jan},}@ARTICLE{9893751,
  author={Li, Yikai and Chen, C. L. Philip and Zhang, Tong},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Survey on Siamese Network: Methodologies, Applications, and Opportunities}, 
  year={2022},
  volume={3},
  number={6},
  pages={994-1014},
  abstract={Siamese network has obtained growing attention in real-life applications. In this survey, we present an comprehensive review on Siamese network from the aspects of methodologies, applications, and interesting topics for further exploration. We first introduce framework designs of Siamese network, followed by methodologies about learning with unlabeled data. Then, we review application scenarios in terms of classification and regression, together with relative methodologies. We also discuss the promising area of few-shot learning, followed by interesting topics about opportunities and challenges.},
  keywords={Artificial intelligence;Optimization;Feature extraction;Learning systems;Self-supervised learning;Semisupervised learning;Unsupervised learning;Broad learning system (BLS);classification;few-shot learning (FSL);regression;self-supervised learning;semi-supervised learning;Siamese network;unsupervised learning},
  doi={10.1109/TAI.2022.3207112},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{9723693,
  author={Mahdavi, Atefeh and Carvalho, Marco},
  booktitle={2021 IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)}, 
  title={A Survey on Open Set Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={37-44},
  abstract={Open Set Recognition (OSR) is about dealing with unknown situations that were not learned by the models during training. In this paper, we provide a survey of existing works about OSR and distinguish their respective advantages and disadvantages to help out new researchers interested in the subject. The categorization of OSR models is provided along with an extensive summary of recent progress. Additionally, the relationships between OSR and its related tasks including multi-class classification and novelty detection are analyzed. It is concluded that OSR can appropriately deal with unknown instances in the real-world where capturing all possible classes in the training data is not practical. Lastly, some new directions for future research topics are suggested.},
  keywords={Training;Knowledge engineering;Conferences;Training data;Machine learning;Learning (artificial intelligence);Multitasking;machine learning;classification;open set recognition;multi-task learning;risk of the unknown},
  doi={10.1109/AIKE52691.2021.00013},
  ISSN={},
  month={Dec},}@ARTICLE{10005797,
  author={Taufique, Abu Md Niamul and Jahan, Chowdhury Sadman and Savakis, Andreas},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Continual Unsupervised Domain Adaptation in Data-Constrained Environments}, 
  year={2024},
  volume={5},
  number={1},
  pages={167-178},
  abstract={Domain adaptation (DA) techniques aim to overcome the domain shift between the source domain used for training and the target domain where testing takes place. However, current DA methods assume that the entire target domain is available during adaptation, which may not hold in practice. We introduce a new, data-constrained DA paradigm where unlabeled target samples are received in batches and adaptation is performed continually. We propose a novel source-free method for continual unsupervised domain adaptation (UDA) that utilizes a buffer for selective replay of previously seen samples. In our continual DA framework, we selectively mix samples from incoming batches with data stored in a buffer using buffer management strategies and use the combination to incrementally update our model. We evaluate and compare the classification performance of the continual DA approach with state-of-the-art (SOTA) DA methods based on the entire target domain. Our results on three popular DA datasets demonstrate the benefits of our method when operating in data constrained environments. We further extend our experiments to adapting over multiple target domains and our method performs favorably with the SOTA methods.},
  keywords={Training;Data models;Adaptation models;Artificial intelligence;Deep learning;Unsupervised learning;Adaptive learning;continual learning (CL);data-constrained learning;unsupervised domain adaptation (UDA)},
  doi={10.1109/TAI.2022.3233791},
  ISSN={2691-4581},
  month={Jan},}@ARTICLE{10601520,
  author={Eldele, Emadeldeen and Ragab, Mohamed and Chen, Zhenghua and Wu, Min and Kwoh, Chee-Keong and Li, Xiaoli},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Label-Efficient Time Series Representation Learning: A Review}, 
  year={2024},
  volume={5},
  number={12},
  pages={6027-6042},
  abstract={Label-efficient time series representation learning, which aims to learn effective representations with limited labeled data, is crucial for deploying deep learning models in real-world applications. To address the scarcity of labeled time series data, various strategies, e.g., transfer learning, self-supervised learning, and semisupervised learning, have been developed. In this survey, we introduce a novel taxonomy for the first time, categorizing existing approaches as in-domain or cross domain based on their reliance on external data sources or not. Furthermore, we present a review of the recent advances in each strategy, conclude the limitations of current methodologies, and suggest future research directions that promise further improvements in the field.},
  keywords={Time series analysis;Surveys;Deep learning;Representation learning;Artificial intelligence;Data models;Transfer learning;Data augmentation;domain adaptation;label-efficient learning;self-supervised learning;semisupervised learning;time series},
  doi={10.1109/TAI.2024.3430236},
  ISSN={2691-4581},
  month={Dec},}@INPROCEEDINGS{10292463,
  author={Khalil, Kasem and Khan Mamun, Mohammad M. R. and Sherif, Ahmed and Elsersy, Mohamed and Imam, Ahmad Abdel-Aliem and Abouzaid, Kamal},
  booktitle={2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)}, 
  title={A Dementia Diagnosis Technique Based on AI and Hardware Acceleration}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The effects of Alzheimer’s disease (AD) are devastating, both personally and within the patient’s family, as the disease progresses slowly over many years. It could significantly affect illness consequences and outcomes if detected early and accurately. Non-invasive medical testing often uses blood bio-samples because they are easy to collect and relatively cheap to analyze. This paper proposes a federated learning (FL)-based diagnostic model for AD using blood bio-samples. We have used blood bio-sample data sets downloaded from the ADNI website to test and compare the efficacy of our models. According to the enormous data collected for early AD detection, we used a hardware acceleration scheme to implement our FL model to fasten the training and testing operations. The hardware-accelerator method is implemented using VHDL and Altera 10 GX FPGA. The simulation results show the proposed algorithms reach an accuracy and sensitivity of 89% and 87%, respectively, for early detection while taking less time to train than other state-of-the-art algorithms. The proposed algorithms also consume 52-172 mW, which makes them suitable for constrained devices.},
  keywords={Training;Biological system modeling;Data models;Classification algorithms;Alzheimer's disease;Artificial intelligence;Blood;Alzheimer’s disease;hardware acceleration;blood bio-samples;federated learning;early diagnosis},
  doi={10.1109/AIBThings58340.2023.10292463},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10762674,
  author={Sui, Mingxiu and Hu, Jiacheng and Zhou, Tong and Liu, Zibo and Wen, Likang and Du, Junliang},
  booktitle={2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={634-638},
  abstract={This paper introduces a novel deep-learning method for the automatic detection and segmentation of lung nodules, aimed at advancing the accuracy of early-stage lung cancer diagnosis. The proposed approach leverages a unique “Channel Squeeze U-Structure” that optimizes feature extraction and information integration across multiple semantic levels of the network. This architecture includes three key modules: shallow information processing, channel residual structure, and channel squeeze integration. These modules enhance the model’s ability to detect and segment small, imperceptible, or ground-glass nodules, which are critical for early diagnosis. The method demonstrates superior performance in terms of sensitivity, Dice similarity coefficient, precision, and mean Intersection over Union (IoU). Extensive experiments were conducted on the Lung Image Database Consortium (LIDC) dataset using five-fold cross-validation, showing excellent stability and robustness. The results indicate that this approach holds significant potential for improving computer-aided diagnosis systems, providing reliable support for radiologists in clinical practice and aiding in the early detection of lung cancer, especially in resource-limited settings},
  keywords={Technological innovation;Accuracy;Lungs;Lung cancer;Information processing;Computer architecture;Feature extraction;Transformers;Computer aided diagnosis;Artificial intelligence;Deep Learning;Lung Nodule Detection;Early Diagnosis;IoU},
  doi={10.1109/ICBASE63199.2024.10762674},
  ISSN={},
  month={Sep.},}@ARTICLE{10197527,
  author={Chhabra, Saheb and Thakral, Kartik and Mittal, Surbhi and Vatsa, Mayank and Singh, Richa},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Low-Quality Deepfake Detection via Unseen Artifacts}, 
  year={2024},
  volume={5},
  number={4},
  pages={1573-1585},
  abstract={The proliferation of manipulated media over the Internet has become a major source of concern in recent times. With the wide variety of techniques being used to create fake media, it has become increasingly difficult to identify such occurrences. While existing algorithms perform well on the detection of such media, limited algorithms take the impact of compression into account. Different social media platforms use different compression factors and algorithms before sharing such images and videos, which amplifies the issues in their identification. Therefore, it has become imperative that fake media detection algorithms work well for data compressed at different factors. To this end, the focus of this article is detecting low-quality fake videos in the compressed domain. The proposed algorithm distinguishes real images and videos from altered ones by using a learned visibility matrix, which enforces the model to see unseen imperceptible artifacts in the data. As a result, the learned model is robust to loss of information due to data compression. The performance is evaluated on three publicly available datasets, namely Celeb-DF, FaceForensics, and FaceForensics++, with three manipulation techniques, viz., Deepfakes, Face2Face, and FaceSwap. Experimental results show that the proposed approach is robust under different compression factors and yields state-of-the-art performance on the FaceForensics++ and Celeb-DF datasets with 97.14% classification accuracy and 74.45% area under the curve, respectively.},
  keywords={Deepfakes;Faces;Image coding;Social networking (online);Artificial intelligence;Training;Deep learning;Artifacts;compression;deepfake},
  doi={10.1109/TAI.2023.3299894},
  ISSN={2691-4581},
  month={April},}@ARTICLE{9868120,
  author={Wang, Ning and Cao, Hui and Zhao, Jun and Chen, Ruilin and Yan, Dapeng and Zhang, Jie},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={M2R2: Missing-Modality Robust Emotion Recognition Framework With Iterative Data Augmentation}, 
  year={2023},
  volume={4},
  number={5},
  pages={1305-1316},
  abstract={This article deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. Present models generally predict the speaker's emotions by its current utterance and context, which is degraded by modality missing considerably. Our work proposes a framework missing-modality robust emotion recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation. First, a network called party attentive network (PANet) is designed to classify emotions, which tracks all the speakers' states and context. Attention mechanism between speaker with other participants and dialogue topic is used to decentralize dependence on multitime and multiparty utterances instead of the possible incomplete one. Moreover, the common representation learning (CRL) problem is defined for modality-missing problem. Data imputation methods improved by the adversarial strategy are used here to construct extra features to augment data. Extensive experiments and case studies validate the effectiveness of our methods over baselines for modality-missing emotion recognition on two different datasets.},
  keywords={Emotion recognition;Task analysis;Oral communication;Representation learning;Iterative methods;Artificial intelligence;Training;Adversarial strategy;attention mechanism;common representation learning (CRL);iterative data augmentation},
  doi={10.1109/TAI.2022.3201809},
  ISSN={2691-4581},
  month={Oct},}@ARTICLE{9781336,
  author={Cheng, Chao and Li, Xuedong and Xie, Pu and Yang, Xiaoyue},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Transfer-Learning-Aided Fault Detection for Traction Drive Systems of High-Speed Trains}, 
  year={2023},
  volume={4},
  number={4},
  pages={689-697},
  abstract={Long-term operation may lead to performance degradation of the traction drive systems. It will naturally increase the difficulty of fault detection (FD). To ensure the safe and stable operation of the traction drive system, data-driven FD has received considerable attention, especially deep learning methods. By exploiting the idea of transfer learning, this article proposes a new FD method for traction converter faults in the traction drive systems of high-speed trains. Its structure consists of a federal neural network based on a variational autoencoder. The significant advantages of the proposed FD method based on transfer learning are summarized as follows: 1) FD is still valid for the systems with performance degradation; 2) it can also realize the FD function even if the physical model and related parameters are not provided; and 3) the proposed framework can adaptively adjust the model parameters by storing and reusing the prior knowledge in the neural network. Finally, the effectiveness of the proposed method is demonstrated through the platform of the traction drive control system.},
  keywords={Degradation;Transfer learning;Neural networks;Artificial intelligence;Circuit faults;Gaussian distribution;Fault detection;Fault detection (FD);federal neural network;performance degradation;transfer learning;variational auto- encoder (VAE)},
  doi={10.1109/TAI.2022.3177387},
  ISSN={2691-4581},
  month={Aug},}@ARTICLE{9780167,
  author={Cheng, Yuhu and Chen, Yang and Kong, Yi and Philip Chen, C. L. and Wang, Xuesong},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Graph Dual Adversarial Network for Hyperspectral Image Classification}, 
  year={2023},
  volume={4},
  number={4},
  pages={922-932},
  abstract={An end-to-end unsupervised domain adaptation method for hyperspectral image (HSI) classification based on a graph dual adversarial network is proposed in this article. First, in order to extract the domain-invariant features of the source and target domains, the rich spectral information and spatial position of HSI are used to construct a spectral–spatial nearest neighbor graph, which is input into the graph convolutional network. Then, a prototype adversarial strategy is proposed, which uses the labeled data of the source domain to reliably calculate the feature prototypes of different classes. Through the prototype adversarial strategy, the distances between different prototypes are appropriately extended, so that the clusters of different classes are far away from their respective decision boundaries, and the discriminability of features are also enhanced. The dual adversarial strategy is composed of the prototype adversarial strategy and the domain adversarial strategy. It is worth noting that the dual adversarial strategy does not require feature extractor and discriminator to work in turn, which can be implemented through a gradient reversal layer. Finally, on the basis of adapting the overall features of both the domains via the domain adversarial strategy, the source- and target-domain features are further adapted by minimizing the correlation alignment loss of each class of samples. Experimental results on two real HSI datasets of Botswana and Kennedy Space Center show the effectiveness of our proposed method.},
  keywords={Feature extraction;Prototypes;Training;Artificial intelligence;Adversarial machine learning;Adaptation models;Kernel;Domain adaptation;dual adversarial strategy;graph convolutional network (GCN);hyperspectral image (HSI) classification},
  doi={10.1109/TAI.2022.3177168},
  ISSN={2691-4581},
  month={Aug},}@INPROCEEDINGS{10760427,
  author={Venketbabu, T and Aswathy, M P and Sangeetha, J and Anish, T.P. and Nalini, M. and Siva Subramanian, R},
  booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Revolutionizing Image Processing: Innovations Impacts and Future Trends}, 
  year={2024},
  volume={},
  number={},
  pages={901-907},
  abstract={Image processing has become one of the key technologies that can be applied to numerous fields of activity such as medicine, astronomy, security, and entertainment. The present survey paper discusses the fundamental notions and offers an overview of the basic and more sophisticated image processing methods. Core techniques such as spatial and frequency domain processing, multiscale methods, and morphological operations are discussed for their roles in enhancing, transforming, and analyzing images. Furthermore, the research discusses about the influence of deep learning and neural networks, especially CNNs and GANs to transform image analysis and synthesis. Some major areas of application are met emphasizing how image processing technologies foster developments in health care, space science, security, and manufacturing industries. This study also focuses on present issues such as computational complexity, data protection, and data ethics. Future directions and trends are discussed as well as directions for future research, focusing on the prospect of combined use with other future technologies such as AI and IoT. Thus, the goal of this research work is to map out the field of image processing to help practitioners and researchers better understand the current and future state of the field.},
  keywords={Surveys;Deep learning;Technological innovation;Ethics;Image processing;Frequency-domain analysis;Entertainment industry;Market research;Security;Artificial intelligence;Image Processing;Image Enhancement;Image Transformation;Image Restoration;Image Segmentation},
  doi={10.1109/ICSSAS64001.2024.10760427},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10097994,
  author={Jakupov, Alibek and Mercadal, Julien and Zeddini, Besma and Longhi, Julien},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Analyzing Deceptive Opinion Spam Patterns: the Topic Modeling Approach}, 
  year={2022},
  volume={},
  number={},
  pages={1251-1261},
  abstract={Deceptive Opinion Spam commonly takes the form of fake reviews (negative or positive) posted by a malicious web user to hurt or inflate a company's image. As these reviews have been deliberately written to deceive the reader, human reviewers are faring little better than a chance in detecting these deceptive statements. Thus, there is a dire need to address this issue as extracting text patterns from the fraudulent texts with meaningful substructures still remains a challenge. In our research, to obtain a deeper understanding of how lies are expressed in texts, we consider the task as a topic modeling problem, in which we constructed a model to learn the patterns that constitute a fake review, and then explore the outputs of this model to identify those patterns. Topic models may be useful in this task due to their ability to group multiple documents into smaller sets of key topics. As the linguistic cues of the lies are still unknown, a key advantage of this approach is that the algorithm encourages the mixtures composed of only few topics, which makes the representation more interpretable and provides additional opportunities to reveal the patterns and structures within the systems of documents. Our methodology proved to be useful for this study, revealing the lexical cues generally applied by human reviewers to generate deceptive language.},
  keywords={Analytical models;Linguistics;Task analysis;Artificial intelligence;topic modeling;deceptive opinion spam;natural language processing},
  doi={10.1109/ICTAI56018.2022.00190},
  ISSN={2375-0197},
  month={Oct},}@ARTICLE{10247603,
  author={Peng, Haipeng and Bao, Shuang and Li, Lixiang},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Survey of Security Protection Methods for Deep Learning Model}, 
  year={2024},
  volume={5},
  number={4},
  pages={1533-1553},
  abstract={In recent years, deep learning (DL) models have attracted widespread concern. Due to its own characteristics, DL has been successfully applied in the fields of object detection, superresolution reconstruction, speech recognition, natural language processing, etc., bringing high efficiency to industrial production and daily life. With the Internet of Things, 6G and other new technologies have been proposed, leading to an exponential growth in data volume. DL models currently suffer from some security issues, such as privacy issues during data collection, defense issues during model training and deployment, etc. The sensitive data of users and special institutions that are directly used as training data of DL models may lead to information leakage and serious privacy problems. In addition, DL models have encountered many malicious attacks in the real world, such as poisoning attack, exploratory attack, adversarial attack, etc., which caused model security problems. Therefore, this article discusses ways of ensuring the security and data privacy of DL models under diversified attack methods and the ways of ensuring the privacy security of edge mobile devices equipped with pretrained deep neural networks. Alternatively, this article analyzes the privacy security of DL models for typical deployment platforms such as server/cloud, edge mobile device, and web browser and, then, summarizes future research direction.},
  keywords={Data models;Security;Training;Computational modeling;Data privacy;Artificial intelligence;Mobile handsets;Data privacy;deep learning (DL);defense method;security},
  doi={10.1109/TAI.2023.3314398},
  ISSN={2691-4581},
  month={April},}@ARTICLE{9966659,
  author={Mehta, Nancy and Murala, Subrahmanyam},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Image Super-Resolution With Content-Aware Feature Processing}, 
  year={2024},
  volume={5},
  number={1},
  pages={179-191},
  abstract={Image super-resolution (SR) is currently a very active research topic with applications spanning from computer vision to videos and graphic industries. The top performers in SR field usually employ deep or wide convolutional neural networks (CNNs) to restore the lost textures from low-resolution images. However, most of these methods adopt pixel shuffle and deconvolution as their upsampling techniques and often generate conspicuous artifacts in the reconstructed image. In addition, the ongoing trend of directly portraying the degraded low-resolution image to a high-resolution image via complex deep CNNs improves the reconstruction performance, but at the cost of high computational complexity. In this article, we propose a multilevel bicubic upsampler network for reconstructing high-quality SR image with a restricted number of parameters. A novel content-aware feature difference (CAFD) block is presented to reform the network by focusing on contextual information. The proposed CAFD block consists of four multilevel attention blocks for a better extraction of low-level features at different scales. Furthermore, we design an innovative upsampling layer that consistently outperforms the traditional upsampling methods. These components collaboratively endow our proposed network with a great performance boost, helping it achieve state-of-the-art accuracy on five synthetic benchmark datasets, both qualitatively and quantitatively. In addition, a detailed ablation study has been accomplished to scrutinize the improvements obtained by different modules in the proposed method.},
  keywords={Feature extraction;Image reconstruction;Superresolution;Artificial intelligence;Spatial resolution;Deconvolution;Convolutional neural networks;Bicubic upsampling;content-aware feature difference (CAFD) block;convolutional neural networks (CNNs);multilevel attention (MLA) block;super-resolution (SR)},
  doi={10.1109/TAI.2022.3225784},
  ISSN={2691-4581},
  month={Jan},}@INPROCEEDINGS{9497894,
  author={Wang, Yawei and Li, Xiu},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Reward function shape exploration in adversarial imitation learning: an empirical study}, 
  year={2021},
  volume={},
  number={},
  pages={52-57},
  abstract={For adversarial imitation learning algorithms (AILs), no true rewards are obtained from the environment for learning the strategy. However, the pseudo rewards based on the output of the discriminator are still required. Given the implicit reward bias problem in AILs, we design several representative reward function shapes and compare their performances by large-scale experiments. To ensure our results' reliability, we conduct the experiments on a series of Mujoco and Box2D continuous control tasks based on four different AILs. Besides, we also compare the performance of various reward function shapes using varying numbers of expert trajectories. The empirical results reveal that the positive logarithmic reward function works well in typical continuous control tasks. In contrast, the so-called unbiased reward function is limited to specific kinds of tasks. Furthermore, several designed reward functions perform excellently in these environments as well.},
  keywords={Shape;Conferences;Computer applications;Trajectory;Reliability;Task analysis;Artificial intelligence;imitation learning;reward function;empirical study;adversarial training;Wasserstein distance},
  doi={10.1109/ICAICA52286.2021.9497894},
  ISSN={},
  month={June},}@INPROCEEDINGS{9643312,
  author={Xu, Jia and Liu, Jing and Lv, Pin and Yang, Panyuan},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Improving Peer Assessment Accuracy by Incorporating Grading Behaviors}, 
  year={2021},
  volume={},
  number={},
  pages={1162-1169},
  abstract={Peer assessment, which asks students to evaluate their peers’ submissions, has become the mainstream paradigm for solving the massive grading challenge of open-ended assignments faced by teachers at MOOC platforms. Since peer grades may be biased and unreliable, a group of probabilistic graph models are proposed to improve the estimation to the true scores of assignments derived based on peer grades, by explicitly modeling the bias and reliability of each grader. However, these models assume that graders’ reliability are only impacted by their knowledge/ability levels while ignoring their grading behaviors. In real life, graders’ grading behaviors (e.g., the time consumed for reviewing an assignment) reflect the seriousness of the graders in the assessment and greatly affect their reliability. Following this intuition, we propose two novel probabilistic graph models for cardinal peer assessment, which optimizes the modeling of the reliability of graders by incorporating various grading behaviors of them. In specific, a GBDT-based regressor is firstly built to quantify the grading seriousness of graders according to their behaviors. Second, the grading seriousness values together with knowledge/ability levels of graders are both employed to model their reliability. Finally, an algorithm based on Gibbs sampling is designed to infer true scores of assignments according to the models. Experimental results on a real peer assessment dataset show the superiority of the proposed models in improving the estimation accuracy to the true scores of assignments by leveraging grader grading behaviors.},
  keywords={Computer aided instruction;Electronic learning;Conferences;Estimation;Probabilistic logic;Reliability;Artificial intelligence;peer assessment;probabilistic graph models;GBDT;online education},
  doi={10.1109/ICTAI52525.2021.00184},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{10495099,
  author={Zhang, Ziyan and Jiang, Bo and Tang, Jin and Tang, Jinhui and Luo, Bin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Incomplete Graph Learning via Partial Graph Convolutional Network}, 
  year={2024},
  volume={5},
  number={9},
  pages={4315-4321},
  abstract={Graph convolutional networks (GCNs) gain increasing attention on graph data learning tasks in recent years. However, in many applications, graph may come with an incomplete form where attributes of graph nodes are partially unknown/missing. Existing graph convolutions (GCs) are generally designed on complete graphs which cannot deal with attribute-incomplete graph data directly. To address this problem, in this article, we extend standard GC and develop an explicit Partial Graph Convolution (PaGC) for attribute-incomplete graph data. Our PaGC is derived based on the observation that the core neighborhood aggregator in GC operation can be equivalently viewed as an energy minimization model. Based on it, we can define a novel partial aggregation function and derive PaGC for incomplete graph data. Experiments demonstrate the effectiveness and efficiency of the proposed PaGCN.},
  keywords={Standards;Training;Minimization;Convolutional neural networks;Data models;Task analysis;Learning (artificial intelligence);Attribute-incomplete graph;Dropout;graph convolutional network (GCN);partial aggregation;semi-supervised learning},
  doi={10.1109/TAI.2024.3386499},
  ISSN={2691-4581},
  month={Sep.},}@INPROCEEDINGS{9288213,
  author={Safovich, Yuri and Azaria, Amos},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Fiction Sentence Expansion and Enhancement via Focused Objective and Novelty Curve Sampling}, 
  year={2020},
  volume={},
  number={},
  pages={835-843},
  abstract={We describe the task of sentence expansion and enhancement, in which a sentence provided by a human is expanded in some creative way. The expansion should be understandable, believably grammatical, and highly related to the original sentence. Sentence expansion and enhancement may serve as an authoring tool, or integrate in dynamic media, conversational agents, and advertising. We implement a neural sentence expander, which is trained on sentence compressions generated from a corpus of modern fiction. We modify the objective loss function to support the task by focusing on new words, and decode at test time with controlled curve-like novelty sampling. We run the sentence expander on sentences provided by human subjects and have humans evaluate these expansions. The generation methods are shown to be comparable to, and as well liked as, subjects' original input sentences, and preferred over baselines.},
  keywords={Authoring systems;Conferences;Focusing;Tools;Media;Task analysis;Artificial intelligence;NLP, Sentence Enhancement, Text Generation},
  doi={10.1109/ICTAI50040.2020.00132},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10073671,
  author={Naga Venkata Satya Sirisha, Tummuri Sri and Venkata Sai Mada, Naga and Haritha, Sriram and Tumuluru, Praveen and Rachapudi, Venubabu},
  booktitle={2023 Third International Conference on Artificial Intelligence and Smart Energy (ICAIS)}, 
  title={Evaluating the Performance of YOLO V5 for Electronic Device Classification}, 
  year={2023},
  volume={},
  number={},
  pages={992-997},
  abstract={This study explores YOLO V5 (You Look Only Once version 5) for image classification. With the input data primarily concentrating on this area, four distinct types of electronic devices were used in this experiment. The task of object detection for electronic device classification is within the scope of computer vision and involves the identification and categorization of various electronic device kinds in both still photos and moving pictures. Among the well-known object detection techniques are Support vector machines (SVMs), decision trees, and more contemporary deep learning techniques such as convolutional neural networks (CNNs), You Only Look Once (YOLO). A variety of factors including scale variation, occlusion, and changes in illumination and position, can have an impact on how well YOLO performs when classifying electronic devices. The usage of ensembles of models, multi -scale training and testing, and data augmentation are a few of the strategies that researchers have suggested to overcome these issues. Despite tremendous advancement, object detection for electronic device classification using YOLO is still an active field of study, and new methods are constantly being developed to boost the precision and effectiveness of this approach. YOLO V5 operates more quickly and accurately. Thus, YOLO V5 has been selected for the most effective training approach. Where the overall average is 90% and above.},
  keywords={Training;Performance evaluation;Support vector machines;Lighting;Object detection;Learning (artificial intelligence);Motion pictures;Device classification;Detection;Classification;Computer Vision;Machine Learning},
  doi={10.1109/ICAIS56108.2023.10073671},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10730117,
  author={Sigua, Gwyneth Patricia D. and Corpuz, Cristelle S. and Villaverde, Jocelyn F.},
  booktitle={2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={Shrimp Freshness Quality Assessment Using YOLOv5}, 
  year={2024},
  volume={},
  number={},
  pages={378-383},
  abstract={Shrimps are known for their benefits in the Philippine aquaculture, health, and food economy. Determining a shrimp's freshness and Quality before consumption is imperative, as it has a sensitive shelf life that can cause serious consumer concerns. We came up with a portable device that captures and determines the type of Shrimp: Penaeus merguiensis (White Shrimp), P. semisulcatus (Tiger shrimp), and Metapenaeus ensis (greasy-back Shrimp) and if the Shrimp is fresh or not based on its color with the utilization of YOLOv5 algorithm. The model is trained and tested using custom datasets processed through a computer. To create the device, Raspberry Pi 4B and a 720P Camera attached to the back of the device is utilized to capture the Shrimp easily. The model's performance is measured and evaluated with a confusion matrix, which produced an accuracy of 89.473% in detecting the Quality of Shrimp and 88.321% accuracy when detecting the types of Shrimp. Results reveal a successfully developed portable device that detects the Quality of Shrimp, classifies the Shrimp, and delivers results in real-time with considerable accuracy.},
  keywords={YOLO;Performance evaluation;Accuracy;Computational modeling;Color;Cameras;Real-time systems;Quality assessment;Artificial intelligence;Aquaculture;YOLOv5 model;object detection;confusion matrix;quality assessment;Raspberry Pi},
  doi={10.1109/IICAIET62352.2024.10730117},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10284705,
  author={Abdullah, Hasna Salsabilla and Ramananda, I Putu Denio Pranatha and Adisaka, Nandatama Bagus and Pangestu, William Suryadharma and Priyanto, Winita Teukeku and Anggreainy, Maria Susan},
  booktitle={2023 4th International Conference on Artificial Intelligence and Data Sciences (AiDAS)}, 
  title={Detection of Cancer Cells in Tissue Histopathology Images Using a Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={75-80},
  abstract={Cancer is a chronic disease with a high increase in the number of cases worldwide. In women, breast cancer is the most common cancer. One way to detect breast cancer is through histopathologic images of the tissue. Currently, histopathology detection is done manually, so it has shortcomings in the form of human limitations in classifying large numbers of images. In this research, a Convolutional Neural Network (CNN) model is developed to detect cancer cells in histopathology images automatically. The dataset used to train the model is 120,000 histopathology images labeled 1 (cancer) and 0 (normal) in the same ratio. Training the model is done with a batch size of 128 and epochs of 30. The confusion matrix and AUC-ROC method measure accuracy. From this study, the CNN model proved accurate in predicting tissues with cancer cells with a precision value of 94.76 percent, recall of 98.07 percent, and F-1 score of 96.39 percent.},
  keywords={Training;Histopathology;Predictive models;Breast cancer;Data models;Convolutional neural networks;Artificial intelligence;CNN;Histopathology;Breast Cancer;Tissue},
  doi={10.1109/AiDAS60501.2023.10284705},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10863625,
  author={Azumah, Sylvia Worlali and Elsayed, Nelly and Elsayed, Zag and Ozer, Murat and Guardia, Amanda La},
  booktitle={2024 2nd International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)}, 
  title={Deep Learning Approaches for Detecting Adversarial Cyberbullying and Hate Speech in Social Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Cyberbullying is a significant concern intricately linked to technology that can find resolution through technological means. Despite its prevalence, technology also provides solutions to mitigate cyberbullying. To address growing concerns regarding the adverse impact of cyberbullying on individuals’ online experiences, various online platforms and researchers are actively adopting measures to enhance the safety of digital environments. While researchers persist in crafting detection models to counteract or minimize cyberbullying, malicious actors are deploying adversarial techniques to circumvent these detection methods. This paper focuses on detecting cyberbullying in adversarial attack content within social networking site text data, specifically emphasizing hate speech. Utilizing a deep learningbased approach with a correction algorithm, this paper yielded significant results. An LSTM model with a fixed epoch of 100 demonstrated remarkable performance, achieving high accuracy, precision, recall, F1-score, and AUC-ROC scores of 87.57 %, 88.73%, 87.57%,88.15%, and 91% respectively. Additionally, the LSTM model’s performance surpassed that of previous studies.},
  keywords={Measurement;Deep learning;Accuracy;Hate speech;Cyberbullying;Learning (artificial intelligence);Safety;Internet of Things;Long short term memory;Standards;Cyberbullying;hate speech;adversarial attacks;deep learning},
  doi={10.1109/AIBThings63359.2024.10863625},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10165632,
  author={Zhang, Yuting and Gu, Yu},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={A Survey of Traditional and Machine Learning-based Malware Detection Techniques}, 
  year={2023},
  volume={3},
  number={},
  pages={1373-1378},
  abstract={Malware activities have become more and more frequent in recent years, and malware has become an essential threat in the field of network security. Malware detection technology has also received more and more attention. Since the machine learning method can effectively solve the classification problem in malware detection technology, the malware detection technology based on machine learning is gradually applied in various scenarios. This paper first summarizes the traditional malware detection technology, then introduces the classification and specific implementation methods of machine learning-based malware detection technology, and finally summarizes the problems faced by the current detection technology and expounds possible future research.},
  keywords={Training;Surveys;Machine learning;Learning (artificial intelligence);Network security;Big Data;Malware;network security;malware;intrusion detection;machine learning},
  doi={10.1109/ICIBA56860.2023.10165632},
  ISSN={},
  month={May},}@ARTICLE{10494066,
  author={Ye, Guifeng and Lu, Shaowen},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Prefetching-based Multiproposal Markov Chain Monte Carlo Algorithm}, 
  year={2024},
  volume={5},
  number={9},
  pages={4493-4505},
  abstract={Our proposed algorithm is a prefetching-based multiproposal Markov Chain Monte Carlo (PMP-MCMC) method that efficiently explores the target distribution by combining multiple proposals with the concept of prefetching. In our method, not all proposals are directly derived from the current state; some are derived from future states. This approach breaks through the inherent sequential characteristics of traditional MCMC algorithms. Compared with single-proposal and multiproposal methods, our approach speeds up by $K$ times and the burn-in period is reduced by a factor of $1/\text{log}_{2}K$ maximally, where $K$ is the number of parallel computational units or processing cores. Compared with prefetching method, our method has increased the number of samples per iteration by a factor of $K/\text{log}_{2}K$. Furthermore, the proposed method is general and can be integrated into MCMC variants such as Hamiltonian Monte Carlo (HMC). We have also applied this method to optimize the model parameters of neural networks and Bayesian inference and observed significant improvements in optimization performance.},
  keywords={Proposals;Prefetching;Parallel processing;Monte Carlo methods;Hidden Markov models;Computational modeling;Artificial intelligence;Bayesian inference;hardware acceleration;Markov Chain Monte Carlo (MCMC);neural networks;parallelization},
  doi={10.1109/TAI.2024.3385384},
  ISSN={2691-4581},
  month={Sep.},}@ARTICLE{10820825,
  author={Wang, Shuyue and Liu, Zhunga and Zhang, Zuowei and Bennamoun, Mohammed},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Weakly Correlated Multimodal Domain Adaptation for Pattern Classification}, 
  year={2025},
  volume={6},
  number={5},
  pages={1360-1372},
  abstract={Multimodal domain adaptation (MMDA) aims to transfer knowledge across different domains that contain multimodal data. Current methods typically assume that both the source and target domains have paired multimodal data with the same modalities, allowing for direct knowledge transfer between corresponding types of data. However, in certain applications, the source domain benefits from advanced sensors and equipment, capturing more modalities than those available in the target domain. As a result, the information from the source modalities may not strongly align with that of the target modalities. This weak correlation hinders the effective utilization of all source data for the target domain. To address this challenge, we propose a weakly correlated multimodal domain adaptation (WCMMDA) method for pattern classification. WCMMDA is designed to acquire the modality-independent and category-related knowledge from the source domain, enabling the full utilization of available source modalities for effective knowledge transfer. Specifically, modality-invariant features are first extracted from the multimodal data to bridge the heterogeneity gap within each domain. Subsequently, domain-invariant features are further learned from these modality-invariant features to align the feature distributions across the source and target domains. A source-specific classifier is employed here, which predicts pseudo-labels for the target data and enables the feature extractor to explore category-related information in source features. Finally, a target-specific classifier is trained using the pseudolabeled target data, where highly reliable pseudolabels are selected based on confidence to improve classification performance. Extensive experiments are performed on the real-world multimodal datasets to demonstrate the superiority of WCMMDA.},
  keywords={Feature extraction;Training;Correlation;Knowledge transfer;Artificial intelligence;Adaptation models;Optical imaging;Data mining;Data models;Synthetic aperture radar;Domain adaptation;invariant feature;knowledge transfer;multimodal data;pattern classification;pseudolabel},
  doi={10.1109/TAI.2024.3524976},
  ISSN={2691-4581},
  month={May},}@ARTICLE{10477772,
  author={Shi, Ruolan and Wang, Zichi and Hao, Yunlong and Zhang, Xinpeng},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Steganography in Style Transfer}, 
  year={2024},
  volume={5},
  number={12},
  pages={6054-6065},
  abstract={Steganography entails concealing secret data within a given medium for covert communication. In recent years, style-transferred images have been widely disseminated on social media, offering a novel multimedia carrier for steganography. However, there is currently a lack of steganographic techniques specifically designed for style-transferred images. In this article, we propose disguising the steganographic tool as a deep neural network (DNN) performing style transfer tasks. In our method, a neural network is manipulated to transfer the style of a given image to a target style, while also embedding secret data into the given image. Meanwhile, a trained receiving network is used to extract the embedded data. The same pretrained network used by the processing network and the receiving network matches the feature maps of secret data at the same layers. Under the guidance of secret data, a stego image is generated after being trained by the processing network and the receiving network an appropriate number of times, and secret data can be extracted from stego image. Experimental results demonstrate that our method is more effective and secure compared to existing steganographic algorithms that achieve data embedding by modifying image content.},
  keywords={Steganography;Watermarking;Feature extraction;Artificial intelligence;Task analysis;Data mining;Discrete cosine transforms;Covert communication;neural style transfer;security;steganography},
  doi={10.1109/TAI.2024.3379946},
  ISSN={2691-4581},
  month={Dec},}@ARTICLE{10896943,
  author={Guo, Honglin and Nie, Weizhi and Chen, Ruidong and Wang, Lanjun and Jin, Guoqing and Liu, Anan},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={ContentDM: A Layout Diffusion Model for Content-Aware Layout Generation}, 
  year={2025},
  volume={6},
  number={8},
  pages={2215-2225},
  abstract={Content-aware layout generation aims to produce fitting element arrangements based on the background contents, which is used for graphic design applications such as automatic poster layout design. In this article, we propose ContentDM, a layout diffusion model specifically designed for the content-aware layout generation task, overcoming the limitations suffered from existing methods: irrational arrangement among layout elements and lack of refining ability for coarse generated results. ContentDM defines the layout diffusion process through random perturbations applied to both the position and type of layout elements. During the denoising training phase, the content-aware layout generator is trained to reconstruct samples from these perturbed layouts. This process enables the model to learn the correct arrangement patterns within the layout elements, thereby enhancing the rationality of generated layouts. Moreover, we develop an iterative layout inference strategy to enable the layout generator to refine the generated layouts progressively, thereby enhancing the overall quality of the generation results. Extensive experiments demonstrate that ContentDM significantly outperforms existing methods, achieving state-of-the-art performance in content-aware layout generation, both in terms of visual quality and quantitative metrics.},
  keywords={Layout;Diffusion models;Training;Visualization;Noise reduction;Generators;Diffusion processes;Artificial intelligence;Perturbation methods;Noise measurement;Design tool;diffusion model;layout generation},
  doi={10.1109/TAI.2025.3544172},
  ISSN={2691-4581},
  month={Aug},}@ARTICLE{10795152,
  author={Li, Changping and Wang, Bingshu and Zheng, Jiangbin and Zhang, Yongjun and Chen, C.L. Philip},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Unsigned Road Incidents Detection Using Improved RESNET From Driver-View Images}, 
  year={2025},
  volume={6},
  number={5},
  pages={1203-1216},
  abstract={Frequent road incidents cause significant physical harm and economic losses globally. The key to ensuring road safety lies in accurately perceiving surrounding road incidents. However, the highly dynamic nature of traffic introduces significant challenges, particularly in detecting sudden and temporary incidents. In this article, we propose a novel detection framework, multihead attention ResNet with dynamic bottleneck (DB_RESNET_MHA), to identify physical unsigned road incidents. Our approach introduces three key innovations. First, we develop a tailored data augmentation strategy to create images that closely mimic the complex variations found in real-world road environments. Second, we enhance model expressiveness by employing attention mechanisms to nonlinearly integrate convolutional kernels within the residual network. Furthermore, we refine the prediction head by applying spatially distinct attention weights, enabling the model to capture intricate correlations between different features more effectively. To demonstrate the effectiveness of our method, we create a dataset for unsigned road incidents (UnsignRI), comprising a total of 16 323 images that capture 12 distinct types of incidents. It stands out as the most comprehensive dataset in the field, encompassing a wide range of geographical features and incident categories. Experimental results show that DB_RESNET_MHA achieves an average accuracy of 96.2% and a f1-score of 0.955 across various categories of unsigned incidents, surpassing other models.},
  keywords={Roads;Artificial intelligence;Accuracy;Vehicles;Landslides;Accidents;Animals;Pedestrians;Monitoring;Manuals;Attention mechanism;incident detection;residual modules;unsigned road incidents},
  doi={10.1109/TAI.2024.3515938},
  ISSN={2691-4581},
  month={May},}@ARTICLE{10613414,
  author={Adhya, Suman and Lahiri, Avishek and Sanyal, Debarshi Kumar and Das, Partha Pratim},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Evaluating Negative Sampling Approaches for Neural Topic Models}, 
  year={2024},
  volume={5},
  number={11},
  pages={5630-5642},
  abstract={Negative sampling has emerged as an effective technique that enables deep learning models to learn better representations by introducing the paradigm of “learn-to-compare.” The goal of this approach is to add robustness to deep learning models to learn better representation by comparing the positive samples against the negative ones. Despite its numerous demonstrations in various areas of computer vision and natural language processing, a comprehensive study of the effect of negative sampling in an unsupervised domain such as topic modeling has not been well explored. In this article, we present a comprehensive analysis of the impact of different negative sampling strategies on neural topic models. We compare the performance of several popular neural topic models by incorporating a negative sampling technique in the decoder of variational autoencoder-based neural topic models. Experiments on four publicly available datasets demonstrate that integrating negative sampling into topic models results in significant enhancements across multiple aspects, including improved topic coherence, richer topic diversity, and more accurate document classification. Manual evaluations also indicate that the inclusion of negative sampling into neural topic models enhances the quality of the generated topics. These findings highlight the potential of negative sampling as a valuable tool for advancing the effectiveness of neural topic models.},
  keywords={Computational modeling;Vectors;Decoding;Analytical models;Context modeling;Training;Artificial intelligence;Contrastive learning;negative sampling;neural topic model;topic coherence;variational autoencoder (VAE)},
  doi={10.1109/TAI.2024.3432857},
  ISSN={2691-4581},
  month={Nov},}@INPROCEEDINGS{9918704,
  author={Widjaja, Denovan and Fustian, Timothy and Lucky, Henry and Suhartono, Derwin},
  booktitle={2022 3rd International Conference on Artificial Intelligence and Data Sciences (AiDAS)}, 
  title={Performance Comparison of Improved Common Sequence to Sequence Paraphrasing Models}, 
  year={2022},
  volume={},
  number={},
  pages={299-304},
  abstract={Paraphrase generation is a widely known and complex task in the field of Natural Language Processing (NLP). With a variety of paraphrasing models, many researchers have developed their own methodology to overcome this task with better efficiency and quality paraphrases. In this paper, Text-to-Text Transfer Transformer (T5) and Back-Translation guided multi-round Paraphrase Generation (BTmPG), two popular algorithms in paraphrasing models, are compared. Paired paraphrased sentences are used to train both models. The goal of paraphrasing models is to generate paraphrases with a high degree of semantic similarity and the fewest possible grammatical errors and sentence structure copying. For the purpose of paraphrase generation, both the T5 and BTmPG models have been optimized. This study aims to compare the performance of the T5 and BTmPG models and identify factors that have a significant influence on output quality. The accuracy and precision of the T5 and BTmPG models' paraphrase quality are calculated using BLEU and ROUGE1. Despite requiring more time for training, the T5 model outperformed the BTmPG model slightly, according to the results. Despite having a smaller dataset size, the MRPC dataset was found to be more efficient than the QQP dataset. With proper grammar and lexicality, both models produced results that were satisfactory.},
  keywords={Training;Semantics;Transformers;Natural language processing;Grammar;Task analysis;Artificial intelligence;Natural Language Processing;paraphrase generation;Seq2Seq},
  doi={10.1109/AiDAS56890.2022.9918704},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10803399,
  author={He, Tao and Jiang, Xiaoming and Wu, Jia and Wang, Wanyun and Zhang, Han and Li, Zhangyong},
  booktitle={2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={Dual-Domain Image Reconstruction Network Integrating Residual Attention for Sparse View Computed Tomography}, 
  year={2024},
  volume={},
  number={},
  pages={282-287},
  abstract={Sparse View Computed Tomography (SVCT) is a promising technology for reducing radiation dose and improving scan speed by reducing the number of required X-ray projection images. However, the limited projection data used in SVCT often results in reconstructed images with noise and artifacts. We propose a dual-domain SVCT image reconstruction network that leverages both residual attention mechanisms and convolutional networks. The network utilizes information from both the sinogram and image domains, enabling complementary information supplementation. To further enhance image restoration, we integrate a residual attention mechanism into the Swin-Transformer architecture to mitigate feature collapse. Additionally, a parallel convolutional branch is incorporated to enhance feature extraction completeness.By leveraging the remote correlation capability of attention mechanisms and the local information extraction capabilities of convolutional networks, our model effectively extracts valuable information for reconstruction. This integration of local and global information effectively guides the reconstruction process. Experiments on the NIH-AAPM dataset demonstrate that our model effectively reduces artifacts and improves the reconstruction of complex structures, outperforming other learning-based reconstruction models.},
  keywords={Measurement;Attention mechanisms;Computational modeling;Computed tomography;Noise;Learning (artificial intelligence);Feature extraction;Convolutional neural networks;Image reconstruction;X-ray imaging;Sparse view;CT Resconstruction;Attention Mechanism;Dual-domain Network;Residual Learning},
  doi={10.1109/MedAI62885.2024.00043},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10605495,
  author={Cai, Zeyu and Huang, Zhelong and Zheng, Xu and Liu, Yexin and Liu, Chao and Wang, Zeyu and Wang, Lin},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Interact360: Interactive Identity-driven Text to 360° Panorama Generation}, 
  year={2024},
  volume={},
  number={},
  pages={728-736},
  abstract={360° images offer an immersive and realistic visual experience for the emerging field, such as virtual tourism, particularly when users dream of recording their sweet moments without physically visiting a place. In this case, it is valuable to generate the 360° scene of a place (e.g., seaside or mountainside) while allowing the users to generate their portraits given a taken face photo, which can be naturally harmonized with the generated 360° scene and their garment can be freely changed. In light of this, we propose a novel Interactive Identity-driven 360° Panorama Generation (Interact360) approach that produces vivid 360° panoramas featuring human portraits based on user inputs and commands, which encompass both the user’s identity and the scene text description. Interact360 consists of three interactive components: prompt refinement module, identity-driven 2D portrait generation module, and 360° panorama generation module. Our approach enables users to modify and iterate on each component according to their individualized and customized requirements. We conduct both objective and subjective analysis to evaluate the effectiveness of our approach. The quantitative and qualitative results for the portrait image generation demonstrate the superiority of our method. Moreover, user studies provide empirical evidence of our method’s effectiveness, demonstrating its strong interactivity and capacity to meet user requirements during the generation process, ultimately yielding satisfactory results for users. Our method highlights the potential benefits of panorama generation, particularly in addressing the personalized and customized needs of users.},
  keywords={Visualization;Image synthesis;Clothing;Recording;Artificial intelligence;Faces;360 Scene Generation;Interactive AI;Text-to-Image Generation},
  doi={10.1109/CAI59869.2024.00141},
  ISSN={},
  month={June},}@INPROCEEDINGS{10710859,
  author={Altintas, Volkan and Kilinc, Murat},
  booktitle={2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={Automated Categorization of Turkish E-commerce Product Reviews Using BERTurk}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Thanks to rapid technological developments, users can now easily access and distribute information. While users have the opportunity to share content and information as they wish, they can also use the information shared to improve themselves in line with their own interests. Especially when buying a product, they make their final decision about the product by examining the content of the comments made about the product. Automatically classifying the comments made about the product by the system and displaying them in the relevant category is one of the issues that has been studied recently. In this study, comments on e-commerce sites were automatically categorized. The dataset was created by collecting comments in Turkish about phones, computers and headphones produced by the same company on Amazon.com.tr, one of the world’s largest e-commerce platforms, with the help of a Python script. User comments were automatically sorted into categories by machine learning algorithms such as Naive Bayes, Linear Support Vector Classifier and Random Forest algorithms and pre-trained and fine-tuned Bert Multilingual and BERTurk models based on transfomer architecture. The results obtained were compared with the help of the F1-score metric. When the results of different machine learning algorithms and BERT models were compared, it was seen that the BERTurk model gave more accurate results than other models.},
  keywords={Measurement;Headphones;Machine learning algorithms;Reviews;Natural languages;Learning (artificial intelligence);Transformers;Vectors;Electronic commerce;Random forests;Natural Language Processing;Classification;Machine Learning},
  doi={10.1109/IDAP64064.2024.10710859},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10849497,
  author={Guo, Xiaoyu and Liu, Yan and Liu, Fenlin},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Adversarially Regularized Graph Embedding for User Identity Linkage Across Social Networks}, 
  year={2024},
  volume={},
  number={},
  pages={226-231},
  abstract={User identity linkage across social networks aims to discover the potential correspondence between users across different social platforms. However, it is non-trivial to solve the practically relevant problem due to the following challenges. 1) User identity linkage requires analyzing relationships within individual networks as well as across networks, making it essential to preserve the intra-network and inter-network relationships. 2) The existing methods mostly ignore the data distribution of latent embeddings and lack additional constraints for enhancing the robustness of representations. Towards this end, we propose an adversarially regularized graph embedding-based method (ARUIL) for user identity linkage across social networks. Specifically, we first adopt a pair of graph auto-encoders with shared weights to embed the source and target networks from the structure space to a common latent vector space. Then, we design an effective dual constraint mechanism, intra-network relationship preserving and inter-network relationship preserving, to optimize the overall framework jointly. The former constraint based on network reconstruction aims to capture the original structural features of input networks. The latter one, inspired by the contrastive learning paradigm, seeks to minimize the distance between positive samples in the latent vector space and increase the distance from negative samples. Finally, adversarial regularization is introduced to enforce latent embeddings to match a prior Gaussian distribution to improve the generalization performance of our proposed method. Extensive experiments on two real-world social network datasets demonstrate that ARUIL outperforms the state-of-the-art baselines. The ablation study also illustrates the rationality and effectiveness of our proposed method.},
  keywords={Couplings;Social networking (online);Contrastive learning;Gaussian distribution;Vectors;Robustness;Artificial intelligence;user identity linkage;social network analysis;adversarially regularized graph embedding},
  doi={10.1109/ICTAI62512.2024.00041},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10920847,
  author={Hsieh, Shao-Kai and Liu, Huey-Ing},
  booktitle={2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Generation of Scene Graph and Semantic Image: A Review and Challenge Ahead}, 
  year={2025},
  volume={},
  number={},
  pages={0990-0997},
  abstract={Scene graph generation creates a structured representation of visual scenes by identifying objects and their attributes and the relationships between them. Conversely, semantic image generation converts semantic representations such as scene graphs, textual descriptions, or object layouts, into photorealistic images. This paper provides an overview of recent advancements in generations of scene graph and semantic image. The survey explores how innovations in both fields can complement each other, paving the way for advanced applications in autonomous systems, semantic communication, visual reasoning, and creative industries. Challenges such as data imbalance, scalability, and semantic coherence and future research directions including multimodal integration, prior processing etc. are throughly discussed.},
  keywords={Surveys;Visualization;Technological innovation;Image synthesis;Reviews;Scalability;Semantics;Semantic communication;Artificial intelligence;Photorealistic images;Scene graph;scene graph generation;semantic image generation},
  doi={10.1109/ICAIIC64266.2025.10920847},
  ISSN={2831-6983},
  month={Feb},}@ARTICLE{10820830,
  author={Hui, Xuemeng and Liu, Zhunga and Liu, Jiaxiang and Zhang, Zuowei and Wang, Longfei},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Visual–Semantic Fuzzy Interaction Network for Zero-Shot Learning}, 
  year={2025},
  volume={6},
  number={5},
  pages={1345-1359},
  abstract={Zero-shot learning (ZSL) aims to recognize unseen class image objects using manually defined semantic knowledge corresponding to both seen and unseen images. The key of ZSL lies in building the interaction between precise image data and fuzzy semantic knowledge. The fuzziness is attributed to the difficulty in quantifying human knowledge. However, the existing ZSL methods ignore the inherent fuzziness of semantic knowledge and treat it as precise data during building the visual–semantic interaction. This is not good for transferring semantic knowledge from seen classes to unseen classes. In order to solve this problem, we propose a visual–semantic fuzzy interaction network (VSFIN) for ZSL. VSFIN utilize an effective encoder–decoder structure, including a semantic prototype encoder (SPE) and visual feature decoder (VFD). The SPE and VFD enable the visual features to interact with semantic knowledge via cross-attention. To achieve visual–semantic fuzzy interaction in SPE and VFD, we introduce the concept of membership function in fuzzy set theory and design a membership loss function. This loss function allows for a certain degree of imprecision in visual–semantic interaction, thereby enabling VSFIN to becomingly utilize the given semantic knowledge. Moreover, we introduce the concept of rank sum test and propose a distribution alignment loss to alleviate the bias towards seen classes. Extensive experiments on three widely used benchmarks have demonstrated that VSFIN outperforms current state-of-the-art methods under both conventional ZSL (CZSL) and generalized ZSL (GZSL) settings.},
  keywords={Semantics;Zero shot learning;Visualization;Vectors;Fuzzy set theory;Artificial intelligence;Knowledge engineering;Prototypes;Overfitting;Image recognition;Fuzzy set theory;knowledge transfer;membership function;object recognition;zero-shot learning},
  doi={10.1109/TAI.2024.3524955},
  ISSN={2691-4581},
  month={May},}@INPROCEEDINGS{10137854,
  author={Chen, Yaqi and Qu, Dan and Zhang, Wenlin and Yu, Fen and Zhang, Hao and Yang, Xukui},
  booktitle={2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Adversarial Meta Learning Improves Low-Resource Speech Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Low-resource automatic speech recognition is a chal- lenging task. To solve this issue, multilingual meta-learning learns a better model initialization from many source language tasks, allowing for rapid adaption to the target language. However, due to the lack of limitations on multilingual pre-training, the shared semantic space of different languages is difficult to learn. In this work, we propose an adversarial meta-learning training approach to solve this problem. By using the adversarial auxiliary aim of language identification in the meta-learning algorithm, it will guide the model encoder to generate language-independent embedding features, which can improve model generalization. And we use Wasserstein distance and temporal normalization to optimize our adversarial training, making the training more stable and easier. The approach is evaluated on the IARPA BABEL. The results reveal that our approach only requires half as many meta learning training epochs to attain comparable multilingual pre-training performance. It also outperforms the meta learning in all target languages fine-tuning and achieves comparable performance in small data scales. Specially, it can reduce CER from 71% to 62% with fine-tuning 25% of Vietnamese data. Finally, we show why our approach is superior than others by using t-SNE.},
  keywords={Training;Adaptation models;Semantics;Task analysis;Artificial intelligence;Automatic speech recognition;adversarial training;meta learning;low resource;speech recognition;IARPA BABEL},
  doi={10.1109/ACAIT56212.2022.10137854},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10920500,
  author={Yu, Bochao and Wang, Jun and Ren, He and Huang, Weiguo and Zhu, Zhongkui},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Single-Source Domain Generalization Fault Diagnosis of Wheel-Set Bearings Based on Flow Model and Contrastive Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Domain generalization methods have demonstrated their effectiveness in identifying mechanical faults under previously unencountered working conditions. However, the majority of these approaches necessitate data from multiple source domains to train the model. Nevertheless, the collection of monitoring data of each health state under different working conditions is unpractical for wheel-set bearings. Aiming at the difficulty of obtaining multi-source domain data, we present a single-source domain generalization model that leverages flow modeling and contrastive learning techniques for health state identification of wheel-set bearings under diverse operating conditions. The proposed model employs flow model as a domain generation module to produce datasets within an extended domain. Then, domain-invariant features are extracted from the source and the generated domains. The diversity of generated dataset and the validity of domain-invariant features are guaranteed by a strategy of adversarial contrastive learning. Finally, single-source domain generalization diagnosis experiments carried out on a wheel-set bearing dataset verify the good performance of the presented method over traditional domain generalization methods.},
  keywords={Fault diagnosis;Employee welfare;Training;Data analysis;Flow production systems;Contrastive learning;Learning (artificial intelligence);Feature extraction;Data models;Monitoring;single-source domain generalization;fault diagnosis;wheel-set bearing;flow model;contrast learning},
  doi={10.1109/ICSMD64214.2024.10920500},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10581585,
  author={Shi, Zhuo and Xiong, Biao},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Fine-Tuning Text-to-Image Generation Models Using Curriculum Learning for Yao Costume Image Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1305-1311},
  abstract={With the emergence of diffusion models, synthesizing images from textual descriptions has become an active research field. However, current descriptions often lack specificity, focusing mostly on natural images rather than attire of Chinese ethnic minorities like the Yao group. Additionally, text-to-image tasks typically demand a large number of annotated pairs, making dataset creation costly. Leveraging multimodal pre-training, we propose a curriculum learning approach to fine-tune stable diffusion models, enabling high-quality Yao costume image generation with color, texture, and pattern fidelity. Through a curriculum-based loss function, we ensure semantic consistency between generated images and text descriptions. Ablation experiments and comparisons with DreamBooth demonstrate the superior performance of our method in terms of both image quality and semantic consistency.},
  keywords={Training;Seminars;Image quality;Image synthesis;Semantics;Text to image;Learning (artificial intelligence);Text-to-Image Generation;Stable Diffusion;Yao Ethnic Costumes Generation;Curriculum Learning;DreamBooth},
  doi={10.1109/AINIT61980.2024.10581585},
  ISSN={},
  month={March},}@INPROCEEDINGS{11063998,
  author={Tulshyan, Kajal and Kumar, Gautam},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Squeeze-and-Excitation Integrated MobileNetV2 for Efficient DeepFake Detection}, 
  year={2025},
  volume={3},
  number={},
  pages={1778-1782},
  abstract={The rapid advancement of deepfake technology has raised significant concerns regarding the authenticity of visual content, creating an urgent need for efficient and accurate detection methods. This paper presents an enhanced MobileNetV2 architecture integrated with a Squeeze-and-Excitation (SE) block, designed to improve performance on image classification tasks, particularly for distinguishing real and fake faces. The proposed SE-enhanced MobileNetV2 model is evaluated using a dataset of 1,40,000 real and fake face images, achieving an impressive 97% accuracy on the test dataset. Comparative analyses demonstrate that the SE-enhanced MobileNetV2 outperforms baseline models, including standard MobileNetV2, EfficientNet-B0, and EfficientNet-B7, in terms of both classification accuracy and resource consumption.},
  keywords={Deep learning;Deepfakes;Visualization;Analytical models;Accuracy;Security;Artificial intelligence;Faces;Standards;Image classification;Deepfake;MobileNetV2;EfficientNet;Deep Learning;Squeeze-and-Excitation (SE) block},
  doi={10.1109/ICCSAI64074.2025.11063998},
  ISSN={},
  month={April},}@INBOOK{10951250,
  author={Hilbush, Brian S.},
  booktitle={In Silico Dreams: How Artificial Intelligence and Biotechnology Will Create the Medicines of the Future}, 
  title={AI in Drug Discovery and Development}, 
  year={2021},
  volume={},
  number={},
  pages={245-268},
  abstract={Summary <p>In pharmacology and drug discovery, computational methods for deriving structure&#x2010;activity relationships date back to the 1950s with the foundational work of Corwin Hansch. Success with AI in drug discovery will require some rethinking of the traditional approaches toward tackling problems and retooling of processes for developing drugs. Pharmacological modeling and in silico drug discovery have advanced primarily from incorporating modern computing and statistical techniques to exploit newly available data&#x2010;generating technologies and the exponential growth of compound databases. Cheminformatics tools are now critical at the starting point for drug design and screening projects. Structure&#x2010;based virtual screening programs and drug design use 3D structures from Protein Data Bank and other sources in small molecule drug discovery. Biotechnology stands as the best model for how technology outside of small molecule chemistry R&amp;D can expand the base of innovation in pharmaceutical research. The pace of advancement in information technology is exponentially greater than in biology and medicine.</p>},
  keywords={Artificial intelligence;Drugs;Chemicals;Compounds;Drug discovery;Biology;Chemistry;Databases;Computational modeling;Libraries},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119745648},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951250},}@INPROCEEDINGS{11021732,
  author={Yang, Yi and Du, Lun and Liu, Meng and Li, Zhuangzhuang and Yan, Danfeng},
  booktitle={2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Open-World Knowledge Graph Based on Text Knowledge Transformation and Alignment}, 
  year={2024},
  volume={},
  number={},
  pages={1240-1247},
  abstract={With the development of knowledge graph completion technology, many models have emerged that can predict missing relations in knowledge graphs with high accuracy. However, these relation prediction models only perform well in predicting relations that have already appeared in the knowledge graph, and perform poorly for new relations that have not appeared in the knowledge graph. In this paper, we propose an open-domain knowledge graph relation prediction model for predicting new relations. The proposed model genarates a relation embedding that can be aligned into the knowledge graph embedding by learning the semantics of a new relation from a textual description of that relation. With this relation embedding, relation prediction can be achieved based on knowledge graph embedding. Since this model considers both neighbor entity features and neighbor topology features of relations when extracting features from text, it performs well on open-domain knowledge graph completion tasks. Experiments show that this model exhibits an excellent ability to predict new relations without a large-scale training dataset.},
  keywords={Training;Accuracy;Semantics;Knowledge graphs;Predictive models;Feature extraction;Topology;Artificial intelligence;knowledge graph;zero shot;open-world},
  doi={10.1109/ACAIT63902.2024.11021732},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9288334,
  author={Stekel, Avigail and Hanukoglu, Moshe and Rovshitz, Aviv and Goldberg, Nissan and Azaria, Amos},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Learning to Conceal: A Method for Preserving Privacy and Avoiding Prejudice in Images}, 
  year={2020},
  volume={},
  number={},
  pages={761-766},
  abstract={We introduce a learning model able to conceal personal information (e.g. gender, age, ethnicity, etc.) from an image while maintaining any additional information present in the image (e.g. smile, hair-style, brightness). Our trained model is not provided the information that it is concealing, and does not try learning it either. Namely, we created a variational autoencoder (VAE) model that is trained on a dataset including labels of the information one would like to conceal (e.g. gender, ethnicity, age). These labels are directly added to the VAE's sampled latent vector. Due to the limited number of neurons in the latent vector and its appended noise, the VAE avoids learning any relation between the given images and the given labels, as those are given directly. Therefore, the encoded image lacks any of the information one wishes to conceal. The encoding may be decoded back into an image according to any provided properties (e.g. a 40-year old woman). Our method successfully conceals the private information; a convolutional neural network trained on the concealed images cannot restore the original private information. In contrast to the private information, a user study shows that the remaining properties of the original image carry-on to the concealed image. The proposed architecture can be used as a mean for privacy preserving and can serve as an input to systems, which will become unbiased and not suffer from prejudice.},
  keywords={Privacy;Image coding;Neurons;Learning (artificial intelligence);Tools;Image restoration;Convolutional neural networks;Privacy, VAE, Fair representation},
  doi={10.1109/ICTAI50040.2020.00121},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{10812947,
  author={Pillai, Gargi V. and Sen, Debashis},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Deep Temporally Recursive Differencing Network for Anomaly Detection in Videos}, 
  year={2025},
  volume={6},
  number={5},
  pages={1414-1428},
  abstract={Intelligent video surveillance systems with anomaly detection capabilities are indispensable for outdoor security. Video anomaly detection (VAD) is usually performed by learning patterns representing normal events and declaring an anomaly when an abnormal pattern is encountered. However, the features of normal patterns in a video often vary with time as real-world videos are non-stationary in nature, which makes its handling essential during VAD. To this end, we propose an approach for anomaly detection in videos, where a novel deep temporally recursive differencing network (DDN) diminishes the adverse effects of the non-stationary nature on VAD. The DDN consists of multiple layers of differencing operators of optimized orders, where every two consecutive layers are separated by a suitable nonlinearity. Spatial and temporal features are extracted from nonoverlapping blocks in video frames and fed to the DDN. While the spatial feature is obtained using a pretrained network, our temporal feature computation involves the use of FlowNetS with a new training strategy that does not require ground truth. The features at the output of DDN are used in a predictor based on autoregression and moving average of the regression errors. Then, the predictor's output estimates are compared to the corresponding actual values for anomaly detection, which also involves block-level selection and consistency check. Qualitative evaluation and quantitative comparison with several existing approaches on multiple standard datasets demonstrate the effectiveness of the proposed VAD approach. An ablation study highlighting the significance of the various components of our approach and a hyperparameter analysis are also provided.},
  keywords={Feature extraction;Videos;Anomaly detection;Optical flow;Security;Training;Long short term memory;Histograms;Artificial intelligence;Video surveillance;Multilayer architecture;non-stationary signal handling;time-recursive differencing;video anomaly detection},
  doi={10.1109/TAI.2024.3521877},
  ISSN={2691-4581},
  month={May},}@INPROCEEDINGS{10224009,
  author={Zhou, Jian and Cao, Qike and Song, Chao and Zhou, Yu and Xi, Xiaowen and Li, Dongmei and Zhang, Xiaoping},
  booktitle={2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)}, 
  title={Multi-Head Self-Attention Network for Multi-View Deep Clustering}, 
  year={2023},
  volume={},
  number={},
  pages={3-8},
  abstract={Multi-view clustering comprehensively processes data objects by studying different views or modalities, as well as the differences or complementary information between different views, then classifies the data objects into different groups by unsupervised learning. However, most of the work these days fuses the information of multiple views only after rough processing, which ignores the difference of information value among different data features in the process of deep fusion representation. In this paper, we propose a Multi-head Self-attention network for Deep Clustering (MSDC) to highlight valuable information features and improve the effect of multi-view clustering. Comprehensive experiments have been carried out on real world datasets and results show that MSDC outperforms other advanced baselines, demonstrating that MSDC provides an excellent approach for multi-view clustering.},
  keywords={Fuses;Computational modeling;Clustering methods;Artificial intelligence;Unsupervised learning;Software engineering;multi-head;self-attention;multi-view;deep clustering},
  doi={10.1109/SNPD-Winter57765.2023.10224009},
  ISSN={},
  month={July},}@INPROCEEDINGS{10356450,
  author={Chen, Katrina and Liang, Xiuqin and Ma, Zheng and Zhang, Zhibin},
  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={GEDI: A Graph-based End-to-end Data Imputation Framework}, 
  year={2023},
  volume={},
  number={},
  pages={723-730},
  abstract={Data imputation is an effective way to handle missing data, which is common in practical applications. In this study, we propose and test a novel data imputation process that achieves two important goals: 1) preserving the row-wise similarities among observations and column-wise contextual relationships among features in the feature matrix. 2) tailoring the imputation process to some specific downstream label prediction task. The proposed imputation process uses Transformer and graph structure learning to iteratively refine the contextual relationships among features and similarities among observations. Moreover, it implements a meta-learning framework to select features that are influential to the downstream prediction task of interest. We conduct experiments on real-world datasets, and show that the proposed method consistently improves imputation and label prediction performance over a variety of benchmark methods.},
  keywords={Metalearning;Predictive models;Benchmark testing;Transformers;Task analysis;Artificial intelligence;Missing Data Imputation;Graph Neural Network;Meta-Learning},
  doi={10.1109/ICTAI59109.2023.00112},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10933611,
  author={Zhao, Weibin and Li, Zhuqiao and Mai, Xitong and Huang, Ruomei},
  booktitle={2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Anti-counterfeiting noise addition strategy model for voice cloning}, 
  year={2024},
  volume={},
  number={},
  pages={351-360},
  abstract={This study aims to explore effective anti-counterfeiting noise addition strategies for voice cloning technology based on VITS. Initially, multiple public voice datasets were collected and normalized to ensure data consistency and comparability. Subsequently, various noise addition processes were applied to the original voice samples based on the proposed anti-counterfeiting methods. In the experiments, we systematically compared the effects of different noise addition methods, including Gaussian noise, salt-and-pepper noise, blank frame combination operations, and frame window standard convolution, on the voice cloning model. The evaluation was conducted using MCD (Mel Cepstral Distortion) scores for voice synthesis quality and MOS (Mean Opinion Score) to weight the merits of each method in terms of anti-counterfeiting effectiveness. The conclusion drawn is that a combination of frame window standard convolution and Gaussian noise can most effectively interfere with AI cloning models without affecting the transmission of vocal information.},
  keywords={Training;Convolution;Gaussian noise;Cloning;Interference;Distortion;Robustness;Artificial intelligence;Standards;Glass box;AI Voice Cloning Crime;Anti-Counterfeiting Noise Addition Strategies;White-Box Attacks;Frame Window Standard Convolution Noise Addition},
  doi={10.1109/ICCBD-AI65562.2024.00066},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10947904,
  author={V, Nikhil and Adithiyan, P V and M M, Sri Vathsan and Kumar, S Praveen and Sendur, G Jeevan and Oviya, I. R.},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={Enhancing Skin Lesion Classification Accuracy: A Deep Learning Approach Using RexNet 150 for Improved Dermatological Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={546-551},
  abstract={The diagnosis of skin lesions is still a major global health concern, requiring the use of sophisticated procedures for early and precise identification. In this work, we suggest a deep learning strategy that makes use of the complex RexNet 150 architecture to improve the segmentation accuracy of skin lesions. Utilizing a broad dataset of more than 60,000 photos representing a range of skin lesions, such as Melanoma, BCC and Keratoses, our approach attains exceptional testing accuracy of 97.91% and training accuracy of 99.21%. RexNet 150 outperforms conventional CNN models, exhibiting markedly improved accuracy and reduced loss, proving its effectiveness in correctly classifying skin lesions. Our goal is to enhance skin cancer treatment and early diagnosis by utilizing RexNet 150's capabilities, which will aid in the continuous fight against this common and potentially lethal illness.},
  keywords={Deep learning;Visualization;Accuracy;Biological system modeling;Medical services;Turning;Skin;Lesions;Artificial intelligence;Skin cancer;Skin lesion;Skin Cancer;Deep Learning;Image Processing;Humans Against Machines 10000 (HAM10000);RexNet 150;Health Technology},
  doi={10.1109/GlobalAISummit62156.2024.10947904},
  ISSN={},
  month={Sep.},}@ARTICLE{11159253,
  author={Nguyen, Tran Thanh Phong and Gulrez, Tauseef and Culpepper, Joanne B. and Phung, Son Lam and Thanh Le, Hoang},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={CamoX: A Diffusion-based Method with Few-shot Learning for Environment-guided Camouflage Pattern Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Effective camouflage is needed for defense personnel and assets to blend seamlessly with the complex and dynamic environments. The camouflage technique must maintain the concealment capability across various environments and geographic regions. However, existing approaches, including manual design, computer-aided techniques, and deep learning methods, face significant challenges in achieving automation, scalability, and generalization across diverse and uncalibrated scenes. To address these limitations, we propose a novel diffusion-based method with few-shot learning to generate environment-guided camouflage patterns. Our method, called CamoX, consists of two major stages: meta learning and few-shot learning. In the meta learning stage, our method introduces a latent diffusion-based architecture that automatically generates camouflage patterns from noise, eliminating manual intervention and enabling scalable production. In the few-shot learning stage, our approach enforces similarity between the latent features of the camouflage patterns and target scenes by optimizing the guided mean absolute error loss. This innovation allows the generated camouflage patterns to adapt seamlessly to multiple environments with minimal retraining. Furthermore, this paper introduces a comprehensive camouflage dataset, called Camo-Meta, comprising 144,750 realistic camouflage patterns and associated metadata to support research in camouflage generation. Experimental results on multiple datasets demonstrate that CamoX outperforms existing state-of-the-art methods in key metrics.},
  keywords={Manuals;Few shot learning;Image color analysis;Metalearning;Artificial intelligence;Training;Noise;Fractals;Diffusion models;Noise reduction;Camouflage Pattern Generation;Diffusion Model;Few-Shot Learning;Guidance Diffusion},
  doi={10.1109/TAI.2025.3608758},
  ISSN={2691-4581},
  month={},}@INPROCEEDINGS{9551065,
  author={Li, Xiaorui and Li, Bin and Xie, Cheng and Liu, Qing},
  booktitle={2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Object-Orient Semantic-to-Visual Generation Model for Stickers Synthetizing}, 
  year={2021},
  volume={},
  number={},
  pages={406-411},
  abstract={Stickers are used by social users to express online chat. The retrieval, matching, and generation of stickers have significant research and application value. However, the current stickers are mainly obtained through “semantic-visual” cross-modal search matching in a pre-defined sticker library. The retrieved images from the pre-defined sticker library are fixed and limited, and often do not conform to the contextual semantics of chatting. This work focuses on synthesizing non-existent stickers instead of retrieving images. The Object-Orient Semantic-to-Visual Generation Model(OSVGAN) is proposed to instantly synthesize the corresponding sticker according to the user's semantic description. First, the intrinsic semantic relationship of each object in the sticker is established by an Object-Orient Meta Model. Then a Semantic-to-Visual model is used to synthesize corresponding images according to each object's text feature, and a Fusion Model fuses each object image to form a complete image. Finally, to enhance the semantic consistency of the generated images, the text-image alignment module(T2I) is applied to the Semantic-to-Visual model and the Fusion Model. OSVGAN can effectively reduce the demand for training samples. On the sticker dataset, our method has achieved good semantic consistency, and is comparable to the existing methods in terms of synthetic image quality.},
  keywords={Training;Image quality;Fuses;Semantics;Libraries;Pattern recognition;Artificial intelligence;stickers;image generation;object-orient;computer semantic-visual fusion},
  doi={10.1109/PRAI53619.2021.9551065},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10462843,
  author={Dzulkifli, Fahmi Akmal and Mashor, Mohd Yusoff and Raof, Rafikha Aliana A. and Jaafar, Hasnan},
  booktitle={2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP)}, 
  title={A Proposed Framework for Improving the Detection and Classification of Ki67 Expression in Astrocytoma Histopathological Images}, 
  year={2023},
  volume={},
  number={},
  pages={315-320},
  abstract={Detecting and classifying the Ki67 expression is a challenging process. The inconsistency in staining intensity and the variations in image quality are the main factors that may reduce the performance of an automated system. Therefore, this study proposes a framework that objectively improves detecting and classifying Ki67 expression in astrocytoma histopathological images. The proposed framework began with implementing the double stain normalization procedure to reduce the colour-staining intensity variations. Then, the system automatically enhanced the morphological features of the Ki67 expression. The following step was to segment the enhanced images by using the U-Net network model. The last step of the proposed framework was to localize and classify the Ki67 expression based on the modified YOLOv3 model. In conclusion, the proposed YOLOv3 model produced a high detection result with a mean average precision of 0.80 for detecting Ki67-positive cells and 0.87 for detecting Ki67-negative cells.},
  keywords={YOLO;Image quality;Image segmentation;Conferences;Artificial intelligence;Astrocytoma;Image Enhancement;Image Segmentation;Ki67 expression;Stain Normalization;YOLOv3},
  doi={10.1109/IWAIIP58158.2023.10462843},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10882461,
  author={Venkatesh, B and Annapoorna, S. and Das, Atoshi and Renusree, P. and Sindhuja, R.},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Melanoma Detection Using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Melanoma is the deadliest type of skin cancer, so early detection is essential and requires accurate identification. With performance on par with dermatologists, Convolutional Neural Network (CNN) based classifiers have become the go- to approach for melanoma identification. In this paper, the use of the VGG-16 convolutional neural network architecture for melanoma detection in dermatoscopic images is investigated. Class imbalance and image variances are addressed by fine- tuning the model using a diverse dataset by utilizing VGG- 16's deep hierarchical feature extraction capabilities. The model is tuned for accuracy and generalization by leveraging aug- mentation and transfer learning approaches. Training entails optimizing hyperparameters and evaluating them on an unknown dataset using quantitative performance metrics such as F1 score, Precision, Recall, and accuracy. VGG16 CNN model achieved an 88.60% of accuracy over other models.},
  keywords={Training;Measurement;Accuracy;Quantum computing;Computational modeling;Transfer learning;Melanoma;Learning (artificial intelligence);Convolutional neural networks;Tuning;Deep Learning;CNN;Melanoma;VGG16;Transfer Learning},
  doi={10.1109/ICAIQSA64000.2024.10882461},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8995218,
  author={Sun, Xiangyuan and Li, Xiaoyong and Ren, Kaijun and Song, Junqiang},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Solving the Defect in Application of Compact Abating Probability to Convolutional Neural Network Based Open Set Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={856-863},
  abstract={Close set is a hypothesis utilized by the majority of machine-learning-based (ML-based) recognition algorithms, assuming all testing classes are known at training time. In real world, the more practical model is Open Set Recognition (OSR), which allows the presence of unknown classes at testing time, but requires the rejection ability of the model. The compact abating probability (CAP) model, which assumes the probability of class membership decreases in value (abates) as points move from known data toward open space, is first raised in traditional ML-based OSR method and soon become the basis of majority of later developed works. Most of convolutional-neural-network-based (CNN-based) OSR methods also adopted this model as their basis. During our exploration, however, we find that the application of CAP model to the CNN-based OSR method is restricted by the difference of its feature space from that of ML-based method. To the best of our knowledge, we are the first group who find this gap. To fill this gap, we propose a method called OpenSoftMax to transform the CNN-based methods' features by the process of SoftMax. In order to investigate performance, we further implement quantitative comparison between our OpenSoftMax method and the well-known CNN-based method OpenMax on caltech256 datasets. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposals.},
  keywords={Training;Accuracy;Transforms;Vectors;Data models;Proposals;Convolutional neural networks;Artificial intelligence;Open data;Testing;Deep Neural Network;Open Set Recognition;Defect;Compact Abating Probability},
  doi={10.1109/ICTAI.2019.00122},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10490542,
  author={Shang, Ziqin and Cheng, Baoping and Xie, Xiaoyan and Fu, Tao and Wu, Zijian},
  booktitle={2023 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Research on Real-Time Pedestrian Detection Based on Infrared-Visible Image Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={With the advancement of AI technology and hardware, intelligent surveillance systems are becoming increasingly widespread, bringing more and more complex application scenes. Pedestrian detection, as one of the tasks within such systems, faces more challenges consequently. Current pedestrian detection methods primarily rely on visiblelight modality, limiting their performance and robustness. Introducing an additional modality, such as infrared, to improve performance imposes higher demands on platforms and resources. Therefore, multi-spectral image fusion emerges as a promising technique to enhance pedestrian detection. This paper proposes a pedestrian detection framework based on infrared-visible light image fusion. Additionally, three fusion methods are employed and compared against single-spectral results using existing detection models, which validate the potential for improving pedestrian detection performance.},
  keywords={Pedestrians;Limiting;Surveillance;Training data;Learning (artificial intelligence);Robustness;Real-time systems;Multispectral Image;Image Fusion;Pedestrian Detection;Deep Learning},
  doi={10.1109/ICSMD60522.2023.10490542},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10974273,
  author={Wang, Xijia and Wang, Yefei and Ai, Chen and Zeng, Jinshan},
  booktitle={2024 3rd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)}, 
  title={One-Shot Font Generation with Masked Diffusion Transformers}, 
  year={2024},
  volume={},
  number={},
  pages={152-159},
  abstract={Font generation is aims to create a new font library by synthesizing characters with specific style references. Existing font generation methods can generally be categorized into two primary types: those based on GANs and those utilizing diffusion models. Owing to adversarial training, GAN-based approaches often encounter issues with training instability and inaccurate character generation. Recently, in the font generation task, the diffusion model has shown outstanding performance due to the stability of its training process. However, existing methods primarily rely on U-Net architecture, and the effectiveness of Transformer architecture has not been fully explored in the field of font generation. To address this, we introduce a Diffusion Transformer-based approach for font generation. Besides, we incorporate masked modeling into the font generation task, allowing the model to better grasp the overall integrity of each character. Extensive experiments demonstrate that our approach yields superior generation results compared to mainstream approaches. Additionally, cross-language font generation experiments confirm our method's effectiveness in generating fonts across diverse languages.},
  keywords={Training;Human computer interaction;Character generation;Transformers;Diffusion models;Stability analysis;Libraries;Artificial intelligence;Robots;Context modeling;component;font generation;diffusion model;Transformer;mask modeling},
  doi={10.1109/AIHCIR65563.2024.00033},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11022168,
  author={Ma, Xiaona and Wang, Enyang and Zhao, Likun and Yang, Qinyi and Liu, Zheng},
  booktitle={2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Customized Experience Design of Human-AI-Collaboration Cultural and Creative Products for China’s Excellent Traditional Culture}, 
  year={2024},
  volume={},
  number={},
  pages={615-621},
  abstract={In response to the profound integration of AI in design and the emphasis on human agency within design activities, this paper investigates a new interactive experience that facilitates efficient user involvement in cultural and creative customization. Drawing on the concepts of human-centered AI and Human-AI-Collaboration design thinking, it proposes an interactive design framework for cultural and creative customization that aligns with the essence of Chinese traditional culture. Additionally, the paper details the design of a corresponding platform, offering a methodological innovation for cultural and creative design development through Human-AI-Collaboration.},
  keywords={Product customization;Technological innovation;User experience;Libraries;Cultural differences;Artificial intelligence;Human-AI-Collaboration;AI;cultural and creative product customization;interaction design},
  doi={10.1109/ACAIT63902.2024.11022168},
  ISSN={},
  month={Nov},}
