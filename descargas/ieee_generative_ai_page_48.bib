@INPROCEEDINGS{11077527,
  author={Siddiqi, Umair F. and Sait, Sadiq M.},
  booktitle={2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)}, 
  title={A Mini-Review of Methods for Forecasting Post Routing Congestion from Placement in FPGA Physical Design}, 
  year={2025},
  volume={},
  number={},
  pages={281-290},
  abstract={Placement and routing are two time-consuming steps in the FPGA physical design flow and can take hours or even days. The placement steps map the logic elements of the netlist onto the computational resources of the FPGA, and the routing step is responsible for finding paths to connect the logic blocks through the FPGA routing fabric. The routing resources have a fixed capacity; using them beyond their capacity results in congestion. Congestion can cause routing failures and make it difficult to achieve timing closure and the placement algorithms should produce solutions that do not generate congestion in routing. Researchers have employed many approaches, from elementary ways (e.g., counting the number of pins) to accurate artificial intelligence-driven methods. This article categorizes congestion estimation methods developed over the last 25 years, providing an overview and survey of methods belonging to each category. To aid in understanding, simple experiments are included to illustrate each method's application. Finally, the article concludes by outlining potential future research directions in the field.},
  keywords={Surveys;Generative AI;Routing;Timing;Pins;Logic;Forecasting;Field programmable gate arrays;Robots;Physical design;FPGA;placement;routing;congestion prediction;machine and deep learning;generative AI},
  doi={10.1109/AIRC64931.2025.11077527},
  ISSN={},
  month={May},}@INPROCEEDINGS{10445563,
  author={Chheang, Vuthea and Sharmin, Shayla and Márquez-Hernández, Rommy and Patel, Megha and Rajasekaran, Danush and Caulfield, Gavin and Kiafar, Behdokht and Li, Jicheng and Kullu, Pinar and Barmaki, Roghayeh Leila},
  booktitle={2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)}, 
  title={Towards Anatomy Education with Generative AI-based Virtual Assistants in Immersive Virtual Reality Environments}, 
  year={2024},
  volume={},
  number={},
  pages={21-30},
  abstract={Virtual reality (VR) and interactive 3D visualization systems have enhanced educational experiences and environments, particularly in complicated subjects such as anatomy education. VR-based systems surpass the potential limitations of traditional training approaches in facilitating interactive engagement among students. However, research on embodied virtual assistants that leverage generative artificial intelligence (AI) and verbal communication in the anatomy education context is underrepresented. In this work, we introduce a VR environment with a generative AI-embodied virtual assistant to support participants in responding to varying cognitive complexity anatomy questions and enable verbal communication. We assessed the technical efficacy and usability of the proposed environment in a pilot user study with 16 participants. We conducted a within-subject design for virtual assistant configuration (avatar- and screen-based), with two levels of cognitive complexity (knowledge- and analysis-based). The results reveal a significant difference in the scores obtained from knowledge- and analysis-based questions in relation to avatar configuration. Moreover, results provide insights into usability, cognitive task load, and the sense of presence in the proposed virtual assistant configurations. Our environment and results of the pilot study offer potential benefits and future research directions beyond medical education, using generative AI and embodied virtual agents as customized virtual conversational assistants.},
  keywords={Visualization;Generative AI;Virtual assistants;Avatars;Complexity theory;Usability;Task analysis;Generative AI;virtual reality;human-computer interaction;embodied virtual assistants;anatomy education},
  doi={10.1109/AIxVR59861.2024.00011},
  ISSN={2771-7453},
  month={Jan},}@INPROCEEDINGS{10165616,
  author={Cai, Jingyi and Song, Shutian and Zhang, Haipeng and Song, Ruiliang and Zhang, Bo and Zheng, Xiang},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Satellite Network Traffic Prediction Based on LSTM and GAN}, 
  year={2023},
  volume={3},
  number={},
  pages={175-178},
  abstract={Satellite networks are characterized by rapid topology changes, quick updates in the coverage of subsatellite points, and large variations in service traffic access in different regions, but they are also likely to cause congestion and blockage in the network. In order to solve this problem, a network traffic prediction method based on long short-term memory (LSTM) and generative adversarial networks (GAN) was put forward. Firstly, the network traffic simulation dataset is constructed based on the population distribution density. Secondly, the dataset is augmented with GAN to prevent the occurrence of training overfitting problems. Finally, LSTM-based model is trained and tested on the dataset obtain a network traffic prediction model with an accuracy of 95.43%, which can provide effective data support for the coordination of satellite network resource scheduling.},
  keywords={Training;Satellites;Sociology;Telecommunication traffic;Predictive models;Generative adversarial networks;Data models;satellite network;traffic priction of network;LSTM;GAN},
  doi={10.1109/ICIBA56860.2023.10165616},
  ISSN={},
  month={May},}@INPROCEEDINGS{9458575,
  author={Bavikadi, Sathwika and Sutradhar, Purab Ranjan and Ganguly, Amlan and Dinakarrao, Sai Manoj Pudukotai},
  booktitle={2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)}, 
  title={uPIM: Performance-aware Online Learning Capable Processing-in-Memory}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={Machine learning and AI-based automated systems are gaining increasing attention for real-time intelligent applications by virtue of a superior co-ordination between the software and the hardware within these systems. Although the majority of the automated systems are implementing Convolutional neural networks (CNNs), and Deep Neural Networks (DNNs) on the hardware with impressive accuracy, a significant amount of cost is associated with data movement in these platforms. Recent advancements in processing-in-memory (PIM), a non-von Neumann computing paradigm, have proven to be very effective in minimizing data communication overheads by performing computations within the memory chip. However, these devices are primarily designed as inference engines and therefore have not been adequately investigated for real-time learning capabilities for applications in changing environments. In this work, we introduce uPIM, a PIM architecture that supports a Generative Adversarial Network (GAN)-based performance-aware online learning model for updating the weights with minimal overheads. Our hardware-software co-design approach exhibits superior performance and efficiency in real-time applications like Autonomous Navigation Systems (ANS) by leveraging massive data-level parallelism and ultra-low data movement latency. The evaluations are performed on multiple state-of-the-art deep learning networks like LeNet, AlexNet, ResNet18, 34, 50 on the German Traffic Sign Recognition Benchmark (GTSRB) dataset and the Belgium Traffic Sign Dataset (BTSD) with several data-precisions. The proposed performance-aware, quantization-friendly online learning based PIM architecture achieves an average accuracy of 72% for GTSRB and 83.4% for BTSD dataset under varying environment for CNNs implemented for Traffic Sign Recognition (TSR) with 8-bit fixed point data-precision.},
  keywords={Performance evaluation;Neural networks;Computer architecture;Parallel processing;Generative adversarial networks;Real-time systems;Hardware},
  doi={10.1109/AICAS51828.2021.9458575},
  ISSN={},
  month={June},}@INPROCEEDINGS{8959911,
  author={Ye, Ru-Ting and Wang, Wei-Li and Chen, Ju-Chin and Lin, Kawuu W.},
  booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)}, 
  title={Interactive Anime Sketch Colorization with Style Consistency via a Deep Residual Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={Anime line sketch colorization is to fill a variety of colors the anime sketch, to make it colorful and diverse. The coloring problem is not a new research direction in the field of deep learning technology. Because of coloring of the anime sketch does not have fixed color and we can't take texture or shadow as reference, so it is difficult to learn and have a certain standard to determine whether it is correct or not. After generative adversarial networks (GANs) was proposed, some used GANs to do coloring research, achieved some result, but the coloring effect is limited. This study proposes a method use deep residual network, and adding discriminator to network, that expect the color of colored images can consistent with the desired color by the user and can achieve good coloring results.},
  keywords={Image color analysis;Generators;Color;Generative adversarial networks;Feature extraction;Training;Residual neural networks;Deep Learning;Colorization},
  doi={10.1109/TAAI48200.2019.8959911},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{9793189,
  author={Poddar, Avhi and Gawade, Surabhi and Varpe, Prasad and Bhagwat, Sumedha},
  booktitle={2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Frontal Face Landmark Generation using GAN}, 
  year={2022},
  volume={},
  number={},
  pages={1172-1177},
  abstract={In the area of video surveillance, there are many challenges identified due to illumination variation, pose variation, angular variation, occlusion etc for face identification. It is very crucial to identify face images from CCTV footages, uncontrolled environment because of low resolution, poor quality of camera and distance between camera and face. This leads to poor quality image generation and difficult to identify face. This paper proposes a novel system which handles pose variation of face. In this paper, GAN (Generative Adversarial Network) is used to generate landmark of frontal face from side view of the face image. 2D face can be generated based on the frontal face landmark we have generated. Experimental results show that frontal face landmark can be generated easily which is further helpful to generate front face.},
  keywords={Image synthesis;Face recognition;Terrorism;Sociology;Lighting;Generative adversarial networks;Cameras;Frontalization;Frontal face;Face detection;Landmark;Synthetize face},
  doi={10.1109/ICAAIC53929.2022.9793189},
  ISSN={},
  month={May},}@INPROCEEDINGS{9643363,
  author={Zhang, Lei and Jiang, Na and Diao, Qishuai and Huang, Danyang and Zhou, Zhong and Wu, Wei},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Object Quality Guided Feature Fusion for Person Re-identification}, 
  year={2021},
  volume={},
  number={},
  pages={1083-1087},
  abstract={Person re-identification (Re-ID) is an essential task in computer vision, which aims to match a person of interest across multiple non-overlapping camera views. It is a fundamental challenging task because of the conflicts between large variations of samples and the limited scale of training sets. Data augmentation method based on generative adversarial network (GAN) is an efficient way to relieve this dilemma. However, existing methods do not consider how to keep identity information and filter the noise of the generated auxiliary samples during Re-ID training. In this paper, we propose object quality guided feature fusion network for person re-identification, which consists of a self-supervised object quality estimation module and a feature fusion module. Specifically, the former evaluates the quality of the auxiliary data to filter the noise and the disturbing features, while the later accomplishes the feature fusion based on object quality estimation in the collection-to-collection recognition manner to make full use of auxiliary data. Extensive performance analysis and experiments are conducted on two benchmark datasets (Market-1501 and DukeMTMC-reID) to show that our proposed approach outperforms or shows comparable results to the existing best performed methods.},
  keywords={Training;Computer vision;Image recognition;Conferences;Estimation;Generative adversarial networks;Information filters;person re-identification;feature fusion;quality estimation;data augmentation},
  doi={10.1109/ICTAI52525.2021.00171},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10201284,
  author={Wu, Hao},
  booktitle={2023 IEEE 3rd International Conference on Computer Communication and Artificial Intelligence (CCAI)}, 
  title={Research on Motion Trend Enhanced 2D Detection on Drones}, 
  year={2023},
  volume={},
  number={},
  pages={221-226},
  abstract={Inspired by the human visual system, we proposed a motion information-based enhancement mechanism for drone detection, named Collaborative Filtering Mechanism (CFM). CFM enhances small object features through GAN-based image translation which is based on a Cycle Generative Adversarial Network (CycleGAN), and filters out unrelated features during the feature extraction cascade of YOLO-V5s, thus improving the performance of object detection. In the experiments, we verified the performance improvement brought by the proposed CFM module on the VisDrone dataset.},
  keywords={Estimation;Transforms;Object detection;Visual systems;Feature extraction;Market research;Generative adversarial networks;Object Detection;Image Processing;Image Translation;Computer Vision},
  doi={10.1109/CCAI57533.2023.10201284},
  ISSN={},
  month={May},}@INPROCEEDINGS{9497934,
  author={Zhang, Chunkang and Cai, Yueqing and Rao, Wenbi},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={A Non-Autoregressivee Network for Chinese Text to Speech and Voice Cloning}, 
  year={2021},
  volume={},
  number={},
  pages={96-101},
  abstract={Text to speech (TTS) has been evolving rapidly these years. Researchers have successfully converted English text into speech which sounds like natural speaker, proposing numerous models from RNN to non-autoregressive network. However, the migration of these models to Chinese TTS is still an issue because of its prosodic phrasing problems and large character set, not to mention the disappointing outcomes of those successfully-migrated models, most of which are autoregressive. In this paper, we successfully migrate FastSpeech2 to the field of Chinese TTS with generative adversarial network (GAN) as its discriminator for training to enhance the outcome. Postnet of Tactron2 is also applied to fine-tune the mel-spectrogram. We also use x-vector-based voiceprint extraction model to extract voiceprint to achieve voice cloning. The experiment is operated on both models which offers results of 3.83 mean opinion score (MOS) in terms of naturalness and 3.82 MOS in terms of similarity.},
  keywords={Training;Vocoders;Computational modeling;Conferences;Cloning;Computer applications;Generative adversarial networks;voice cloning;Chinese text to speech;voiceprint feature extraction;non-autoregressive network},
  doi={10.1109/ICAICA52286.2021.9497934},
  ISSN={},
  month={June},}@INBOOK{10951344,
  author={Xiao, Perry},
  booktitle={Artificial Intelligence Programming with Python: From Zero to Hero}, 
  title={GAN and Neural&#x2010;Style Transfer}, 
  year={2022},
  volume={},
  number={},
  pages={465-489},
  abstract={Summary <p>This chapter introduces Generative Adversarial Networks (GAN) and neural&#x2010;style transfer and then introduce adversarial machine learning and music generation. It shows the schematic diagram of a GAN, including two key components: Generator and Discriminator. Neural&#x2010;style transfer is an optimization technique that uses deep learning to compose one image in the style of another image. It typically takes two images as inputs; one image is used as a content image and another as a style reference image (such as an artwork by a famous painter) and blends them together so the output image looks like the content image, but in the style of the style reference image. Adversarial machine learning is a research area that attempts to fool machine learning models by supplying deceptive input. It arranges the input data to exploit specific vulnerabilities and compromise the results.</p>},
  keywords={Generative adversarial networks;Internet;Codes;Python;Generators;Tutorials;Software development management;Deep learning;Convolutional neural networks;Browsers},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119820949},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951344},}@INPROCEEDINGS{10205662,
  author={Lin, Chia-Yu and Lin, Pin-Fan and Chung, Wei-Kuang and Lee, Yu-Hsien},
  booktitle={2023 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={ResUnet-GAN with Dynamic Memory for Mura Defect Detection}, 
  year={2023},
  volume={},
  number={},
  pages={350-353},
  abstract={“Mura” is a phenomenon in which panels have uneven display defects, irregular shapes, and different sizes. It is impossible to produce perfect panels on production lines, so panel inspection is necessary to differentiate between “light Mura” and “serious Mura” manually. The performance of conventional defect detection models for Mura detection is worse since they only differentiate between “normal” and “abnormal” samples. To reduce human cost and increase the accuracy of Mura detection, we propose a “ResUnet-GAN with Dynamic Memory Model an unsupervised anomaly detection method based on a Generative Adversarial Network (GAN) with a memory module to distinguish panel defects. In the dynamic memory, we designed a dynamic feature filtering (DFF) method to choose important features of images, enhancing the ability to recognize light Mura features of the ResUnet-GAN. The proposed model can achieve an Area Under Curve (AUC) of approximately 0.8 for accurate Mura detection. The mechanism of this paper is novel, and the result contributes to practical application.},
  keywords={Image recognition;Filtering;Shape;Memory modules;Production;Inspection;Generative adversarial networks;Mura detection;ResNet;U-Net;GAN;dynamic feature filtering},
  doi={10.1109/IAICT59002.2023.10205662},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{9832103,
  author={Gao, Huachao and Mao, Wei and Lin, Yongping},
  booktitle={2022 IEEE 2nd International Conference on Software Engineering and Artificial Intelligence (SEAI)}, 
  title={Generating face images from fine-grained sketches based on GAN with global-local joint discriminator}, 
  year={2022},
  volume={},
  number={},
  pages={50-54},
  abstract={This paper explores the face image generating with clear details from fine-grained sketches. Edge maps are usually used as sketches in face image generation tasks. However, there are some problems such as discontinuous lines and a lack of detailed information in the edge map, so the generated face image is not clear enough and lacks details. To address this problem, a face sketch dataset with rich details is made. The discriminator combines a global discriminator and a local discriminator, which ensures that the generated face image has a complete face structure and generates clearer face details. A self-attention mechanism is employed in the generative model to establish long-term dependencies on partial feature information. The model is quantitatively evaluated by using the valuation criteria IS, FID, and KID. Among them, the evaluation scores of the model on IS, FID, and KID are 1.956 ± 0.053, 17.941 ± 0.970, and 0.009 ± 0.001, respectively. The evaluation results show that the model achieves good performance. Furthermore, the visual effects of the model are analyzed by comparing with the pix2pixHD and CycleGAN models on the generated images. The final results show that our model outperforms the other two models and can generate high-quality face images with sharp details.},
  keywords={Analytical models;Image synthesis;Image edge detection;Conferences;Generative adversarial networks;Visual effects;Task analysis;image-to-image translation;fined-grained sketches;self-attention;global-local joint discriminator},
  doi={10.1109/SEAI55746.2022.9832103},
  ISSN={},
  month={June},}@INPROCEEDINGS{10505426,
  author={Xu, Yiyao and Huang, Zhen},
  booktitle={2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)}, 
  title={Enhancing Medical Image Segmentation: A Source-Free GAN-Based Approach}, 
  year={2023},
  volume={},
  number={},
  pages={460-464},
  abstract={This study addresses the challenge of training deep neural networks for medical image analysis, where data annotation demands expertise and incurs high costs, and privacy concerns hinder neural network training. We introduce an innovative source-free domain adaptation method for medical image segmentation, leveraging generative adversarial networks. This technique employs a discriminator to identify and assimilate target-domain features, transferring these parameters to the segmentation network's downsampling section. Experimentation with public medical image datasets shows our method effectively reduces segmentation model loss during domain transfer without source data, leading to progressive improvements.},
  keywords={Training;Representation learning;Image segmentation;Adaptation models;Data privacy;Image analysis;Generative adversarial networks;generating adversarial networks;medical image segmentation;source-free domain adaptation},
  doi={10.1109/AIHCIR61661.2023.00081},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11117022,
  author={Jiang, Rui and Cai, Hongxia and Yu, Tao},
  booktitle={2024 5th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)}, 
  title={Agaricus Bisporus Growth Status Prediction in Digital Twin Smart Farm}, 
  year={2024},
  volume={},
  number={},
  pages={578-582},
  abstract={In recent years, digital twin has gradually begun to be promoted in the field of agriculture, and the establishment of an agricultural digital twin system is able to simultaneously realize the following: digital scene mapping driven by physical monitoring data in real time; remote control of the physical scene by the digital scene; and monitoring and utilization of multisource dynamic data in the life cycle of crops to provide a theoretical basis for decision-making in the planting process. In this paper, based on the function of the digital twin system growth scene projection, due to the intensive growth characteristics of Agaricus bisporus, a spatio-temporal prediction algorithm of Agaricus bisporus growth state image is proposed. Currently, there are fewer image prediction algorithms for the growth state of dense-growing crops such as Agaricus bisporus, and the algorithm proposed in this paper considers the spatial location information contained in the image, integrates the self-attention module, generative adversarial network and spatio-temporal prediction model, and realizes better growth prediction accuracy, to guide the harvesting of Agaricus bisporus in digital twin smart farm.},
  keywords={Smart agriculture;Crops;Predictive models;Prediction algorithms;Generative adversarial networks;Digital twins;Image sequences;Spatiotemporal phenomena;Spatial resolution;Monitoring;Growth status prediction;Agaricus bisporus;Digital twin},
  doi={10.1109/ICBAIE63306.2024.11117022},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10710966,
  author={Yilmaz, Berkay and Vatansever, Saffet},
  booktitle={2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={An Overview of Deepfake Video Detection Using Remote Photoplethysmography}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Deepfake technology, which can create remarkably realistic videos through deep learning techniques, has many applications that serve humanity, including cinema and television productions, education, social media applications, art, fashion, and virtual assistants. However, this technology also brings with it potential abuse scenarios through manipulative content. Particularly in cases like fake news, identity fraud, blackmail, and slander, it can hasten the dissemination of false information and violate people’s privacy. Moreover, it may cause major legal and societal issues in crucial fields like politics and public security. In this context, developing solutions for detecting deepfake videos has become imperative. While some methods developed in the literature to detect deepfake video are based on spatial and frequency-based analysis of digital traces and residues, which emerge intrinsically during the fake content production stage, some are based on examining physiological signs. Remote photoplethysmography (rPPG)-based physiological approaches, which analyze imperceptible color changes on the skin surfaces of individuals, have gained significant attention due to their high performance. This study examined rPPG-based techniques and their effectiveness in detecting fake videos made with Generative Adversarial Network (GAN) and Autoencoder (AE), two of the most popular deep learning algorithms used to produce deepfake content, and discussed the technical challenges encountered.},
  keywords={Deep learning;Deepfakes;TV;Social networking (online);Virtual assistants;Production;Photoplethysmography;Generative adversarial networks;Physiology;Skin;remote photoplethysmography;rPPG;deep learning;fake video;deepfake},
  doi={10.1109/IDAP64064.2024.10710966},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10165577,
  author={Liu, Xinhua and Zhang, Yizhou and Ma, Xiaolin and Kuang, Hailan},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={A Network of Generating High Quality Hemoglobin Images}, 
  year={2023},
  volume={3},
  number={},
  pages={787-791},
  abstract={In recent years, image-based facial skin pigment extraction has made great progress, and the extraction methods are mainly divided into traditional methods and deep learning methods. Due to the early introduction of traditional methods, there are some defects, such as low efficiency, some black pixels will go white, etc. Therefore, this paper mainly proposes a deep learning method based on Generative Adversarial Network (GAN) to achieve hemoglobin image generation. Our goal is similar to image translation. Currently, the mainstream GAN networks for image translation include pix2pix, pix2pixHD, cycleGAN, etc. This paper proposes a network GHNet based on UNet to generate higher quality images. In the data set part, we built the paired data set of polarized light-hemoglobin image by ourselves. In the network part, we reduced the number of encoders and decoders in the traditional UNet network, which made the network more lightweight. Based on this, we define a neural network module and integrate the attention mechanism SE module to enhance the ability to capture the input image features. The experimental results show that, compared with ASAPNet, DGC and CLD networks, the proposed network training speed is the same as that of the baseline network, and the PSNR index and SSIM index of the generated images are improved.},
  keywords={Training;Deep learning;Image synthesis;Neural networks;Pigments;Generative adversarial networks;Skin;hemoglobin image;deep learning;GAN;attention mechanism},
  doi={10.1109/ICIBA56860.2023.10165577},
  ISSN={},
  month={May},}@INPROCEEDINGS{10463405,
  author={Ning, Dian and Han, Dong Seog},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Image Translation of Vehicle Front Camera Frame Failures Based on CycleGAN}, 
  year={2024},
  volume={},
  number={},
  pages={313-316},
  abstract={More and more kinds of sensors are used including cameras in the vehicle to proactively address safety issues, either directly or indirectly. Camera failures, such as abnormal frames caused by muzzy, obstruction, and flutter, can lead to system exceptions and even traffic accidents because of their important role in the vehicle's system. We hope to reduce those exception faults by recovering abnormal frames. Therefore, in this paper, we first collect the video from the front-facing camera and define the abnormal frames. Then, this dataset is learned by a cycle generative adversarial network (CycleGAN) to generate more abnormal frames because sufficient samples are needed for better training. Moreover, CycleGAN can also restore the abnormal frames to normal frames, which reduces the system faults. This method can mitigate the consequence of camera failures and also works as a generator of corresponding failure frames.},
  keywords={Training;Fault detection;Cameras;Transformers;Generative adversarial networks;Generators;Image restoration;Fault Detection;GAN;Vehicle Inpainting;Camera Faulty},
  doi={10.1109/ICAIIC60209.2024.10463405},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10176821,
  author={Li, Jun and Wang, Sinuo and Zhao, Shuanbao and Xu, Changbin and Liu, Wei and Dong, Shidanjie},
  booktitle={2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI)}, 
  title={Research on the security strategy of power system under cyber attack}, 
  year={2023},
  volume={},
  number={},
  pages={107-111},
  abstract={In order to solve the security and stability problems occurred in the cyber-attack power system, this paper proposes a controller based on graph generation adversarial network algorithm, and simulates and verifies it in the load frequency control and automatic voltage regulation model. The generator of the graph generation adversarial network algorithm collects normal data and data that have been attacked and tampered with, and the discriminator compares the generated data with the historical data to determine whether it is under cyber attack; then, the discriminator outputs normal and safe data; finally, the attention network aggregates the output data of the discriminator and selects the data most similar to the historical signal for effective defense against the attack data. Simulation results show that the controller has better robustness than the PID controller and the generative adversarial network controller, can better suppress the frequency and voltage fluctuations caused by the network attack, and can maintain the safe and stable operation of the power system.},
  keywords={Time-frequency analysis;Power system stability;Generative adversarial networks;Data models;Stability analysis;Security;Voltage control;component;load frequency control;automatic voltage regulation;network attacks;graph generation against networks},
  doi={10.1109/ICECAI58670.2023.10176821},
  ISSN={},
  month={May},}@INBOOK{10951498,
  author={Koh, Immanuel},
  booktitle={Diffusions in Architecture: Artificial Intelligence and Image Generators}, 
  title={3D Diffusion or 3D Disfiguration?}, 
  year={2024},
  volume={},
  number={},
  pages={202-211},
  abstract={Summary <p>Stable diffusion model is the only opensource model, thus granting the opportunity in extending its text&#x2010;to&#x2010;image 2D diffusion model codebase with neural radiance fields as text&#x2010;to&#x2010;3D diffusion models for architecture. In retrospect, despite the recent shift from generative adversial networks (GAN) to Diffusion models as SOTA, a recurring aesthetics of disfiguration and hypercubism remains somewhat visible, especially in 3D&#x2010;Diffusion models. Inspired by the figure&#x2010;ground volumetric configuration alluded by the bodies of pandas, a text prompt is used as an input to a custom 3D&#x2010;Diffusion model in kick&#x2010;starting the sequence of denoising process, from an initial spherical noise to a hypercubist architectural configuration. As a comparative study, the 3D&#x2010;GAN generated chairs exhibit a disfiguration aesthetics while the 3D&#x2010;Diffusion generated chairs reveal a stronger hypercubist aesthetics.</p>},
  keywords={Three-dimensional displays;Diffusion models;Solid modeling;Noise reduction;Generative adversarial networks;Hands;Face recognition;Text to image;Sun;Museums},
  doi={10.1002/9781394191802.ch22},
  ISSN={},
  publisher={Wiley},
  isbn={9781394191796},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951498},}@INPROCEEDINGS{10670779,
  author={Wu, Bing and Wang, Wenqi and Gao, Hong and Dong, Qingshuang},
  booktitle={2024 3rd International Conference on Robotics, Artificial Intelligence and Intelligent Control (RAIIC)}, 
  title={Research on Denoising Technology for Yimeng Red Films Based on Swin Transformer}, 
  year={2024},
  volume={},
  number={},
  pages={41-44},
  abstract={This research introduces a novel method for denoising Yimeng red films by integrating Swin Transformer and Generative Adversarial Network (GAN). The Swin Transformer effectively extracts both local and global features of video frames through its hierarchical structure and window attention mechanism. Meanwhile, the GAN enhances the realism and detail preservation of the denoised videos through adversarial training. Experimental results show that the proposed method achieves a PSNR value of 36.47 and an SSIM value of 0.940 in the YOUKUVESR dataset, outperforming other advanced algorithms. This provides a new solution for old film restoration and contributes to the preservation of cultural heritage.},
  keywords={Training;Films;Noise reduction;Process control;Transformers;Generative adversarial networks;Feature extraction;Swin Transformer;GAN;Old film Restoration;Video Denoising},
  doi={10.1109/RAIIC61787.2024.10670779},
  ISSN={},
  month={July},}@ARTICLE{10321729,
  author={Zhou, Xiaokang and Zheng, Xuzhe and Shu, Tian and Liang, Wei and Wang, Kevin I-Kai and Qi, Lianyong and Shimizu, Shohei and Jin, Qun},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Information Theoretic Learning-Enhanced Dual-Generative Adversarial Networks With Causal Representation for Robust OOD Generalization}, 
  year={2025},
  volume={36},
  number={2},
  pages={2066-2079},
  abstract={Recently, machine/deep learning techniques are achieving remarkable success in a variety of intelligent control and management systems, promising to change the future of artificial intelligence (AI) scenarios. However, they still suffer from some intractable difficulty or limitations for model training, such as the out-of-distribution (OOD) issue, in modern smart manufacturing or intelligent transportation systems (ITSs). In this study, we newly design and introduce a deep generative model framework, which seamlessly incorporates the information theoretic learning (ITL) and causal representation learning (CRL) in a dual-generative adversarial network (Dual-GAN) architecture, aiming to enhance the robust OOD generalization in modern machine learning (ML) paradigms. In particular, an ITL- and CRL-enhanced Dual-GAN (ITCRL-DGAN) model is presented, which includes an autoencoder with CRL (AE-CRL) structure to aid the dual-adversarial training with causality-inspired feature representations and a Dual-GAN structure to improve the data augmentation in both feature and data levels. Following a newly designed feature separation strategy, a causal graph is built and improved based on the information theory, which can enhance the causally related factors among the separated core features and further enrich the feature representation with the counterfactual features via interventions based on the refined causal relationships. The ITL is incorporated to improve the extraction of low-dimensional feature representations and learn the optimized causal representations based on the idea of “information flow.” A dual-adversarial training mechanism is then developed, which not only enables the generator to expand the boundary of feature distribution in accordance with the optimized feature representation from AE-CRL, but also allows the discriminator to further verify and improve the quality of the augmented data for OOD generalization. Experiment and evaluation results based on an open-source dataset demonstrate the outstanding learning efficiency and classification performance of our proposed model for robust OOD generalization in modern smart applications compared with three baseline methods.},
  keywords={Data models;Training;Feature extraction;Task analysis;Representation learning;Predictive models;Object oriented modeling;Autoencoder (AE);causal representation learning (CRL);deep learning;generative adversarial network (GAN);information theoretic learning (ITL);out-of-distribution (OOD)},
  doi={10.1109/TNNLS.2023.3330864},
  ISSN={2162-2388},
  month={Feb},}@ARTICLE{10586974,
  author={Kim, Dong-Jin and Oh, Tae-Hyun and Choi, Jinsoo and Kweon, In So},
  journal={IEEE Access}, 
  title={Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data}, 
  year={2024},
  volume={12},
  number={},
  pages={93580-93592},
  abstract={We present a novel data-efficient semi-supervised framework to improve the generalization of image captioning models. Constructing a large-scale labeled image captioning dataset is expensive in terms of labor, time, and cost. In contrast to manually annotating all the training samples, separately collecting uni-modal datasets is immensely easier, e.g., a large-scale image dataset and a sentence dataset. We leverage such massive unpaired image and caption data upon standard paired data by learning to associate them. To this end, our novel semi-supervised learning method assigns pseudo-labels to unpaired images and captions in an adversarial learning fashion, where the joint distribution of image and caption is learned. This approach shows noticeable performance improvement even in challenging scenarios, including out-of-task data and web-crawled data. We also show that our proposed method is theoretically well-motivated and has a favorable global optimal property. Our extensive and comprehensive empirical results on captioning datasets, followed by a comprehensive analysis of the scarcely-paired COCO dataset, demonstrate the consistent effectiveness of our method compared to competing ones.},
  keywords={Task analysis;Data models;Training;Semisupervised learning;Visualization;Natural languages;Generative adversarial networks;Closed captioning;Image captioning;unpaired captioning;semi-supervised learning;generative adversarial networks},
  doi={10.1109/ACCESS.2024.3423790},
  ISSN={2169-3536},
  month={},}@INBOOK{10952927,
  author={Baker, Pam},
  booktitle={Generative AI For Dummies}, 
  title={Delving into Specialized GenAI Tools}, 
  year={2025},
  volume={},
  number={},
  pages={203-216},
  abstract={Summary <p>Specialized GenAI models are tools designed to perform specific functions. GenAI enterprise tools differ substantially from those accessible to the broader public. GenAI can significantly reduce the time and resources required for tasks such as content creation, data discovery, data analysis, and customer engagement. As GenAI models continue to evolve, the impact on large&#x2010;scale operations will only grow, making it an exciting and essential field for businesses and workers to explore and leverage. Mini GPTs offer specialized GenAI capabilities that are customized to meet specific requirements. The GPT App Store represents a significant shift in the GenAI paradigm. Autonomous AI agents and personalized AI represent two distinct paradigms within the field of artificial intelligence, each with its own set of capabilities, focus areas, and potential impacts on society and technology. GenAI's advanced capabilities enable autonomous AI agents to work with digital twins with a higher degree of autonomy and intelligence.</p>},
  keywords={Biological system modeling;Data models;Adaptation models;Artificial intelligence;Retrieval augmented generation;Maintenance;Data security;Analytical models;Writing;Training data},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394270767},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952927},}@ARTICLE{9526351,
  author={Zhang, Yabin and Lian, Hairong and Yang, Guang and Zhao, Suyun and Ni, Peng and Chen, Hong and Li, Cuiping},
  journal={IEEE Transactions on Cybernetics}, 
  title={Inaccurate-Supervised Learning With Generative Adversarial Nets}, 
  year={2023},
  volume={53},
  number={3},
  pages={1522-1536},
  abstract={Inaccurate-supervised learning (ISL) is a weakly supervised learning framework for imprecise annotation, which is derived from some specific popular learning frameworks, mainly including partial label learning (PLL), partial multilabel learning (PML), and multiview PML (MVPML). While PLL, PML, and MVPML are each solved as independent models through different methods and no general framework can currently be applied to these frameworks, most existing methods for solving them were designed based on traditional machine-learning techniques, such as logistic regression, KNN, SVM, decision tree. Prior to this study, there was no single general framework that used adversarial networks to solve ISL problems. To narrow this gap, this study proposed an adversarial network structure to solve ISL problems, called ISL with generative adversarial nets (ISL-GANs). In ISL-GAN, fake samples, which are quite similar to real samples, gradually promote the Discriminator to disambiguate the noise labels of real samples. We also provide theoretical analyses for ISL-GAN in effectively handling ISL data. In this article, we propose a general framework to solve PLL, PML, and MVPML, while in the published conference version, we adopt the specific framework, which is a special case of the general one, to solve the PLL problem. Finally, the effectiveness is demonstrated through extensive experiments on various imprecise annotation learning tasks, including PLL, PML, and MVPML.},
  keywords={Phase locked loops;Training;Generators;Noise measurement;Annotations;Supervised learning;Task analysis;Generative adversarial nets (GANs);imprecise annotation;inaccurate-supervised learning (ISL);multiview partial multilabel learning (MVPML);partial label learning (PLL);partial multilabel learning (PML)},
  doi={10.1109/TCYB.2021.3104848},
  ISSN={2168-2275},
  month={March},}@ARTICLE{10746627,
  author={Zeng, Ye and Qiao, Li and Gao, Zhen and Qin, Tong and Wu, Zhonghuai and Khalaf, Emad and Chen, Sheng and Guizani, Mohsen},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={CSI-GPT: Integrating Generative Pre-Trained Transformer With Federated-Tuning to Acquire Downlink Massive MIMO Channels}, 
  year={2025},
  volume={74},
  number={3},
  pages={5187-5192},
  abstract={In massive multiple-input multiple-output (MIMO) systems, how to reliably acquire downlink channel state information (CSI) with low overhead is challenging. In this work, by integrating the generative pre-trained Transformer (GPT) with federated-tuning, we propose a CSI-GPT approach to realize efficient downlink CSI acquisition. Specifically, we first propose a Swin Transformer-based channel acquisition network (SWTCAN) to acquire downlink CSI, where pilot signals, downlink channel estimation, and uplink CSI feedback are jointly designed. Furthermore, to solve the problem of insufficient training data, we propose a variational auto-encoder-based channel sample generator (VAE-CSG), which can generate sufficient CSI samples based on a limited number of high-quality CSI data obtained from the current cell. The CSI dataset generated from VAE-CSG will be used for pre-training SWTCAN. To fine-tune the pre-trained SWTCAN for improved performance, we propose an online federated-tuning method, where only a small amount of SWTCAN parameters are unfrozen and updated using over-the-air computation, avoiding the high communication overhead caused by aggregating the complete CSI samples from user equipment (UEs) to the BS for centralized fine-tuning. Simulation results verify the advantages of the proposed SWTCAN and the communication efficiency of the proposed federated-tuning method.},
  keywords={Downlink;Transformers;Ear;Channel estimation;Feature extraction;Massive MIMO;Vectors;Training;Computer architecture;Uplink;Channel estimation;CSI feedback;federated learning;generative AI;massive MIMO;swin transformer},
  doi={10.1109/TVT.2024.3493463},
  ISSN={1939-9359},
  month={March},}@INPROCEEDINGS{10387603,
  author={Barr, Joseph R. and Haass, Jon C. and Proctor, Neil},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={On Cyber Protection with AI}, 
  year={2023},
  volume={},
  number={},
  pages={91-94},
  abstract={AI has become increasingly significant risk as well as defense factor in cybersecurity. On the attack side, adversaries utilize generative tools to create malware, ‘phishing’ and ‘spear phishing’ campaigns with far more convincing content and lessen the likelihood of detection. The defense must also utilize AI often in the form of machine learning to discover and respond to the changing patterns of attacks.},
  keywords={Machine learning;Organizations;Malware;Computer networks;Computer security;Investment;Artificial Intelligence;Machine Learning;Cyber Protection;Attack Vectors;Early Detection;Incidence Management;Anomaly Detection;Autonomous Defense Systems},
  doi={10.1109/TransAI60598.2023.00024},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10787161,
  author={Mongkolrob, Sonram and Intarasema, Sarayut and Premchaiswadi, Wichian},
  booktitle={2024 22nd International Conference on ICT and Knowledge Engineering (ICT&KE)}, 
  title={From Data to Decisions: Enhancing Loan Applications with Process Mining Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This research investigates the loan application process using Process Mining techniques combined with Generative AI to identify inefficiencies and propose improvements. Data from the BPI Challenge 2017 was analyzed using Process Mining tools, particularly the Fuzzy Miner algorithm, to discover and assess workflow patterns and bottlenecks in the loan application process. Key activities were examined, including W _ Validate Application, W _Call after offers, and W _Handle leads, which exhibited excessive processing times. The analysis revealed significant delays, particularly in the W _ Validate Application step, which took an unprecedented 103.3 years. Redundant activities were also identified, negatively impacting the overall efficiency of the process. The integration of Generative AI facilitated accurate data analysis, but the need for expert interpretation remains critical for actionable insights. To enhance process efficiency, implementing Robotic Process Automation (RPA) and Generative AI in areas with identified delays is recommended. Improving interdepartmental communication and coordination is essential for a streamlined workflow. Additionally, utilizing AI systems for risk detection in complex steps, such as W _Assess Potential Fraud, can significantly improve processing speed and accuracy. This study contributes to the field of process improvement by highlighting the effectiveness of combining Process Mining with Generative AI., offering a systematic approach to identify and address bottlenecks in business processes. The findings provide valuable insights for organizations aiming to enhance operational efficiency.},
  keywords={Process mining;Intelligent automation;Accuracy;Systematics;Data analysis;Generative AI;Redundancy;Organizations;Delays;Fraud;Process Mining;Generative AI;loan application process;efficiency Improvement;bottlenecks;Robotic Process Automation;data analysis},
  doi={10.1109/ICTKE62841.2024.10787161},
  ISSN={2157-099X},
  month={Nov},}@INPROCEEDINGS{11166608,
  author={Tambde, Ridima Chetan and Muntean, Cristina Hava and Yaqoob, Abid},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Polycystic Ovary Syndrome Detection Utilizing SRGAN-Generated Synthetic Images and Advanced CNN Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Polycystic Ovary Syndrome (PCOS) is a common hormonal disorder affecting women worldwide and is detected using ultrasound scans. Accurate diagnosis is crucial but often hindered by limited data availability. High-quality data is needed for building reliable models, and with advancements in artificial intelligence, data generation through synthetic images has shown promising results. This paper explores the use of Super-Resolution Generative Adversarial Networks (SRGAN) and convolutional neural networks (CNN) for PCOS detection on a limited dataset. Synthetic images from four SRGAN variants and original ultrasound scans are used to train and test CNN models, including NasNetMobile, ResNet152, and Xception. Additionally, hybrid models combining these CNNs with the CatBoost classifier are evaluated. The Xception model and its hybrid version with CatBoost achieved the best performance, up to 99% accuracy. Some models showed a slight drop when trained with synthetic data. This study concludes that SRGAN-generated images can expand datasets and support diagnosis, though performance depends on the architecture used.},
  keywords={Ultrasonic imaging;Accuracy;Computational modeling;Superresolution;Data models;Convolutional neural networks;Reliability;Artificial intelligence;Medical diagnostic imaging;Synthetic data;Polycystic;Ovary Syndrome;(PCOS);Ultrasound;Imaging;Super-Resolution GAN(SRGAN);Convolutional Neural Networks(CNN);Synthetic Medical Data;CatBoost Classifier},
  doi={10.1109/ACDSA65407.2025.11166608},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10914765,
  author={Chaykam, Varun Reddy and Koduru, Avinash Reddy and S, Karthick and Arun, C.},
  booktitle={2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Enhanced Tuberculosis Detection Using Deep Learning and GAN Data Augmentation Techniques for Chest x-Ray Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1171-1178},
  abstract={With around 1.6 million deaths yearly, primarily in low-resource environments, abstract-tuberculosis (TB) is a serious worldwide health issue. While human analysis can be error-prone and labor-intensive, early discovery using chest X-rays is vital. We used advanced machine learning methods including a mix of Histogram of Oriented Gradients (HOG) and Convolutional Neural Networks (CNN), alongside Deep Convolutional Generative Adversarial Networks (DCGANs) to create synthetic TB images, so helping to balance the dataset and reduce class imbalance. We applied explainable artificial intelligence (XAI) methods including LIME and Grad-CAM for greater interpretability in order to improve model transparency. The outcomes showed notable progress; our hybrid model obtained a stunning 99.5% diagnosis accuracy. With respective accuracy of 96.87% and 94.67%, models such as Xception and MobileNetV2 also fared rather well. Especially in resource-limited healthcare settings, this study provides a consistent and interpretable strategy for tuberculosis screening.},
  keywords={Deep learning;Accuracy;Tuberculosis;Explainable AI;Generative adversarial networks;Data models;Convolutional neural networks;Reliability;X-ray imaging;Biomedical imaging;Tuberculosis;Machine Learning;Deep Learning;Transfer Learning;Hog Features;Explainable AI;CNN;DCGAN},
  doi={10.1109/IDCIOT64235.2025.10914765},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10859066,
  author={Kang, Ha Ye Jin and Ko, Min Sam and Ryu, Kwang Sun},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Generation of Synthetic Data for Sharing and Utilization in Healthcare Data}, 
  year={2024},
  volume={},
  number={},
  pages={778-781},
  abstract={The potential value of healthcare data in the context of artificial intelligence and data mining is widely acknowledged. However, there are significant limitations to the utilization and sharing of this data. In our work, we propose the production of synthetic data based on generative adversarial networks algorithms as a means of addressing this problem. The data employed in this study is the Cancer Public Staging Database (CPSD) in the National Cancer Data Centre. The experimental results demonstrate that the synthetic data exhibits a similar distribution to the original data. As a future work, it is necessary to compare the performance of the synthetic data based on predictive modelling to verify its objective effectiveness of the synthetic data.},
  keywords={Statistical analysis;Databases;Soft sensors;Medical services;Production;Predictive models;Generative adversarial networks;Reliability;Synthetic data;Cancer;Synthetic tabular data;Healthcare},
  doi={10.1109/DSC63484.2024.00116},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11051404,
  author={Kapoor, Kaustubh and Singh, Shourya Pratap and Aggarwal, Garima and Sharma, Meghna},
  booktitle={2025 International Conference on Engineering, Technology & Management (ICETM)}, 
  title={Deepfake Detection Using CNN-Based Architecture}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Deepfakes refer to synthetic media generated using artificial intelligence (AI) and deep learning techniques, capable of manipulating or fabricating visual and audio content to produce highly realistic yet deceptive multimedia. The rapid evolution and widespread accessibility of these technologies have raised serious concerns about their potential misuse across various domains, including politics, entertainment, and security, where the implications could be far-reaching and disruptive. This paper delves into the application of convolutional neural network (CNN)-based architectures for effectively detecting deepfakes in images, aiming to provide a robust and reliable solution to mitigate their adverse impacts. Through a detailed investigation of different CNN models, this study evaluates their ability to distinguish between authentic and fabricated visual data by leveraging feature extraction and classification mechanisms. Furthermore, the research explores mathematical formulations central to image analysis, such as feature maps, loss functions, and performance metrics, providing insights into the intricate processes driving accurate deepfake detection. The findings from this study contribute significantly to advancing the field of imagebased AI and offer practical solutions for real-world applications where preserving authenticity is paramount.},
  keywords={Deepfakes;Visualization;Image analysis;Media;Feature extraction;Mathematical models;Convolutional neural networks;Security;Reliability;Artificial intelligence;Deepfakes;Convolutional Neural Networks;Image Processing;Generative Adversarial Networks;Performance Metrics},
  doi={10.1109/ICETM63734.2025.11051404},
  ISSN={},
  month={May},}@INPROCEEDINGS{11024685,
  author={Yao, Sucheng and Budthimedhee, Kanjanee},
  booktitle={2024 5th International Conference on Intelligent Design (ICID)}, 
  title={From Sketches to Renderings: A Comparison of Rapid Visualization Methods for Sketches Based on ControlNet (CN)}, 
  year={2024},
  volume={},
  number={},
  pages={181-184},
  abstract={The use of the ControlNet (CN)-controlled Stable Diffusion (SD) model increases the potential for artificial intelligence to enhance efficiency in architectural design. This study compared the results of using different CN processors (Canny, MLSD, Lineart, Scribble, Softedge) to process various styles of design sketches and generate architectural renderings. The aim was to explore the potential of each CN processor to visualize architectural design sketches. The results indicate that with appropriate CN control, architectural hand-drawn sketches can be quickly visualized, yielding satisfactory outcomes. The comparisons revealed that: (1) CN performs better with orderly hand-drawn sketches; (2) CN weights between 0.5 and 1.0 are more effective for image control; (3) minimizing conceptual lines in hand-drawn sketches reduces the likelihood of rendering failures. This study provides a foundation for future research in architectural and planning image generation and offers a more efficient and cost-effective design method for architectural professionals.},
  keywords={Visualization;Program processors;Image synthesis;Design methodology;Process control;Rendering (computer graphics);Planning;Artificial intelligence;Creativity;generative design;design method;ControlNet;Stable Diffusion;architectural},
  doi={10.1109/ICID64166.2024.11024685},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11147246,
  author={Marimekala, Daniel and Krishnan, Sidharth and Lamb, John},
  booktitle={2025 IEEE Integrated STEM Education Conference (ISEC)}, 
  title={Harnessing AI to Optimize Energy Consumption in Academic Institutions to Combat Climate Change}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Climate change is an escalating challenge in the modern era. While the race to adopt AI and achieve leadership in the field is pushing boundaries, the focus and impact on addressing climate change are gradually shifting. In our paper, we focus on academic institutions such as schools and colleges, and we investigate the current sources of energy consumption and effective measures currently adopted to promote sustainability. Academic Institutions are significant consumers of energy, driven by their diverse operations and facilities. From building operations such as heating, cooling, and lighting to parking lots, libraries, recreation facilities, energy-intensive research laboratories, and data centers, the demand for power is multifaceted. Additionally, the integration of IT infrastructure, residential facilities, auditoriums, charging stations for cellular devices, and electric vehicles contributes to overall consumption. With the increasing focus on sustainability, understanding power consumption patterns in academic institutions is crucial for optimizing energy usage and reducing carbon footprints. In our paper, we will explore current challenges in optimizing energy consumption at academic institutions and how GenAI can help solve these challenges. In this paper, our primary focus is to demonstrate how Generative AI can assist in optimizing energy consumption in academic institutions by using two case studies and how it plays a vital role in bringing awareness to students and educators about the importance of optimizing energy consumption in academic institutions.},
  keywords={Energy consumption;Climate change;Adaptation models;Costs;Generative AI;Buildings;Lighting;Data models;Artificial intelligence;Sustainable development;Generative AI;ChatGPT;Guardrails;Singularity;AI Models;Large Language Models;ML;Sustainability;Optimizing Energy},
  doi={10.1109/ISEC64801.2025.11147246},
  ISSN={2473-7623},
  month={March},}@INPROCEEDINGS{10137845,
  author={Liu, Xuefeng and Zhang, Guangjian},
  booktitle={2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={IPGD: A Dataset for Robotic Inside-Propped Grasp Detection}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Grasping skills are the basic skills required by robots in many practical applications. Recent research on robotic grasping detection generally focuses on grasping poses similar to human grasping. However, this grasping pose is not suitable for all grasping scenarios in practical applications. Therefore, this paper uses a new inside-propped grasping pose to label a large number of images with inside-propped grasping potential. In this way, an inside-propped grasp dataset is completed. Based on this dataset, this paper constructs a generative deep neural network for the inside-propped grasping prediction. The experimental results show that the success rate of the inside-propped grasping prediction network is 65.59%, and the average prediction time is 82ms, which has achieved good results in accuracy and real-time performance.},
  keywords={Deep learning;Neural networks;Grasping;Manipulators;Real-time systems;Artificial intelligence;Robotic Grasp Detection;Inside-propped Grasp Dataset;Inside-propped Grasp Detection;Multi-Dimension Attention Fusion;Convolutional Neural Network},
  doi={10.1109/ACAIT56212.2022.10137845},
  ISSN={},
  month={Dec},}@ARTICLE{10944630,
  author={Sedjelmaci, Hichem and Ayaida, Marwane},
  journal={IEEE Wireless Communications}, 
  title={Robust Zero Trust Systems Based on Collaborative ai to Secure the 6G-Enabled VANETs}, 
  year={2025},
  volume={32},
  number={2},
  pages={164-170},
  abstract={The research in cyber security for vehicular ad-hoc networks (VANETs) has received great attention from the scientific community. However, the intrusion detection and prevention for Sixth Generation (6G)-enabled VANETs has not attracted much attention up to this point. In this research article, we propose new robust zero trust agents based on collaborative artificial intelligence (AI) algorithms to protect the 6G-enabled VANETs from attacks targeting simultaneously the VANETs and 6G infrastructure. Collaborative AI is based on generative AI and transfer learning (TL) algorithms. Two kinds of zero trust agents are proposed - local zero trust systems (LZTS) and global zero trust systems (GZTS) - that monitor the network and infrastructure with the goal of detecting malicious behaviors promptly.},
  keywords={6G mobile communication;Measurement;Generative AI;Prevention and mitigation;Transfer learning;Collaboration;Lead;Zero Trust;Smart grids;Monitoring},
  doi={10.1109/MWC.003.2300571},
  ISSN={1558-0687},
  month={April},}@INBOOK{10880579,
  author={Shukla, Seema and Pandey, Babita and Pandey, Devendra Kumar and Mishra, Brijendra Pratap and Khamparia, Aditya},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Exploring Trust and Mistrust Dynamics: Generative AI&#x2010;Curated Narratives in Health Communication Media Content Among Gen X}, 
  year={2025},
  volume={},
  number={},
  pages={417-433},
  abstract={Summary <p>Large language models, generative adversarial networks (GANs), and variational autoencoders (VAEs) are basic technologies used in interfaces like Chat Generative Pre&#x2010;Trained Transformer (a textual content creator) and DALL&#x2010;E 2 (a text&#x2010;to&#x2010;image creator), poised to revolutionize the way users access and understand health information. The rapid uptake and investment in these technologies suggest they will be transformative, yet their implications for health communications remain unclear. In this viewpoint, we present a research study measuring individual trust using a previously established trust scale and examining the impact of displaying disclaimers on trust in content generated by artificial intelligence (AI). The results of data analysis using SmartPLS indicate that the three components of trust have a positive impact on individual trust. Semi&#x2010;structured interviews further reinforce these findings. This study sheds light on the adoption of new information technologies, focusing on how generative AI tools such as large language models, GANs, and VAEs may alter the production and consumption of health information. We explore how these technologies may influence the content people encounter, the blending of marketing and misinformation with evidence, and the factors that influence trust.</p>},
  keywords={Data analysis;Mathematical models;Fake news;Chatbots;Writing;Transformers;Text to image;Systematic literature review;Software;Psychology},
  doi={10.1002/9781394280735.ch21},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880579},}@INPROCEEDINGS{10973855,
  author={Zhou, Dezijian and Zhao, Yi and Xie, Jialing and Bian, Wenkai and Deng, Yuanyuan and Globa, Anastasia},
  booktitle={2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Participatory Design Investigation for Spatial Interactions in Ageing Population}, 
  year={2025},
  volume={},
  number={},
  pages={1759-1764},
  abstract={With the accelerating growth of the aging population and changes in family structures, the demand for supportive living environments and social interaction among older adults has grown significantly. However, traditional qualitative methods often fail to fully capture the nuanced needs of older adults due to communication barriers, memory limitations, and psychological stress. This study proposes an innovative research approach using an AI-driven age-inclusive digital agent in Mixed Reality (MR) to explore long-term interactions with older adults ageing in place. By creating intelligent digital agents designed after individuals, pets, or objects familiar to older adults, these agents aim to unobtrusively investigate how the residential environment influences daily routines among older people. Insights gained through natural interactions in MR can inform the design of residential spaces, better satisfying the needs of aging populations. This proposed approach aims to improve both home redevelopments and future residential designs for older adults.},
  keywords={Emotion recognition;Generative AI;Mixed reality;Human-robot interaction;Psychology;Prototypes;Human factors;Older adults;Faces;Guidelines;Ageing in Place;Artificial Intelligence Agent;Mixed Reality;Anthropomorphic Avatar},
  doi={10.1109/HRI61500.2025.10973855},
  ISSN={},
  month={March},}@ARTICLE{10795250,
  author={Jiang, Fenlong and Huang, Bo and Wu, Husheng and Feng, Dan and Zhou, Yu and Zhang, Mingyang and Gong, Maoguo and Zhao, Wei and Guan, Ziyu},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Change Masked Modality Alignment Network for Multimodal Change Detection}, 
  year={2025},
  volume={63},
  number={},
  pages={1-16},
  abstract={Using multimodal remote sensing images for change detection (CD) can significantly improve the feasibility and reliability in challenging environments. However, the differences in imaging mechanisms make multimodal images highly heterogeneous. A key challenge for multimodal CD (MCD) is that the heterogeneity of the modalities and changes in ground objects are intertwined during processing. To address this issue, this article proposes a change masked modality alignment network (CMMAN), which uses a multitask framework consisting of one CD branch and two image modal transformation (IMT) branches. Specifically, to ensure a unified feature space, bi-temporal multimodal images are first input into the same Swin-Transformer-based encoder. The extracted features are then fed simultaneously into the CD branch and separately into the two IMT branches. In the CD branch, the decoder is also designed based on the Swin-Transformer, and a weakly modality-correlated feature enhancement (WMCFE) module is introduced to mitigate the interference of modality heterogeneity on CD. For the two IMT branches, both employ a generative adversarial network (GAN) to transform between modalities, and the distributions of features from different modalities are aligned through simultaneous optimization. Uniquely, the change probability map predicted by the CD branch is utilized to mask the change regions in IMT, further decoupling ground object changes and modal heterogeneity. Experimental results on multiple public datasets demonstrate that the proposed CMMAN significantly improves MCD performance and shows good compatibility and portability with various common backbone networks.},
  keywords={Transformers;Feature extraction;Sensors;Interference;Image sensors;Multitasking;Intelligent sensors;Generative adversarial networks;Remote sensing;Optical sensors;Change detection (CD);generative adversarial network (GAN);multimodal;multitask learning;transformer},
  doi={10.1109/TGRS.2024.3516001},
  ISSN={1558-0644},
  month={},}@INBOOK{11173777,
  author={},
  booktitle={Handbook of Intelligent Automation Systems Using Computer Vision and Artificial Intelligence}, 
  title={Front Matter}, 
  year={2025},
  volume={},
  number={},
  pages={i-xxii},
  abstract={<p>The prelims comprise: <ul> <li>Half&#x2010;Title Page</li> <li>Publisher Page</li> <li>Title Page</li> <li>Copyright Page</li> <li>Table of Contents</li> <li>Preface</li> </ul> </p>},
  keywords={},
  doi={10.1002/9781394302734.fmatter},
  ISSN={},
  publisher={Wiley},
  isbn={9781394302703},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11173777},}@INPROCEEDINGS{11144574,
  author={Cui, Haoyi and Deng, Hua and Deng, Yangwu and Li, Kexin and Wang, Wanguo and Wang, Zhenli and Sun, Weifan},
  booktitle={2025 2nd International Conference on Artificial Intelligence and Digital Technology (ICAIDT)}, 
  title={Few-Shot Learning Based on GAN and Model Fine-Tuning}, 
  year={2025},
  volume={},
  number={},
  pages={18-24},
  abstract={Accurate identification of devices such as transformers, switches, transmission lines and poles is becoming increas $\boldsymbol{\gamma}$ ingly important as the demand for automated monitoring and control in the power and energy industries increases. Traditional image classification methods often face performance bottlenecks due to insufficient samples and data imbalance. In this paper, we propose a few-shot learning based on data enhancement and model fine-tuning, which first generates images using conditional generative adversarial network (cGAN) to expand the training dataset, enhance the sample diversity, and solve the problem of uneven sample sizes in different categories. Subsequently, the pretrained ResNet50 model is fine-tuned according to the enhanced dataset to better adapt to the characteristics of power equipment. The experimental results show that after data enhancement and model fine-tuning, the classification accuracy of transformers and poles reaches 92% and 90% respectively, and the classification accuracy of switches and transmission lines is 88% and 85% respectively. The overall average accuracy is 89.75%, which proves the effectiveness of the model in the power equipment image classification task.},
  keywords={Training;Adaptation models;Accuracy;Power transmission lines;Transformers;Generative adversarial networks;Data models;Few shot learning;Image classification;Residual neural networks;Data Augmentation;Model Fine-Tuning;Power Equipment Image Classification},
  doi={10.1109/ICAIDT66272.2025.00013},
  ISSN={},
  month={April},}@INPROCEEDINGS{10667177,
  author={Zhou, Jiehan and Cao, Yang and Lu, Quanbo and Zhang, Weishan and Liu, Xin and Ni, Weijian},
  booktitle={2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={Industrial Large Model: Toward A Generative AI for Industry}, 
  year={2024},
  volume={},
  number={},
  pages={80-81},
  abstract={Large Language Models (LLMs) are creating new opportunities in industrial intelligence and have the potential to be the driving forces behind its transformation. This new era of industrial intelligence is characterized by universality, reusability, cross-domain generalization, multimodality, and support for multi-task decision-making. We refer to this new form of industrial intelligence as Industrial Large Models (ILMs). To accelerate the development of ILMs, this paper proposes an integrated framework for Industrial Large Model.},
  keywords={Industries;Generative AI;Computational modeling;Large language models;Decision making;Multitasking;Large Language Models;Industrial Large Models},
  doi={10.1109/CCECE59415.2024.10667177},
  ISSN={2576-7046},
  month={Aug},}@INPROCEEDINGS{10935967,
  author={Wu, Yi-Chieh and Hsu, Yu-Jung},
  booktitle={2024 International Symposium on Multimedia (ISM)}, 
  title={Generating and Evaluating Cursive Chinese Calligraphy by Semi-Classifying Style: A Case Study Using a Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={9-16},
  abstract={Generative AI offers a promising approach to overcoming the challenges of optical character recognition (OCR) for traditional Chinese cursive calligraphy. By generating text-image data, we can address data insufficiency and imbalance, enhancing the effectiveness of deep learning models in capturing the variability of cursive styles. In this study, we developed a text-to-text-content-image generation model utilizing the Cursive Chinese Calligraphy Dataset and the WordStylist framework. By categorizing images according to style characteristics, our method reduces the learning curve and improves the quality of generated artifacts while tackling common issues in handwritten datasets, such as the absence of author annotations and mixed styles without proper labeling.We trained three models with varying style categories: four-class, eight-class, and sixteen-class, and introduced a semi-annotating method for style categorization. Experimental results indicate that fewer style categories lead to greater variation in cursive characters within each class. The sixteen-class model exhibited more consistent styles, resulting in faster convergence and more stable generation, thereby offering users a broader range of style choices.To evaluate the quality of the generated content, we employed OCR-Embedding with Maximum Mean Discrepancy(MMD), derived from CMMD, as well as expert evaluations. The results from OCR-Embedding MMD were generally consistent with those of expert assessments. According to the experts, the four-class model performed best in zero-shot sample generation. We infer that zero-shot characters benefit from features of other characters within the same style class, as these characters have more training data available, contributing to improved generation of zero-shot characters.},
  keywords={Generative AI;Annotations;Optical character recognition;Zero shot learning;Training data;Diffusion models;Data models;Labeling;Convergence;Diffusion Model;Cursive Chinese Calligraphy;OCR-Embedding with Maximum Mean Discrepancy;Zero-shot Learning;Generative AI},
  doi={10.1109/ISM63611.2024.00007},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10837633,
  author={Gillet, Denis and Notari, Michele and Spacnlchauer, Basile and Reidy, Thibault},
  booktitle={2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)}, 
  title={Implementation Framework and Strategies for AI-Augmented Open Educational Resources (OER): A Comprehensive Approach Applied to Secondary and Higher Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper aims to propose an implementation framework for the adoption and management of Open Educational Resources (OER), focusing on their life cycle, as well as the integration of AI for supporting educators in their classification of the created content, the creation of tutoring agents for the learning process and learners in deepening their learning experience and exploitation. We explore the incentives for educators, connections to educational programs, and propose a participatory design model for effective implementation. The application of this framework to the Graasp11https://graasp.org learning experience platform and its associated open OER library is also discussed, along with future implementation strategies.},
  keywords={Training;Educational programs;Focusing;Open Educational Resources;Libraries;Artificial intelligence;Information technology;digital education;open educational resources;OER;digital library;generative AI},
  doi={10.1109/ITHET61869.2024.10837633},
  ISSN={2473-2060},
  month={Nov},}@INPROCEEDINGS{10795498,
  author={Wu, Xiaolin and Zhu, Shuliang and Zhou, Jinjia and Yao, Raymond},
  booktitle={2024 IEEE Cyber Science and Technology Congress (CyberSciTech)}, 
  title={Paradigm Shift in Foreign Language Teaching Practices in the Age of AI - Case Studies of Chinese and Japanese Learners}, 
  year={2024},
  volume={},
  number={},
  pages={496-501},
  abstract={Over the past decade, with the continuous enhancement of smartphone functionalities, numerous applications have been developed for Android and iOS systems, including speech-to-text, text-to-speech, and translation functions. Simultaneously, translation software has made remarkable progress, providing increasingly accurate and context-sensitive translations. The rise of AI-driven language models, such as ChatGPT, has revolutionized traditional language education by offering dynamic interaction and personalized learning experiences. This paper proposes to shift the paradigm of language learning involving learners, teachers, and AI working in tandem. And introducing a novel paradigm-based AI Tri-Party Collaborative Language Learning Model(AI-TriCoLL), which significantly enhances language learning by focusing on improving learners' pronunciation, understanding of grammar, and overall language fluency. This shift in language education promotes not only language skills but also cultural understanding. The teaching methods and learning experiences explored here provide fresh insights into creative strategies for both educators and students as they engage with foreign language learning in the context of AI advancements.},
  keywords={Vocabulary;Visualization;Translation;Education;Chatbots;Software;Grammar;Text to speech;Artificial intelligence;Speech to text;Generative AI;Foreign Language Education;Learning Outcome Visualization;Human-Computer Interaction;Adaptive Learning Technologies},
  doi={10.1109/CyberSciTech64112.2024.00088},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11126635,
  author={Cordero, David and Zhao, Yijun and Diaz, Priscilla and Weiss, Gary M.},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Unveiling Bias: Analyzing Race and Gender Disparities in AI-Generated Imagery}, 
  year={2025},
  volume={},
  number={},
  pages={318-323},
  abstract={This study explores gender and racial bias in AI-generated images. DALL-E 3 was used to generate 2800 images based on prompts related to occupations, activities, and positive/negative personal characteristics, and human reviewers classified the generated images by gender and race. Our analysis reveals that certain prompts are disproportionately associated with specific races and/or genders, suggesting that the AI model may be biased. Race and gender statistics are compared with real-world statistics to determine whether the generated images mirror existing societal biases or introduce new biases. Our findings raise ethical concerns about fairness and representation in AI technologies and discuss the consequences of biased image generation. This research is motivated by the growing integration of AI in media generation and the associated risks of perpetuating and amplifying existing biases. The dataset used in this study is provided via a GitHub repository to support reproducibility, transparency, and broader studies in the research community.},
  keywords={Ethics;Technological innovation;Image synthesis;Text to image;Production;Media;Software;Reproducibility of results;Artificial intelligence;Software development management;AI bias;generative models;text-to-image;race and gender representation;image generation},
  doi={10.1109/COMPSAC65507.2025.00051},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11048179,
  author={Ouyang, Song and Chen, Hui},
  booktitle={2025 5th International Conference on Mechanical, Electronics and Electrical and Automation Control (METMS)}, 
  title={Research on Unbalanced Data Fault Diagnosis of Diesel Engine Based on WACGAN-GP}, 
  year={2025},
  volume={},
  number={},
  pages={324-332},
  abstract={Aiming at the problem of unbalanced data caused by few samples of vibration signal faults in the actual operation of ship diesel engines, a fault diagnosis model based on conditional Wasserstein Assisted Classification Generative Adversarial Network with Gradient Penalty(WACGAN-GP) optimization is proposed. The WACGAN-GP is used to enhance the data for the category data with few samples to make the datasets balanced, the hyperparameters of Support Vector Machine (SVM) are optimized by Grey Wolf Optimizer(GWO), and finally the balanced datasets is imported into SVM for the experimental validation of fault diagnosis. The comparison experimental results of different data enhancement algorithms and different classification algorithms show that the artificial samples generated by the WACGAN-GP method have highly similar characteristics to the original samples, and the Grey Wolf Optimizer-Support Vector Machine (GWO-SVM) has a higher accuracy rate than other fault diagnosis models.},
  keywords={Fault diagnosis;Support vector machines;Diesel engines;Data enhancement;Clustering algorithms;Generative adversarial networks;Data models;Classification algorithms;Marine vehicles;Optimization;marine diesel engine;fault diagnosis;support vector machine;generative adversarial networks},
  doi={10.1109/METMS65303.2025.11048179},
  ISSN={},
  month={May},}@ARTICLE{11072910,
  author={Rutecka, Paulina and Cicha, Karina and Rizun, Mariia and Strzelecki, Artur},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Generative AI in Curriculum Design: Empirical Insights Into Model Performance and Educational Constraints}, 
  year={2025},
  volume={18},
  number={},
  pages={757-768},
  abstract={This study verifies the ability of large language models (LLMs) to generate a curriculum and develop syllabi for specific courses. We prompted four models to generate two sets of curricula for a bachelor’s degree in Economics and Management. We also generated syllabi for the courses included in the curriculum. We chose five Polish public economics universities offering those degree programs for comparison. Four LLMs were used in this experiment: ChatGPT-3.5, ChatGPT-4, Google Bard, and Gemini. Two of them are multimodal models. The study used an iterative approach, increasing the detail of the prompt in each iteration. The results show that the more specific prompt is given to the LLM, the less accurate the results are. Moreover, the experiment shows that none of the LLMs developed a complete curriculum at a level comparable to that generated by humans. However, LLMs can significantly help create a curriculum and develop syllabi by humans, provided that there is close human–artificial intelligence (AI) collaboration. The results obtained from the AI-assisted curriculum design differ depending on the model. By analyzing the differences between the tools and the real degree programs and syllabi, we determined that multimodal models are better suited for this task than older models.},
  keywords={Chatbots;Economics;Accuracy;Testing;Systematic literature review;Support vector machines;Plagiarism;Large language models;Bibliometrics;Analytical models;Artificial intelligence (AI)-generated content;curriculum design;generative AI (GenAI);higher education;large language model (LLM)},
  doi={10.1109/TLT.2025.3587081},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{11016463,
  author={Bhatt, Vishwa and Yu, Zhixin and Hou, Yunfei and Jin, Jennifer},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={ChatGPT as a Programming Tutor: Student Perceptions, Effectiveness, and Challenges}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={This research examines the impact of ChatGPT on computer science education, focusing on its application in learning programming languages like SQL, C++, Python, and C#. Through a survey of 149 university students, the study identifies both the advantages and challenges of using ChatGPT as a virtual lab assistant. The results show that ChatGPT offers considerable assistance to students, especially in providing prompt feedback, facilitating debugging, and clarifying complex programming concepts. However, the research also points out significant challenges, including the potential for over-reliance on AI tools and worries about the accuracy of the responses generated by AI. Several students reported experiencing misleading or incomplete information from ChatGPT, indicating a need for enhancements in its accuracy and dependability. To address these challenges, the study proposes a transition from assignments focused on theory to personalized, project-based activities that foster independent problem-solving. In summary, this research sheds light on the advantages and obstacles of incorporating ChatGPT into computer science courses. Although ChatGPT provides beneficial support for learning, its function should be supplementary rather than central to programming education. The results highlight the necessity of thoughtful integration, promoting self-directed learning while utilizing AI's capabilities to foster engagement and offer immediate assistance.},
  keywords={Training;Surveys;Accuracy;Systematics;Debugging;Chatbots;Prompt engineering;Problem-solving;Artificial intelligence;Programming profession;ChatGPT;Generative AI;Programming education;AI dependency;AI accuracy;Programming Tutor},
  doi={10.1109/EDUCON62633.2025.11016463},
  ISSN={2165-9567},
  month={April},}@ARTICLE{9854196,
  author={Oh, Kwanseok and Yoon, Jee Seok and Suk, Heung-Il},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learn-Explain-Reinforce: Counterfactual Reasoning and its Guidance to Reinforce an Alzheimer's Disease Diagnosis Model}, 
  year={2023},
  volume={45},
  number={4},
  pages={4843-4857},
  abstract={Existing studies on disease diagnostic models focus either on diagnostic model learning for performance improvement or on the visual explanation of a trained diagnostic model. We propose a novel learn-explain-reinforce (LEAR) framework that unifies diagnostic model learning, visual explanation generation (explanation unit), and trained diagnostic model reinforcement (reinforcement unit) guided by the visual explanation. For the visual explanation, we generate a counterfactual map that transforms an input sample to be identified as an intended target label. For example, a counterfactual map can localize hypothetical abnormalities within a normal brain image that may cause it to be diagnosed with Alzheimer's disease (AD). We believe that the generated counterfactual maps represent data-driven knowledge about a target task, i.e., AD diagnosis using structural MRI, which can be a vital source of information to reinforce the generalization of the trained diagnostic model. To this end, we devise an attention-based feature refinement module with the guidance of the counterfactual maps. The explanation and reinforcement units are reciprocal and can be operated iteratively. Our proposed approach was validated via qualitative and quantitative analysis on the ADNI dataset. Its comprehensibility and fidelity were demonstrated through ablation studies and comparisons with existing methods.},
  keywords={Visualization;Cognition;Brain modeling;Magnetic resonance imaging;Transforms;Perturbation methods;Location awareness;Visual explanation;counterfactual reasoning;representation reinforcement;explanation-guided attention;deep learning;explainable AI (XAI);structural magnetic resonance imaging;Alzheimer's disease},
  doi={10.1109/TPAMI.2022.3197845},
  ISSN={1939-3539},
  month={April},}@INBOOK{10950574,
  author={Khan, Ian},
  booktitle={The Quick Guide to Prompt Engineering: Generative AI Tips and Tricks for ChatGPT, Bard, Dall-E, and Midjourney}, 
  title={Diving Deeper: Structure and Nuances of Prompts}, 
  year={2024},
  volume={},
  number={},
  pages={47-61},
  abstract={Summary <p>As with any tool, understanding the anatomy of prompts is paramount. It empowers users to harness AI's potential fully. Context in prompts acts as the guiding light, enabling AI models to navigate through vast seas of information and zero in on a specific response. In the realm of AI, the way we communicate our inquiries is pivotal in obtaining desired outcomes. Specificity and clarity in prompt formulation are crucial determinants of this success. In the domain of generative AI, the concept of max tokens is crucial for controlling the length and precision of the model's outputs. Incorporating external knowledge and diverse contexts into prompts not only refines the accuracy of AI&#x2010;generated content but also tailors responses to resonate more profoundly with the intended audience or objective. Incorporating user feedback for dynamic prompting fosters a symbiotic relationship between the user and the AI system.</p>},
  keywords={Artificial intelligence;Temperature control;Creativity;Anatomy;Iterative methods;Temperature distribution;Prompt engineering;Hands;Context modeling;Airplanes},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394243341},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950574},}@INBOOK{10950803,
  author={Khan, Ian},
  booktitle={The Quick Guide to Prompt Engineering: Generative AI Tips and Tricks for ChatGPT, Bard, Dall-E, and Midjourney}, 
  title={Practical Guide to Prompt Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={83-102},
  abstract={Summary <p>Crafting effective prompts for a generative AI system is pivotal in guiding its responses and ensuring meaningful and accurate results. This chapter offers a step&#x2010;by&#x2010;step breakdown to ensure our initiation into prompt engineering is smooth and productive. It guides on how to systematically test and evaluate our prompts for AI systems. Prompt engineering, akin to any skill, requires continual refinement. The chapter presents an in&#x2010;depth look into iterating and refining prompts within the realm of prompt engineering. It elucidates how prompt engineering can be tailored for various domains, ensuring effective and relevant AI interactions. In the realm of prompt engineering, machine learning (ML) has emerged as a powerful ally, pushing the boundaries of what's possible. By integrating ML techniques, we can optimize prompts dynamically, making interactions with AI models more precise and insightful.</p>},
  keywords={Prompt engineering;Artificial intelligence;Testing;History;Feedback loop;Analytical models;Adaptation models;Refining;Iterative methods;Urban planning},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394243341},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10950803},}@INPROCEEDINGS{10980597,
  author={Anand, Jonathan and Kumar, Anand},
  booktitle={2025 17th International Conference on Computer and Automation Engineering (ICCAE)}, 
  title={Assessing Fatality in AI, ML, and NN Solutions for Critical-Safety Systems in Transportation, Industrial, and Construction Industries}, 
  year={2025},
  volume={},
  number={},
  pages={385-388},
  abstract={This study explores the trend to apply artificial intelligent (AI), machine learning (ML), and neural network (NN) based solutions to safety-critical systems in transportation, industrial, and construction sectors of the industry. NN is introduced as subsets of AI and ML. The fatality rates for the three industrial sectors are presented as a reference for the expectation of AI, ML, and NN solutions in safety-critical systems. The reliability of such systems has to significantly improve and probability of fatality in such AI solutions to safety-critical systems should be hundred or thousand times or higher order of magnitude lower. Reliability standards and methods for AI solutions have been and are being developed to meet the stringent requirements.},
  keywords={Deep learning;Standards organizations;Transportation;Artificial neural networks;Organizations;Market research;Safety;Risk management;Reliability;Artificial intelligence;AI;ML;NN;Deep Learning;Engineering Applications;Critical-Safety},
  doi={10.1109/ICCAE64891.2025.10980597},
  ISSN={2154-4360},
  month={March},}@ARTICLE{10965579,
  author={Wu, Yanyi and Zhang, Weijia and Lin, Chenghua},
  journal={IT Professional}, 
  title={Generative Artificial Intelligence in University Education}, 
  year={2025},
  volume={27},
  number={2},
  pages={69-74},
  abstract={Generative AI (GenAI) is rapidly changing the landscape of university education, posing significant ethical and pedagogical dilemmas for institutions, educators, and students. The pursuit of GenAI’s educational advantages must be carefully balanced with the imperative to uphold academic integrity, necessitating a reevaluation of authorship, assessment methodologies, and the very definition of original work. Although AI-driven personalized learning offers enticing convenience, it simultaneously introduces critical concerns regarding student data privacy and intellectual autonomy. As GenAI becomes increasingly integrated into education, universities are challenged to develop pedagogical approaches that cultivate enduring “AI-proof” skills alongside leveraging GenAI to enrich the learning experience. Constructing a robust AI-enhanced education system demands a foundational commitment to ethical principles, transparent governance structures, and a renewed emphasis on nurturing critical thinking and creativity. This article proposes pathways for GenAI integration in education, empowering learning while safeguarding core educational values, especially critical thinking.},
  keywords={Ethics;Data privacy;Generative AI;Education;Creativity;Educational technology;Educational programs},
  doi={10.1109/MITP.2025.3545629},
  ISSN={1941-045X},
  month={March},}@INBOOK{10880618,
  author={Kumar, Bagesh and Kumar, Sohan and Vikram Singh Rathore, Yash and Raj, Akash and Singh Andotra, Vanshika and Gupta, Rishik and Shukla, Prakhar},
  booktitle={Generative Artificial Intelligence for Biomedical and Smart Health Informatics}, 
  title={Secure Decentralized ECG Prediction: Balancing Privacy, Performance, and Heterogeneity}, 
  year={2025},
  volume={},
  number={},
  pages={607-621},
  abstract={Summary <p>In recent years, there has been a surge in utilizing generative artificial intelligence (AI), particularly generative adversarial networks (GANs), to enhance decentralized prediction models for analyzing electrocardiogram (ECG) data. This book chapter explores how generative AI methods address limitations of conventional techniques in ECG analysis, such as data scarcity and privacy concerns associated with centralized storage. It discusses the technical aspects of generative AI, including generating synthetic ECG signals, improving data quality, detecting anomalies, and creating realistic clinical scenarios. In addition, the chapter examines the implementation of decentralized prediction models using federated learning frameworks to ensure data privacy while enabling collaborative training across distributed ECG datasets. This advancement in generative AI has the potential to revolutionize ECG data processing and predictive modeling in decentralized healthcare systems, enabling personalized treatment plans and remote monitoring, thereby fostering a new era of patient&#x2010;centric care delivery.</p>},
  keywords={Electrocardiography;Heart beat;Medical diagnostic imaging;Training;Time-frequency analysis;Noise;Medical services;Generators;Generative AI;Deep learning},
  doi={10.1002/9781394280735.ch29},
  ISSN={},
  publisher={IEEE},
  isbn={9781394280728},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10880618},}@INPROCEEDINGS{9879058,
  author={Hu, Zhanhao and Huang, Siyuan and Zhu, Xiaopei and Sun, Fuchun and Zhang, Bo and Hu, Xiaolin},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Adversarial Texture for Fooling Person Detectors in the Physical World}, 
  year={2022},
  volume={},
  number={},
  pages={13297-13306},
  abstract={Nowadays, cameras equipped with AI systems can capture and analyze images to detect people automatically. However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., physical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack success rate when the viewing angle (i.e., the camera's angle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detectors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTexure and then made T-shirts, skirts, and dresses in the physical world. Experiments showed that these clothes could fool person detectors in the physical world.},
  keywords={Printing;Computer vision;Shape;Detectors;Cameras;Generators;Pattern recognition;Adversarial attack and defense; Recognition: detection;categorization;retrieval},
  doi={10.1109/CVPR52688.2022.01295},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10498685,
  author={Patel, Sanskruti and Patel, Unnati and Nanavati, Jay and Patel, Atul},
  booktitle={2024 11th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Analyzing Generative Models for Realistic Data Augmentation across Modalities and Applications}, 
  year={2024},
  volume={},
  number={},
  pages={1601-1606},
  abstract={In the emerging era of artificial intelligence, Generative AI models such as Transformer-based models, Generative Adversarial Networks, Diffusion models and Variational Autoencoders offer a powerful paradigm for data augmentation by enabling the generation of artificial data that be like to the original data distribution. This study discusses the use of generative models in place of traditional augmentation techniques and reviewing their history and drawbacks. It explains the workings of GANs and VAEs, and discusses strategies for effectively integrating them into the augmentation pipeline to boost model performance. Both perceptual metrics like Inception Score and Frechet Inception Distance as well as domain-specific metrics like F1-score, IoU and MSE are presented for rigorously evaluating the quality of generated data. Through case studies and experimental results, the paper demonstrates quantitatively how generative augmentation can enhance dataset diversity, prevent overfitting on limited data, and significantly improve metrics across tasks like classification and object detection. It discusses challenges and future scope of generative models for data augmentation.},
  keywords={Measurement;Training;Analytical models;Computational modeling;Object detection;Predictive models;Data augmentation;Generative Models;Data Augmentation;GANs;VAEs},
  doi={10.23919/INDIACom61295.2024.10498685},
  ISSN={},
  month={Feb},}@ARTICLE{9826746,
  author={Lee, Tae Bok and Han, Sujy and Heo, Yong Seok},
  journal={IEEE Access}, 
  title={Continuous Facial Motion Deblurring}, 
  year={2022},
  volume={10},
  number={},
  pages={76079-76094},
  abstract={We introduce a novel framework for continuous facial motion deblurring that restores the continuous sharp moment latent in a single motion-blurred face image via a moment control factor. Although a motion-blurred image is the accumulated signal of continuous sharp moments during the exposure time, most existing single image deblurring approaches aim to restore a fixed number of frames using multiple networks and training stages. To address this problem, we propose a continuous facial motion deblurring network based on GAN (CFMD-GAN), which is a novel framework for restoring the continuous moment latent in a single motion-blurred face image with a single network and a single training stage. To stabilize the network training, we train the generator to restore continuous moments in the order determined by our facial motion-based reordering process (FMR) utilizing domain-specific knowledge of the face. Moreover, we propose an auxiliary regressor that helps our generator produce more accurate images by estimating continuous sharp moments. Furthermore, we introduce a control-adaptive (ContAda) block that performs spatially deformable convolution and channel-wise attention as a function of the control factor. Extensive experiments on the 300VW datasets demonstrate that the proposed framework generates a various number of continuous output frames by varying the moment control factor. Compared with the recent single-to-single image deblurring networks trained with the same 300VW training set, the proposed method show the superior performance in restoring the central sharp frame in terms of perceptual metrics, including LPIPS, FID and Arcface identity distance. The proposed method outperforms the existing single-to-video deblurring method for both qualitative and quantitative comparisons. In our experiments on the 300VW test set, the proposed framework reached 33.14 dB and 0.93 for recovery of 7 sharp frames in PSNR and SSIM, respectively.},
  keywords={Image restoration;Faces;Training;Face recognition;Generative adversarial networks;Feature extraction;Decoding;Continuous facial motion deblurring;AC-GAN;control-adaptive block},
  doi={10.1109/ACCESS.2022.3190089},
  ISSN={2169-3536},
  month={},}@INBOOK{10785895,
  author={Haque, Enamul},
  booktitle={AI Horizons: Shaping a Better Future Through Responsible Innovation and Human Collaboration}, 
  title={Chapter 8: Generative AI: Conversational Agents and Beyond}, 
  year={2024},
  volume={},
  number={},
  pages={187-200},
  abstract={},
  keywords={Generative AI;Artificial intelligence;Chatbots;Problem-solving;Pattern recognition;History;Creativity;Shape;Biological neural networks;Technological innovation},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9781501518485},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10785895},}@INPROCEEDINGS{10968413,
  author={Chaudhari, Pawan Dilip and Gudadhe, Amit},
  booktitle={2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)}, 
  title={Deep Learning Approach and its Application in the Cybersecurity Domain}, 
  year={2025},
  volume={},
  number={},
  pages={520-524},
  abstract={Cyber Security has become especially prominent and critical for organizations all over the world because of the fast advancements in technology and the increasing sophistication of modern cyber threats. Most traditional methods related to cyber security are unable to match the scale and complexity of modern cyber-attacks. Deep learning has emerged in this context as a powerful tool for offering brand-new approaches and avenues toward threat detection, anomaly identification, and protection of data. In this paper, we investigate the basic principles of deep learning and its significant architectures Convolutional Neural Networks, Recurrent Neural Networks, and Generative Adversarial Networks which provide models for solving various issues in the field of cybersecurity. We describe the contribution of deep learning to intrusion detection systems, malware detection, phishing protection, DDoS attack mitigation, and fraud detection. Deep learning also presents some advantages in cybersecurity. In this paper, I reviewed the basic applications, challenges, and potential solutions for enhancing cybersecurity systems by techniques of deep learning. Every organization, right from financial institutes to the entire globe employing itself to reach a developmental stage, has been forced to consider the implications of cybersecurity threats carefully. Most traditional methods relevant to cybersecurity have been ineffective at combatting modern cyber-attacks' scale and complexity.},
  keywords={Deep learning;Recurrent neural networks;Prevention and mitigation;Phishing;Organizations;Threat assessment;Complexity theory;Computer security;Protection;Cyberattack;Deep Learning;Cybersecurity;Anomaly Detection;Explainable AI (XAI);Applications},
  doi={10.1109/ICMLAS64557.2025.10968413},
  ISSN={},
  month={March},}@ARTICLE{11008900,
  author={Bi, Xia-An and Yang, Zicheng and Chen, Dayou and Wang, Jie and Xing, Zhaoxu and Xu, Luyun},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={FHG-GAN: Fuzzy Hypergraph Generative Adversarial Network With Large Foundation Models for Alzheimer’s Disease Risk Prediction}, 
  year={2025},
  volume={33},
  number={8},
  pages={2599-2613},
  abstract={Risk prediction for Alzheimer’s disease (AD) is crucial for delaying disease progression in clinics. However, existing methods face significant challenges in leveraging the high-order and fuzzy associations in biomedical data. To address this limitation, we propose a fuzzy hypergraph-based deep learning framework to explore multiomics associations and enhance predictive performances efficiently. In specific, we first establish a mathematical model of fuzzy structural entropy propagation. This model represents multiomics associations by fuzzy hypergraphs, characterizing the progression of AD as the topological evolution of fuzzy hypergraphs. Next, we design a fuzzy hypergraph generative adversarial network (FHG-GAN). Particularly, large foundation models are used to generate high-quality feature representations, alleviating data noise and inconsistencies. FHG-GAN captures the evolutionary patterns of diseases using fuzzy hypergraph convolutional layers designed based on the mathematical model, thereby achieving accurate disease risk prediction and pathogeny extraction. Finally, we experimentally demonstrate the superiority of FHG-GAN compared with advanced methods, indicating FHG-GAN’s ability to support the early diagnosis and treatment of AD-like diseases.},
  keywords={Diseases;Mathematical models;Feature extraction;Deep learning;Generative adversarial networks;Entropy;Data mining;Noise;Functional magnetic resonance imaging;Biology;Alzheimer’s disease (AD);disease risk prediction;fuzzy hypergraph generative adversarial network (FHG-GAN);imaging genetics;large foundation model (LFM);pathogeny extraction},
  doi={10.1109/TFUZZ.2025.3572479},
  ISSN={1941-0034},
  month={Aug},}@ARTICLE{10504865,
  author={Park, Chae-Won and Palakonda, Vikas and Yun, Sangseok and Kim, Il-Min and Kang, Jae-Mo},
  journal={IEEE Internet of Things Journal}, 
  title={OCR-Diff: A Two-Stage Deep Learning Framework for Optical Character Recognition Using Diffusion Model in Industrial Internet of Things}, 
  year={2024},
  volume={11},
  number={15},
  pages={25997-26000},
  abstract={Optical character recognition (OCR) is one of the key enabling technologies in industrial Internet of Things (IIoT) for extracting and utilizing useful textual information, but it is technically challenging due to poor environmental conditions. To deal with such challenges, in this letter, we propose a novel two-stage deep learning framework for OCR using a generative diffusion model, namely, OCR-Diff. In the first stage, our customized conditional U-Net is pretrained jointly with a feature extractor with the aid of the forward diffusion process such that the quality of a low-resolution text image is improved via the reverse diffusion process. In the next stage, the pretrained conditional U-Net and feature extractor are jointly fine tuned for an off-the-shelf text recognizer to precisely recognize the texts in the image. Experimental results on TextZoom data sets substantiate the superiority and effectiveness of the proposed scheme.},
  keywords={Feature extraction;Text recognition;Optical character recognition;Image recognition;Image quality;Industrial Internet of Things;Diffusion processes;Deep learning (DL);generative diffusion model;industrial Internet of Things (IIoT);low resolution text image;optical character recognition (OCR);text recognition},
  doi={10.1109/JIOT.2024.3390700},
  ISSN={2327-4662},
  month={Aug},}@INPROCEEDINGS{10065737,
  author={Li, Wanjun and Liu, Zhe and Deng, Hongtao},
  booktitle={2022 IEEE 8th International Conference on Computer and Communications (ICCC)}, 
  title={A Self-Attention Based SRGAN for Super-Resolution of Astronomical Image}, 
  year={2022},
  volume={},
  number={},
  pages={1977-1981},
  abstract={High-resolution (HR) astronomical image play a vital role in the development of scientific research, cosmic exploration, astronomy, and physics. In this paper, we propose a self- attention based generative adversarial network of astronomical image super-resolution (SR) aiming at the problem of low- resolution (LR) of astronomical imaging systems. We adopt SRGAN as the benchmark model and add self-attention, which captures more global dependencies and deepens the network for enhanced high-frequency feature representation. To achieve fast and stable training, the BN layer is deleted from the proposed networks. The Charbonnier loss is introduced as the loss function to handle outliers and improve SR performance. Experimental results demonstrate that the proposed method is able to reduce artifacts and obtains better performance in Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) on the astronomical image testset.},
  keywords={Training;Visualization;PSNR;Computational modeling;Superresolution;Imaging;Generators;generative adversarial network;astronomical image;super-resolution;self-attention},
  doi={10.1109/ICCC56324.2022.10065737},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10972721,
  author={Cheng, Long and Gisler, Joy and Lao, Kenli and Kunz, Andreas},
  booktitle={2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Generative Artificial Intelligence Chatbot Integration for Virtual Reality Language Learning}, 
  year={2025},
  volume={},
  number={},
  pages={137-142},
  abstract={This paper describes the potential of integrating generative artifi-cial intelligence (GenAI) based chatbots into virtual reality (VR) for language learning. Existing VR and AI-based systems primarily focus on practicing vocabulary or written skills, often neglecting speaking practice. To address this gap, a VR-enabled conversation trainer, chatbot AI for VR (CHAI) 1, is introduced. The system integrates speech-to-text, a large language model, text-to-speech, and an avatar to enable natural conversations with CHAI. It is evaluated for its language error detection capabilities in English and German. The results show high accuracy in detecting language errors and demonstrate substantial reproducibility in error detection.},
  keywords={Vocabulary;Accuracy;Three-dimensional displays;Large language models;Oral communication;Learning (artificial intelligence);User interfaces;Chatbots;Text to speech;Speech to text;Virtual Reality;Large Language Model;Chatbot},
  doi={10.1109/VRW66409.2025.00035},
  ISSN={},
  month={March},}@ARTICLE{10700792,
  author={Zhang, Weidong and Li, Zexu and Li, Guohou and Zhou, Ling and Zhao, Wenyi and Pan, Xipeng},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={AGANet: Attention-Guided Generative Adversarial Network for Corn Hyperspectral Images Augmentation}, 
  year={2025},
  volume={71},
  number={2},
  pages={3683-3694},
  abstract={Hyperspectral imaging represents a spectral technique that facilitates the non-destructive detection of corn seeds. However, the application of deep learning techniques often necessitates a substantial volume of training data, a requirement that becomes challenging due to the limited availability of labeled hyperspectral images, a limitation imposed by equipment and labor costs. To overcome these challenges, this paper proposes an attention-guided generative adversarial network designed for data augmentation. By integrating an attention module and a classifier within the GAN framework, this approach enables the generator to produce hyperspectral samples with precise class labels from random noise and class labels as inputs. Additionally, the classifier establishes a linkage between the generated images and their corresponding labels, thereby reducing the necessity for extensive labeled datasets. Moreover, the incorporation of attention modules in both the generator and discriminator enhances the spatial feature extraction, resulting in more realistic sample production. The experimental outcomes confirm that the proposed methodology significantly elevates the resemblance of the generated hyperspectral corn images to actual images, thereby underscoring its potent generative capabilities.},
  keywords={Hyperspectral imaging;Generators;Generative adversarial networks;Training;Seeds (agriculture);Noise;Consumer electronics;Diffusion models;Noise reduction;Deep learning;Data augmentation;generating adversarial networks;corn seed;hyperspectral images},
  doi={10.1109/TCE.2024.3470846},
  ISSN={1558-4127},
  month={May},}@ARTICLE{10669055,
  author={Li, Zhiyuan and Zhou, Yanhui and Wei, Hao and Ge, Chenyang and Jiang, Jingwen},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Toward Extreme Image Compression With Latent Feature Guidance and Diffusion Prior}, 
  year={2025},
  volume={35},
  number={1},
  pages={888-899},
  abstract={Image compression at extremely low bitrates (below 0.1 bits per pixel (bpp)) is a significant challenge due to substantial information loss. In this work, we propose a novel two-stage extreme image compression framework that exploits the powerful generative capability of pre-trained diffusion models to achieve realistic image reconstruction at extremely low bitrates. In the first stage, we treat the latent representation of images in the diffusion space as guidance, employing a VAE-based compression approach to compress images and initially decode the compressed information into content variables. The second stage leverages pre-trained stable diffusion to reconstruct images under the guidance of content variables. Specifically, we introduce a small control module to inject content information while keeping the stable diffusion model fixed to maintain its generative capability. Furthermore, we design a space alignment loss to force the content variables to align with the diffusion space and provide the necessary constraints for optimization. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art approaches in terms of visual performance at extremely low bitrates. The source code and trained models are available at https://github.com/huai-chang/DiffEIC.},
  keywords={Image coding;Diffusion models;Image reconstruction;Bit rate;Text to image;Decoding;Visualization;Image compression;diffusion models;content variables;extremely low bitrates},
  doi={10.1109/TCSVT.2024.3455576},
  ISSN={1558-2205},
  month={Jan},}@ARTICLE{10556508,
  author={Chen, Wei and Li, Jin},
  journal={IEEE Access}, 
  title={SIGAN-CNN: Convolutional Neural Network Based Stepwise Improving Generative Adversarial Network for Time Series Classification of Small Sample Size}, 
  year={2024},
  volume={12},
  number={},
  pages={85499-85510},
  abstract={With the development of artificial intelligence technology, time series classification has attracted greater attention. Various methods have been considered for using deep learning models to perform the task. Training such models, however, requires a large amount of high-quality labeled samples, which may not be available due to the expensive cost of collection. Therefore, we proposed a novel framework named Convolutional Neural Network based Stepwise Improving Generative Adversarial Network (SIGAN-CNN) to solve this problem by generating more samples from the original distribution. We designed the structure of the generator and discriminator of SIGAN to be suitable for time series data. We also explored a method for extracting trend information from time series data to obtain trend samples for stepwise training. Therefore, the generator can fit the original time series data with subtle variations. More importantly, SIGAN can improve the diversity of the generated samples and the stability of the training process to achieve a higher quality of the generated samples. The generated samples are then combined with CNN based time series data classification methods to improve the classification performance of time series data. Especially, the combination of SIGAN and Multi-scale Attention Convolutional Neural Network (MACNN) is suitable for this task. We conducted a comprehensive evaluation of 8 standard datasets from various domains. The results demonstrate that SIGAN-MACNN achieves the best performance and outperforms the other state-of-the-art methods by a large margin. Therefore, SIGAN-MACNN offers an effective solution for addressing the time series classification task of small sample size.},
  keywords={Time series analysis;Market research;Data models;Generative adversarial networks;Training;Generators;Convolutional neural networks;Time series classification;generative adversarial network;convolutional neural network},
  doi={10.1109/ACCESS.2024.3413948},
  ISSN={2169-3536},
  month={},}@ARTICLE{9369348,
  author={Bazrafkan, Shabab and Varkarakis, Viktor and Lemley, Joseph and Javidnia, Hossein and Corcoran, Peter},
  journal={IEEE Access}, 
  title={Versatile Auxiliary Classification and Regression With Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={38810-38825},
  abstract={One of the most interesting challenges in Artificial Intelligence is to train conditional generators which are able to provide labeled adversarial samples drawn from a specific distribution. For a successful implementation of conditional generators, the created samples are constrained to a specific class. In this work, a new framework is presented to train a deep conditional generator by placing a classifier or regression model in parallel with the discriminator and back propagate the classification or regression error through the generator network. Special cases for binary classification, multi-class classification, and regression are studied. Experimental results on several data-sets are provided and the results are compared with similar state-of-the-art techniques. The main advantage of the method is that it is versatile and applicable to any variation of Generative Adversarial Network (GAN) implementation but also it is shown to obtain superior results compared to other methods. The mathematical proofs for the proposed scheme for both classification and regression are presented.},
  keywords={Generators;Gallium nitride;Generative adversarial networks;Task analysis;Databases;Training;Data models;Conditional generators;deep neural networks;generative adversarial networks},
  doi={10.1109/ACCESS.2021.3063793},
  ISSN={2169-3536},
  month={},}@ARTICLE{11129246,
  author={Yang, Ziyi and Liu, Zhen and Lu, Yaojun and Zhang, Rui and Wang, Shuai and Pan, Gaofeng and An, Jianping},
  journal={IEEE Consumer Electronics Magazine}, 
  title={Generative Foundation Model-aided Secure Satellite Terrestrial Communication}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Secure Satellite Terrestrial Communication (SSTC) systems are of critical importance across a range of sectors, including military, scientific, and commercial domains, especially in consumer electronics. As emergent threats become increasingly sophisticated, ensuring the security, reliability, and effectiveness of SSTC in consumer electronic scenarios has become paramount. However, traditional communication solutions are no longer adequate to address the evolving challenges of SSTC systems. To address these limitations, this paper proposes a novel SSTC system based on Generative Foundation Models (GFMs) to enhance security, reliability, and effectiveness. A fundamental aspect of this approach entails the proposal of a GFM structure to optimize signal design. Notably, the Knowledge Distillation (KD) technique has been incorporated to mitigate complexity. A range of critical performance parameters is taken into consideration for GFM, including the bit error rate and secrecy outage probability. The findings illustrate that the proposed GFM enhances the security and reliability of information transmission in SSTC systems. Finally, the paper engages with the challenges and potential future directions for the application of GFM to SSTC systems for future consumer electronic scenarios.},
  keywords={Security;Transformers;Real-time systems;Consumer electronics;Satellites;Artificial intelligence;Adaptation models;Interference;Terrestrial communications;Resource management},
  doi={10.1109/MCE.2025.3599657},
  ISSN={2162-2256},
  month={},}@INPROCEEDINGS{9191196,
  author={Han, Chuchu and Gao, Changxin and Sang, Nong},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={Keypoint-Based Feature Matching For Partial Person Re-Identification}, 
  year={2020},
  volume={},
  number={},
  pages={226-230},
  abstract={As a derivative of person re-identification (re-ID), partial re- ID aims to retrieve a partial pedestrian across holistic person images captured by non-overlapping cameras. This task is more challenging and closer to real-world applications. Since we cannot locate the part of the partial image, the misaligned region compromises the performance greatly when directly (a) compare a partial pedestrian with a holistic one. To alleviate this issue, we propose a Keypoint-Based Feature Matching (KBFM) network, which constructs a simple and effective framework for partial re-ID. Specifically, our architecture explicitly leverages the keypoints generated by pose estimation. Based on the visible keypoints, coordinates of the corresponding visible region can be computed. And the keypoint-based feature embeddings can be generated by bilinear sampling. When matching two images, we extract their features on the basis of shared visible keypoints, avoiding the misalignment and disturbance. Moreover, considering the triplet loss cannot be flexibly built in the partial re-ID pipeline, we improve the original sampling method and achieve significant performance. Extensive experimental results on two widely used benchmarks demonstrate significant performance improvements of our method over most state-of-the-art methods.},
  keywords={Feature extraction;Cameras;Task analysis;Pose estimation;Training;Pipelines;Generative adversarial networks;Partial;Keypoint;Matching;Re-ID},
  doi={10.1109/ICIP40778.2020.9191196},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{10561673,
  author={Wan, Gang and Li, Xinyu and Zhu, Sisi and Chen, Chen and Sun, Chao and Shi, Pengfei and Zhou, Xuan and Fan, Xinnan},
  booktitle={2024 IEEE 14th International Conference on Electronics Information and Emergency Communication (ICEIEC)}, 
  title={Underwater Image Enhancement Embedded Algorithm Based on Improved Ghost Module}, 
  year={2024},
  volume={},
  number={},
  pages={302-306},
  abstract={Underwater image enhancement algorithms create conditions for subsequent processing tasks such as recognition, classification, and reconstruction. Usually, the stronger the performance of embedded devices, the higher their image processing capabilities and speed. However, due to limitations in application environment and current embedded development board performance, algorithms on embedded development boards should seek a subtle balance between high performance and high processing speed. This article is based on the FUnIE GAN network, using an improved Ghost module to compress the network model, using the Fused MBConv module to shorten the network training time, and using TensorRT strategy to further optimize the algorithm.},
  keywords={Performance evaluation;Training;Visualization;Image recognition;Image coding;Generative adversarial networks;Classification algorithms;Underwater Image Enhancement;Improved Ghost Module;Embedded Algorithm},
  doi={10.1109/ICEIEC61773.2024.10561673},
  ISSN={2377-844X},
  month={May},}@INPROCEEDINGS{10463404,
  author={Kwon, Ohbyung and Bae, Sujin and Lee, Dongjae and Lee, Kyuho and Choi, Ryunhee and Hwang, Kyunghwa and Kwon, Seongjun and Kim, Taeyoung},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={An Empirical Study on the Psychological Improvement Effects and Satisfaction of Korean Traditional Painting Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={135-140},
  abstract={Recent advancements in generative AI technology have made it possible to generate art pieces for traditional Korean painting as well. Visual image generation through generative AI can occur autonomously as well as through co-creation with creators. This technological progress has opened up possibilities for utilizing generative AI in art therapy, aiming to achieve psychological well-being during the creation or appreciation of artworks or improving the therapeutic process. However, the measurement of psychological improvement resulting from the collaborative creation process using generative AI has not been attempted thus far. Therefore, the purpose of this study is to experimentally ascertain whether the creation of visual images through generative AI leads to psychological improvement for creators or viewers. 40 participants experienced the generative AI-based traditional Korean painting experience web developed by ALLBIGDAT Inc, a generative AI company in Korea, and conducted a survey on the psychological state of each before and after the experience. As a result, it was found that imperfections and serendipity had a significant effect on enhancing satisfaction with collaborative creations. In addition, the age considered as a control factor also showed statistically significant results, indicating that the higher the age, the higher user's satisfaction.},
  keywords={Surveys;Visualization;Art;Generative AI;Image synthesis;Psychology;Collaboration;Generative AI;Artwork;Catharsis;Imperfection;Serendipity;User satisfaction},
  doi={10.1109/ICAIIC60209.2024.10463404},
  ISSN={2831-6983},
  month={Feb},}@ARTICLE{10583864,
  author={Sunilkumar, Anusree P. and Keshari Parida, Bikram and You, Wonsang},
  journal={IEEE Access}, 
  title={Recent Advances in Dental Panoramic X-Ray Synthesis and Its Clinical Applications}, 
  year={2024},
  volume={12},
  number={},
  pages={141032-141051},
  abstract={X-ray imaging plays a pivotal role in clinical diagnostics by facilitating tasks ranging from organ segmentation to disease detection. With the integration of artificial intelligence (AI), the capabilities of X-ray analysis have been greatly enhanced. However, acquiring a comprehensive dataset for training AI models remains challenging due to various factors including patient privacy, radiation exposure, and cost constraints. To overcome this hurdle, researchers have turned to create synthetic X-ray datasets. In dental imaging, the synthesis of Panoramic X-rays (PXs) poses unique challenges owing to the intricate nature of the oral cavity and the inherent diversity among patients. Existing efforts predominantly rely on Cone Beam Computed Tomography (CBCT) or phantoms for such tasks. Synthesizing PX can be beneficial when the PX imaging cannot be performed immediately or when dentists need to find a one-to-one correspondence between PX and CBCT. This review paper comprehensively examines the methodologies employed for synthesizing dental PXs, specifically, automated synthesis, elucidating the methods utilized and the associated advantages, limitations, and potential clinical applications in routine practice. By shedding light on these approaches, this review aims to provide valuable insights for researchers and practitioners in the dental imaging domain.},
  keywords={X-ray imaging;Dentistry;Imaging;Image synthesis;Data models;Computed tomography;Medical diagnostic imaging;Deep learning;CBCT;CT;deep learning;maximum intensity projection;multi-planar reformatting;panoramic X-ray},
  doi={10.1109/ACCESS.2024.3422650},
  ISSN={2169-3536},
  month={},}@ARTICLE{10925559,
  author={Wu, Zuohan and Zhang, Chen Jason and Yin, Han and Meng, Rui and Zheng, Libin and Zhu, Huaijie and Liu, Wei},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={DRLPG: Reinforced Opponent-Aware Order Pricing for Hub Mobility Services}, 
  year={2025},
  volume={37},
  number={6},
  pages={3298-3311},
  abstract={A modern service model known as the “hub-oriented” model has emerged with the development of mobility services. This model allows users to request vehicles from multiple companies (agents) simultaneously through a unified entry (a ‘hub’). In contrast to conventional services, the “hub-oriented” model emphasizes pricing competition. To address this scenario, an agent should consider its competitors when developing its pricing strategy. In this paper, we introduce DRLPG, a mixed opponent-aware pricing method, which consists of two main components: the two-stage guarantor and the end-to-end deep reinforcement learning (DRL) module, as well as interaction mechanisms. In the guarantor, we design a prediction-decision framework. Specifically, we propose a new objective function for the spatiotemporal neural network in the prediction stage and utilize a traditional reinforcement learning method in the decision stage, respectively. In the end-to-end DRL framework, we explore the adoption of conventional DRL in the “hub-oriented” scenario. Finally, a meta-decider and an experience-sharing mechanism are proposed to combine both methods and leverage their advantages. We conduct extensive experiments on real data, and DRLPG achieves an average improvement of 99.9% and 61.1% in the peak and low peak periods, respectively. Our results demonstrate the effectiveness of our approach compared to the baseline.},
  keywords={Pricing;Companies;Artificial intelligence;Urban areas;Deep reinforcement learning;Training;Neural networks;Data mining;Convergence;Technological innovation;Order pricing;ride-hailing;reinforcement learning;quantile learning},
  doi={10.1109/TKDE.2025.3551147},
  ISSN={1558-2191},
  month={June},}@INPROCEEDINGS{11040679,
  author={Zhang, Peng and Fan, Xiaoyu and Du, Zexu and Xia, Weishang},
  booktitle={2025 10th Asia Conference on Power and Electrical Engineering (ACPEE)}, 
  title={Transformer Fault Sample Generation Method Based on Embedding Typical Transformer Fault Evolution Laws}, 
  year={2025},
  volume={},
  number={},
  pages={2645-2649},
  abstract={To ensure the safe and stable operation of power transformers, mining the variation characteristics of state quantities in typical fault samples of power transformers based on machine learning approaches and constructing diagnostic analysis models is a significant research direction. Nevertheless, transformer diagnostic models encounter challenges such as a shortage of fault samples and subpar sample quality, which impedes the enhancement of model accuracy. The samples generated by existing methods based on data interpolation and general generative models are typically of inferior quality and fail to fulfill the actual requirements of model construction. To address the aforementioned issues, this paper proposes a nearest neighbor sample segment splicing generation method embedded with the evolution patterns of typical faults in power transformers. Firstly, various typical faults are simulated on the power transformer fault simulation platform, and the variations of key state quantities are measured to fit the evolution laws of each state quantity during the fault development process. Subsequently, the state quantities during the typical fault development process are segmented, with the evolution laws of the state quantities retained in each segment. The state quantity segment data are regarded as similar image patches and embedded in the GPNN (Generative Patch Nearest-Neighbor) generative model. Finally, a GPNN model for power transformer sample generation is constructed, and a large number of similar transformer fault samples are generated through the generative model and local block matching technology. Utilizing the method proposed in this paper, with 117 samples obtained from simulation experiments as training data, 536 samples were generated through the GPNN model. The matching degree of the 536 samples with the original samples was evaluated from multiple dimensions such as completeness, coverage, overlap, and identification quality, reaching $\mathbf{9 0. 0 3 \%}$.},
  keywords={Electrical engineering;Image segmentation;Interpolation;Power measurement;Splicing;Asia;Training data;Machine learning;Data models;Power transformers;Power Transformer;Fault Evolution Laws;Sample Generationt;Generative Patch Nearest-Neighbor},
  doi={10.1109/ACPEE64358.2025.11040679},
  ISSN={2996-2951},
  month={April},}@ARTICLE{10342856,
  author={Ghahremani, Shahram and Bidgoly, Amir Jalaly and Nguyen, Uyen Trang and Yau, David K. Y.},
  journal={IEEE Access}, 
  title={A Novel Semi-Supervised Adversarially Learned Meta-Classifier for Detecting Neural Trojan Attacks}, 
  year={2023},
  volume={11},
  number={},
  pages={138303-138315},
  abstract={Deep neural networks (DNNs) are highly vulnerable to neural Trojan attacks. To carry out such an attack, an adversary retrains a DNN with poisoned data or modifies its parameters to produce incorrect output. These attacks can remain unnoticed until triggered by a specific pattern in the input, making detection challenging. In this article, we propose a novel semi-supervised adversarially learned meta-classifier (SESALME) to detect if a target model has been trojaned. Unlike previous Trojan detection methods, SESALME assumes that the defender has no knowledge of the attack mechanisms, and no access to training data, poisoned data, or parameters/layers of a target model. In the absence of poisoned data and knowledge of the attack mechanisms, we use a set of shadow models to emulate normal behavior of the target model. Having learned the normal behavior of the target model, SESALME then uses one-class learning, implemented within a semi-supervised generative adversarial network (GAN), to detect abnormal behavior of a model to be investigated, if any. Behavior that deviates from the learned normal behavior indicates a high likelihood that the model is trojaned. We compare the performance of SESALME with that of state-of-the-art neural Trojan detectors using popular datasets such as MNIST, CIFAR-10, and SC. Experimental results show that SESALME outperforms state-of-the-art Trojan detection methods in terms of detection performance and inference time in almost all cases, while being attack-agnostic and requiring no access to training data, poisoned data, or parameters of the target model.},
  keywords={Trojan horses;Data models;Training data;Behavioral sciences;Artificial neural networks;Training;Semisupervised learning;Artificial neural networks;Generative adversarial networks;Deep neural networks;neural Trojan attacks;generative adversarial networks;one-class learning;semi-supervised learning},
  doi={10.1109/ACCESS.2023.3339542},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10541601,
  author={Wall, Emily and Matzen, Laura and El-Assady, Mennatallah and Masters, Peta and Hosseinpour, Helia and Endert, Alex and Borgo, Rita and Chau, Polo and Perer, Adam and Schupp, Harald and Strobelt, Hendrik and Padilla, Lace},
  booktitle={2024 IEEE 17th Pacific Visualization Conference (PacificVis)}, 
  title={Trust Junk and Evil Knobs: Calibrating Trust in AI Visualization}, 
  year={2024},
  volume={},
  number={},
  pages={22-31},
  abstract={Many papers make claims about specific visualization techniques that are said to enhance or calibrate trust in AI systems. But a design choice that enhances trust in some cases appears to damage it in others. In this paper, we explore this inherent duality through an analogy with "knobs". Turning a knob too far in one direction may result in under-trust, too far in the other, over-trust or, turned up further still, in a confusing distortion. While the designs or so-called "knobs" are not inherently evil, they can be misused or used in an adversarial context and thereby manipulated to mislead users or promote unwarranted levels of trust in AI systems. When a visualization that has no meaningful connection with the underlying model or data is employed to enhance trust, we refer to the result as "trust junk." From a review of 65 papers, we identify nine commonly made claims about trust calibration. We synthesize them into a framework of knobs that can be used for good or "evil," and distill our findings into observed pitfalls for the responsible design of human-AI systems.},
  keywords={Reviews;Data visualization;Distortion;Turning;Data models;Calibration;Artificial intelligence;Computing methodologies;Artificial intelligence; Human-centered computing;Human computer interaction (HCI)},
  doi={10.1109/PacificVis60374.2024.00012},
  ISSN={2165-8773},
  month={April},}@ARTICLE{11030805,
  author={Hasan, Syed Mhamudul and Islam, Taminul and Saifuzzaman, Munshi and Ahmed, Khaled R. and Huang, Chun-Hsi and Shahid, Abdur R.},
  journal={IEEE Transactions on Sustainable Computing}, 
  title={Carbon Emission Quantification of Machine Learning: A Review}, 
  year={2025},
  volume={},
  number={},
  pages={1-19},
  abstract={The rapid growth of machine learning (ML) technologies has raised significant concerns about their environmental impact, particularly regarding energy consumption and carbon emissions. This comprehensive review examines the intersection of ML and sustainability, synthesizing research from 2014 to 2024 to provide a holistic view of sustainable ML practices. This systematic review, encompassing over 200 peer-reviewed publications, reveals a growing emphasis on quantifying and mitigating the environmental footprint of ML systems. Key findings include: (1) a 300% increase in sustainable ML research since 2020; (2) the emergence of specialized carbon footprint quantification tools for ML; and (3) promising advancements in energy-efficient algorithms and green computing infrastructure. This research identifies critical challenges, including the lack of standardized sustainability metrics and the need for more robust life-cycle assessments of ML systems. The review also highlights the potential of transfer learning, federated learning, and hardware innovations in reducing ML's environmental impact. The analysis culminates in a novel framework for implementing sustainable practices in ML projects and a detailed roadmap for future research. This work provides researchers, practitioners, and policymakers with crucial insights to drive the development of more environmentally responsible ML technologies, ultimately contributing to global sustainability goals.},
  keywords={Carbon dioxide;Sustainable development;Artificial intelligence;Systematic literature review;Atmospheric measurements;Atmosphere;Security;Carbon neutral;Bibliometrics;Terminology;Sustainability;Artificial Intelligence (AI);Machine Learning (ML);Carbon Emission;Sustainable Computing;ML Emission},
  doi={10.1109/TSUSC.2025.3578834},
  ISSN={2377-3782},
  month={},}@ARTICLE{11125963,
  author={Fu, Xinyi and Li, Meng and Li, Xiaomeng and Chen, Wen and Yu, Lening and Chen, Zixin and Wen, Shuting and Li, Yilin and Du, Jiachen and Wang, Yun and Xu, Yingqing and Chen, Yunbing},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Emerging Computing Technology for Digital Culture Heritage Preservation and Inheritance: A Literature Review}, 
  year={2025},
  volume={},
  number={},
  pages={1-38},
  abstract={Cultural heritage preservation has entered a transformative era with the integration of advanced computing technologies, reshaping how heritage is acquired, archived, restored, analyzed, and presented. This study conducts a comprehensive literature review of the digital cultural heritage process (DCHP), analyzing 146 relevant case studies from the past five years (2020–2024), focusing on the role of computing technologies, especially emerging technologies, in revolutionizing heritage preservation. The DCHP is examined through five key stages—acquisition, archive, restoration, analysis and valuation, and presentation and promotion—highlighting the application of computing technologies and tools. Additionally, we explore the interdisciplinary integration of computing disciplines and multimodal data. Despite significant advancements, challenges persist in areas such as sustainability, data reuse, and ethical concerns. This study addresses research questions regarding exploring the impact of computing technologies on DCHP, the integration of hybrid disciplines and modalities, and future trends and challenges in emerging technologies for cultural heritage preservation. Our contributions include a detailed analysis of emerging technologies in DCHP, insights into multimodal and interdisciplinary integration, and a roadmap for future research. By synthesizing the current state-of-the-art and identifying gaps in the literature, this work aims to guide researchers and practitioners in leveraging emerging technologies to preserve and promote cultural heritage effectively.},
  keywords={Computers;Cultural differences;Artificial intelligence;Three-dimensional displays;Market research;Image restoration;Art;Digitization;Technological innovation;Systematic literature review;Artificial intelligence (AI);digital culture heritage (DCH);emerging computing technology;interdisciplinary research;literature review;preservation and inheritance;technology integration},
  doi={10.1109/TCSS.2025.3589324},
  ISSN={2329-924X},
  month={},}@ARTICLE{8553661,
  author={Xu, Jun and Wu, Kaishun},
  journal={IEEE Network}, 
  title={Living with Artificial Intelligence: A Paradigm Shift toward Future Network Traffic Control}, 
  year={2018},
  volume={32},
  number={6},
  pages={92-99},
  abstract={Future Internet is expected to meet explosive traffic growth and extremely complex architecture, which tend to make the traditional NTC strategies inefficient and even ineffective. Inspired by the latest breakthroughs of AI and its power to address large-scale and complex difficulties, the network community has begun to consider shifting the NTC paradigm from legacy rule-based to novel AI-based. As an applied inter-discipline, design and implementation are important. Although there have been some preliminary explorations along this frontier, they are either limited by only envisioning the prospects, or too scattered to provide high-level insight into a general methodology. To this end, we start with the domain knowledge relationships of AI and NTC, summarizing a baseline workflow toward deep reinforcement learning, which will be the dominant method for the AI-NTC paradigm. On top of that, we argue that AI-NTC training and running must be carried out in online environments in closed-loop fashion for the purpose of putting ti into practice. A series of challenges and opportunities are discussed from a realistic viewpoint, and a set of new architecture and mechanism to enable the online and closed-loop AI-NTC paradigm are proposed. Hopefully, this work can help the AI community to better understand NTC and the NTC community to better live with AI.},
  keywords={Support vector machines;Feature extraction;Computer architecture;Telecommunication traffic;Traffic control;Networked control systems;Artificial intelligence},
  doi={10.1109/MNET.2018.1800119},
  ISSN={1558-156X},
  month={November},}@INPROCEEDINGS{10578646,
  author={Vishnumolakala, Sai Krishna and C, Sobin C and Subheesh, N P and Kumar, Prabhat and Kumar, Randhir},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={AI-Based Research Companion (ARC): An Innovative Tool for Fostering Research Activities in Undergraduate Engineering Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The engineering education today emphasizes the need to combine book learning with real-world application. However, much of the research done by undergraduates, which could be very valuable, is scattered and not fully used. To address this, a new tool called “AI-based Research Companion (ARC)” has been developed. ARC leverages advanced Generative AI technology, including GPT-4, to systematically organize, enhance, and offer personalized recommendations for undergraduate research projects. This platform is more than a simple tool; it aims to inspire undergraduates to dive into research by making the process approachable and engaging, thus increasing participation in research activities. Initial assessments of ARC have revealed an encouraging rise in student engagement with research, indicating a shift towards more research-oriented projects. The integration of GPT-4 within ARC stands out significantly; it precisely addresses the detailed demands of undergraduate research by providing a tailored, intelligent exploration pathway. By incorporating GPT-4's advanced features with a user-centric design, ARC emerges as an innovative platform, emphasizing the pivotal role of Generative AI in enhancing and expanding undergraduate research initiatives.},
  keywords={Technological innovation;Generative AI;Information age;Research initiatives;Engineering education;Testing;AI-based Research Companion (ARC);Dynamic recommendations;Engineering education;Generative AI;GPT-4;Undergraduate research},
  doi={10.1109/EDUCON60312.2024.10578646},
  ISSN={2165-9567},
  month={May},}@ARTICLE{10716502,
  author={Cao, Xianghai and Yu, Jiayu and Xu, Ruijie and Wei, Jiaxuan and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Mask-Enhanced Contrastive Learning for Hyperspectral Image Classification}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={Recently, self-supervised learning (SSL) has gained great prominence in hyperspectral image classification (HSIC) due to its powerful capability to alleviate data-hunger problem. The generative-based method and the contrastive-based method have become two main streams in the field of SSL. To fully integrate the merits of both of them, we propose an efficient hybrid SSL method, that is, mask-enhanced contrastive learning (MECL). Essentially, MECL remains a prototypical contrastive-learning (CL) method, but incorporates the masking-and-predicting idea of the generative-based method. When the sample gradually matches the prototypes, feature reconstruction is implicitly performed in MECL step by step. Furthermore, we design a spatial-spectral multimasking mechanism for hyperspectral data and also propose two complementary strategies to prevent MECL from collapsing. To demonstrate the performance of MECL, a wide range of experiments are carried out in our work. On the one hand, the results of ablation experiments prove that HSIC does benefit from the fusion of SSL methods. On the other hand, the classification results on three hyperspectral datasets confirm that our MECL is an effective SSL method with high stability and strong reliability.},
  keywords={Hyperspectral imaging;Feature extraction;Generative adversarial networks;Contrastive learning;Image classification;Classification algorithms;Machine learning algorithms;Image reconstruction;Attention mechanisms;Annotations;Contrastive learning (CL);hyperspectral image classification (HSIC);masking;self-supervised learning (SSL)},
  doi={10.1109/TGRS.2024.3479220},
  ISSN={1558-0644},
  month={},}@ARTICLE{10706204,
  author={Jiang, Huajie and Li, Zhengxian and Hu, Yongli and Yin, Baocai and Yang, Jian and van den Hengel, Anton and Yang, Ming-Hsuan and Qi, Yuankai},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dual Prototype Contrastive Network for Generalized Zero-Shot Learning}, 
  year={2025},
  volume={35},
  number={2},
  pages={1111-1122},
  abstract={Generalized zero-shot learning (GZSL) requires that models are able to recognize classes they were trained on, and new classes they haven't seen before. Feature-generation approaches are popular due to their effectiveness in mitigating overfitting to the training classes. Existing generative approaches usually adopt simple discriminators for distribution or classification supervision, however, thus limiting their ability to generate visual features that are discriminative of and transferable to novel categories. To overcome this limitation and improve the quality of generated features, we propose a dual prototype contrastive augmented discriminator for the generative adversarial network. Specifically, we design a Dual Prototype Contrastive Network (DPCN), which leverages complementary information between visual space and semantic space through multi-task prototype contrastive learning. Contrastive learning of the visual prototypes enhances the ability of the generated features to distinguish between classes, while the contrastive learning of the semantic prototypes improves their transferability. Furthermore, we introduce margins into the contrastive learning process to ensure both intra-class compactness and inter-class separation. To demonstrate the effectiveness of the proposed approach, we conduct experiments on three widely-used zero-shot learning benchmark datasets, where DPCN achieves state-of-the-art performance for GZSL.},
  keywords={Visualization;Semantics;Prototypes;Contrastive learning;Zero shot learning;Generative adversarial networks;Object recognition;Feature extraction;Training;Face recognition;Generalized zero-shot learning;prototype learning;contrastive learning},
  doi={10.1109/TCSVT.2024.3474910},
  ISSN={1558-2205},
  month={Feb},}@INPROCEEDINGS{10775373,
  author={Faridh Suni, Alfa and Utomo, Aryo Baskoro and Fathoni, Khoiruddin and Yusuf Mahendra, Budiandra and Gemi Seinsiani, Izzati and Hastawan, Ahmad Fashiha},
  booktitle={2024 4th International Conference on Electronic and Electrical Engineering and Intelligent System (ICE3IS)}, 
  title={Text-to-Speech User Interface for ChatGPT}, 
  year={2024},
  volume={},
  number={},
  pages={311-315},
  abstract={The emergence of OpenAI's ChatGPT is paying a lot of attention to Generative AI and its impact on Academic integrity. Generative AI is a system designed to generate content or output such as text, image, audio, simulation, video and code based on their respective training data. ChatGPT itself is in the form of a chatbot where users can communicate with AI through dialogue applications via text. GPT 3 API is the technology behind the ChatGPT. However, interaction via text is not as flexible as dialogue using voice. Therefore, speech-to-text technology is needed to interface between the user and the AI. Chatbot AI powered by GPT3 with the optional voice UI powered by Google Text to Speech clearly benefits accessibility and usability. The output of dialogue between human users and AI can be done through text or sound. This research attempts to implement a Text to Speech and vice versa Speech to Text as a User Interface on a Gen-AI using the GPT 3 API so that it will be more user-friendly and accessible. This research opens many possibilities for human-AI interaction (HAX) in implementing virtual assistants. A seamless dialogue between humans and AI will be our goal in the future.},
  keywords={Electrical engineering;Generative AI;Virtual assistants;Training data;Chatbots;Text to speech;Internet;Usability;Intelligent systems;Speech to text;Text to Speech;ChatGPT;Generative AI;Speech to Text},
  doi={10.1109/ICE3IS62977.2024.10775373},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11143482,
  author={Yuan, Weiwen and Ren, Jinke and Sun, Rui and Han, Yatong and Cui, Shuguang},
  booktitle={2025 IEEE 26th International Workshop on Signal Processing and Artificial Intelligence for Wireless Communications (SPAWC)}, 
  title={Large Semantic Agents for Wireless Image Transmission}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Semantic communication (SemCom) is recognized as a promising technology to improve communication efficiency by transmitting the meaning of data rather than raw bits. However, existing SemCom systems are typically designed for specific tasks with limited generalization and reasoning abilities. To address these issues, this paper considers a point-to-point image transmission system and proposes two large language model (LLM)-based semantic agents, namely LSAs, which serve as the semantic codecs at the transmitter and receiver, respectively. Each LSA comprises three key components, including a multi-source prompt module, a pre-trained LLM, and an auxiliary tool. The multi-source prompt module first transforms the input data and environmental information into token-like features that can be processed by the pre-trained LLM. Then, the LLM performs semantic understanding at the transmitter or content regeneration at the receiver, thereby producing semantic embeddings or generating images. The auxiliary tool consists of application programming interfaces and databases, assisting the LLM in semantic understanding and image optimization. Experimental results demonstrate that the proposed LSA-based SemCom scheme achieves superior image transmission performance than four baseline schemes.},
  keywords={Image communication;Auxiliary transmitters;Wireless networks;Large language models;Semantics;Receivers;Transforms;Signal processing;Semantic communication;Optimization},
  doi={10.1109/SPAWC66079.2025.11143482},
  ISSN={1948-3252},
  month={July},}@ARTICLE{9508371,
  author={Kwon, Hye-Jeong and Shin, Dong-Hoon and Chung, Kyungyong},
  journal={IEEE Access}, 
  title={PGGAN-Based Anomaly Classification on Chest X-Ray Using Weighted Multi-Scale Similarity}, 
  year={2021},
  volume={9},
  number={},
  pages={113315-113325},
  abstract={To use artificial intelligence to assist in diagnoses applications, a model to utilize quality data is required, which results in massive time and cost. In medical data, data imbalance occurs because the amount of data with lesions is less than that without lesions. To overcome this limitation, this study proposes a progressive growth of generative adversarial networks (PGGAN)-based anomaly classification on chest X-rays using weighted multi-scale similarity. An anomaly detection method is applied to learn the distribution of normal images to solve the problem of data imbalance. The use of PGGAN, which is a model that generates high-resolution images by gradually adding layers, enables to find image characteristics on a multi-scale and define the similarity between an original image and a generated image. The anomaly score is calculated by applying the weighted arithmetic mean to a resolution-by-resolution similarity. The threshold is defined after the analysis of the F1-score, and then the classification performance is evaluated. The accuracy of the proposed model was assessed using a confusion matrix and compared with that of a conventional classification model, and the efficiency was demonstrated through ablation studies. The classification accuracy of the test dataset was 0.8525. Compared to a U-net-based disease classifier with low-resolution which accuracy was 0.8410, the performance of the proposed model was 0.8507, exhibited an improvement.},
  keywords={Image resolution;Diseases;Data models;Lesions;Artificial intelligence;X-ray imaging;Mathematical model;Artificial intelligence;computer-aided diagnostics;deep learning;healthcare;PGGAN;unsupervised learning;X-ray data},
  doi={10.1109/ACCESS.2021.3102954},
  ISSN={2169-3536},
  month={},}@INBOOK{10201347,
  author={Porambage, Pawani and Liyanage, Madhusanka and Senevirathna, Thulitha},
  booktitle={Security and Privacy Vision in 6G: A Comprehensive Guide}, 
  title={Role of Explainable AI in 6G Security}, 
  year={2023},
  volume={},
  number={},
  pages={267-290},
  abstract={Accountability and resilience of AI/ML based 6G services is paramount now more than ever. In this chapter we are going to discuss the potential of Explainable AI (XAI) to address the resilience of 6G services and technologies and thereby improve the accountability as a whole. The readers will be able to gain an overview of the use cases where XAI would be necessary in 6G applications and enabling technologies as well as the security concerns related to XAI.},
  keywords={Mathematical models;Stakeholders;Data models;Closed box;6G mobile communication;Training;Resilience},
  doi={10.1002/9781119875437.ch18},
  ISSN={},
  publisher={IEEE},
  isbn={9781119875413},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10201347},}@INBOOK{9536286,
  author={Kiekintveld, Christopher D. and Kamhoua, Charles A. and Fang, Fei and Zhu, Quanyan},
  booktitle={Game Theory and Machine Learning for Cyber Security}, 
  title={Introduction}, 
  year={2021},
  volume={},
  number={},
  pages={1-19},
  abstract={Computing technologies are increasingly vital to the functioning of the global economy, national security, and our everyday lives. The threat of cyber attacks is not new, but both the impact and sophistication of these attacks on our computing infrastructure has grown dramatically with the widespread adoption and integration of computing. Attackers will adopt any technological edge to develop new and more effective attacks, so we also need a broad and rigorous approach to cyber defense that draws on both established methods and emerging technologies to more effectively secure computer networks. Since it is not possible to prevent all vulnerabilities and attacks, we also need to develop better approaches for making cyber security decisions that balance risks and costs, and that are able to make good uses of data to adapt and improve defensive strategies over time.},
  keywords={Computer security;Machine learning;Game theory;Uncertainty;Games;Computer crime;Computational modeling},
  doi={10.1002/9781119723950.ch1},
  ISSN={},
  publisher={IEEE},
  isbn={9781119723912},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9536286},}@BOOK{10769246,
  author={Guilmette, Aaron and Miles, Steve and Tender, Peter De},
  booktitle={Microsoft Azure AI Fundamentals AI-900 Exam Guide: Gain proficiency in Azure AI and machine learning concepts and services to excel in the AI-900 exam},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Get ready to pass the certification exam on your first attempt by gaining actionable insights into AI concepts, ML techniques, and Azure AI services covered in the latest AI-900 exam syllabus from two industry experts Key FeaturesDiscover Azure AI services, including computer vision, Auto ML, NLP, and OpenAIExplore AI use cases, such as image identification, chatbots, and moreWork through 145 practice questions under chapter-end self-assessments and mock examsPurchase of this book unlocks access to web-based exam prep resources, including mock exams, flashcards, and exam tipsBook DescriptionThe AI-900 exam helps you take your first step into an AI-shaped future. Regardless of your technical background, this book will help you test your understanding of the key AI-related topics and tools used to develop AI solutions in Azure cloud. This exam guide focuses on AI workloads, including natural language processing (NLP) and large language models (LLMs). You’ll explore Microsoft’s responsible AI principles like safety and accountability. Then, you’ll cover the basics of machine learning (ML), including classification and deep learning, and learn how to use training and validation datasets with Azure ML. Using Azure AI Vision, face detection, and Video Indexer services, you’ll get up to speed with computer vision-related topics like image classification, object detection, and facial detection. Later chapters cover NLP features such as key phrase extraction, sentiment analysis, and speech processing using Azure AI Language, speech, and translator services. The book also guides you through identifying GenAI models and leveraging Azure OpenAI Service for content generation. At the end of each chapter, you’ll find chapter review questions with answers, provided as an online resource. By the end of this exam guide, you’ll be able to work with AI solutions in Azure and pass the AI-900 exam using the online exam prep resources.What you will learnDiscover various types of artificial intelligence (AI)workloads and services in AzureCover Microsoft's guiding principles for responsible AI development and useUnderstand the fundamental principles of how AI and machine learning workExplore how AI models can recognize content in images and documentsGain insights into the features and use cases for natural language processingExplore the capabilities of generative AI servicesWho this book is forWhether you're a cloud engineer, software developer, an aspiring data scientist, or simply interested in learning AI/ML concepts and capabilities on Azure, this book is for you. The book also serves as a foundation for those looking to attempt more advanced AI and data science-related certification exams (e.g. Microsoft Certified: Azure AI Engineer Associate). Although no experience in data science and software engineering is required, basic knowledge of cloud concepts and client-server applications is assumed. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835885673},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10769246},}@INPROCEEDINGS{10650414,
  author={Li, Jun and Li, Dingcheng and Li, Ping and Samorodnitsky, Gennady},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Generalized Pareto GAN: Generating Extremes of Distributions}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Extreme events are ubiquitous in various domains, including finance, meteorology, and network analysis. Nevertheless, modern deep learning techniques exhibit limitations in capturing extreme samples, treating them as outliers. In this paper, we propose Generalized Pareto Generative Adversarial Network (GPGAN), a novel generative model for tackling the generation of extreme values in distributions. Our methodology specifically entails the direct modeling of the multi-dimensional Generalized Pareto Distribution (GPD) using deep generative neural networks. Compared to prior work, GPGAN excels at faithfully representing the dependence structure exhibited by extreme values. To ensure effective generation, we employ an adaptive Generalized Pareto parameter strategy. This strategy dynamically selects appropriate GPD parameters for each generation step, enabling compatibility with user-defined extremeness functions within the multi-dimensional GPD framework. Extensive evaluations on a benchmark precipitation dataset demonstrate the effectiveness of our model.},
  keywords={Deep learning;Adaptation models;Precipitation;Neural networks;Finance;Network analyzers;Benchmark testing;generative model;generalized Pareto distribution;extreme value theory},
  doi={10.1109/IJCNN60899.2024.10650414},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11016386,
  author={Johri, Aditya and Schleiss, Johannes and Ranade, Nupoor},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Lessons for GenAI Literacy from a Field Study of Human-GenAI Augmentation in the Workplace}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Generative artificial intelligence (GenAI) is increasingly becoming a part of work practices across the technology industry and being used across a range of industries. This has necessitated the need to better understand how GenAI is being used by professionals in the field so that we can better prepare students for the workforce. An improved understanding of the use of GenAI in practice can help provide guidance on the design of GenAI literacy efforts including how to integrate it within courses and curriculum, what aspects of GenAI to teach, and even how to teach it. This paper presents a field study that compares the use of GenAI across three different functions - product development, software engineering, and digital content creation - to identify how GenAI is currently being used in the industry. This study takes a human augmentation approach with a focus on human cognition and addresses three research questions: how is GenAI augmenting work practices; what knowledge is important and how are workers learning; and what are the implications for training the future workforce. Findings show a wide variance in the use of GenAI and in the level of computing knowledge of users. In some industries GenAI is being used in a highly technical manner with deployment of fine-tuned models across domains. Whereas in others, only off-the-shelf applications are being used for generating content. This means that the need for what to know about GenAI varies, and so does the background knowledge needed to utilize it. For the purposes of teaching and learning, our findings indicated that different levels of GenAI understanding needs to be integrated into courses. From a faculty perspective, the work has implications for training faculty so that they are aware of the advances and how students are possibly, as early adopters, already using GenAI to augment their learning practices.},
  keywords={Industries;Training;Generative AI;Employment;Learning (artificial intelligence);Intellectual property;Human augmentation;Product development;Engineering education;Software engineering;generative artificial intelligence;engineering education;computing education;AI literacy;workplace studies},
  doi={10.1109/EDUCON62633.2025.11016386},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{10575682,
  author={Vimal, S. P. and Kiruthika, V. and Hariharan, M and Midhilesh, S and Kabilan, G},
  booktitle={2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Design of Hardware Interfacing with ChatGPT as an Inference Engine for Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={137-143},
  abstract={This study proposes the design and implementation of a hardware interface system that leverages ChatGPT, a powerful natural language processing model, as an inference engine to assist visually impaired individuals in accessing information and performing everyday tasks. The proposed system aims to provide an intuitive and user-friendly interface that enables visually impaired users to interact with ChatGPT using voice commands and receive spoken responses, thereby enhancing their accessibility to digital content and services. The hardware interface consists of a portable device equipped with a microphone for input and a speaker for output, along with additional tactile and auditory feedback mechanisms to facilitate user interaction. The proposed system architecture integrates ChatGPT’s natural language understanding capabilities with the hardware interface, enabling seamless communication between the user and the AI model.},
  keywords={Systems architecture;Personal voice assistants;Chatbots;Transformers;Hardware;Internet;Artificial intelligence;Chat Generative Pre-Training Transformer;Voice Assistant;Artificial Intelligence;Natural language processing},
  doi={10.1109/ICAAIC60222.2024.10575682},
  ISSN={},
  month={June},}@ARTICLE{10857645,
  author={Li, Yunxiang and Chen, Meixu and Wang, Kai and Ma, Jun and Bovik, Alan C. and Zhang, You},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={SAMScore: A Content Structural Similarity Metric for Image Translation Evaluation}, 
  year={2025},
  volume={6},
  number={8},
  pages={2027-2040},
  abstract={Image translation has wide applications, such as style transfer and modality conversion, usually aiming to generate images having both high degrees of realism and faithfulness. These problems remain difficult, especially when it is important to preserve content structures. Traditional image-level similarity metrics are of limited use, since the content structures of an image are high-level and not strongly governed by pixelwise faithfulness to an original image. To fill this gap, we introduce SAMScore, a generic content structural similarity metric for evaluating the faithfulness of image translation models. SAMScore is based on the recent high-performance segment anything model (SAM), which allows content similarity comparisons with standout accuracy. We applied SAMScore on 19 image translation tasks and found that it is able to outperform all other competitive metrics on all tasks. We envision that SAMScore will prove to be a valuable tool that will help to drive the vibrant field of image translation, by allowing for more precise evaluations of new and evolving translation models.},
  keywords={Measurement;Translation;Image segmentation;Artificial intelligence;Biomedical imaging;Accuracy;Training;Data mining;PSNR;Computed tomography;Evaluation metric;generative artificial intelligence;image translation},
  doi={10.1109/TAI.2025.3535456},
  ISSN={2691-4581},
  month={Aug},}@INPROCEEDINGS{11030713,
  author={Posner, M. T. and McKee, M.},
  booktitle={2025 IEEE Conference on Education and Training in Optics and Photonics (ETOP)}, 
  title={Train the Trainer: Everything You Wanted to Know About Outreach}, 
  year={2025},
  volume={},
  number={},
  pages={1-2},
  abstract={"Train the Trainer: Everything You Wanted to Know About Outreach", is an interactive course for professionals who are interested in presenting outreach and training others in how to conduct workshops in optics and photonics. The paper outlines the course structure and didactic content developed, including lessons learned from the pandemic and the use of generative artificial intelligence tools for instructional design. Implementations and evaluations of the workshops at conferences and on-site training are presented as case studies for this paper, with the training carried out for 53 people in the period 2022-24.},
  keywords={Training;Generative AI;Pandemics;Conferences;Optics;Hybrid power systems;Standards;Photonics;Guidelines;Photonics outreach;Continuous education},
  doi={10.1109/ETOP64842.2025.11030713},
  ISSN={},
  month={May},}@INPROCEEDINGS{10398408,
  author={Chan, Henry C. B.},
  booktitle={2023 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Grading Generative AI-based Assignments Using a 3R Framework}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={With the advent of generative artificial intelligence (GenAI), there is a strong need to revisit the grading or assessment mechanism. In this paper, we present a 3R framework to facilitate the grading of GenAI-based assignments. Basically, there are three essential components: Report, Revise and Reflect. Students should report on how they use GenAI tool(s). They should also revise its output by providing their own input or contributions. Last but not least, they should provide a learning reflection. We also present a 3R rubric for evaluation purposes and propose a GPT formula for determining an effective grade. For illustration purposes, we discuss two cases, covering essay assignments and programming assignments. Furthermore, to evaluate the 3R framework from the student perspective, we present and discuss student survey results. The 3R framework can provide the basis for further research study as well.},
  keywords={Surveys;Systematics;Generative AI;Education;Programming;Reflection;generative AI;ChatGPT;assessment},
  doi={10.1109/TALE56641.2023.10398408},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10737803,
  author={Kumar, Aman and Gadde, Deepak Narayan},
  booktitle={2024 IEEE 37th International System-on-Chip Conference (SOCC)}, 
  title={Generative AI Augmented Induction-based Formal Verification}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={Generative Artificial Intelligence (GenAI) has demonstrated its capabilities in the present world that reduce human effort significantly. It utilizes deep learning techniques to create original and realistic content in terms of text, images, code, music, and video. Researchers have also shown the capabilities of modern Large Language Models (LLMs) used by GenAI models that can be used to aid hardware development. Formal verification is a mathematical-based proof method used to exhaustively verify the correctness of a design. In this paper, we demonstrate how GenAI can be used in induction-based formal verification to increase the verification throughput.},
  keywords={Deep learning;Codes;Generative AI;Large language models;Throughput;Mathematical models;Human in the loop;Hardware;System-on-chip;Formal verification;Generative AI;k-Induction;Formal Verification},
  doi={10.1109/SOCC62300.2024.10737803},
  ISSN={2164-1706},
  month={Sep.},}@INPROCEEDINGS{10857538,
  author={Chatvichienchai, Somchai},
  booktitle={2025 19th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Effective Development of Database Manipulation Skills Using Generative AI Tools}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={As data management continues to evolve, mastering database manipulation has become crucial for achieving business success. This paper presents a methodology that effectively integrates GAI (Generative Artificial Intelligence) tools into database courses to enhance the development of database manipulation skills. Although current GAI tools are adept at supporting query generation, their ability to teach fundamental database manipulation concepts remains limited. This paper addresses this gap by outlining policies and best practices for optimizing the use of GAI tools in educational settings. This paper also offers a comparative analysis of various GAI tools, highlighting their strengths and specializations to provide insights into their effectiveness for developing database manipulation skills.},
  keywords={Databases;Generative AI;Information management;Best practices;Business;Generative AI;databases;SQL;skill;classrooms},
  doi={10.1109/IMCOM64595.2025.10857538},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10975950,
  author={Shao, Huijun and Tang, Gaohua},
  booktitle={2025 14th International Conference on Educational and Information Technology (ICEIT)}, 
  title={Empowering Mathematical Problem-Posing Pedagogy with Generative AI: A Theoretical Framework and Case Study}, 
  year={2025},
  volume={},
  number={},
  pages={111-115},
  abstract={In the era of artificial intelligence, fostering creativity, critical thinking, and problem-solving skills has become increasingly important, particularly in mathematics education. Problem-posing pedagogy is recognized as an effective approach to achieving these educational goals. However, teachers often face challenges in task design, creating effective prompts, and managing classroom interactions. This study proposes a novel theoretical framework powered by Generative AI (GAI) to address these challenges. The framework consists of four interconnected stages: Design, Simulate, Engage, and Refine. By leveraging GAI, teachers can generate personalized tasks, anticipate classroom dynamics, and dynamically adjust teaching strategies. A case study demonstrates the framework's potential to enhance student engagement and improve problem-posing activities. Despite its promising potential, the framework requires further empirical validation and optimization through real-world application. Future research will explore the integration pathways of GAI in mathematical problem-posing pedagogy.},
  keywords={Training;Technological innovation;Generative AI;Face recognition;Problem-solving;Iterative methods;Information technology;Optimization;Creativity;Guidelines;Mathematical Problem-Posing;Generative AI;Theoretical Framework;Case Study},
  doi={10.1109/ICEIT64364.2025.10975950},
  ISSN={},
  month={March},}@INPROCEEDINGS{11147449,
  author={Spjut, Josef},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={A Generative Ai Game Jam Case Study From October 2024}, 
  year={2025},
  volume={},
  number={},
  pages={612-618},
  abstract={Generative Artificial Intelligence (GenAI) promises to democratize many creative endeavors, from art, to music, to writing. However, video games are an underexplored field for GenAI given the highly multi-modal and interactive nature. In this work, we present a case study game-jam-style game development process (performed over only a few days!) making heavy use of available GenAI tools (as of October 2024) to create a game called Plunderwater: Sunken Treasure, a title selected from among GenAI suggestions. We share our impressions of what worked well and what could use improvement, and hope that this paper will serve as a guide for GenAI tool developers to focus on the highest impact future improvements and as a starting point for future GenAI for game development benchmarks.},
  keywords={Video games;Computer vision;Generative AI;Image synthesis;Conferences;Games;Tutorials;Writing;Pattern recognition;Engines;generative ai;games;content generation;game jam},
  doi={10.1109/CVPRW67362.2025.00066},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{11101900,
  author={Berkol, Ali},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Exploring AI-Based Techniques for the Generation of Synthetic Infrared Images and Their Practical Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Infrared (IR) imaging plays a vital role in a wide range of applications, including military surveillance, autonomous driving, and medical diagnostics. However, the acquisition of high-quality IR data is often constrained by sensor cost, environmental limitations, and data availability. Recent advances in artificial intelligence, particularly in generative models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models, have enabled the synthesis of realistic IR images from alternative modalities like visible spectrum data. This paper presents a comprehensive survey of AI-based techniques for synthetic IR image generation, categorizing state-of-the-art approaches based on architecture, training strategy, and data modality. We identify current limitations in realism, generalizability, and explainability, while highlighting open research challenges. Building on the insights from this survey, we propose a novel deep learning framework designed to generate enhanced synthetic IR imagery using limited paired data. The proposed approach aims to improve domain adaptation performance and reduce computational overhead for real-time applications. Finally, we explore potential use cases in defense, healthcare, and intelligent systems, illustrating the transformative potential of AI-driven IR image synthesis.},
  keywords={Surveys;Training;Translation;Image synthesis;Image edge detection;Imaging;Generative adversarial networks;Diffusion models;Real-time systems;Medical diagnosis;Infrared image synthesis;Generative adversarial networks (GANs);Thermal image translation;Domain adaptation;Edge-aware deep learning},
  doi={10.1109/ISAS66241.2025.11101900},
  ISSN={},
  month={June},}@ARTICLE{11079701,
  author={Li, Fengling and Wang, Zequn and Wang, Tianshi and Zhu, Lei and Chang, Xiaojun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Generative Augmentation Hashing for Few-shot Cross-Modal Retrieval}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Deep cross-modal hashing has demonstrated strong performance in large-scale retrieval but remains challenging in few-shot scenarios due to limited data and weak cross-modal alignment. We propose Generative Augmentation Hashing (GAH), a new framework that synergizes Visual-Language Models (VLMs) and generation-driven hashing to address these limitations. GAH first introduces a cycle generative augmentation mechanism: VLMs generate descriptive textual captions for images, which, combined with label semantics, guide diffusion models to synthesize semantically aligned images via inconsistency filtering. These images then regenerate coherent textual descriptions through VLMs, forming a self-reinforcing cycle that iteratively expands cross-modal data. To resolve the diversity-alignment trade-off in augmentation, we design cross-modal perturbation enhancement, injecting synchronized perturbations with controlled noise to preserve inter-modal semantic relationships while enhancing robustness. Finally, GAH employs dual-level adversarial hash learning, where adversarial alignment of modality-specific and shared latent spaces optimizes both cross-modal consistency and discriminative hash code generation, effectively bridging heterogeneous gaps. Extensive experiments on benchmark datasets show that GAH outperforms state-of-the-art methods in few-shot cross-modal retrieval, achieving significant improvements in retrieval accuracy. Our source codes and datasets are available at https://github.com/xiaolaohuuu/GAH.},
  keywords={Semantics;Perturbation methods;Cross modal retrieval;Codes;Training;Data augmentation;Noise;Metalearning;Filtering;Few shot learning;Cross-modal retrieval;few-shot learning;generative augmentation;perturbation enhancement},
  doi={10.1109/TCSVT.2025.3588769},
  ISSN={1558-2205},
  month={},}
