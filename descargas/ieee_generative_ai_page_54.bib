@INPROCEEDINGS{11040765,
  author={Qiu, Hui and Li, Zheng and Song, Weize},
  booktitle={2025 10th Asia Conference on Power and Electrical Engineering (ACPEE)}, 
  title={Generative AI Application in Power System Asset Management}, 
  year={2025},
  volume={},
  number={},
  pages={161-166},
  abstract={As the global efforts to address climate change goals advance, the demand for renewable energy grows, and governments around the world actively promote low-carbon economies and green energy policies, the power system is undergoing profound changes. This transformation not only affects the mode of power production but also profoundly alters the way, focus, and technical means of power equipment asset management. The diversification of equipment types has increased management complexity, as new types of equipment such as photovoltaic inverters, wind turbine blades, and energy storage systems require specialized maintenance knowledge and technologies. To enhance the reliability and safety of power equipment, and reduce maintenance costs, this paper aims to explore the integration path of generative AI technology and the knowledge base of power equipment asset management, establish a knowledge base for the life cycle of asset equipment, adopt emerging generative AI information technology, and build an intelligent power asset equipment management question-answer platform to achieve intelligent management of various assets, thereby reducing unplanned downtime and improving the stability of power grid operation. Firstly, establish a knowledge base for power equipment management and combine it with the retrieval-augmented generation technology of generative AI to enable it to understand and generate professional knowledge and technical guidance related to power equipment. Then, deploy the application on a cloud platform and create an interactive intelligent assistant that can analyze emerging problems in real time and provide targeted solution suggestions. At the same time, it enables front-line technicians to quickly obtain effective help and support even when facing complex situations, they have never encountered before.},
  keywords={Costs;Generative AI;Knowledge based systems;Retrieval augmented generation;Power system stability;Power grids;Asset management;Maintenance;Power system reliability;Safety;Generative AI;Asset equipment maintenance;intelligent assistant;retrieval-augmented generation},
  doi={10.1109/ACPEE64358.2025.11040765},
  ISSN={2996-2951},
  month={April},}@INPROCEEDINGS{8959837,
  author={Huang, Yi-Hao and Li, Chih-Hung G. and Chang, Yu-Ming},
  booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)}, 
  title={Illumination-Robust Object Coordinate Detection by Adopting Pix2Pix GAN for Training Image Generation}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Illumination effects often result in error or failure in visual object localization. Whereas ConvNet-based object localization frameworks have shown tremendous robustness to the illumination effect, under some strong illumination effects, the system may still fail. In this paper, the authors propose a data augmentation method utilizing pix2pix GAN for automatic generation of object images under various illumination effects. Upon training for the object localization ConvNet, the generated images are included to enrich the training set for better performance under strong illumination effects. Experimental evidence shows that the accuracy of object coordinate detection can be improved significantly. The proposed framework maintains our concept of “one-shot” where the user only needs to take a basis photo of the target object; the rest of the process including image processing and data annotation are all automated.},
  keywords={Lighting;Training;Gallium nitride;Generative adversarial networks;Annotations;Detectors;Image synthesis;computer vision;convolutional neural network;generative adversarial network;pix2pix;localization;data augmentation;one-shot},
  doi={10.1109/TAAI48200.2019.8959837},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{10704907,
  author={Mariano, Federico and Pitzalis, Roberto Francesco and Monica, Luigi and Ortiz, Jesus and Berselli, Giovanni},
  booktitle={2024 20th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)}, 
  title={Traditional vs Generative Design Optimization for Novel Wrist Exoskeleton}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper will focus on two possible optimizations for a novel wrist exoskeleton. Exoskeletons are structures designed to support human joints using mechanical components and are used in various fields such as rehabilitation, military, and industrial sectors. The study focuses on a new exoskeleton for the wrist with the intention of optimizing its weight and mechanical strength. Two optimization methods are employed. the first one aims to achieve traditional optimization using Computer-Aided Design (CAD) and Finite Element Method (FEM) analysis. In the second the use of Generative Design (GD) optimization, which combine topological optimization and Artificial Intelligence (AI), wants to create new organic components almost impossible to obtain without using this tool. The results indicate that GD optimization significantly reduces maximum displacement while maintaining stress levels within material limits, often improving the weight of the part. Moreover, GD optimization streamlines the design process, offering quicker iterations than traditional methods. This study highlights the efficacy of GD in achieving superior exoskeleton designs with improved mechanical properties and reduced weight, presenting it as a valuable tool for designers seeking optimal solutions in less time.},
  keywords={Wrist;Design automation;Three-dimensional displays;Force;Exoskeletons;Finite element analysis;Topology;Artificial intelligence;Optimization;Stress;Generative Design;Exoskeleton;FEM analysis},
  doi={10.1109/MESA61532.2024.10704907},
  ISSN={2835-902X},
  month={Sep.},}@INPROCEEDINGS{9930290,
  author={Huang, Zhigao and Zhou, Yixin and Shi, Yu and Chen, Jisong and Lai, Ting and Shao, Changjiang},
  booktitle={2022 International Conference on Artificial Intelligence and Computer Information Technology (AICIT)}, 
  title={Gradient-guided GAN for dynamic scene deblurring}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Dynamic scene blur, mainly caused by camera shake and motions, is one of the most common causes of image degradation. Recent GAN-based strategies have performance on deblurring tasks. To further improve the performance of GAN-based approaches on deblurring tasks, we propose Gradient-guided GAN for dynamic scene deblurring, it includes image restoration branch and gradient branch, which uses the gradient as a guide to supervise the restoration process. In particular, perform an attention fusion of feature image generated by restoration branch and gradient feature image generated by gradient branch, which using gradient information to guide the network to fully learn the deep feature information. Extensive experiments on GOPRO dataset show that our method achieve state-of-the-art performance in dynamic scene deblurring.},
  keywords={Measurement;Degradation;Image synthesis;Dynamics;Generative adversarial networks;Cameras;Image restoration;Dynamic scene deblurring;generative adversarial network;gradient guide;feature fusion},
  doi={10.1109/AICIT55386.2022.9930290},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9457913,
  author={Maru, Chihiro and Kobayashi, Ichiro},
  booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Collective Anomaly Detection for Multivariate Data using Generative Adversarial Networks}, 
  year={2020},
  volume={},
  number={},
  pages={598-604},
  abstract={Generative adversarial network (GAN) is used to model complex high-dimensional distributions of real-world scenarios. It has been applied to anomaly detection and making great achievements. However, most of the existing GAN-based anomaly detection methods cannot detect collective anomalies that change the behavior of multipoint data instances. Moreover, although many GAN-based methods for time-series anomaly detection have been proposed, there are few studies to handle collective anomalies in time-series data. Besides, there is still much room to improve the methods in terms of computational cost and the accuracy for detecting anomaly. We thus aim to propose a GAN-based method to detect multi-dimensional collective anomalies with high accuracy. To correctly detect collective anomalies, we especially introduce an encoder into a GAN-based anomaly detection method to obtain the latent states of the real data. We furthermore adopt a sequence to sequence technique to both encoder and generator, recurrent neural network, and fully connected neural network for the discriminator. We conducted experiments using two types of datasets: artificial and natural, and verified the effectiveness of our GAN model.},
  keywords={Recurrent neural networks;Scientific computing;Generative adversarial networks;Generators;Data models;Computational efficiency;Anomaly detection;anomaly detection;GAN;collective anomalies;time-series},
  doi={10.1109/CSCI51800.2020.00106},
  ISSN={},
  month={Dec},}@ARTICLE{10922144,
  author={Yang, Chuang and Zhao, Bingxuan and Zhou, Qing and Wang, Qi},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={MMO-IG: Multiclass and Multiscale Object Image Generation for Remote Sensing}, 
  year={2025},
  volume={63},
  number={},
  pages={1-12},
  abstract={The rapid advancement of deep generative models (DGMs) has significantly advanced research in computer vision, providing a cost-effective alternative to acquiring vast quantities of expensive imagery. However, existing methods predominantly focus on synthesizing remote sensing (RS) images aligned with real images in a global layout view, which limits their applicability in RS image object detection (RSIOD) research. To address these challenges, we propose a multiclass and multiscale object (MMO) image generator based on DGMs, termed MMO-IG, designed to generate RS images with supervised object labels from global and local aspects simultaneously. Specifically, from the local view, MMO-IG encodes various RS instances using an iso-spacing instance map (ISIM). During the generation process, it decodes each instance region with iso-spacing value in ISIM—corresponding to both background and foreground instances—to produce RS images through the denoising process of diffusion models. Considering the complex interdependencies among MMOs, we construct a spatial-cross dependence knowledge graph (SCDKG). This ensures a realistic and reliable multidirectional distribution among MMOs for region embedding, thereby reducing the discrepancy between source and target domains. Besides, we propose a structured object distribution instruction (SODI) to guide the generation of synthesized RS image content from a global aspect with SCDKG-based ISIM together. Extensive experimental results demonstrate that our MMO-IG exhibits superior generation capabilities for RS images with dense MMO-supervised labels, and RS detectors pretrained with MMO-IG show excellent performance on real-world datasets. Code is available at https://github.com/omtcyang/MMO-IG.},
  keywords={Remote sensing;Layout;Image synthesis;Data models;Object detection;Diffusion models;Process control;Monitoring;Knowledge graphs;Gray-scale;Diffusion model;image generation;object detection;remote sensing (RS)},
  doi={10.1109/TGRS.2025.3550404},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10076700,
  author={Ma, Xuebin and Yang, Ren and Zheng, Maobo},
  booktitle={2022 18th International Conference on Mobility, Sensing and Networking (MSN)}, 
  title={RDP-WGAN: Image Data Privacy Protection Based on Rényi Differential Privacy}, 
  year={2022},
  volume={},
  number={},
  pages={320-324},
  abstract={In recent years, artificial intelligence technology based on image data has been widely used in various industries. Rational analysis and mining of image data can not only promote the development of the technology field but also become a new engine to drive economic development. However, the privacy leakage problem has become more and more serious. To solve the privacy leakage problem of image data, this paper proposes the RDP-WGAN privacy protection framework, which deploys the Rényi differential privacy (RDP) protection techniques in the training process of generative adversarial networks to obtain a generative model with differential privacy. This generative model is used to generate an unlimited number of synthetic datasets to complete various data analysis tasks instead of sensitive datasets. Experimental results demonstrate that the RDP-WGAN privacy protection framework provides privacy protection for sensitive image datasets while ensuring the usefulness of the synthetic datasets.},
  keywords={Training;Industries;Differential privacy;Privacy;Generative adversarial networks;Data models;Sensors;Rényi differential privacy;generative adversarial networks;image data privacy protection},
  doi={10.1109/MSN57253.2022.00060},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10009340,
  author={Singh, Neetu and F, Abdul Manaf and Rastogi, Mudit and Prasad, Rahul},
  booktitle={2022 8th International Conference on Signal Processing and Communication (ICSC)}, 
  title={Performance Analysis of Conditional GANs based Image-to-Image Translation Models for Low-Light Image Enhancement}, 
  year={2022},
  volume={},
  number={},
  pages={468-474},
  abstract={With the evolution of generative adversarial networks, popularly known as GANs for image-to-image translations, conditional GANs (cGANs) are explored and employed for various digital image preprocessing (enhancement and de-noising) tasks. The series of tasks includes image processing such as image enhancement, de-hazing, de-noising, resolution enhancement, and many more. In image enhancement, the area of increasing light (brightness) in low-light images (or poorly-illuminated images) is investigated in this work. For low-light image enhancement, the performance of pix2pix and pix2pixHD models has been demonstrated and analyzed. An analysis of low-light image enhancement using pix2pix model with other loss functions is also presented. Furthermore, pix2pix performance with instance normalization layers for low-light image enhancement is studied, and improved full-reference Image Quality Assessment (FIQA) metrics values along with entropy (a no-reference IQA (NIQA) metric) are reported. The quantitative and qualitative results are also compared with selected cutting-edge deep learning frameworks for low-light image enhancement. In this research, it is found that pix2pix model enhancement metrics are better than RetinexNet model. And the pix2pixHD results are comparable to the latest low-light image enhancement deep learning frameworks such as MIRNet and LLFlow. Furthermore, pix2pix models are lighter in size than MIRNet. The inference times achieved using pix2pix are the minimum on both the CPU and the GPU.},
  keywords={Measurement;Deep learning;Image quality;Analytical models;Image resolution;Noise reduction;Signal processing;conditional GANs;Discriminator;Generator;instance normalization;inference time;low-light image enhancement;loss function;pix2pix model;pix2pixHD model},
  doi={10.1109/ICSC56524.2022.10009340},
  ISSN={2643-444X},
  month={Dec},}@INPROCEEDINGS{10134626,
  author={Singh, Neetu and Dubey, Ekaagra and Joshi, Prateek and Prasad, Rahul},
  booktitle={2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={Experimental Investigation of the Pix2pixHD Model for the Improvement of the Fairly Substantial Quality of Low-Light Images}, 
  year={2023},
  volume={},
  number={},
  pages={426-433},
  abstract={In image synthesis, conditional generative adversarial networks (cGANs) have been investigated numerous times and are used for diverse image generation applications. In this work, the domain of low-light image enhancement that involves transforming low-light images into realistic, bright images employing a cGAN is explored. The pix2pixHD model, a cGAN, has been tested, examined, and improved qualitatively and quantitatively for low-light image enhancement. The employed quantitative metrics include both a no-reference Image Quality Assessment (NIQA) metric such as entropy and full-reference IQA (FIQA) metrics such as MSE, PSNR, SSIM, and LPIPS. Additionally, the pix2pixHD model with different combinations of loss functions is being investigated. Furthermore, the efficacy of histogram-equalized versions of input images in generating well-illuminated images is investigated by employing the lighter pix2pixHD model. The qualitative and quantitative simulation results are also compared with state-of-the-art deep learning models, and this analysis showed that pix2pixHD can challenge the most recent deep learning models for low-light image enhancement, namely MIRNet, LLFlow, and MIRNetv2, with a minimum inference time of approximately 0. 01s on GPU.},
  keywords={Measurement;Deep learning;Image quality;Analytical models;Image synthesis;Simulation;Graphics processing units;Conditional GANs;Low-light image;Discriminator;Generator;image enhancement;image quality assessment;loss functions;pix2pixHD},
  doi={10.1109/CSNT57126.2023.10134626},
  ISSN={2473-5655},
  month={April},}@INPROCEEDINGS{9965222,
  author={Xu, Guixun and Guo, Wenhui and Wang, Yanjiang},
  booktitle={2022 16th IEEE International Conference on Signal Processing (ICSP)}, 
  title={Memory Enhanced Replay for Continual Learning}, 
  year={2022},
  volume={1},
  number={},
  pages={218-222},
  abstract={Currently, artificial intelligence can not continually learn the new knowledge and retain the acquired knowledge in the past like human beings. It is affected by a phenomenon called catastrophic forgetting. Although continual learning based on generative replay could alleviate the problem to a certain extent without accessing the past data, it utilizes not fully potentials of the framework. Inspired by the mechanism of the interaction between the brain hippocampus and neocortex, we propose a novel memory enhanced replay networks (MER) based on generative replay for continual learning. This is a generative adversarial networks (GANs) integrating memory enhancement module. Specifically, the method adopts GANs to simulate the learning and memory process in the brain and add a memory enhancement module in the final process of training. This approach allows the network to relieve the uneven distribution of tasks in the previous sequence training, so as to better preserve the past learned knowledge. Class-incremental experiments show that the proposed framework is able to promote knowledge consolidation after training and has a competitive result compared with other advanced methods.},
  keywords={Training;Knowledge engineering;Databases;Signal processing;Brain modeling;Generative adversarial networks;Task analysis;continual learning;catastrophic forgetting;class-incremental;generative replay;memory enhancement;GANs},
  doi={10.1109/ICSP56322.2022.9965222},
  ISSN={2164-5221},
  month={Oct},}@INPROCEEDINGS{10505597,
  author={Wang, Guangyi and Cai, Yuren and Liu, Pei and Xia, Jincun and Su, Songzhi},
  booktitle={2023 13th International Conference on Information Technology in Medicine and Education (ITME)}, 
  title={Principles of Diffusion Models and Their Applications on Medicine}, 
  year={2023},
  volume={},
  number={},
  pages={724-729},
  abstract={Diffusion models, as a paradigm of generative models, have achieved unprecedented success in the field of image generation since 2020, owing to their advantage of simplicity in optimization. They have been widely applied to various tasks in fields such as image, text, and speech processing. Notably, due to a series of breakthroughs achieved by diffusion models in natural image generation, the field of medical imaging has actively explored the potential of diffusion models. This paper begins with a detailed introduction to the principles of diffusion models. It then categorizes the applications of diffusion models in the field of medical imaging into various tasks, including generation, 3D reconstruction, segmentation, image-to-image translation, and classification, highlighting the main methods for each task. Finally, the paper summarizes the research progress of diffusion models in the field of medical imaging, discusses key challenges and open issues, and proposes promising directions for future improvements.},
  keywords={Image segmentation;Three-dimensional displays;Image synthesis;Reviews;Task analysis;Speech processing;Information technology;Diffusion models;Score-based;Applications on Medical},
  doi={10.1109/ITME60234.2023.00149},
  ISSN={2474-3828},
  month={Nov},}@INPROCEEDINGS{10667570,
  author={Zhuang, Gancheng},
  booktitle={2024 5th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)}, 
  title={Research on Large-scale Data Anomaly Detection based on Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={249-252},
  abstract={With the continuous progress of big data, Internet of Things, artificial intelligence and other technologies, we are living in an era of information explosion. A large amount of data is constantly recorded, and this data contains important information at the critical moment. With the rapid increase in data volume, traditional statistical methods are gradually unable to meet the needs due to high computational complexity and poor adaptability. In this work, we utilize autoencoder and generative adversarial network deep learning methods to detect anomalies in large-scale data. Autoencoders learn low-dimensional representations of data through the encoding and decoding process, and are mainly used to reconstruct normal data and identify anomalies by reconstructing errors. The generative adversarial network grasps the distribution characteristics of the data through the adversarial learning of its generative model and discriminant model, and uses this ability to detect abnormal patterns in the data. The effectiveness of the proposed model was verified by conducting experiments on multiple real-world datasets. Experimental results show that compared with the traditional method, the proposed deep learning model has significant improvement in detection accuracy and computing efficiency.},
  keywords={Deep learning;Adaptation models;Information science;Statistical analysis;Computational modeling;Generative adversarial networks;Data models;data anonaly detection;autoencoder;adversarial learning;generative network},
  doi={10.1109/ISPDS62779.2024.10667570},
  ISSN={},
  month={May},}@INPROCEEDINGS{9457739,
  author={Naritomi, Yusuke and Adachi, Takanori},
  booktitle={2020 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={Data Augmentation of High Frequency Financial Data Using Generative Adversarial Network}, 
  year={2020},
  volume={},
  number={},
  pages={641-648},
  abstract={In recent years, the field of machine learning, as seen in deep learning, has grown tremendously and is being applied to the financial field. For example, in the area of high frequency trading, machine learning techniques are used to predict stock prices. However, the non-stationary nature of financial time series makes it quite difficult to use machine learning to create a good stock price prediction model in the area. A large amount of training data may be required to overcome the problem. As another way to solve the problem, this study proposes a data augmentation method using artificial market simulations based on Generative Adversarial Networks (GANs). We use the Wasserstein GAN (WGAN), which is one of several GANs and is formulated for the optimal transport problem. Our method trains the GAN using a time series of trading data provided by the Tokyo Stock Exchange called FLEX Full. The results confirmed that the probability distribution of synthetic order events generated by the GAN was close to reality, and we also obtained data augmentation of execution prices from the artificial market simulations using the synthetic order events. The results showed that the accuracy of the prediction of increase/decrease in execution prices was better than that of the case without data augmentation.},
  keywords={Flexible printed circuits;Time series analysis;Training data;Predictive models;Generative adversarial networks;Data models;Probability distribution;Data Augmentation;GAN;Artificial Market},
  doi={10.1109/WIIAT50758.2020.00097},
  ISSN={},
  month={Dec},}@ARTICLE{10041900,
  author={Lee, Youngchan and You, Wonsang},
  journal={IEEE Access}, 
  title={EBAT: Enhanced Bidirectional and Autoregressive Transformers for Removing Hairs in Hairy Dermoscopic Images}, 
  year={2023},
  volume={11},
  number={},
  pages={14225-14235},
  abstract={A great progress in deep learning technologies for skin cancer detection from dermoscopic images has been made for a decade. While its performance is vulnerable to a large amount of hairs densely covering the skin surface, the existing image processing methods frequently fail to remove hairs in hairy skin images. In this paper, we propose, as a deep learning approach to removing hairs, a generative image inpainting network where bidirectional autoregressive transformers (BATs) are employed to learn image features and are systematically integrated with convolutional neural networks (CNNs) in multiple spatial scales in order to reconstruct missing regions. Each patch split from a masked image is unfolded and processed through BATs, and re-folded to constitute diverse shapes of feature maps through kernel-based unfolding-folding operations. By introducing the multi-scale features extracted by collaborative learning of transformers and CNNs to the texture generator network, our method can effectively reconstruct minute details of local regions as well as global structure which might not be easily inferred from neighbor pixels in hairy skin images. Quantitative and qualitative evaluations show not only that our multi-scale dual-modality strategy is much robust to reconstruct hair-shaped missing regions compared to the existing transformer-based image inpainting method called BAT-Fill, but also that our framework outperforms the state-of-the-art image inpainting models in removing hairs from hairy dermoscopic images.},
  keywords={Hair;Deep learning;Transformers;Generators;Feature extraction;Image reconstruction;Generative adversarial networks;Imaging;Hair removal;skin image;image inpainting;transformer;deep learning;generative adversarial networks},
  doi={10.1109/ACCESS.2023.3243911},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10060406,
  author={J, Praveen Gujjar and H R, Prasanna Kumar},
  booktitle={2022 International Conference on Artificial Intelligence and Data Engineering (AIDE)}, 
  title={Natural language processing using text augmentation for chatbot}, 
  year={2022},
  volume={},
  number={},
  pages={248-251},
  abstract={Chatbot is an application that helps in virtual interactions. Chatbot simulates the interaction in a more effective way by natural language processing. An artificial neural network makes better understanding of the user intent and entity. There are two broad classifications for chatbot i.e rule based and conversational AI chatbot. In the case of rule based chatbot, for all user inquiries automated responses are generated based on set of rules. Whereas in conversational AI chatbot, it requires better natural language processing for understanding user inquiries. Text augmentation provides the platform for the chatbot system to train more on the limited amount of the dataset. This paper focuses more on the text augmentation techniques for creating the robust chatbot system using the text augmentation open source library NLPAug. The recent studies outline that machine learning models are more vulnerable to even for small adversarial attack. The result shows that NLPAug perform the text augmentation in three levels those are character level, word level and sentence level to avoid the adversarial attacks. Text augmentation enables the chatbot to provide accurate result for the user query. Multimedia files can be augmented in the future work.},
  keywords={Semantics;Artificial neural networks;Chatbots;Data engineering;Libraries;Tokenization;Data models;Adversarial attacks;Chatbot;neural network;NLPAug;text augmentation},
  doi={10.1109/AIDE57180.2022.10060406},
  ISSN={},
  month={Dec},}@ARTICLE{9399430,
  author={Li, Dejian and Qi, Wenqian and Sun, Shouqian},
  journal={IEEE Access}, 
  title={Facial Landmarks and Expression Label Guided Photorealistic Facial Expression Synthesis}, 
  year={2021},
  volume={9},
  number={},
  pages={56292-56300},
  abstract={Facial expression manipulation plays an increasingly important role in the field of computer graphics and has been widely used in generating facial animations. However, it is still a very challenging task as it needs full understanding of the input face and very depending on the facial appearance. In this paper, we present an end-to-end generative adversarial network for facial expression synthesis. Given the facial landmarks and the expression label of a target image, our method automatically generates a corresponding expression facial image with the identity information and facial details well preserved. Both qualitative and quantitative experiments are conducted on the CK+ and Oulu-CASIA datasets. Experimental results show that our method has the compelling perceptual results even there exist large differences in facial shapes for unseen subjects.},
  keywords={Generative adversarial networks;Faces;Gallium nitride;Generators;Two dimensional displays;Three-dimensional displays;Shape;Facial expression synthesis;generative adversarial networks},
  doi={10.1109/ACCESS.2021.3072057},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10620592,
  author={Helmy, Mona and El-Din, Yomna Safaa and Mohamed, Omar Tarek and Kader, Omar Salah Abdel and Ramadan, Shehab Adel and Kamal, Amr Essam and Selim, Mohamed Reda Mohamed},
  booktitle={2024 International Telecommunications Conference (ITC-Egypt)}, 
  title={Navigating the World with an Intelligent Tourist Guide Using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper addresses the intersection of artificial intelligence and cultural heritage exploration. The study investigates the utilization of NeRF (Neural Radiance Fields) for generating 3D scene meshes derived from reality, and the implementation of a virtual tour-guide chat-bot using LLaMA 3 LLM (Large Language Model). The aim is to facilitate the exploration of Egyptian Heritage Sites for tourists and explorers. By examining these AI-driven solutions, the paper sheds light on the challenges and opportunities in leveraging advanced technologies to enhance cultural heritage exploration and preservation endeavors.},
  keywords={Bridges;Solid modeling;Three-dimensional displays;Navigation;Generative AI;Large language models;Neural radiance field;Neural Radiance Fields (NeRF);Mesh;Multi- Layer Perceptron (MLP);Nerfstudio;Large Language Model (LLM);Large Language Model Meta AI (LLaMA);Natural Language Processing (NLP)},
  doi={10.1109/ITC-Egypt61547.2024.10620592},
  ISSN={},
  month={July},}@ARTICLE{10813021,
  author={Li, Peichun and Dong, Huanyu and Qian, Liping and Zhou, Sheng and Wu, Yuan},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={FlexGen: Efficient On-Demand Generative AI Service With Flexible Diffusion Model in Mobile Edge Networks}, 
  year={2025},
  volume={11},
  number={2},
  pages={961-973},
  abstract={Generative artificial intelligence (AI) in edge networks has excelled in delivering human-level creative services close to the end users. However, providing customized intelligence services to a wide range of end clients remains challenging due to the diverse demands of edge applications. In this paper, we present FlexGen, an efficient generative AI framework with flexible diffusion models, to tailor the intelligence service for different client-side requests under diverse quality and efficiency constraints. To this end, we first design and train a flexible diffusion model to support quality-and-cost adjustable image synthesis. After that, we focus on the server-side energy minimization problem subject to the quality level of generative service and the client-side latency constraint. We further theoretically characterize the relationship between the width of the diffusion model and the expected quality of the synthetic image. Following that, the decomposition solution is applied to optimize the generative service, where the image synthesis strategy and resource allocation policy are personalized for different client-side requests. Experiments indicate that, compared to existing image generation schemes, our framework can save up to two times energy consumption without sacrificing the quality of the service.},
  keywords={Generative AI;Diffusion models;Computational modeling;Servers;Load modeling;Image quality;Adaptation models;Resource management;Image synthesis;Switches;Generative AI;flexible diffusion model;resource management},
  doi={10.1109/TCCN.2024.3522084},
  ISSN={2332-7731},
  month={April},}@INBOOK{10951742,
  author={Kihn, Martin and Lin, Andrea Chen},
  booktitle={Customer 360: How Data, AI, and Trust Change Everything}, 
  title={AI in Action Today!}, 
  year={2025},
  volume={},
  number={},
  pages={107-122},
  abstract={Summary <p>Today's prolific generative artificial intelligence (AI) grew from the neural network approach spurring the potential for technological innovation to reach unprecedented levels of awareness and accessibility. AI in combination with big data, blockchain, IoT, gene sequencing innovations, and other disruptive technologies&#x2014;these inspire societal transformations that impact how we communicate, work, and entertain ourselves. Evidence among enterprise AI users is that using &#x201c;hybrid AI&#x201d; or a combination of predictive and generative AI together is a powerful option. The chapter provides their examples of hybrid AI use cases where generative AI helps companies to take action based on predictive AI insights. The degree of real&#x2010;time personalization generative AI offers is significant as long as the Customer 360 team has its curated data unified, harmonized, and orchestrated to serve as high octane fuel. The chapter presents examples of how different companies have successfully built the foundations of a next generation, AI&#x2010;empowered company.</p>},
  keywords={Artificial intelligence;Digital audio broadcasting;Web sites;Technological innovation;Social networking (online);Generative AI;Video on demand;Fourth Industrial Revolution;Data models;Computational modeling},
  doi={10.1002/9781394308668.ch12},
  ISSN={},
  publisher={Wiley},
  isbn={9781394273638},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10951742},}@INPROCEEDINGS{10504342,
  author={Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
  booktitle={2024 IEEE First International Conference on Artificial Intelligence for Medicine, Health and Care (AIMHC)}, 
  title={AI-Powered Social Determinants of Health Extraction from Patient Records: A GPT-Based Investigation}, 
  year={2024},
  volume={},
  number={},
  pages={109-112},
  abstract={Social Determinants of Health (SDOH) play a critical role in understanding the factors affecting patient well-being, and the identification of addiction status is a crucial component in this realm. This paper investigates the use of GPT (Generative Pretrained Transformer) model for extraction of addiction status from patient discharge summaries. The study demonstrates the capacity of GPT -based natural language processing to effectively parse patient notes and ascertain the presence or absence of addiction-related information. To harness the capability of this model and align it with our objective, prompt engineering was adopted with involving the iterative exploration of various prompt. The results reveal the model's remarkable capability to discern addiction-related information within patient records with a high accuracy. By effectively identifying addiction status, clinical care teams can address addiction-related issues promptly and comprehensively, leading to more targeted support and improved patient outcomes.},
  keywords={Addiction;Refining;Medical services;Transformers;Natural language processing;Iterative methods;Artificial intelligence;Social determinants of health extraction;GPT;Prompt engineering},
  doi={10.1109/AIMHC59811.2024.00028},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9253125,
  author={Ciolino, Matthew and Kalin, Josh and Noever, David},
  booktitle={2020 Third International Conference on Artificial Intelligence for Industries (AI4I)}, 
  title={The Go Transformer: Natural Language Modeling for Game Play}, 
  year={2020},
  volume={},
  number={},
  pages={23-26},
  abstract={This work applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project’s game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons & Connect 4/6)},
  keywords={Sequential analysis;Training data;Games;Learning (artificial intelligence);Data models;Natural language processing;Task analysis;Natural Language Processing (NLP);Transformers;Game Play;Deep Learning},
  doi={10.1109/AI4I49448.2020.00012},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10123624,
  author={Jeong, Young-Seob and Kim, EunJin and Hwang, JunHa and Mswahili, Medard E. and Kim, YoungJin},
  booktitle={2022 5th International Conference on Artificial Intelligence for Industries (AI4I)}, 
  title={Language Model for Statistics Domain}, 
  year={2022},
  volume={},
  number={},
  pages={54-54},
  abstract={Since transformer has appeared, there were many studies that proposed variants of some representative language models (e.g., Bidirectional Encoder Representations from Transformers (BERT) [1] and Generative Pre-Training (GPT) series [2]). Huge language models are appearing recently (e.g., Chinchilla [3], Megatron LM), whereas there are studies of domain-specific (or language-specific) language models. For example, BioBERT for bio-informatics [4], SwahBERT for Swahili language [5], and FinBERT for financial domain [6]. Without doubt, statistics must be one of the domains with many collected data (e.g., reports of statistics). Pre-trained language model for the statistic domain will probably deliver much performance improvement in down-stream tasks such as industry code classification and job code classification, and more accurate system for the code classification tasks will contribute to better national statistics and taxation. Indeed, many countries are trying to develop such system, and this paper summarizes some relevant findings and provides suggestions to develop language models for statistics domain.},
  keywords={Industries;Codes;Biological system modeling;Bit error rate;Transformers;Task analysis;Artificial intelligence;Language model;Transformer;Statistics;Domain specific language model},
  doi={10.1109/AI4I54798.2022.00020},
  ISSN={2770-4718},
  month={Sep.},}@INPROCEEDINGS{10445554,
  author={Xi, Mingze and Perera, Madhawa and Anderson, Stuart and Adcock, Matt},
  booktitle={2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)}, 
  title={Towards Situated Imaging}, 
  year={2024},
  volume={},
  number={},
  pages={85-89},
  abstract={Integrating augmented reality (AR) with externally hosted computer vision (CV) models can provide enhanced AR experiences. For instance, by utilising an advanced object detection model, an AR system can recognise a range of predefined objects within the user’s immediate surroundings. However, existing AR-CV workflows rarely incorporate user-defined contextual information, which often come in the form of multi-modal queries blending both natural and body language. Interpreting these intricate user queries, processing them via a sequence of deep learning models, and then adeptly visualising the outcomes remains a formidable challenge.In this paper, we describe Situated Imaging (SI), an extensible array of techniques for in-situ interactive visual computing. We delineate the architecture of the Situated Imaging framework, which enhances the conventional AR-CV workflow by incorporating a range of advanced interactive and generative computer vision techniques. We also describe a demonstration implementation to illustrate the pipeline’s capabilities, enabling users to engage in activities such as labelling, highlighting, or generating content within a user-defined context. Furthermore, we provide initial guidance for tailoring this framework to example use cases and identify avenues for future research. Our model-agnostic Situated Imaging pipeline acts as a valuable starting point for both academic scholars and industry practitioners interested in enhancing the AR experience by incorporating computationally intensive AI models.},
  keywords={Solid modeling;Visualization;Computer vision;Computational modeling;Query processing;Imaging;Artificial intelligence;Augmented Reality;Situated Imaging;Situated Visualisation;Computer Vision;Mixed Reality},
  doi={10.1109/AIxVR59861.2024.00019},
  ISSN={2771-7453},
  month={Jan},}@INPROCEEDINGS{10355001,
  author={Ruangchutiphophan, Pavaris and Saetia, Chanatip and Ayutthaya, Thititorn Seneewong Na and Chalothorn, Tawunrat},
  booktitle={2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)}, 
  title={Thai Knowledge-Augmented Language Model Adaptation (ThaiKALA)}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Large language models have exhibited considerable prowess in diverse NLP tasks, demonstrating promising performance. However, they still have limitations in effectively capturing domain-specific knowledge and contextually-relevant information, resulting in hallucination issues. To address these challenges, This paper presents ThaiKALA, a framework designed for the Thai language to augment domain-specific knowledge into the language model. The framework utilizes three modules to handle Thai language specifically: event extraction, a self-defined ID database, and a multilingual language model. To confirm the performance, the framework is also evaluated with strong generative baselines like GPT-3 and GPT-3.5-turbo-16k. As a result, ThaiKALA, with only Entity Memory, outperforms all baselines including GPT-3 and GPT-3.5 in extractive Question Answering (EQA) tasks, achieving a higher exact match (42.48%) and competitive F1 scores (67.07%). These results demonstrate that ThaiKALA is effective in enhancing the language model’s performance on Thai extractive QA by augmenting the extracted knowledge.},
  keywords={Adaptation models;Databases;Question answering (information retrieval);Task analysis;Artificial intelligence;Knowledge-Augmented;Language Model;Question Answering},
  doi={10.1109/iSAI-NLP60301.2023.10355001},
  ISSN={2831-4565},
  month={Nov},}@ARTICLE{10231323,
  author={Wu, Chin Ta and Tsou, Ching Shih and Li, Shing Han},
  journal={IEEE Access}, 
  title={Data Augmentation With CycleGAN to Build a Classifier for Novel Defects From the Dicing Stage of Semiconductor Package Assembly}, 
  year={2023},
  volume={11},
  number={},
  pages={93012-93018},
  abstract={Industry 4.0, a concept first proposed by Germany, has resulted in an increasing number of companies adopting a mass customization strategy. This strategy is widely used across various industries, enabling the production of small batches of diversified products to meet the diverse needs of customers. It encompasses the business process of providing customized goods that best fulfill individual customer requirements, thereby necessitating small-scale production of multiple products. Therefore, the product life cycle of mass customization is much shorter than other production strategies. When product line changes are frequent and customized products have high yield rates, accurately detecting potential defects from a limited number of images is a daunting challenge. If the defect identification classification model needs to maintain a certain level of identification accuracy and the model needs to be deployed quickly, it is impossible to wait until a large number of defect images are collected before deploying an accurate model for new defects. Obtaining a high-precision defect identification classification model is crucial. In this study, we employed the style transfer method of CycleGAN, which takes advantage of the unmatched training images, to successfully transfer the style of defective images from old products to defect-free images of new products. However, CycleGAN requires a large number of images for training, so this study primarily focuses on rare sample categories. We first obtained the defect mask through a semantic segmentation model and then separated the foreground defect from the wafer background using digital image processing techniques. We then copied and pasted the separated defect onto a new wafer background to generate fake defect images. Finally, a generative adversarial network architecture was used to perform image blending to make the fake defect images more natural and realistic. The effectiveness of the data augmentation method was verified through a convolutional neural network model. Through the proposed method in this study, the number of defect images in new products was successfully increased, which helps to deploy a defect identification classification model for new products quickly.},
  keywords={Semiconductor device modeling;Data models;Training;Integrated circuit modeling;Artificial intelligence;Production facilities;Semantic segmentation;Outsourcing;Assembly systems;Automated optical inspection (AOI);outsourced semiconductor assembly and test (OSAT);artificial intelligence;vision defects;GAN;image fusion},
  doi={10.1109/ACCESS.2023.3309159},
  ISSN={2169-3536},
  month={},}@INBOOK{10953116,
  author={Shah, Priten},
  booktitle={AI and the Future of Education: Teaching in the Age of Artificial Intelligence}, 
  title={Automating Administrative Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={136-158},
  abstract={Summary <p>Recent AI developments promise opportunities for increased efficiency, personalized support, and enhanced communication. In addition, through various tools and generative AI bots, education has the potential to reshape logistics significantly. AI can improve communication among educators, students, and parents. For classroom organization and efficiency, AI can optimize class operations and reduce conflicts through intelligent scheduling and automatic organization of digital resources. AI systems can break down rubrics, grading requirements, or assignment instructions and then grade assignments with feedback for students. ChatGPT provides clear, actionable feedback to the student that can be a starting place for the teacher's own work with the student. Concerns about privacy, data security, and potential biases in the existing AI systems must be addressed before AI tools can be fully trusted to decide grades unanimously. AI tools can also help create templates, forms, and other paperwork necessary for the daily operations of our schools.</p>},
  keywords={Artificial intelligence;Chatbots;Education;Electronic mail;Virtual assistants;Scheduling;Organizations;Turning;Translation;Sentiment analysis},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394219261},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10953116},}@INPROCEEDINGS{11166129,
  author={Nuseir, Aya},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Performance Analysis of Vision-Language Pre-Training Models on Unbiased VQA Tasks}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This study evaluates the performance of Visual Language Pre-training (VLP) models on a colour recognition task using unbiased Visual Question Answering (VQA) datasets. We focused on three state-of-the-art multimodal VLP models: Vision-and-Language Transformer (ViLT), Generative Image-to-text Transformer (GIT), and Bootstrapping Language-Image Pre-training (BLIP). To assess these models, we developed three novel balanced datasets with binary question-answer pairs, each designed to detect the colour of objects. The datasets differ in image creation methods, including fully synthetic images, photographs of coloured objects, and varied objects with realistic backgrounds. Our results indicate that dataset characteristics significantly influence model performance. ViLT consistently performed well on simpler binary datasets, achieving an accuracy of around 62%, but struggled with more complex formats. GIT showed stable performance in simpler binary datasets but had difficulty with complex questions, especially in combined colour identification tasks. BLIP is highly adaptable, excelling in complex question formats with perfect accuracy in colour identification tasks. 1},
  keywords={Visualization;Adaptation models;Accuracy;Image color analysis;Computational modeling;Transformers;Question answering (information retrieval);Data models;Performance analysis;Artificial intelligence;Visual Question Answering;unbiased dataset;multimodal Visual Language Pre-training;colour recognition},
  doi={10.1109/ACDSA65407.2025.11166129},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9960275,
  author={Gong, Mengmeng and Song, Hui and Zhou, Haoran and Xu, Bo},
  booktitle={2022 17th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)}, 
  title={Enhancing Response Relevance and Emotional Consistency for Dialogue Response Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={VAE (Variational Autoencoder) and CVAE (Conditional V AE) encode the sentence with the latent variable to generate response in Dialogue. However, studies have shown that the latent variables obtained are more inclined to remember the first words and the length of the sentence, and only represents limited local features. In order to alleviate this problem, we propose to involve contrastive learning to generate positive and negative samples for training process, which enriches the latent variables representation with the global information of sentence and generates more relevant response. On the other hand, those generative models do not consider emotional information of dialogue, a sentiment discrimination module is introduced in our model to maintain the emotional consistency. Experiments on two public datasets - DailyDialog and PERSONA-CHAT demonstrate the effectiveness of our method, the evaluation results of BLEU and Rouge are both improved. The sentiment discrimination network also forces the model to generating emotional consistency response with share embedding.},
  keywords={Training;Natural language processing;Artificial intelligence;Response Generation;CV AE;Contrastive Learning;Emotional Consistency},
  doi={10.1109/iSAI-NLP56921.2022.9960275},
  ISSN={2831-4565},
  month={Nov},}@ARTICLE{10056354,
  author={Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Normalization Techniques in Training DNNs: Methodology, Analysis and Application}, 
  year={2023},
  volume={45},
  number={8},
  pages={10173-10196},
  abstract={Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
  keywords={Training;Optimization;Covariance matrices;Task analysis;Tensors;Decorrelation;Biological neural networks;Batch normalization;deep neural networks;image classification;survey;weight normalization},
  doi={10.1109/TPAMI.2023.3250241},
  ISSN={1939-3539},
  month={Aug},}@INPROCEEDINGS{8906971,
  author={Burgueño, Loli and Cabot, Jordi and Gérard, Sébastien},
  booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={An LSTM-Based Neural Network Architecture for Model Transformations}, 
  year={2019},
  volume={},
  number={},
  pages={294-299},
  abstract={Model transformations are a key element in any model-driven engineering approach. But writing them is a time-consuming and error-prone activity that requires specific knowledge of the transformation language semantics. We propose to take advantage of the advances in Artificial Intelligence and, in particular Long Short-Term Memory Neural Networks (LSTM), to automatically infer model transformations from sets of input-output model pairs. Once the transformation mappings have been learned, the LSTM system is able to autonomously transform new input models into their corresponding output models without the need of writing any transformation-specific code. We evaluate the correctness and performance of our approach and discuss its advantages and limitations.},
  keywords={Codes;Neural networks;Semantics;Transforms;Writing;Model driven engineering;Artificial intelligence;Long short term memory;MDE, model transformations, LSTM ANN},
  doi={10.1109/MODELS.2019.00013},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9125079,
  author={Fourastier, Yannick and Baron, Claude and Thomas, Carsten and Esteban, Philippe},
  booktitle={2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)}, 
  title={Assurance levels for decision making in autonomous intelligent systems and their safety}, 
  year={2020},
  volume={},
  number={},
  pages={475-483},
  abstract={The autonomy of intelligent systems and their safety rely on their ability for local decision making based on collected environmental information. This is even more for cyber-physical systems running safety critical activities. While this intelligence is partial and fragmented, and cognitive techniques are of limited maturity, the decision function must produce results whose validity and scope must be weighted in light of the underlying assumptions, unavoidable uncertainty and hypothetical safety limitation. Besides the cognitive techniques dependability, it is about the assurance level of the decision self-making. Beyond the pure decision-making capabilities of the autonomous intelligent system, we need techniques that guarantee the system assurance required for the intended use. Security mechanisms for cognitive systems may be consequently tightly intricated. We propose a trustworthiness module which is part of the system and its resulting safety. In this paper, we briefly review the state of the art regarding the dependability of cognitive techniques, the assurance level definition in this context, and related engineering practices. We elaborate regarding the design of autonomous intelligent systems safety, then we discuss its security design and approaches for the mitigation of safety violations by the cognitive functions.},
  keywords={Uncertainty;Runtime;Protocols;Decision making;Cyber-physical systems;Lead;Safety;Artificial intelligence;autonomous system decision making;safety monitoring design;assurance level},
  doi={10.1109/DESSERT50317.2020.9125079},
  ISSN={},
  month={May},}@INBOOK{10952931,
  author={Agrawal, Poorva and Kaur, Gagandeep and Gupta, Vansh and Agarwal, Kruthika and Pinjarkar, Latika and Patil, Seema},
  booktitle={Genomics at the Nexus of AI, Computer Vision, and Machine Learning}, 
  title={AI Applications in Analyzing Gene Expression for Cancer Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={285-307},
  abstract={Summary <p>In the domain of cancer detection, machine learning (ML), deep learning (DL), and transfer learning (TL) have become innovative instruments that hold great promise for early diagnosis and improved outcomes for patients. Massive amounts of medical data, such as genetic information, imaging data, and clinical records, can be swiftly checked by ML algorithms to find tendencies and mutations that can point to cancer. Accurate detection of carcinoma can be further improved by developing intelligent systems that can read and learn from complex data by artificial intelligence (AI). This more general term includes ML. Cancer detection relies heavily on TL, an ML technique that uses pre&#x2010;trained models to speed learning, especially in small datasets. Data preprocessing, feature extraction, model training, and model evaluation contain significant phases in applying ML, DL, and TL in cancer detection. The chapter aims to offer insights into the current state of AI applications in the field, exploring their efficacy and potential impact on enhancing the accuracy and efficiency of cancer diagnosis by examining gene expression patterns.</p>},
  keywords={Cancer;Genomics;Bioinformatics;Artificial intelligence;Sequential analysis;Medical services;Prediction algorithms;Machine learning;Biological neural networks;Libraries},
  doi={10.1002/9781394268832.ch13},
  ISSN={},
  publisher={Wiley},
  isbn={9781394268825},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952931},}@INPROCEEDINGS{10616356,
  author={Raut, Abhishek and Katti, Jayashree},
  booktitle={2024 International Conference on Innovations and Challenges in Emerging Technologies (ICICET)}, 
  title={The Future of Chatbots: Survey on Generative Pretrained Models, Ernie, and Gemini}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The evolution of chatbots from rule-based, retrieval-based, contextual systems, to hybrid and generative AIpowered models. It aimsto solve complex natural language processing and task-specific challenges. The finding explores innovative self-attention mechanisms in the Generative Pretrained Transformer (GPT). The GPT model training and fine-tuning methods include discriminative fine-tuning, zeroshot and few-shot learning, parallel processing, and reinforcement learning with human feedback. Revolutionary multimodal AI models like GPT-4 (Open-AI), Ernie (Baidu), and Gemini (Google). The study discovers the advantages and limitations of chatbots.},
  keywords={Training;Surveys;Technological innovation;Web and internet services;Training data;Reinforcement learning;Chatbots;Chatbots;Transformer;GPT Series;Ernie;Gemini},
  doi={10.1109/ICICET59348.2024.10616356},
  ISSN={},
  month={June},}@INBOOK{10790206,
  author={Babu, R. Ganesh and Glorindal, G. and Maurya, Sudhanshu and Yuvaraj, S. and Karthika, P.},
  booktitle={Semantic Intelligent Computing and Applications}, 
  title={9 Evolutionary computation and streaming analytics machine learning with IoT for urban intelligent systems}, 
  year={2024},
  volume={},
  number={},
  pages={157-182},
  abstract={In the Internet of things (IoT) era, for a wide range of fields and applications, a vast number of detecting gadgets collect or potentially produce different tactile information after a while. These gadgets can cause large or fast/constant streams of information. Examining these information sources in order to find new data predict future bits of knowledge, and decide on control choices is a critical process that makes IoT a commendable worldview for organizations and a personal satisfaction that enhances innovation. In this chapter, we give a careful review on the use of a class of cutting-edge artificial intelligence (AI) systems to advance deep learning (DL), thus enhancing IoT space inspection and encouraging learning. We begin by articulating IoT information attributes and recognizing two essential IoT information medicines from an AI perspective: IoT massive evolutionary computation and streaming information analysis and IoT gushing information review are two expels of IoT. We also explore why DL is a good way to perfectly handle investigation in these kinds of data and applications. The capacity to use DL procedures for investigation of IoT information is then addressed, and its guarantees and difficulties are identified. On various DL systems and measurements, we pose a far-ranging base. We also research and detail major research projects that used DL in IoT space. Furthermore, elegant IoT gadgets that fused DL into their knowledge base are investigated. On the basis of IoT applications, the mist and cloud-based DL implementation approach is also overviewed. We finally shed light on some problems and possible reasons that need further research.},
  keywords={Internet of Things;Artificial intelligence;Streams;Reviews;Information analysis;Deep learning;Technological innovation;Evolutionary computation;Testing;Dispersion},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783110781748},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790206},}@ARTICLE{10856705,
  author={Liao, Yaxin and Wang, Yingze and Cui, Qimei and Chen, Kwang-Cheng and Nan, Guoshun and Tao, Xiaofeng},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Data-Driven Cyber-Physical Anomaly Detection With GAN in Federated Smart Factories}, 
  year={2025},
  volume={21},
  number={4},
  pages={3067-3076},
  abstract={Resilient operation of a wireless networked multirobot system (MRS) in a smart factory relies on the effective detection of physical anomalies from robots and cyber anomalies from wireless transmission errors or imprecise artificial intelligence decisions, which leads to a new technological frontier in data-driven industrial informatics: cyber-physical anomaly detection (AD). Furthermore, data patterns in a single smart factory are unlikely enough to train high-quality learning models for this new cyber-physical AD, which suggests the necessity to utilize operating data from multiple smart factories while keeping the privacy of each factory's data. To overcome the aforementioned technical challenges for cyber-physical AD in smart factories, this article proposes an integral mechanism of generative adversarial networks, federated learning, and fuzzy clustering acceleration. Generative adversarial networks facilitate data imputation to regenerate complete datasets alleviating anomalies caused by wireless communications. Federated learning enables rich privacy-preserving datasets to be jointly used among multiple collaborative factories. Furthermore, fuzzy clustering acceleration is embedded to speed up the factory selection algorithm such that efficient training and real-time physical AD in the large-scale operation of multiple smart factories can be achieved. Extensive computational experiments based on the KDD-99 dataset demonstrate the effective and efficient cyber-physical AD of wireless networked MRS in collaborative multiple smart factories.},
  keywords={Wireless communication;Wireless sensor networks;Smart manufacturing;Production facilities;Service robots;Generative adversarial networks;Data models;Artificial intelligence;Real-time systems;Anomaly detection;Cyber-physical anomaly detection (AD);federated learning (FL);generative adversarial networks (GANs);multirobot system (MRS);smart factory},
  doi={10.1109/TII.2024.3523542},
  ISSN={1941-0050},
  month={April},}@INPROCEEDINGS{11108794,
  author={Wang, Li-Chen and Lin, Ming-Yi and Lee, Yu-Hsuan and Chen, Meng-Chun and Chuang, Chung-Yueh and Liu, Chi-Fan and Hsieh, Sheng-Hsien},
  booktitle={2025 9th International Conference on Robotics and Automation Sciences (ICRAS)}, 
  title={Interactive Companion Robot and AR-Assisted Learning System for Innovative Children's Education}, 
  year={2025},
  volume={},
  number={},
  pages={254-258},
  abstract={The rapid development of digital technologies has introduced new opportunities for children's education. This study presents an AR-assisted interactive learning system designed to support dual-income families by enhancing children's learning experiences and promoting parent-child interaction. The system integrates Time-of-Flight (ToF) cameras, augmented reality (AR), and generative AI (GAI) to create an intuitive, contactless learning environment. Built on projector equipment, the system includes an e-book platform, gesture recognition, and interactive software. By combining ToF and RGB cameras, it enables precise gesture tracking, allowing children to interact with projected content through simple gestures, increasing usability and engagement. GAI further enriches the experience by generating multilingual content, voice narration, and interactive elements. Compared to traditional tools, this system offers a more natural and immersive interaction, boosting focus and supporting cognitive development. It delivers personalized educational resources, making it a promising solution for modern families and a valuable reference for smart education development.},
  keywords={Learning systems;Generative AI;Robot vision systems;Learning (artificial intelligence);Cameras;Software;Multilingual;Usability;Robots;Augmented reality;Augmented Reality;Children's Education;Generative Artificial Intelligence;Gesture Tracking;Interactive Companion Robot;Interactive Learning;Time-of-Flight Camera;Virtual Touch},
  doi={10.1109/ICRAS65818.2025.11108794},
  ISSN={2694-3506},
  month={June},}@INPROCEEDINGS{10873764,
  author={Zheng, Peixiang and Shen, Zhao},
  booktitle={2024 21st International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)}, 
  title={Biologically Inspired Spike-UNet for Enhanced and Efficient Image Generati}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Achieving high-quality image generation remains a challenging problem, particularly when balancing computational efficiency, representational richness, and energy consumption. Traditional deep learning models often lack the biological plausibility and dynamic processing capabilities found in natural neural systems. To address these limitations, we propose SUM (Spike-UNet Model), a novel Spiking Neural Network (SNN) architecture inspired by the U-Net framework By harnessing the temporal dynamics of spiking neurons, SUM aims to enhance image realism while improving computational efficiency. Experiments on benchmark datasets, including MNIST and CIFAR-10, demonstrate that SUM not only outperforms existing SNN architectures but also surpasses conventional Artificial Neural Networks (ANNs) in certain tasks. The architecture effectively captures intricate patterns and textures, resulting in images with enhanced realism and detail. Moreover, its low energy consumption makes it suitable for neuromorphic hardware and edge devices with power constraints. This work underscores the potential of SNNs in computationally intensive visual tasks, demonstrating that biologically inspired architectures can deliver both efficiency and quality. By advancing the integration of spike-based processing into modern AI pipelines, SUM paves the way for more adaptive, sustainable, and high-performing image generation approaches.},
  keywords={Energy consumption;Visualization;Image synthesis;Computational modeling;Biological system modeling;Neurons;Computer architecture;Spiking neural networks;Computational efficiency;Artificial intelligence;Spiking Neural Networks (SNNs);Image Generation;Spike-UNet Model (SUM)},
  doi={10.1109/ICCWAMTIP64812.2024.10873764},
  ISSN={2576-8964},
  month={Dec},}@INBOOK{10954875,
  author={Musiol, Martin},
  booktitle={Generative AI: Navigating the Course to the Artificial General Intelligence Future}, 
  title={Front Matter}, 
  year={2024},
  volume={},
  number={},
  pages={i-xii},
  abstract={<p>The prelims comprise: <ul> <li>Series Page</li> <li>Title Page</li> <li>Copyright Page</li> <li>Dedication</li> <li>Contents</li> <li>Introduction</li> </ul> </p>},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394205950},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10954875},}@INBOOK{10952350,
  author={Musiol, Martin},
  booktitle={Generative AI: Navigating the Course to the Artificial General Intelligence Future}, 
  title={Index}, 
  year={2024},
  volume={},
  number={},
  pages={409-420},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394205950},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952350},}@INBOOK{10790757,
  author={},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={About the editors}, 
  year={2024},
  volume={},
  number={},
  pages={XIII-XIV},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790757},}@INBOOK{10790370,
  author={},
  booktitle={Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI}, 
  title={List of authors}, 
  year={2024},
  volume={},
  number={},
  pages={IX-XII},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783111324166},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10790370},}@ARTICLE{9371691,
  author={Sundaram, K. Mohana and Hussain, Azham and Sanjeevikumar, P. and Holm-Nielsen, Jens Bo and Kaliappan, Vishnu Kumar and Santhoshi, B. Kavya},
  journal={IEEE Access}, 
  title={Deep Learning for Fault Diagnostics in Bearings, Insulators, PV Panels, Power Lines, and Electric Vehicle Applications—The State-of-the-Art Approaches}, 
  year={2021},
  volume={9},
  number={},
  pages={41246-41260},
  abstract={Deep learning (DL) is an exciting field of interest for many researchers and business. Due to a massive leap in DL based research, many domains like Business, science and government sectors make use of DL for various applications. This work puts forward the importance of DL and its application in a few critical electrical segments. Initially, an introduction to Artificial Intelligence (AI) and Machine Learning (ML) is presented. Then the need for DL and the popular architectures, algorithms and frameworks used are presented. A summary of different techniques used in DL is outlined, and finally, a review on the application of deep learning techniques in some popular electrical applications is presented. Five critical electrical applications, namely identification of bearing faults, hot spots on the surface of PV panels, insulator faults, an inspection of power lines and Electric vehicles have been considered for review in this work. The primary aim of this work is to present chronologically, a survey of different areas in which it applies DL along with their architectures, frameworks and techniques to provide a deeper understanding of DL for widespread use in real-time applications.},
  keywords={Artificial intelligence;Training;Real-time systems;Insulators;Deep learning;Speech recognition;Unsupervised learning;Artificial intelligence (AI);deep learning (DL);machine learning (ML);power distribution faults;power system faults;fault diagnosis},
  doi={10.1109/ACCESS.2021.3064360},
  ISSN={2169-3536},
  month={},}@ARTICLE{10379487,
  author={Zheng, Xiaolong and Li, Jingyu and Lu, Mengyao and Wang, Fei-Yue},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={New Paradigm for Economic and Financial Research With Generative AI: Impact and Perspective}, 
  year={2024},
  volume={11},
  number={3},
  pages={3457-3467},
  abstract={In the past few years, we have witnessed the rapid development and exponential growth of generative artificial intelligence (GAI) technologies including large language models (LLMs)-enabled ChatGPT and peripheral innovations. These technologies are designed to be humanlike intelligence and intuitive by providing direct access to systems using application programming interfaces (APIs). The GAI applications can fundamentally change economic and financial activities, through revolutionizing the ways that humans interact with machines and giving rise to new modes of production and behavior patterns. It is imperative to develop a new research paradigm that is more suitable than the currently dominating conventional research paradigms. This article presents the new paradigm for economic and financial research with GAI, covering the research objectives, scientific data, and models, and explores the underlying impact and perspective that bring to this field. We elaborate on the potential five scenarios including portfolio management, economic and financial prediction, extreme scenario analysis, policy analysis, and financial fraud detection. The new research paradigm with GAI proposed in this article can provide significant insights for a comprehensive understanding of innovation and transformation in this domain.},
  keywords={Economics;Biological system modeling;Data models;Task analysis;Behavioral sciences;Big Data;Analytical models;Economics;finance;generative artificial intelligence (GAI);reinforcement learning from human feedback (RLHF);research paradigm},
  doi={10.1109/TCSS.2023.3334306},
  ISSN={2329-924X},
  month={June},}@ARTICLE{10570408,
  author={Yang, Weijia and Shen, Lin and Huang, Chih-Fang and Lee, Johee and Zhao, Xian},
  journal={IEEE Access}, 
  title={Development Status, Frontier Hotspots, and Technical Evaluations in the Field of AI Music Composition Since the 21st Century: A Systematic Review}, 
  year={2024},
  volume={12},
  number={},
  pages={89452-89466},
  abstract={In recent years, “Artificial Intelligence (AI)” has become a focal point of discussion. AI music composition, an interdisciplinary field blending computer science and musicology, has emerged as a prominent area of research. Despite rapid advancements in AI music creation technology, there remains a dearth of comprehensive surveys addressing the core technologies within this domain. To address this gap, this study conducted a comprehensive search across multiple databases spanning a 23-year period (2000–2023) on the topic of “AI music composition.” Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) standard for literature screening, the study systematically organized the development status, frontier hotspots, and technical evaluations of the field. Drawing from literature data, the study verified Price’s Law, Lotka’s Law, and Bradford’s Law—three scientific productivity laws—while summarizing the current landscape from four perspectives: authors, organizations, countries, and journals. Subsequently, utilizing VOSviewer and CiteSpace, two technical software tools, the study conducted an in-depth analysis consisting of four steps: clustering, time zone, burst words, and high-frequency referenced literature. The study presented the evolution trajectory of frontiers and hotspots through visualization maps. Finally, building upon quantitative statistical insights, the study qualitatively expanded research efforts by organizing and evaluating the latest AI music generation algorithm technologies. The systematic literature analysis, both quantitative and qualitative, aims to furnish researchers and practitioners in related fields with systematic references.},
  keywords={Artificial intelligence;Databases;Bibliometrics;Reviews;Systematics;Software tools;Visualization;Music;Automatic programming;Artificial intelligence;automatic music composition;bibliometrics;PRISMA},
  doi={10.1109/ACCESS.2024.3419050},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10309814,
  author={Meng, Han and Wagner, Christian and Triguero, Isaac},
  booktitle={2023 IEEE International Conference on Fuzzy Systems (FUZZ)}, 
  title={An Initial Step Towards Stable Explanations for Multivariate Time Series Classifiers with LIME}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={LIME, or ‘Local Interpretability Model-agnostic Explanations’ is a well-known post-hoc explanation technique for the interpretation of black-box models. While very useful, recent studies show that LIME suffers from stability problems: explanations provided for the same process can be different, making it difficult to trust their reliability. This paper investigates the stability of LIME when explaining multivariate time series classification problems. We demonstrate that due to the temporal dependency in time series data, the traditional artificial neighbour generation methods used in LIME have a higher risk of creating out-of-distribution inputs. We disucss how this behavior is one of the reasons resulting in unstable explanations. In addition, LIME weights neighbours based on user-defined hyperparameters which are problem-dependent and hard to tune, and we show how unsuitable hyperparameters can contribute to the generation of unstable explanations. As a preliminary step towards addressing these issues, we propose to employ a generative approach with an adaptive weighting method in the LIME framework. Specifically, we adopt a generative model based on variational autoencoder to create within-distribution neighbours, reducing the out-of-distribution problem, while the adaptive weight method eliminates the need for user-defined hyperparameters. Experiments on real-world datasets demonstrate the effectiveness of the proposed method in providing more stable explanations using the LIME framework.},
  keywords={Adaptation models;Time series analysis;Closed box;Fuzzy neural networks;Stability analysis;Behavioral sciences;Reliability;Multivariate Time Series Classification;Explainable Artificial Intelligence;LIME;Stability;Trustworthiness},
  doi={10.1109/FUZZ52849.2023.10309814},
  ISSN={1558-4739},
  month={Aug},}@ARTICLE{8807270,
  author={Ji, Rongrong and Li, Ke and Wang, Yan and Sun, Xiaoshuai and Guo, Feng and Guo, Xiaowei and Wu, Yongjian and Huang, Feiyue and Luo, Jiebo},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Semi-Supervised Adversarial Monocular Depth Estimation}, 
  year={2020},
  volume={42},
  number={10},
  pages={2410-2422},
  abstract={In this paper, we address the problem of monocular depth estimation when only a limited number of training image-depth pairs are available. To achieve a high regression accuracy, the state-of-the-art estimation methods rely on CNNs trained with a large number of image-depth pairs, which are prohibitively costly or even infeasible to acquire. Aiming to break the curse of such expensive data collections, we propose a semi-supervised adversarial learning framework that only utilizes a small number of image-depth pairs in conjunction with a large number of easily-available monocular images to achieve high performance. In particular, we use one generator to regress the depth and two discriminators to evaluate the predicted depth, i.e., one inspects the image-depth pair while the other inspects the depth channel alone. These two discriminators provide their feedbacks to the generator as the loss to generate more realistic and accurate depth predictions. Experiments show that the proposed approach can (1) improve most state-of-the-art models on the NYUD v2 dataset by effectively leveraging additional unlabeled data sources; (2) reach state-of-the-art accuracy when the training set is small, e.g., on the Make3D dataset; (3) adapt well to an unseen new dataset (Make3D in our case) after training on an annotated dataset (KITTI in our case).},
  keywords={Estimation;Generators;Training;Image reconstruction;Sensors;Adaptation models;Data models;Monocular depth estimation;generative adversarial learning;semi-supervise learning},
  doi={10.1109/TPAMI.2019.2936024},
  ISSN={1939-3539},
  month={Oct},}@INPROCEEDINGS{10384921,
  author={Buselic, Vjeran},
  booktitle={2023 International Conference on Computing, Networking, Telecommunications & Engineering Sciences Applications (CoNTESA)}, 
  title={Teaching Information Literacy and Critical Thinking Skills in Chat GPT Time}, 
  year={2023},
  volume={},
  number={},
  pages={14-20},
  abstract={In this article, the unprecedented popularity of Chat GPT is acknowledged, generating positive interest but also raising concerns about AI’s potential to replace humans. Despite factual mistakes and hallucinations, Chat GPT’s enduring presence is emphasized, acknowledged with the record-breaking sales growth and Generative AI market projected almost 50% YoY growth. Amid controversies, particularly in education with New York City Public Schools banning its use due to concerns over cheating and setback of critical thinking and slowdown of problem-solving skills development, the author advocates for incorporating Chat GPT into the learning process. The article proposes an ‘upgrade’ of a Class designed in 2016, reinforcing the usage of mental models as the cognitive framework for effective prompt formulation. By effectively using Chat GPT in their Class assignment students will master its use for any given task, simultaneously enforcing generic skills needed for successful study and easier employment.},
  keywords={Ethics;Generative AI;Education;Urban areas;Employment;Problem-solving;Task analysis;Information Literacy;Critical Thinking;Education;Mental Models;Generative AI;Chat GPT;ethical engagement},
  doi={10.1109/CoNTESA61248.2023.10384921},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8995210,
  author={Li, Xiu and Duan, Guichun and Wang, Zhouxia and Ren, Jimmy and Zhang, Yongbing and Zhang, Jiawei and Song, Kaixiang},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Recovering Extremely Degraded Faces by Joint Super-Resolution and Facial Composite}, 
  year={2019},
  volume={},
  number={},
  pages={524-530},
  abstract={In the past a few years, we witnessed rapid advancement in face super-resolution from very low resolution(VLR) images. However, most of the previous studies focus on solving such problem without explicitly considering the impact of severe real-life image degradation (e.g. blur and noise). We can show that robustly recover details from VLR images is a task beyond the ability of current state-of-the-art method. In this paper, we borrow ideas from "facial composite" and propose an alternative approach to tackle this problem. We endow the degraded VLR images with additional cues by integrating existing face components from multiple reference images into a novel learning pipeline with both low level and high level semantic loss function as well as a specialized adversarial based training scheme. We show that our method is able to effectively and robustly restore relevant facial details from 16x16 images with extreme degradation. We also tested our approach against real-life images and our method performs favorably against previous methods.},
  keywords={Degradation;Training;Superresolution;Semantics;Noise;Pipelines;Generative adversarial networks;Image restoration;Artificial intelligence;Faces;super resolution;deep learning;facial composite},
  doi={10.1109/ICTAI.2019.00079},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{10478489,
  author={Tsai, An-Chao and Huang, Patrick Po-Han and Wu, Zhong-Chong and Wang, Jhing-Fa},
  journal={IEEE Access}, 
  title={Advanced Pigmented Facial Skin Analysis Using Conditional Generative Adversarial Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={46646-46656},
  abstract={In recent years, artificial intelligence (AI) approaches in computer vision and medical technology have been combined to create various convenient and accurate tools to assist medical treatments. In this work, we propose conditional generative adversarial networks (conditional GANs)-based pigmented facial skin analysis system for melasma diagnosis. In the past, melasma diagnosis was based on subjective diagnoses from doctors, and there were few automatic melasma analysis methods. The proposed system helps to determine the region according to the melasma’s severity. Areas associated with melasma and hemoglobin are detected to determine whether they may require special treatments. Furthermore, the proposed work cooperates with HUANGDERM dermatology to collect a facial skin pigmented dataset. We divide the dataset into 3,000 groups for training datasets and 678 groups for testing. Each group contains four categories of images: standard white light, polarized light, melanin and hemoglobin distribution. As a result, the proposed system successfully generates melasma and hemoglobin images and performs well with respect to subjective and objective evaluations.},
  keywords={Skin;Image color analysis;Lesions;Generators;Image segmentation;Generative adversarial networks;Pigments;Conditional GANs;hemoglobin;melasma;pigmented facial skin analysis},
  doi={10.1109/ACCESS.2024.3381535},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9124828,
  author={Lai, Xuan and Qu, Yanyun},
  booktitle={2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)}, 
  title={Adversarial Deep Mutual Learning}, 
  year={2019},
  volume={},
  number={},
  pages={324-329},
  abstract={Recently more attention has been paid to neural network compression due to the increasing requirements about limit memory with high performance. The typical methods based on knowledge distillation have to fix a pre-trained teacher model, and most result-orient methods are limited by the teacher model. In this work, we propose a novel lightweight network training method, which apply adversarial learning to deep mutual learning and train multiple networks in just one framework without pre-trained. We focus on the potential relationships among all networks in our framework to form the ingenious loss rather than these manual designed. Extensive experimental results on several datasets illustrate that the proposed method can significantly outperform other state-of-the-art methods.},
  keywords={Training;Autonomous systems;Neural network compression;Manuals;Learning (artificial intelligence);Generative adversarial networks;Adversarial machine learning;Knowledge transfer;lightweight model;adversarial learning;mutual learning},
  doi={10.1109/ICUSAI47366.2019.9124828},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10972941,
  author={Behravan, Majid and Gračanin, Denis},
  booktitle={2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={From Voices to Worlds: Developing an AI-Powered Framework for 3D Object Generation in Augmented Reality}, 
  year={2025},
  volume={},
  number={},
  pages={150-155},
  abstract={This paper presents Matrix, an advanced AI-powered framework designed for real-time 3D object generation in Augmented Reality (AR) environments. By integrating a cutting-edge text-to-3D generative AI model, multilingual speech-to-text translation, and large language models (LLMs), the system enables seamless user interactions through spoken commands. The framework processes speech inputs, generates 3D objects, and provides object recommendations based on contextual understanding, enhancing AR experiences. A key feature of this framework is its ability to optimize 3D models by reducing mesh complexity, resulting in significantly smaller file sizes and faster processing on resource-constrained AR devices. Our approach addresses the challenges of high GPU usage, large model output sizes, and real-time system responsiveness, ensuring a smoother user experience. Moreover, the system is equipped with a pre-generated object repository, further reducing GPU load and improving efficiency. We demonstrate the practical applications of this framework in various fields such as education, design, and accessibility, and discuss future enhancements including image-to-3D conversion, environmental object detection, and multimodal support. The open-source nature of the framework promotes ongoing innovation and its utility across diverse industries.},
  keywords={Solid modeling;Three-dimensional displays;Generative AI;Large language models;Graphics processing units;User interfaces;Real-time systems;User experience;Multilingual;Augmented reality;Augmented reality;generative AI;multilingual speech interaction;large language models;3D object generation},
  doi={10.1109/VRW66409.2025.00038},
  ISSN={},
  month={March},}@INPROCEEDINGS{10774607,
  author={Jaiswal, Anuj and Tiwari, Garima and Jha, Aakash and Mangulkar, Rushikesh and Rani, Pushpi},
  booktitle={2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)}, 
  title={Retrieval Augmented Generation Approach for Multipdf Chatbot using LangChain}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents an AI-driven application designed to facilitate information retrieval and conversational interactions through uploaded PDF documents. In an era where a vast amount of information is contained within PDFs, it is essential to have efficient extraction and interaction mechanisms. The purpose of this paper is to bridge the gap between users and the wealth of knowledge contained in these documents. The research problem is addressed by utilizing advanced AI technologies to streamline document processing and enhance user engagement. The paper emphasizes the importance of efficient information retrieval and user-friendly interfaces in addressing existing challenges in accessing and discussing content within PDF documents. Methodologically, the paper integrates various technologies, including Google’s Generative AI and FAISS, to enable seamless document processing, text segmentation, and embedding generation. Key findings underscore the effectiveness of the implemented methodologies in facilitating interactive communication and extracting relevant information from PDFs. The paper’s conclusions highlight the it’s utility as a versatile tool for knowledge sharing and information retrieval. Limitations are acknowledged, emphasizing the need for ongoing research and improvement in addressing emerging challenges. Overall, this research contributes to advancing AI-driven applications for document processing and interactive communication, paving the way for enhanced knowledge accessibility and user engagement in diverse domains.},
  keywords={Automation;Generative AI;Process control;User interfaces;Portable document format;Information retrieval;Chatbots;User experience;Data mining;Information retrieval;Conversational interactions;PDF documents;Google’s Generative AI;FAISS;Text segmentation;Embedding generation;User-friendly interfaces;Knowledge accessibility},
  doi={10.1109/ICCUBEA61740.2024.10774607},
  ISSN={2771-1358},
  month={Aug},}@ARTICLE{10836866,
  author={Li, Shuangliang and Wang, Jinwei and Wu, Hao and Zhang, Jiawei and Cheng, Xin and Luo, Xiangyang and Ma, Bin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Defense Against Adversarial Faces at the Source: Strengthened Faces Based on Hidden Disturbances}, 
  year={2025},
  volume={6},
  number={7},
  pages={1761-1775},
  abstract={Face recognition (FR) systems, while widely used across various sectors, are vulnerable to adversarial attacks, particularly those based on deep neural networks. Despite existing efforts to enhance the robustness of FR models, they still face the risk of secondary adversarial attacks. To address this, we propose a novel approach employing “strengthened face” with preemptive defensive perturbations. Strengthened face ensures original recognition accuracy while safeguarding FR systems against secondary attacks. In the white-box scenario, the strengthened face utilizes gradient-based and optimization-based methods to minimize feature representation differences between face pairs. For the black-box scenario, we propose shielded gradient sign descent (SGSD) to optimize the gradient update direction of strengthened faces, ensuring the transferability and effectiveness against unknown adversarial attacks. Experimental results demonstrate the efficacy of strengthened faces in defending against adversarial faces without compromising the performance of FR models or face image visual quality. Moreover, SGSD outperforms conventional methods, achieving an average performance improvement of 4% in transferability across different attack intensities.},
  keywords={Faces;Face recognition;Perturbation methods;Closed box;Security;Glass box;Artificial intelligence;Generative adversarial networks;Deep learning;Visualization;Adversarial attacks;face recognition;facial protection},
  doi={10.1109/TAI.2025.3527923},
  ISSN={2691-4581},
  month={July},}@INPROCEEDINGS{11152472,
  author={Gross, David C. and Beddingfield, R. Byron and Popovski, Anthony and Lewis, Patrick},
  booktitle={2025 IEEE Electric Ship Technologies Symposium (ESTS)}, 
  title={A Case Study in Scalable, Low-Cost Deployment of a Digital Ecosystem for Engineering Complex Systems}, 
  year={2025},
  volume={},
  number={},
  pages={299-307},
  abstract={The transition to more highly integrated naval vessels with significant, high power electric power systems for advanced sensors, weapons, and propulsion boost introduces significant complexity in managing power distribution, thermal control, and cross-domain system interactions-necessitating a cohesive digital engineering approach. This paper presents a lowcost, rapidly deployable Digital Ecosystem architecture tailored to support model-based systems engineering (MBSE) and digital twin development in electric ship programs. Aligned with DoDI 5000.97, the study implements a “three-legged” integration strategy using CSV files, Python scripts, and MATLAB routines to connect a SysML system model in Cameo with external analytical tools. This architecture enables seamless validation of system behavior and performance against requirements, including integration with CAD-derived geometries and physics-based simulations such as antenna pattern and structural analysis. Generative AI (ChatGPT) was used to accelerate model development by transforming legacy documents into structured, import-ready datasets. The result is a scalable, interoperable framework for digital thread and digital twin workflows that supports the agility, traceability, and cross-discipline coordination demanded by modern electric ship design and sustainment.},
  keywords={Analytical models;Generative AI;Weapons;Ecosystems;Thermal engineering;Thermal sensors;Thermal management;Mathematical models;Digital twins;Marine vehicles;electric ship;digital engineering;MBSE;SysML;technology integration;digital twin;digital thread;generative AI},
  doi={10.1109/ESTS62818.2025.11152472},
  ISSN={2768-3508},
  month={Aug},}@INPROCEEDINGS{11076664,
  author={S, Senthil Pandi and S, Murshid Ahmed and P, Nikhil and R, Karthick},
  booktitle={2025 Global Conference in Emerging Technology (GINOTECH)}, 
  title={Enhanced LegalTech: AI Voice Assistant for Legal Queries with Dynamic Lawyer Recommendations}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This Research gives an AI-pushed voice assistant that gives prison information the use of Generative AI and herbal Language Processing (NLP) technology, mainly targeted on Indian laws. The assistant is designed to offer real-time prison causes primarily based totally on consumer activates and inquiries, helping each English and Tamil. By leveraging a big Language version (LLM) skilled on Indian prison documents, the machine can offer unique regulation sections, give an explanation for their implications, and provide steerage. Additionally, it complements accessibility through supplying responses in spoken Tamil, making prison statistics greater inclusive for nearby language speakers. A key function of this undertaking is the change of the set of rules primarily based totally on epoch, which improves the version choice manner for extra accuracy. The machine additionally indicates a listing of practising attorneys in India primarily based totally at the consumer’s prison difficulty and location, with real-time updates on to be had practitioners. This voice assistant addresses the constraints posed through conventional text-primarily based totally prison statistics systems, streamlining the prison question manner and making prison steerage greater accessible, specifically in rural groups in India.},
  keywords={Law;Navigation;Generative AI;Large language models;Personal voice assistants;Natural language processing;Real-time systems;Regulation;Multilingual;Guidelines;Keywords-Generative AI;Natural Language Processing (NLP);Indian Law;Large Language Model (LLM);Speech Recognition;Text-to-Speech (TTS);Tamil Language Support;Lawyer Recommendation System;LegalTech;Algorithm Optimization;Epoch-based Selection},
  doi={10.1109/GINOTECH63460.2025.11076664},
  ISSN={},
  month={May},}@ARTICLE{11046201,
  author={Roy, Prasun and Bhattacharya, Saumik and Ghosh, Subhankar and Pal, Umapada and Blumenstein, Michael},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Human affordance learning investigates contextually relevant novel pose prediction such that the estimated pose represents a valid human action within the scene. While the task is fundamental to machine perception and automated interactive navigation agents, the exponentially large number of probable pose and action variations make the problem challenging and non-trivial. However, the existing datasets and methods for human affordance prediction in 2D scenes are significantly limited in the literature. In this paper, we propose a novel cross-attention mechanism to encode the scene context for affordance prediction by mutually attending spatial feature maps from two different modalities. The proposed method is disentangled among individual subtasks to efficiently reduce the problem complexity. First, we sample a probable location for a person within the scene using a variational autoencoder (VAE) conditioned on the global scene context encoding. Next, we predict a potential pose template from a set of existing human pose candidates using a classifier on the local context encoding around the predicted location. In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class. Our experiments show significant improvements over the previous baseline of human affordance injection into complex 2D scenes.},
  keywords={Affordances;Semantics;Feature extraction;Artificial intelligence;Encoding;Deformation;Generative adversarial networks;Visualization;Root mean square;Generators;Affordance generation;cross-attention;VAE},
  doi={10.1109/TAI.2025.3581897},
  ISSN={2691-4581},
  month={},}@ARTICLE{10746445,
  author={Liu, Tsung-Jung and Wang, Chia-Ching},
  journal={IEEE Access}, 
  title={Face Aging Synthesis by Deep Cycle Generative Adversarial Networks and Bias Loss}, 
  year={2024},
  volume={12},
  number={},
  pages={166439-166458},
  abstract={In this work, we propose a deep learning-based adversarial network for human face aging synthesis. We use an image dataset consisting of young (Domain X) and elderly (Domain Y) individuals to generate aged images at various target ages. Our network employs two adversarial components: a generator that synthesizes aged images and a discriminator that verifies their authenticity. Through adversarial training, the generator iteratively improves its output based on feedback from the discriminator, ultimately achieving a Nash equilibrium where the generated images closely match the target ages. Moreover, we integrate deep ResNet blocks in the generator, with the best image quality achieved using a 7-layer configuration. To address artifacts caused by overly strict discriminators, we introduce a novel bias loss function that relaxes the discriminator’s harshness, resulting in more realistic aging effects. Additionally, we incorporate perceptual loss using a pre-trained VGG16 network to preserve identity features during age progression. Experimental results show that the proposed architecture significantly enhances aging realism and identity preservation, outperforming existing models both qualitatively and quantitatively. The source code and pre-trained models are available at (https://github.com/Tony00728/FAS-by-CGAN-and-BiasLoss).},
  keywords={Aging;Generative adversarial networks;Generators;Training;Accuracy;Vectors;Testing;Skin;Facial features;Deep learning;Unsupervised learning;Deep learning;face aging;generative adversarial network (GAN);residual block (Resblock);unsupervised learning},
  doi={10.1109/ACCESS.2024.3493376},
  ISSN={2169-3536},
  month={},}@ARTICLE{10994352,
  author={Khan, Salabat and Khan, Mansoor and Khan, Muhammad Asghar and Luo, Fei and Usman, Muhammad and Irshad, Azeem and Wu, Kaishun and Wang, Lu and Khan, Muhammad Attique},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={PBatch: Pseudonym Certificate Batch Authentication With Generative AI-Based Cache for Cooperative Intelligent Transportation Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Authentication and revocation are the key mechanisms to ensure the security of the Cooperative Intelligent Transportation System (C-ITS). C-ITS relies on the Vehicular Public Key Infrastructure (VPKI) for anonymous authentication and device revocation. Several works complemented the VPKI-based authentication and revocation process. However, several security and performance issues exist in both mechanisms. This article presents PBatch: Pseudonym Certificate Batch Authentication based on Distributed Ledger Technology. PBatch addresses challenges specific to the authentication and revocation process to achieve 1000 authentications per second. PBatch relies on the concept of batching pseudonym certificates by offloading heavy validation operations such as certificate chain and revocation status validation to local edge servers. This enables vehicles to validate a batch of pseudonym certificates with a fixed number of verification operations, thus simplifying the authentication of the pseudonym certificate at the end devices. Furthermore, a caching-based message authentication mechanism is introduced to validate a relatively larger number of safety messages. We also introduced a Generative Artificial Intelligence (GAI) based cache management mechanism for safety messages caching and fetching. Finally, experiments and security analysis are conducted to investigate PBatch performance and security. The results show that PBatch is more secure, feasible, and scalable than the leading VPKI-based authentication proposals.},
  keywords={Authentication;Security;Electronic mail;Vehicle-to-everything;Protocols;Privacy;Servers;Public key;Internet of Things;Urban areas;Authentication;batching;generative artificial intelligence;vehicular public key infrastructure;cooperative intelligent transportation system},
  doi={10.1109/TITS.2025.3558366},
  ISSN={1558-0016},
  month={},}@INPROCEEDINGS{10402256,
  author={Ran, Junyi},
  booktitle={2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Evaluation of Data Calculus Quality Based on Computer Intelligent Image Recognition Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={557-561},
  abstract={In recent years, computer intelligent image recognition technology has developed rapidly and has been widely used in various fields. Its main research direction is text-based and object-oriented. This paper uses computer vision recognition algorithm to preprocess the collected pictures. Firstly, the contents and key technologies involved in the paper are introduced. Secondly, the defects of the method in practical application are analyzed in detail and the solutions are given. Finally, the image matching process and the algorithm design and implementation of the result operation feature extraction are briefly described, and the test results are obtained. The results show that the data calculus based on the computer intelligent image recognition algorithm performs very well in quality, speed, time and efficiency, The calculation efficiency basically reaches more than 90%. This shows that the data calculus quality evaluation model based on the recognition algorithm can meet the basic needs of users.},
  keywords={Industries;Computer vision;Image segmentation;Image recognition;Machine learning algorithms;Feature extraction;Calculus;computer intelligent image;image recognition algorithm;data calculus;quality evaluation},
  doi={10.1109/CICN59264.2023.10402256},
  ISSN={2472-7555},
  month={Dec},}@INPROCEEDINGS{11102522,
  author={Das, Sunanda},
  booktitle={2025 International Conference on Networks and Cryptology (NETCRYPT)}, 
  title={Generative AI and Sustainability Optimizing Retail Supply Chain}, 
  year={2025},
  volume={},
  number={},
  pages={472-477},
  abstract={The kingdom is changing previous than we continuously hypothetical probable. Powerful original knowhows bear sacking conservatively uncooperative before difficult proceedings, creation a large change in what technique we alive previously effort. Currently is the period meant at skills to revenue a robed lengthy arrival on in what way they're employed formerly in pardon method to industrialized a part of this enormous increase of innovation? A productive occupational strategy often is depending on being a first Digital change through adopters of these new services, which is added goal vital to pay maintenance to innovations parallel artificial intelligence (AI), block chain, as well cyber security. Numerical / digital change ensues the minute alphanumeric data is collective bowed on a industry's goods, dealings, in accumulation products to gain waged proficiency, rally patron facts, expand curved on different markets, then as well realize peril. Convinced corporations power detection it inspiring to bear arithmetical change. However thru method of arithmetical services alike the haze, IoT, AI, big data, also portable continue increasingly used in extra subdivisions of saleable before development, it eats grow robust that digitalization is the only idea that ampule give skills a shy advantage. Sole of the chief interests of relaxed AI is its aptitude to assistance businesses delivers healthier purchaser provision. This software permits manufactures to automate errands analogous replying investigations or responding to comments on shared TV seats similar Facebook besides Twitter. This pays they won't vital by technique of many folks working in sound canters or organization infrastructures afterward patrons announcement up old-fashioned intended at runs who vessel do slightly diverse supplementary inspired via their dated. In today's arithmetical creation, employments recurrently attendance used for behaviours to tradition facts to rally their procurer data. Artificial intelligence (AI) is a convincing tool to provision finished this box. The main goals of this education are to discovery out: tranquil AI destined at improved customer delivery; powerful aide's support nearby bonds former; uniqueness society; clash resolve; documentations party; Designed for this task regular crucial cystography as well as lopsided key cryptography, mess function——these arrangements will be castoff.},
  keywords={Technological innovation;TV;Social networking (online);Supply chains;Organizations;Software;Hardware;Maintenance;Artificial intelligence;Sustainable development;CPU mining;Mining Hardware;FPGAs and ASICs;Mining cloud services;sustainability},
  doi={10.1109/NETCRYPT65877.2025.11102522},
  ISSN={},
  month={May},}@ARTICLE{10145414,
  author={Wang, Clinton J. and Rost, Natalia S. and Golland, Polina},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Spatial-Intensity Transforms for Medical Image-to-Image Translation}, 
  year={2023},
  volume={42},
  number={11},
  pages={3362-3373},
  abstract={Image-to-image translation has seen major advances in computer vision but can be difficult to apply to medical images, where imaging artifacts and data scarcity degrade the performance of conditional generative adversarial networks. We develop the spatial-intensity transform (SIT) to improve output image quality while closely matching the target domain. SIT constrains the generator to a smooth spatial transform (diffeomorphism) composed with sparse intensity changes. SIT is a lightweight, modular network component that is effective on various architectures and training schemes. Relative to unconstrained baselines, this technique significantly improves image fidelity, and our models generalize robustly to different scanners. Additionally, SIT provides a disentangled view of anatomical and textural changes for each translation, making it easier to interpret the model’s predictions in terms of physiological phenomena. We demonstrate SIT on two tasks: predicting longitudinal brain MRIs in patients with various stages of neurodegeneration, and visualizing changes with age and stroke severity in clinical brain scans of stroke patients. On the first task, our model accurately forecasts brain aging trajectories without supervised training on paired scans. On the second task, it captures associations between ventricle expansion and aging, as well as between white matter hyperintensities and stroke severity. As conditional generative models become increasingly versatile tools for visualization and forecasting, our approach demonstrates a simple and powerful technique for improving robustness, which is critical for translation to clinical settings. Source code is available at github.com/clintonjwang/spatial-intensity-transforms.},
  keywords={Transforms;Generators;Biomedical imaging;Task analysis;Brain modeling;Biological system modeling;Predictive models;Spatial-intensity transform;image-to-image translation;generative adversarial network;longitudinal image prediction;counterfactual image generation},
  doi={10.1109/TMI.2023.3283948},
  ISSN={1558-254X},
  month={Nov},}@INPROCEEDINGS{10762111,
  author={Xu, ShengNan and Dumlao, Menchita F.},
  booktitle={2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)}, 
  title={Face Image Super-resolution based on GAN Constrained by Residual Dense Network and Channel Attention Mechanism}, 
  year={2024},
  volume={},
  number={},
  pages={291-294},
  abstract={Face image carries the key feature recognition information of human, and has wide application in intelligent security, electronic payment, public security and other fields. High resolution face images often contain more real and abundant face features. How to acquire high resolution face images quickly, efficiently and at low cost has become a hot research topic in the field of image super resolution reconstruction. In this paper, the generation model of SRGAN algorithm is improved to solve the problems of the traditional super-resolution reconstruction algorithm, such as too smooth reconstruction image and insufficient detail reconstruction ability. The Residual Dense Network (RDN) and Channel Attention (CA) Mechanism is used to form a joint unit, and dense connections are adopted between the joint learning units to fully retain as much shallow information as possible. CA algorithm can extract the key information of face image effectively. The loss function in this paper includes three parts: pixel loss, feature loss and countermeasure loss, which can not only ensure that the reconstructed image has as complete content information as possible, but also ensure that the reconstructed image is consistent with the real HR image in terms of high-level semantic features. Experiments show that the image reconstructed by the proposed algorithm has more detailed information in terms of visual effects. The average peak signal-to-noise ratio(PSNR) of the proposed algorithm on the test data set is 30.5109, the average structural similarity(SSIM) index is 0.8363, and the average LPIPS index is 0.0504, which is superior to other classical algorithms.},
  keywords={Face recognition;Superresolution;Software algorithms;Reconstruction algorithms;Generative adversarial networks;Feature extraction;Visual effects;Indexes;Image reconstruction;Software engineering;component;Residual Dense Network;Channel Attention (CA) Mechanism;face image},
  doi={10.1109/ICBASE63199.2024.10762111},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9943612,
  author={Huang, Jiaorui and Yang, Chungang and Kou, Shiwen and Song, Yanbo},
  booktitle={2022 27th Asia Pacific Conference on Communications (APCC)}, 
  title={A Brief Survey and Implementation on AI for Intent-Driven Network}, 
  year={2022},
  volume={},
  number={},
  pages={413-418},
  abstract={Intent-driven network (IDN, or intent-based network, IBN) is a novel networking paradigm, which can enable user intents to drive network management autonomously and improve the network’s operational efficiency. Although artificial intelligence (AI) has been found for several applications to the IDN, there lacks a systematic discussion and research on this topic. In this work, we present a survey of the application of AI at each layer of IDN. Then, a general IDN management architecture, State-Action-Intent (SAI), is proposed. The presented SAI is a new IDN implement framework to automate the operational intents in a closed loop to overcome the challenges of complex network services. To verify the availability and effectiveness of SAI, a proof-of-concept demonstration is provided, and the obtained performance is discussed.},
  keywords={Systematics;Asia;Complex networks;Network architecture;Artificial intelligence;Artificial Intelligence;Intent-driven Network;Network Architecture},
  doi={10.1109/APCC55198.2022.9943612},
  ISSN={2163-0771},
  month={Oct},}@ARTICLE{9785655,
  author={Mandelli, Sara and Cozzolino, Davide and Cannas, Edoardo D. and Cardenuto, João P. and Moreira, Daniel and Bestagini, Paolo and Scheirer, Walter J. and Rocha, Anderson and Verdoliva, Luisa and Tubaro, Stefano and Delp, Edward J.},
  journal={IEEE Access}, 
  title={Forensic Analysis of Synthetically Generated Western Blot Images}, 
  year={2022},
  volume={10},
  number={},
  pages={59919-59932},
  abstract={The widespread diffusion of synthetically generated content is a serious threat that needs urgent countermeasures. As a matter of fact, the generation of synthetic content is not restricted to multimedia data like videos, photographs or audio sequences, but covers a significantly vast area that can include biological images as well, such as western blot and microscopic images. In this paper, we focus on the detection of synthetically generated western blot images. These images are largely explored in the biomedical literature and it has been already shown they can be easily counterfeited with few hopes to spot manipulations by visual inspection or by using standard forensics detectors. To overcome the absence of publicly available data for this task, we create a new dataset comprising more than 14K original western blot images and 24K synthetic western blot images, generated using four different state-of-the-art generation methods. We investigate different strategies to detect synthetic western blots, exploring binary classification methods as well as one-class detectors. In both scenarios, we never exploit synthetic western blot images at training stage. The achieved results show that synthetically generated western blot images can be spot with good accuracy, even though the exploited detectors are not optimized over synthetic versions of these scientific images. We also test the robustness of the developed detectors against post-processing operations commonly performed on scientific images, showing that we can be robust to JPEG compression and that some generative models are easily recognizable, despite the application of editing might alter the artifacts they leave.},
  keywords={Probabilistic logic;Detectors;Forensics;Training;Data models;Robustness;Generative adversarial networks;Western blots;GANs;denoising diffusion probabilistic models;synthetically generated images;image forensics},
  doi={10.1109/ACCESS.2022.3179116},
  ISSN={2169-3536},
  month={},}@ARTICLE{9857944,
  author={Liu, Yanan and Zhang, Libao},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Weakly Supervised Region of Interest Extraction Based on Uncertainty-Aware Self-Refinement Learning for Remote Sensing Images}, 
  year={2022},
  volume={60},
  number={},
  pages={1-16},
  abstract={Region of interest (ROI) extraction plays a significant role in the field of remote sensing image (RSI) processing. Recently, weakly supervised ROI extraction methods have attracted considerable attention due to low labeling cost. Most of them follow the pipeline of first generating pseudo labels and then using the pseudo labels to train a segmentation model. However, there remain problems to be solved: 1) the unbalanced distribution of foreground and background samples in the RSI dataset influences the network performance; 2) the pseudo labels mainly cover the most discriminative part of object regions that are incomplete; and 3) training with pseudo labels inevitably causes noise issues that degrade the model performance. To solve these issues, we propose a weakly supervised uncertainty-aware self-refinement learning (UASRL) method, where the initial unbalanced image-level labels are progressively refined to high-quality pixel-level annotations. In the proposed UASRL, we first present a deep generative model combined with self-attention modules to improve the unbalanced distribution in the weakly labeled dataset. Then, we design a confidence-weighted complementary erasing-based weakly supervised method to generate pseudo labels with high integrity. Finally, for training with noisy pseudo labels, we develop an uncertainty-aware joint optimization (UAJO) training strategy to reduce the negative effect caused by noisy labels and further refine pixelwise labels in a coarse-to-accurate manner, which in turn jointly promotes the model’s performance. Extensive experiments on three types of RSI datasets reveal that our proposed method is superior to other competing methods and shows a preferable tradeoff between annotation cost and detection performance.},
  keywords={Training;Annotations;Data models;Generative adversarial networks;Noise measurement;Costs;Analytical models;Region of interest (ROI) extraction;remote sensing images (RSIs);saliency analysis;weakly supervised learning},
  doi={10.1109/TGRS.2022.3199028},
  ISSN={1558-0644},
  month={},}@ARTICLE{9505697,
  author={Xu, Mingle and Lee, Jaehwan and Fuentes, Alvaro and Park, Dong Sun and Yang, Jucheng and Yoon, Sook},
  journal={IEEE Access}, 
  title={Instance-Level Image Translation With a Local Discriminator}, 
  year={2021},
  volume={9},
  number={},
  pages={111802-111813},
  abstract={Instance-level image translation aims to only translate instance of interest and can be operated more finely and flexibly than object-level and holistic-level image translation. However, current algorithms are not suitable to do it since they employ a holistic or object level’s discriminator that tends to change the whole image or all instances. To address the issue, we propose a simple yet effective local discriminator, in which the input image is split into two parts, region of interest (ROI) and background. Instance mask is employed to align the ROI and the background is design to be random in a prior distribution to mitigate a divergence between the ROI and the background. In this way, we obtain translated instance with decent margins without artifacts as current algorithms get. Moreover we propose a new architecture to simultaneously realize versatile instance-level image translation. Experimental results prove that our proposed algorithm outperforms the state-of-the-art in position accuracy and background retainment by a clear margin.},
  keywords={Generators;Generative adversarial networks;Feature extraction;Image synthesis;Training data;Semantics;Image translation;local discriminator;generative adversarial network},
  doi={10.1109/ACCESS.2021.3102263},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9157770,
  author={Shen, Yuming and Qin, Jie and Chen, Jiaxin and Yu, Mengyang and Liu, Li and Zhu, Fan and Shen, Fumin and Shao, Ling},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Auto-Encoding Twin-Bottleneck Hashing}, 
  year={2020},
  volume={},
  number={},
  pages={2815-2824},
  abstract={Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods. Our source code can be found at https://github.com/ymcidence/TBH.},
  keywords={Decoding;Encoding;Binary codes;Training;Image reconstruction;Neurons;Data structures},
  doi={10.1109/CVPR42600.2020.00289},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9171481,
  author={Jia, Sen and Lin, Zhijie and Xu, Meng and Huang, Qiang and Zhou, Jun and Jia, Xiuping and Li, Qingquan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={A Lightweight Convolutional Neural Network for Hyperspectral Image Classification}, 
  year={2021},
  volume={59},
  number={5},
  pages={4150-4163},
  abstract={In the hyperspectral image, each pixel corresponds to a small area on the Earth's surface and represents the intrinsic characteristic of objects, which can be applied for recognition of land covers. Nevertheless, hyperspectral image processing should face some critical issues, and a small sample set problem may be the most challenging one in the research. Deep learning (DL), which has successfully been applied in many fields, has also been introduced for hyperspectral image classification. However, the large gap between the massive parameters to be tuned and limited labeled samples can lead to overfitting scenario, inevitably deteriorating the generalization ability of the DL model. In this article, a lightweight convolutional neural network (LWCNN) is proposed for hyperspectral image classification to mainly tackle the small sample set problem. Especially, spatial-spectral Schroedinger eigenmaps (SSSE) feature extraction is first adopted to obtain the joint spatial-spectral information, and the compressed dimensionality could significantly reduce the number of parameters in the following DL model. Second, a dual-scale convolution (DSC) module is carefully designed to address the SSSE features from a 1-D vector viewpoint (the number of parameters is further decreased), and the DSC procedure is successively employed to obtain the hierarchical structure description that could represent data distribution from different aspects. Subsequently, the feature vectors from all DSC layers are separately filtered by a new bichannel fusion (BCF) module, which could well encode both the intrinsic and contextual information inside DSC features. Finally, the filtered features are concatenated together and imported into a global average pooling classifier to achieve the predicted probability of each category. Experimental results on three famous hyperspectral image data sets illustrate that the developed LWCNN approach is advantageous in both the efficiency and robustness sides for hyperspectral image classification tasks and outperforms other state-of-the-art methods (both traditional-based and DL-based) with very limited labeled samples.},
  keywords={Hyperspectral imaging;Feature extraction;Convolution;Convolutional neural networks;Deep learning;Principal component analysis;Deep learning (DL);hyperspectral imagery},
  doi={10.1109/TGRS.2020.3014313},
  ISSN={1558-0644},
  month={May},}@ARTICLE{10103611,
  author={Martín-Isla, Carlos and Campello, Víctor M. and Izquierdo, Cristian and Kushibar, Kaisar and Sendra-Balcells, Carla and Gkontra, Polyxeni and Sojoudi, Alireza and Fulton, Mitchell J. and Arega, Tewodros Weldebirhan and Punithakumar, Kumaradevan and Li, Lei and Sun, Xiaowu and Al Khalil, Yasmina and Liu, Di and Jabbar, Sana and Queirós, Sandro and Galati, Francesco and Mazher, Moona and Gao, Zheyao and Beetz, Marcel and Tautz, Lennart and Galazis, Christoforos and Varela, Marta and Hüllebrand, Markus and Grau, Vicente and Zhuang, Xiahai and Puig, Domenec and Zuluaga, Maria A. and Mohy-ud-Din, Hassan and Metaxas, Dimitris and Breeuwer, Marcel and van der Geest, Rob J. and Noga, Michelle and Bricq, Stephanie and Rentschler, Mark E. and Guala, Andrea and Petersen, Steffen E. and Escalera, Sergio and Palomares, José F. Rodríguez and Lekadir, Karim},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Deep Learning Segmentation of the Right Ventricle in Cardiac MRI: The M&Ms Challenge}, 
  year={2023},
  volume={27},
  number={7},
  pages={3302-3313},
  abstract={In recent years, several deep learning models have been proposed to accurately quantify and diagnose cardiac pathologies. These automated tools heavily rely on the accurate segmentation of cardiac structures in MRI images. However, segmentation of the right ventricle is challenging due to its highly complex shape and ill-defined borders. Hence, there is a need for new methods to handle such structure's geometrical and textural complexities, notably in the presence of pathologies such as Dilated Right Ventricle, Tricuspid Regurgitation, Arrhythmogenesis, Tetralogy of Fallot, and Inter-atrial Communication. The last MICCAI challenge on right ventricle segmentation was held in 2012 and included only 48 cases from a single clinical center. As part of the 12th Workshop on Statistical Atlases and Computational Models of the Heart (STACOM 2021), the M&Ms-2 challenge was organized to promote the interest of the research community around right ventricle segmentation in multi-disease, multi-view, and multi-center cardiac MRI. Three hundred sixty CMR cases, including short-axis and long-axis 4-chamber views, were collected from three Spanish hospitals using nine different scanners from three different vendors, and included a diverse set of right and left ventricle pathologies. The solutions provided by the participants show that nnU-Net achieved the best results overall. However, multi-view approaches were able to capture additional information, highlighting the need to integrate multiple cardiac diseases, views, scanners, and acquisition protocols to produce reliable automatic cardiac segmentation algorithms.},
  keywords={Image segmentation;Pathology;Deep learning;Magnetic resonance imaging;Bioinformatics;Myocardium;Computer architecture;Cardiovascular magnetic resonance;data augmentation;image segmentation;multi-view segmentation;public dataset},
  doi={10.1109/JBHI.2023.3267857},
  ISSN={2168-2208},
  month={July},}@ARTICLE{9943552,
  author={Lu, Xiaoqiang and Jiao, Licheng and Liu, Fang and Yang, Shuyuan and Liu, Xu and Feng, Zhixi and Li, Lingling and Chen, Puhua},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Simple and Efficient: A Semisupervised Learning Framework for Remote Sensing Image Semantic Segmentation}, 
  year={2022},
  volume={60},
  number={},
  pages={1-16},
  abstract={Semantic segmentation based on deep learning has achieved impressive results in recent years, but these results are supported by a large amount of labeled data, which requires intensive annotation at the pixel level, particularly for high-resolution remote sensing (RS) images. In this work, we propose a simple yet efficient semisupervised learning framework based on linear sampling (LS) self-training, named LSST, to improve the performance of RS image semantic segmentation. Specifically, the classical pseudolabeling-based self-training paradigm is enhanced by injecting strong data augmentations (SDAs) applicable to RS images, based on which a powerful baseline is constructed. Nevertheless, the problem of insufficient data training to generate pseudolabels with a high level of noise persists, and the noisy pseudolabels will continue to accumulate and impede model improvement during the retraining phase. Previous works commonly employ a predefined threshold to remove noise, but it will lead to overfitting the model to easily identified classes. To address it, a method using LS is presented for assigning thresholds to different classes in an adaptive manner, which provides noiseless regions for retraining. Experiments prove that the proposed pixelwise selection is more available for segmentation than image-level selection in RS images. Finally, LSST achieves state of the art on several datasets and different evaluation metrics. The source code of this article is available at https://github.com/xiaoqiang-lu/LSST.},
  keywords={Data models;Remote sensing;Perturbation methods;Semantics;Predictive models;Semisupervised learning;Training;Data augmentation;remote sensing (RS) images;self-training;semantic segmentation;semisupervised learning (SSL)},
  doi={10.1109/TGRS.2022.3220755},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10204851,
  author={Bai, Weiming and Liu, Yufan and Zhang, Zhipeng and Li, Bing and Hu, Weiming},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={AUNet: Learning Relations Between Action Units for Face Forgery Detection}, 
  year={2023},
  volume={},
  number={},
  pages={24709-24719},
  abstract={Face forgery detection becomes increasingly crucial due to the serious security issues caused by face manipulation techniques. Recent studies in deepfake detection have yielded promising results when the training and testing face forgeries are from the same domain. However, the problem remains challenging when one tries to generalize the detector to forgeries created by unseen methods during training. Observing that face manipulation may alter the relation between different facial action units (AU), we propose the Action-Units Relation Learning framework to improve the generality of forgery detection. In specific, it consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). The ART constructs the relation between different AUs with AU-agnostic Branch and AU-specific Branch, which complement each other and work together to exploit forgery clues. In the Tampered AU Prediction, we tamper AU-related regions at the image level and develop challenging pseudo samples at the feature level. The model is then trained to predict the tampered AU regions with the generated location-specific supervision. Experimental results demonstrate that our method can achieve state-of-the-art performance in both the in-dataset and cross-dataset evaluations.},
  keywords={Training;Gold;Subspace constraints;Predictive models;Transformers;Forgery;Security;Humans: Face;body;pose;gesture;movement},
  doi={10.1109/CVPR52729.2023.02367},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9743385,
  author={Song, Yucheng and Li, Jingrun and Gao, Peng and Li, Linfeng and Tian, Tian and Tian, Jinwen},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Two-Stage Cross-Modality Transfer Learning Method for Military-Civilian SAR Ship Recognition}, 
  year={2022},
  volume={19},
  number={},
  pages={1-5},
  abstract={Military-civilian attribute recognition of ships in synthetic aperture radar (SAR) imagery plays an important role in marine surveillance. However, high-quality labeled data are hard to obtain for SAR ships, which hinder the development of deep learning models. Considering that models directly transferred from labeled optical images cannot achieve satisfactory performance for SAR applications due to the great discrepancy of different modalities, we propose a two-stage transfer learning method by combining the data-level and feature-level knowledge transfer. First, CycleGAN is adopted in the first stage to transfer the labeled optical image domain to the intermediate SAR-like image domain with the attribute labels. Then, a novel network called Domain Transfer using Adversarial learning and Metric learning (DTAM) is proposed to realize the task of military-civilian ship recognition by the domain adaption of the intermediate domain and the target SAR domain with joint adversarial learning and metric learning. To validate the proposed method, we establish a high-resolution SAR ship recognition dataset (HRSSRD), containing SAR and optical images of military and civilian ships. The experimental results show that the proposed two-stage architecture exhibits promising performance on the problem of SAR military-civilian ship recognition.},
  keywords={Optical imaging;Synthetic aperture radar;Marine vehicles;Optical sensors;Image recognition;Task analysis;Radar polarimetry;Domain adaption;military-civilian ship recognition;optical remote sensing images;synthetic aperture radar (SAR) imagery;transfer learning},
  doi={10.1109/LGRS.2022.3162707},
  ISSN={1558-0571},
  month={},}@ARTICLE{9511491,
  author={Mo, Xianbo and Tan, Shunquan and Li, Bin and Huang, Jiwu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={MCTSteg: A Monte Carlo Tree Search-Based Reinforcement Learning Framework for Universal Non-Additive Steganography}, 
  year={2021},
  volume={16},
  number={},
  pages={4306-4320},
  abstract={Recent research has shown that non-additive image steganographic frameworks effectively improve security performance through adjusting distortion distribution. However, as far as we know, all of the existing non-additive proposals are based on handcrafted policies, and can only be applied to a specific image domain, which heavily prevent non-additive steganography from releasing its full potentiality. In this paper, we propose an automatic non-additive steganographic distortion learning framework called MCTSteg to remove the above restrictions. Guided by the reinforcement learning paradigm, we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental model to build MCTSteg. MCTS makes sequential decisions to adjust distortion distribution without human intervention. Our proposed environmental model is used to obtain feedbacks from each decision. Due to its self-learning characteristic and domain-independent reward function, MCTSteg has become the first reported universal non-additive steganographic framework which can work in both spatial and JPEG domains. Extensive experimental results show that MCTSteg can effectively withstand the detection of both hand-crafted feature-based and deep-learning-based steganalyzers. In both spatial and JPEG domains, the security performance of MCTSteg steadily outperforms the state of the art by a clear margin under different scenarios.},
  keywords={Distortion;Reinforcement learning;Security;Cost function;Transform coding;Monte Carlo methods;Media;Steganography;steganalysis;Monte Carlo tree search;reinforcement learning},
  doi={10.1109/TIFS.2021.3104140},
  ISSN={1556-6021},
  month={},}@ARTICLE{10041967,
  author={Mo, Xianbo and Tan, Shunquan and Tang, Weixuan and Li, Bin and Huang, Jiwu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={ReLOAD: Using Reinforcement Learning to Optimize Asymmetric Distortion for Additive Steganography}, 
  year={2023},
  volume={18},
  number={},
  pages={1524-1538},
  abstract={Recently, the success of non-additive steganography has demonstrated that asymmetric distortion can remarkably improve security performance compared with symmetric cost functions. However, most of current existing additive steganographic methods are still based on symmetric distortion. In this paper, for the first time we optimize asymmetric distortion for additive steganography and propose an A3C (Asynchronous Advantage Actor-Critic) based steganographic framework, called ReLOAD. ReLOAD is composed of an actor and a critic, where the former guides action selection for pixel-wise distortion modulation, and the latter evaluates the performance of modulated distortion. Meanwhile, a reward function that considers embedding effects is proposed to unify the goal of steganography and reinforcement learning, so that the minimization of embedding effects can be achieved by learning secure policy to maximize total rewards. Statistical analysis shows that compared with non-additive steganography, ReLOAD achieves lower change rates and makes embedding traces more consistent with cover image textures. Comprehensive experiments conducted on both hand-crafted feature-based and deep learning-based steganalyzers show that ReLOAD significantly promotes the state-of-the-art security performance of current additive methods and even outperforms non-additive steganography when the modification distribution gets sparser.},
  keywords={Distortion;Steganography;Additives;Security;Reinforcement learning;Modulation;Costs;Additive steganography;asymmetric distortion;automatic cost modulation;reinforcement learning},
  doi={10.1109/TIFS.2023.3244094},
  ISSN={1556-6021},
  month={},}@ARTICLE{10114409,
  author={Lu, Xiaoqiang and Jiao, Licheng and Li, Lingling and Liu, Fang and Liu, Xu and Yang, Shuyuan and Feng, Zhixi and Chen, Puhua},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Weak-to-Strong Consistency Learning for Semisupervised Image Segmentation}, 
  year={2023},
  volume={61},
  number={},
  pages={1-15},
  abstract={Supervised remote sensing (RS) image segmentation has achieved remarkable success with large amounts of manually labeled data, which may be difficult to acquire in some practical application scenarios. Semisupervised RS image segmentation can efficiently utilize the knowledge embedded in unlabeled data to improve recognition performance, which is of great significance for the generalization application of segmentation models. In this work, we propose an end-to-end semisupervised RS image segmentation method based on weak-to-strong consistency learning (WSCL). Specifically, a common strong data augmentation technique for image segmentation is introduced to provide powerful input perturbation to decouple self-biased cognition. By forcing weakly augmented and strongly augmented perspectives from the same sample to be consistent, WSCL not only enables the model to steadily learn knowledge contained in unlabeled data but also alleviates overfitting. In addition, a novel sparse dual-view cross-sample image generation method is presented to generate new training samples, which helps provide a more comprehensive diversity of perturbations. Furthermore, an adaptive reweighting strategy based on the entropy maps of the outputs of strongly perturbed samples is proposed to suppress noise, guiding the training process in a positive direction. Extensive experiments demonstrate the significant advantage of WSCL over other advanced methods, achieving new state-of-the-art under several evaluation metrics on DFC22, iSAID, MER, MSL, Vaihingen, and GID-15 datasets. The source code is open-sourced at https://github.com/xiaoqiang-lu/WSCL.},
  keywords={Image segmentation;Training;Data models;Perturbation methods;Predictive models;Entropy;Adaptation models;End-to-end;remote sensing (RS) images;semisupervised image segmentation;weak-to-strong consistency learning (WSCL)},
  doi={10.1109/TGRS.2023.3272552},
  ISSN={1558-0644},
  month={},}@ARTICLE{10445244,
  author={Zhang, Tianyang and Zhang, Xiangrong and Zhu, Xiaoqian and Wang, Guanchun and Han, Xiao and Tang, Xu and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Multistage Enhancement Network for Tiny Object Detection in Remote Sensing Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-12},
  abstract={With the rapid advances in deep learning techniques, remote sensing object detection (RSOD) has achieved remarkable achievements in recent years. However, tiny object detection remains unsatisfactory and suffers from two main drawbacks: 1) the high sensitivity of IoU for location deviation in tiny objects and 2) the poor-quality feature representations of tiny objects. To address the aforementioned problems, we propose a multistage enhancement network (MENet) that achieves the instance-level and feature-level enhancement of tiny objects from different stages of the detector. Since the IoU-based label assignment drastically deteriorates the positive samples for tiny objects, we first propose a central region (CR)-based label assignment to substitute it in the region proposal network (RPN). The CR label assignment regards the anchors that fall into the CR of ground-truth boxes as positive samples, which provides more positive samples for tiny objects. Then, we design a gated context aggregation (GCA) module that selectively aggregates valuable context information to enhance the feature representation of tiny objects. Additionally, we devise a positive RoI (pRoI) feature generator in the region convolutional neural network (R-CNN) to generate a rich diversity of high-quality pRoI features for tiny objects. We conduct extensive experiments on AI-TOD and SODA-A datasets, and the results demonstrate the effectiveness of our proposed method.},
  keywords={Object detection;Feature extraction;Remote sensing;Detectors;Superresolution;Deep learning;Generators;Context aggregation;label assignment;remote sensing images (RSIs);tiny object detection},
  doi={10.1109/TGRS.2024.3363614},
  ISSN={1558-0644},
  month={},}@ARTICLE{10261222,
  author={Cao, Shidong and Chai, Wenhao and Hao, Shengyu and Zhang, Yanting and Chen, Hangyue and Wang, Gaoang},
  journal={IEEE Transactions on Multimedia}, 
  title={DiffFashion: Reference-Based Fashion Design With Structure-Aware Transfer by Diffusion Models}, 
  year={2024},
  volume={26},
  number={},
  pages={3962-3975},
  abstract={Image-based fashion design with AI techniques has attracted increasing attention in recent years. We focus on the reference-based fashion design task, where we aim to combine a reference appearance image and a clothing image to generate a new fashion clothing image. Although existing diffusion-based image translation methods have enabled flexible style transfer, it is often difficult to transfer the appearance of the image realistically during reverse diffusion. When the referenced appearance domain greatly differs from the source domain, it often leads to the collapse in the translation. To tackle this issue, we present a novel diffusion model-based unsupervised structure-aware transfer method, namely DiffFashion. Our method is free of model tuning and structure-preserving and has high flexibility in transferring from images with large domain gaps. Specifically, based on the optimal transport properties, we keep a shared latent across the clothing image and reference appearance image to bridge the gap between the two domains in the denoising process, and the latent of the reference image is gradually adapted to the clothing domain. Simultaneously, the structure is transferred from the source clothing to the output fashion image with mixed guidance, including pre-trained Vision Transformer (ViT) guidance and a foreground mask guidance, to further preserve the structure and appearance semantics from source and reference images. Our experimental results show that the proposed method outperforms state-of-the-art baseline models, generating more realistic images in the fashion design task.},
  keywords={Clothing;Task analysis;Noise reduction;Training;Semantics;Electronic mail;Artificial intelligence;Fashion design;diffusion models;structure-aware},
  doi={10.1109/TMM.2023.3318297},
  ISSN={1941-0077},
  month={},}@ARTICLE{10777578,
  author={Li, ChaoRong and Ling, XuDong and Xue, YiLan and Luo, Wenjie and Zhu, LiHong and Qin, FengQing and Zhou, Yaodong and Huang, Yuanyuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Precipitation Nowcasting Using Diffusion Transformer With Causal Attention}, 
  year={2024},
  volume={62},
  number={},
  pages={1-16},
  abstract={Short-term precipitation forecasting remains challenging due to the difficulty in capturing long-term spatiotemporal dependencies. Current deep learning methods fall short in establishing effective dependencies between conditions and forecast results, while also lacking interpretability. To address this issue, we propose a precipitation nowcasting using a diffusion transformer with causal attention (DTCA) model. Our model leverages the transformer and combines causal attention mechanisms to establish spatiotemporal queries between conditional information (causes) and forecast results (results). This design enables the model to effectively capture long-term dependencies, allowing forecast results to maintain strong causal relationships with input conditions over a wide range of time and space. We explore four variants of spatiotemporal information interactions for DTCA, demonstrating that global spatiotemporal labeling interactions yield the best performance. In addition, we introduce a channel-to-batch shift (CTBS) operation to further enhance the model’s ability to represent complex rainfall dynamics. We conducted experiments on two datasets. Compared to state-of-the-art U-Net-based methods, our approach improved the critical success index (CSI) for predicting heavy precipitation by approximately 15% and 8%, respectively, achieving state-of-the-art performance. Our project is open source and available on GitHub at: https://github.com/ybu-lxd/DTCA.},
  keywords={Transformers;Diffusion models;Spatiotemporal phenomena;Forecasting;Weather forecasting;Rain;Predictive models;Meteorology;Accuracy;Image synthesis;Causal attention;diffusion model (DM);nowcasting prediction;transformer},
  doi={10.1109/TGRS.2024.3510693},
  ISSN={1558-0644},
  month={},}@ARTICLE{10113373,
  author={Chen, Hung-Jen and Shuai, Hong-Han and Cheng, Wen-Huang},
  journal={IEEE Signal Processing Magazine}, 
  title={A Survey of Artificial Intelligence in Fashion}, 
  year={2023},
  volume={40},
  number={3},
  pages={64-73},
  abstract={The fashion industry is on the verge of an unprecedented change. Fashion applications are benefiting greatly from the development of machine learning, computer vision, and artificial intelligence. In this article, we present an overview of three major topics of fashion and associated state-of-the-art techniques: 1) fashion analysis, including popularity prediction and fashion trend analysis; 2) fashion recommendation, including fashion compatibility and outfit matching; and 3) fashion synthesis, including makeup transfer and virtual try-on. Problem formulations, method comparisons, and evaluation metrics are illustrated for each topic of fashion research. Additionally, promising directions for development in each area are outlined to inspire future research.},
  keywords={Artificial intelligence;Clothing industry;Deep learning;Computer vision;Computational modeling;Taxonomy;Market research},
  doi={10.1109/MSP.2022.3233449},
  ISSN={1558-0792},
  month={May},}@ARTICLE{10151944,
  author={Wang, Jiahao and Liu, Fang and Wang, Hao and Liu, Xu and Jiao, Licheng and Yang, Hua and Li, Lingling and Chen, Puhua},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={SDCDNet: A Semi-Dual Change Detection Network Framework With Super-Weak Label for Remote Sensing Image}, 
  year={2023},
  volume={61},
  number={},
  pages={1-14},
  abstract={Most current change detection methods require a large amount of labeled data to train huge parameters. To break this limitation, this article proposes a novel semi-supervised learning (SSL) framework for remote sensing change detection, named a semi-dual change detection network (SDCDNet). The SDCDNet consists of a dual shared network and dual branching networks. The dual shared network is designed to exploit the full potential of the data, and the dual branching network is proposed to differentiate the kinds of annotated data and eliminate the disturbance between different types of data. In addition, the adaptive weighting module (AWM) enhances the features of weak branching, and the mask constraint module (MCM) is proposed to increase the ability of the network to extract foreground features. To solve the complex problem of data labeling, a patch-based weak label construction method is proposed to build super-weak labels. Experiments show that the proposed SDCDNet achieves excellent results on two remote sensing image change detection datasets.},
  keywords={Feature extraction;Remote sensing;Training;Task analysis;Semantics;Annotations;Transformers;Change detection;dual branch network;remote sensing image;semi-supervised learning (SSL)},
  doi={10.1109/TGRS.2023.3286113},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10224995,
  author={Fysarakis, Konstantinos and Lekidis, Alexios and Mavroeidis, Vasileios and Lampropoulos, Konstantinos and Lyberopoulos, George and Vidal, Ignasi Garcia-Milà and Terés i Casals, José Carles and Luna, Eva Rodriguez and Moreno Sancho, Alejandro Antonio and Mavrelos, Antonios and Tsantekidis, Marinos and Pape, Sebastian and Chatzopoulou, Argyro and Nanou, Christina and Drivas, George and Photiou, Vangelis and Spanoudakis, George and Koufopavlou, Odysseas},
  booktitle={2023 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={PHOENI2X – A European Cyber Resilience Framework With Artificial-Intelligence-Assisted Orchestration, Automation & Response Capabilities for Business Continuity and Recovery, Incident Response, and Information Exchange}, 
  year={2023},
  volume={},
  number={},
  pages={538-545},
  abstract={As digital technologies become more pervasive in society and the economy, cyber-security incidents become more frequent, but also more impactful. Based on the NIS & NIS2 Directives, EU Member States and their Operators of Essential Services (OES) must establish a minimum baseline set of capabil- ities while providing cross-border coordination and cooperation. But this is only a small step towards European cyber resilience. In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective crisis management and cyber-security resilience. This paper presents PHOENI2X which, motivated by the above, aims to design, develop, and deliver a Cyber Resilience Framework (CRF) providing Artificial Intelligence (AI) - assisted orchestration, automation & response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of OES and of the EU Member State (MS) National Authorities entrusted with cyber-security.},
  keywords={Crisis management;Automation;Europe;Business continuity;Artificial intelligence;Computer crime;Information exchange},
  doi={10.1109/CSR57506.2023.10224995},
  ISSN={},
  month={July},}@ARTICLE{10897566,
  author={Xu, Yifan and Wei, Huapeng and Lin, Minxuan and Deng, Yingying and Sheng, Kekai and Zhang, Mengdan and Tang, Fan and Dong, Weiming and Huang, Feiyue and Xu, Changsheng},
  journal={Computational Visual Media}, 
  title={Transformers in computational visual media: A survey}, 
  year={2022},
  volume={8},
  number={1},
  pages={33-62},
  abstract={Transformers, the dominant architecture for natural language processing, have also recently attracted much attention from computational visual media researchers due to their capacity for long-range representation and high performance. Transformers are sequence-to-sequence models, which use a self-attention mechanism rather than the RNN sequential structure. Thus, such models can be trained in parallel and can represent global information. This study comprehensively surveys recent visual transformer works. We categorize them according to task scenario: backbone design, high-level vision, low-level vision and generation, and multimodal learning. Their key ideas are also analyzed. Differing from previous surveys, we mainly focus on visual transformer methods in low-level vision and generation. The latest works on backbone design are also reviewed in detail. For ease of understanding, we precisely describe the main contributions of the latest works in the form of tables. As well as giving quantitative comparisons, we also present image results for low-level vision and generation tasks. Computational costs and source code links for various important works are also given in this survey to assist further development.},
  keywords={Transformers;Visualization;Predictive models;Deformable models;Surveys;Computational modeling;Media;Computer architecture;Adaptation models;Training;visual transformer;computational visual media (CVM);high-level vision;low-level vision;image generation;multi-modal learning},
  doi={10.1007/s41095-021-0247-3},
  ISSN={2096-0662},
  month={March},}@ARTICLE{10508127,
  author={Liu, Qiang and Wu, Junfei and Wu, Shu and Wang, Liang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Out-of-Distribution Evidence-Aware Fake News Detection via Dual Adversarial Debiasing}, 
  year={2024},
  volume={36},
  number={11},
  pages={6801-6813},
  abstract={Evidence-aware fake news detection aims to conduct reasoning between news and evidences, which are retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and minimize the impact of content biases. To be noted, our proposed DAL approach is a plug-and-play module that works well with existing backbones. We conduct comprehensive experiments under two OOD settings, and plug DAL in four evidence-aware fake news detection backbones. Results demonstrate that, DAL significantly and stably outperforms the original backbones and some competitive debiasing methods.},
  keywords={Fake news;Feature extraction;Correlation;Training;Task analysis;Cognition;Data models;Fake news detection;evidence-aware;out-of-distribution;debiasing;adversarial learning},
  doi={10.1109/TKDE.2024.3390431},
  ISSN={1558-2191},
  month={Nov},}@INPROCEEDINGS{10234977,
  author={Anoop, R and Kiran, Niharika and Raj, Bhargav P and Kodipalli, Ashwini and Rao, Trupthi and Rohini, B R},
  booktitle={2023 World Conference on Communication & Computing (WCONF)}, 
  title={MNSIT Handwritten Digit Recognition Using Machine Learning Classification Algorithms}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The paper discusses the use of machine learning in recognizing handwritten digits and text, which has wide applications in areas such as surveillance, healthcare, and document analysis. The study focuses on evaluating the accuracy and variability of classifying handwritten digits with different numbers of hidden layers using the Modified National Institute of Standards and Technology (MNIST) dataset, and compares the performance of common machine learning algorithms such as SVM, KNN, and RFC. The study notes that recognizing handwritten digits and text is challenging due to their dissimilarities in size, thickness, position, and orientation. The ability to accurately recognize handwritten digits is essential in various fields, including banking, post offices, and tax files. The paper demonstrates handwritten digit recognition (HDR) using the MNIST dataset and selected classification algorithms. Overall, handwriting recognition is a major area of development with many possibilities for applications.},
  keywords={Support vector machines;Handwriting recognition;Machine learning algorithms;Text analysis;Text recognition;Banking;Classification algorithms;Random Forest Classifier;Decision Tree Classifier;Gini Index;Entropy;Handwritten Digit Recognition;K-Nearest Neighbor;Supports Vector Machine;Gaussian Naive Bayes;Ada Boosting;Bagging Classifier},
  doi={10.1109/WCONF58270.2023.10234977},
  ISSN={},
  month={July},}@ARTICLE{10265178,
  author={Zhao, Qianfan and Zhang, Lu and He, Bin and Liu, Zhiyong},
  journal={IEEE Robotics and Automation Letters}, 
  title={Semantic Policy Network for Zero-Shot Object Goal Visual Navigation}, 
  year={2023},
  volume={8},
  number={11},
  pages={7655-7662},
  abstract={The task of zero-shot object goal visual navigation (ZSON) aims to enable robots to locate previously “unseen” objects by visual observations. This task presents a significant challenge since the robot must transfer the navigation policy learned from “seen” objects to “unseen” objects through auxiliary semantic information without training samples, a process known as zero-shot learning. In order to address this challenge, we propose a novel approach termed the Semantic Policy Network (SPNet). The SPNet consists of two modules that are deeply integrated with semantic embeddings: the Semantic Actor Policy (SAP) module and the Semantic Trajectory (ST) module. The SAP module generates actor network weight bias based on semantic embeddings, creating unique navigation policies for different target classes. The ST module records the robot's actions, visual features, and semantic embeddings at each step, and aggregates information in both the spatial and temporal dimensions. To evaluate our approach, we conducted extensive experiments using MP3D dataset, HM3D dataset, and RoboTHOR. Experimental results indicate that the proposed method outperforms other ZSON methods for both seen and unseen target classes.},
  keywords={Semantics;Navigation;Visualization;Robots;Task analysis;Training;Feature extraction;Deep learning;path planning;reinforcement learning;vision-based navigation},
  doi={10.1109/LRA.2023.3320014},
  ISSN={2377-3766},
  month={Nov},}@ARTICLE{10284005,
  author={Wang, Changwei and Xu, Rongtao and Xu, Shibiao and Meng, Weiliang and Xiao, Jun and Zhang, Xiaopeng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Accurate Lung Nodule Segmentation With Detailed Representation Transfer and Soft Mask Supervision}, 
  year={2024},
  volume={35},
  number={12},
  pages={18381-18393},
  abstract={Accurate lung lesion segmentation from computed tomography (CT) images is crucial to the analysis and diagnosis of lung diseases, such as COVID-19 and lung cancer. However, the smallness and variety of lung nodules and the lack of high-quality labeling make the accurate lung nodule segmentation difficult. To address these issues, we first introduce a novel segmentation mask named “ soft mask,” which has richer and more accurate edge details description and better visualization, and develop a universal automatic soft mask annotation pipeline to deal with different datasets correspondingly. Then, a novel network with detailed representation transfer and soft mask supervision (DSNet) is proposed to process the input low-resolution images of lung nodules into high-quality segmentation results. Our DSNet contains a special detailed representation transfer module (DRTM) for reconstructing the detailed representation to alleviate the small size of lung nodules images and an adversarial training framework with soft mask for further improving the accuracy of segmentation. Extensive experiments validate that our DSNet outperforms other state-of-the-art methods for accurate lung nodule segmentation, and has strong generalization ability in other accurate medical segmentation tasks with competitive results. Besides, we provide a new challenging lung nodules segmentation dataset for further studies (https://drive.google.com/file/d/15NNkvDTb_0Ku0IoPsNMHezJRTH1Oi1wm/view?usp=sharing).},
  keywords={Lung;Image segmentation;Biomedical imaging;Labeling;Pipelines;Lesions;Task analysis;Detailed representation transfer;lung nodules segmentation;medical images segmentation;soft mask},
  doi={10.1109/TNNLS.2023.3315271},
  ISSN={2162-2388},
  month={Dec},}@INPROCEEDINGS{10616971,
  author={Rbah, Yahya and Mahfoudi, Mohammed and Fattah, Mohammed and Balboul, Younes and Mazer, Said and Elbekkali, Moulhime and Bernoussi, Benaissa},
  booktitle={2024 International Conference on Circuit, Systems and Communication (ICCSC)}, 
  title={A Comprehensive Survey on Deep Learning Approaches for Safeguarding the Internet of Medical Things from Malicious Intrusions}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The integration of smart medical devices and applications within the Internet of Medical Things (IoMT) is transforming the healthcare industry by enhancing patient engagement, real-time monitoring, and data analytics. However, the open nature of IoMT networks introduces vulnerabilities to various cyber threats, particularly malware attacks. These attacks compromise the integrity, confidentiality, and availability of sensitive data and system resources. Consequently, securing IoMT systems against such malicious activities is crucial. Notably, detecting unknown malware in IoMT environments remains a significant challenge for both research and operational teams. This paper surveys deep learning-based approaches for malware detection within the IoMT domain. We comprehensively compare existing solutions based on their employed detection methods, utilized datasets, and achieved detection accuracy. Furthermore, we explore the challenges and considerations of developing efficient malware classifiers for IoMT. Finally, we outline potential future directions for creating robust malware detection systems in this critical healthcare application domain.},
  keywords={Surveys;Industries;Deep learning;Medical devices;Data analysis;Accuracy;Internet of Medical Things;Internet of Medical Things (IoMT);Security;IoMT malware;Malware Detection;Deep learning (DL)},
  doi={10.1109/ICCSC62074.2024.10616971},
  ISSN={},
  month={June},}@ARTICLE{10835760,
  author={Chaddad, Ahmad and Wu, Yihang and Jiang, Yuchen and Bouridane, Ahmed and Desrosiers, Christian},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Simulations of Common Unsupervised Domain Adaptation Algorithms for Image Classification}, 
  year={2025},
  volume={74},
  number={},
  pages={1-17},
  abstract={Traditional machine learning assumes that training and test sets are derived from the same distribution; however, this assumption does not always hold in practical applications. This distribution disparity can lead to severe performance drops when the trained model is used in new datasets. Domain adaptation (DA) is a machine learning technique that aims to address this problem by reducing the differences between domains. This article presents simulation-based algorithms of recent DA techniques, mainly related to unsupervised DA (UDA), where labels are available only in the source domain. Our study compares these techniques with public datasets and diverse characteristics, highlighting their respective strengths and drawbacks. For example, safe self-refinement for transformer-based DA (SSRT) achieved the highest accuracy (91.6%) in the office-31 dataset during our simulations, however, the accuracy dropped to 72.4% in the Office-Home dataset when using limited batch sizes. In addition to improving the reader’s comprehension of recent techniques in DA, our study also highlights challenges and upcoming directions for research in this domain. The codes are available at https://github.com/AIPMLab/Domain_Adaptation.},
  keywords={Training;Deep learning;Data models;Internet;Feature extraction;Adaptation models;Transformers;Training data;Sun;Medical services;Domain adaptation (DA);image classification;machine learning;medical imaging},
  doi={10.1109/TIM.2025.3527531},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10361328,
  author={Xie, Yibing and Gardi, Alessandro and Sabatini, Roberto},
  booktitle={2023 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, 
  title={Cybersecurity Risks and Threats in Avionics and Autonomous Systems}, 
  year={2023},
  volume={},
  number={},
  pages={0814-0819},
  abstract={Ongoing advances in digitization and automation of critical infrastructures, particularly in terms of aeronautical Communication, Navigation, and Surveillance (CNS) technologies, has led to the fusion of intricate physical and information networks with artificial intelligence (AI) systems. This integration is empowered avionics and Air Traffic Management (ATM) systems with enhanced data processing capabilities, interactive information exchange, and expansive geographic distribution. However, these advancements also expose the systems to increasing cybersecurity threats, physical vulnerabilities, and data integrity risks. The complexity and interconnectedness of CNS infrastructure, combined with ATM systems, amplify the potential scope and depth of attacks and increase their ability to spread through interconnected components. As a result, both ATM and UAS Traffic Management (UTM) systems face an escalation in security challenges. While the concept of cybersecurity within aviation has a long history, its seamless integration into aviation systems remains a significant challenge. Just as AI technology is harnessed to improve the overall operational efficiency and reliability of aviation systems, it has also emerged as a pivotal battleground for cybersecurity risks and threats. Increasingly, AI-driven intrusion and theft methods are replacing traditional approaches. In response, researchers have put forth defensive strategies rooted in AI technology. This paper critically evaluates cybersecurity vulnerabilities and threats that may confront ATM and UTM systems. It systematically categorizes a variety of potential threat actors along with their corresponding targets, based on their objectives, motivations, and capabilities. Simultaneously, the paper delves deeply into an exploration of possible attack methodologies founded on AI technology, accompanied by their corresponding defensive tactics.},
  keywords={Navigation;Surveillance;Aerospace electronics;Network security;Complexity theory;Reliability;Computer security;Cybersecurity;Avionics system;Autonomous Systems;Cyber-Threat;ATM;UTM;CNS+A},
  doi={10.1109/DASC/PiCom/CBDCom/Cy59711.2023.10361328},
  ISSN={2837-0740},
  month={Nov},}@INPROCEEDINGS{10446418,
  author={Yu, Linfeng and Zhang, Wangyou and Du, Chenpeng and Zhang, Leying and Liang, Zheng and Qian, Yanmin},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Generation-Based Target Speech Extraction with Speech Discretization and Vocoder}, 
  year={2024},
  volume={},
  number={},
  pages={12612-12616},
  abstract={Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker’s proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.},
  keywords={Vocoders;Interference;Boosting;Frequency estimation;Acoustics;Recording;Noise measurement;Target speech extraction;speech discretization;speech synthesis;vocoder},
  doi={10.1109/ICASSP48485.2024.10446418},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10589720,
  author={Zhu, Hao and Yan, Fengtao and Guo, Pute and Li, Xiaotong and Hou, Biao and Chen, Kefan and Wang, Shuang and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={High-Low-Frequency Progressive-Guided Diffusion Model for PAN and MS Classification}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={With the rapid development of remote sensing technology, satellites can easily acquire multispectral (MS) and panchromatic (PAN) images. It is challenging to utilize their complementarity to effectively combine each other’s advantages and mitigate the differences between different modes. In this article, we propose a high-low-frequency progressive-guided diffusion model. It is used to generate an image with the advantages of both MS and PAN, which can be complementary to MS and PAN and, thus, can better reduce the modal differences between them. Therefore, we use the fusion image as an auxiliary mode and an intermediate bridge, which can better connect the characteristics between various sources. First, we design guidance information that contains the advantages of MS and PAN, and some operations can make this information better guide the generation stage. In addition, we design a high-low-frequency progressive guidance strategy; by using this strategy, we can first ensure the overall structure and layout of the image in the generation stage and then refine the local details and features of the image. This dramatically improves the quality of the generated image. Finally, we use mathematical knowledge to explain the rationality of the strategy. We validate our method on multiple datasets and achieve the best performance. Our code is https://github.com/Xidian-AIGroup190726/HLF-GDiffusion.},
  keywords={Diffusion models;Feature extraction;Noise reduction;Noise;Data integration;Training;Task analysis;Deep learning (DL);diffusion model;fusion classification;multispectral (MS) images;panchromatic (PAN) images;remote sensing},
  doi={10.1109/TGRS.2024.3425370},
  ISSN={1558-0644},
  month={},}@ARTICLE{10679601,
  author={Zhang, Huanyu and Zhang, Yi-Fan and Zhang, Zhang and Wen, Qingsong and Wang, Liang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={LogoRA: Local-Global Representation Alignment for Robust Time Series Classification}, 
  year={2024},
  volume={36},
  number={12},
  pages={8718-8729},
  abstract={Unsupervised domain adaptation (UDA) of time series aims to teach models to identify consistent patterns across various temporal scenarios, disregarding domain-specific differences, which can maintain their predictive accuracy and effectively adapt to new domains. However, existing UDA methods struggle to adequately extract and align both global and local features in time series data. To address this issue, we propose the Local-Global Representation Alignment framework (LogoRA), which employs a two-branch encoder–comprising a multi-scale convolutional branch and a patching transformer branch. The encoder enables the extraction of both local and global representations from time series. A fusion module is then introduced to integrate these representations, enhancing domain-invariant feature alignment from multi-scale perspectives. To achieve effective alignment, LogoRA employs strategies like invariant feature learning on the source domain, utilizing triplet loss for fine alignment and dynamic time warping-based feature alignment. Additionally, it reduces source-target domain gaps through adversarial training and per-class prototype alignment. Our evaluations on four time-series datasets demonstrate that LogoRA outperforms strong baselines by up to 12.52%, showcasing its superiority in time series UDA tasks.},
  keywords={Feature extraction;Time series analysis;Data mining;Adaptation models;Training;Transformers;Legged locomotion;Domain adaptation;time series classification},
  doi={10.1109/TKDE.2024.3459908},
  ISSN={1558-2191},
  month={Dec},}@ARTICLE{10552302,
  author={Yan, Tao and Zhu, Xiangjie and Chen, Xianglong and He, Weijiang and Wang, Chenglong and Yang, Yang and Wang, Yinghui and Chang, Xiaojun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={GLGFN: Global-Local Grafting Fusion Network for High-Resolution Image Deraining}, 
  year={2024},
  volume={34},
  number={11},
  pages={10860-10873},
  abstract={Image deraining is a hot research topic, which aims to remove various rain streaks (raindrops) from rainy images and restore the backgrounds. Though image deraining has been extensively studied in recent years, few methods are able to effectively and efficiently derain real-world high-resolution rainy images. In general, existing image deraining methods are restricted by two main factors while processing high-resolution images. First, the computational complexity and memory usage of existing deep learning-based methods are high when it comes to derain high-resolution images. Second, as the image resolution increases, it is difficult to simultaneously extract and aggregate both global and local features for clean rain removal. In this paper, we propose a novel network, called Global-Local Grafting Fusion Network (GLGFN), for deraining real-world high-resolution images. Our GLGFN utilizes a staggered connection structure to achieve deeper sampling depth while maintaining low computational cost. It adopts the Transformer and CNN based encoders (backbones) to extract global and local features, respectively, and then grafts global features into local features to guide the extraction of rain streaks. In addition, for well fusing global and local features, we also propose a Grafting Fusion Module (GFM), which adopts Cross Sparse Attention (CSA) and Selective Kernel Fusion (SK Fusion) to efficiently aggregate global and local features. Extensive experiments conducted on several high-resolution real rainy datasets have demonstrated the effectiveness and efficiency of our proposed GLGFN. We will release our code and dataset.},
  keywords={Feature extraction;Transformers;Image restoration;Image resolution;Decoding;Deep learning;Precipitation;Rain;High-resolution image;real-world image;deraining;deep learning;staggered connection structure;grafting fusion},
  doi={10.1109/TCSVT.2024.3411655},
  ISSN={1558-2205},
  month={Nov},}@INPROCEEDINGS{10354922,
  author={Sun, Yuhao and Zhang, Shixin and Shan, Jianhua and Zhao, Lei and Wang, Xiangbo and Sun, Fuchun and Yang, Yiyong and Fang, Bin},
  booktitle={2023 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
  title={Simulation of Vision-based Tactile Sensors with Efficiency-tunable Rendering}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Vision-based tactile sensors (VBTS) leverages visual modality to present high-resolution tactile information. The vision-based sensing mechanism has good adaptability to robot manipulation because of available information capture and data processing. However, the simulation of VBTS is a challenging problem because it deals with computer graphics and elasto-plastic deformation. In this paper, we propose a simulation approach for VBTS using a rendering approach based on path tracing. The rendering method can be integrated with existing VBTS and robotic simulators for the simulation of robotic systems. In addition, this method can control the rendering efficiency and quality by controlling the number of voxels in the fitting deformation area to meet the requirements of efficient robot training. The experimental results verify that our method can provide high-quality images at low rendering efficiency or reduce image quality to improve rendering efficiency. Our work has the potential to advance the sim-to-real research on VBTS.},
  keywords={Training;Image quality;Visualization;Deformation;Biomimetics;Fitting;Tactile sensors},
  doi={10.1109/ROBIO58561.2023.10354922},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10717135,
  author={Gopal, J Vijay and Prasad, K. Sai and Kumar, Dhatrika Kamal and Prashanth, Maddhe Sai and Kumar, Vuppala Praneeth and Nikhil Sri Sai Teja, I},
  booktitle={2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)}, 
  title={A Deep Insight on Cricket Video to Text Summarization Using Neural Networks}, 
  year={2024},
  volume={1},
  number={},
  pages={1865-1872},
  abstract={In this modern era, the demand for efficient and automated cricket video summarization techniques is rapidly increasing. This paper introduces an innovative and advance neural network system that transforms the way cricket match videos are summarized. Cricket video-to-text summarization system overcomes the limitations of traditional manual summarization approach by utilizing various deep learning techniques to completely automate the summarization process, our system can extract crucial insights from lengthy cricket match footage and convert them into easily readable text formats. The system employs three-pronged approach which involves extraction of visual features using VGG16 Convolutional Neural Network (CNN), scoreboard information is extracted through Optical Character Recognition (OCR) technology, and Text Summarization performed by Long Short-Term Memory (LSTM) network. Our system revolutionizes the way cricket enthusiasts engage with match videos, providing a game-changing experience for fans worldwide.},
  keywords={Visualization;Recurrent neural networks;Optical character recognition;Text summarization;Transforms;Feature extraction;Convolutional neural networks;Data mining;Long short term memory;Sports;advanced neural networks;text summarization;Convolutional neural networks;optical character recognition;recurrent neural networks;Long Short-Term Memory cricket videos},
  doi={10.1109/ICACCS60874.2024.10717135},
  ISSN={2575-7288},
  month={March},}@ARTICLE{10579799,
  author={Chen, Siru and Yu, Lingxin and Liu, Yuxuan and Ding, Zhifei and Zhang, Jiacheng and Wang, Xinyue and Han, Jiahao and Liu, Richen},
  journal={IEEE Internet of Things Journal}, 
  title={Diminished Reality Techniques for Metaverse Applications: A Perspective From Evaluation}, 
  year={2024},
  volume={11},
  number={21},
  pages={34734-34748},
  abstract={The extended reality (XR) is one of the most widely used approaches for accessing the metaverse world. The metaverse and XR aim to blend the virtual and real parts, offering an immersive and interactive experience. Diminished reality (DR) is a subset of XR that specifically addresses the real-time occlusion, removal, and transparency of objects in the environment. As an immersive technology, DR has been utilized in academia and industry to tackle a wide range of engineering problems. However, there is a little investigative work about DR technique evaluations. In this survey, we categorize the state-of-the-art research into two major categories and six subcategories, providing a novel perspective. We further analyze and evaluate the application effects and performance of these approaches from both quantitative and qualitative perspectives, considering the technical performance and user experience of DR techniques. Finally, we provide an overview of potential future directions for DR applications.},
  keywords={Metaverse;Extended reality;Real-time systems;Surveys;Visualization;Measurement;Standards;Augmented reality (AR);diminished reality;qualitative evaluation;quantitative evaluation},
  doi={10.1109/JIOT.2024.3418034},
  ISSN={2327-4662},
  month={Nov},}@INPROCEEDINGS{11093065,
  author={Tu, Siwei and Fei, Ben and Yang, Weidong and Ling, Fenghua and Chen, Hao and Liu, Zili and Chen, Kun and Fan, Hang and Ouyang, Wanli and Bai, Lei},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution}, 
  year={2025},
  volume={},
  number={},
  pages={28071-28079},
  abstract={Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields. If spatial interpolation is applied directly to obtain meteorological states for specific locations, there will often be significant discrepancies compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km. The code is available at https://github.com/Tusiwei/SGD},
  keywords={Training;Interpolation;Satellites;Accuracy;Atmospheric modeling;Weather forecasting;Diffusion models;Pattern recognition;Kernel;Spatial resolution},
  doi={10.1109/CVPR52734.2025.02614},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10691066,
  author={Rao, Abhishek S. and B, Rakesh Kumar and K, Ranganatha and Poojary, Ramaprasad and B H, Karthik Pai and M S, Lisa Bojamma and Jaleesh, Jeeva},
  booktitle={2024 International Conference on Data Science and Network Security (ICDSNS)}, 
  title={Moral Storytelling Model Using Artificial Intelligence-Driven Image-to-Text Synthesis}, 
  year={2024},
  volume={},
  number={},
  pages={01-07},
  abstract={Artificial intelligence in storytelling often struggles with conveying complex moral narratives due to its reliance on short and simplistic captions. This study introduces a novel approach to overcome this challenge by developing a model designed to craft more extensive and profound moral stories. Utilizing unsupervised deep learning techniques, the proposed model autonomously learns from a diverse corpus of short stories and images to generate extended narratives without requiring explicit human guidance. The study highlights three primary contributions: the creation of an autonomous model for generating moral stories with minimal human intervention, the development of a substantial reservoir of moral narratives for comprehensive training, and the empirical validation of the model's superior ability to produce innovative and captivating stories compared to existing methodologies. The proposed model exhibits remarkable accuracy, achieving 91.0% on the COCO dataset and 88% on the Flickr30K dataset, outperforming models such as Encoder-Decoder, Informative Cross-modal, LSTM, and AESOP. Additionally, the model's performance was evaluated using the BLEU and CIDEr scores, yielding 0.81 and 0.75, respectively. This model not only enhances the depth and quality of AI-generated moral stories but also has potential applications in education, entertainment, and virtual assistants, providing richer and more engaging content for users.},
  keywords={Deep learning;Training;Ethics;Virtual assistants;Decision making;Entertainment industry;Network security;Reservoirs;Artificial intelligence;Long short term memory;Image Captioning;Text Generation;Storytelling;Deep Learning;Natural language Processing},
  doi={10.1109/ICDSNS62112.2024.10691066},
  ISSN={},
  month={July},}@INPROCEEDINGS{10689821,
  author={Adithya and S, Gokulan and Reddy, Mahati and M, Varshith and K, Afnaan and Singh, Tripty and Sharma, Mansi},
  booktitle={2024 IEEE Recent Advances in Intelligent Computational Systems (RAICS)}, 
  title={Enhanced Image Registration for Precise Alignment of Brain and Liver CT Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study aims to assess improved methods of image registration to obtain accurate alignment of CT scans of the liver and brain. A collection of more than 1600 CT scans of the brain and liver is used in our research. We employ a variety of techniques, such as FFT-based registration with frequency domain analysis, Cv2 (OpenCV) for complete computer vision capabilities including picture alignment, and PystackReg, a Python package with a variety of registration algorithms. While Cv2 enables feature detection and geometric transformations, PystackReg offers a variety of transformation models. FFT uses spectral shifts to align images by taking advantage of frequency domains. By addressing particular aspects of the image, these techniques allow for accurate alignment of CT images throughout the registration process. As our CT dataset consists of two different anatomic data (Brain and Liver), we will divide the outcomes of each approach into two sections. To conclude our findings, we employ a variety of evaluation measures, including the Euclidean distance, Mean Square Error, Sum of Absolute Differences, and Jaccard Index. The FFT model is found to be more accurate in the dataset including brain CT scans, as evidenced by its higher Jaccard Index value (0.909) in comparison to the other models (0.439-PystackReg, 0.866-Cv2) and its lower Mean Square Error (0.090) in comparison to the other models (37.708-PystackReg, 0.130-Cv2). However, with the lowest Euclidean Measure (0.5974) and mean square error (0.0050) as compared to PystackReg (14.9810, 0.0386) and FFT (5.3996, 0.5727) models, respectively, the Cv2 Model performs better on liver CT images. These advancements could lead to better therapeutic evaluation, diagnostic capabilities, and personalized healthcare. They might also bring about a paradigm shift towards the analysis and interpretation of medical pictures that is more successful and efficient.},
  keywords={Image registration;Accuracy;Computed tomography;Frequency-domain analysis;Computational modeling;Measurement uncertainty;Liver;Mean square error methods;Brain modeling;Indexes;Image registration;CT scans;Computer vision;Transformation models;Feature detection;Geometric transformations;Dataset analysis;Diagnostic capabilities},
  doi={10.1109/RAICS61201.2024.10689821},
  ISSN={2769-5565},
  month={May},}@INPROCEEDINGS{11058471,
  author={Solomon, Enoch and Woubie, Abraham and Awulachew, Eden Solomon and Chekole, Eyasu Getahun},
  booktitle={2025 8th International Conference on Information and Computer Technologies (ICICT)}, 
  title={Leveraging Artificial Intelligence Approaches for Smart Farming}, 
  year={2025},
  volume={},
  number={},
  pages={384-389},
  abstract={In recent years, the integration of deep learning techniques, particularly computer vision, into smart agriculture has emerged as a powerful tool to revolutionize traditional farming practices. As global demand for food production continues to rise, coupled with the increasing need for resource-efficient and sustainable agricultural systems, leveraging artificial intelligence (AI) has become crucial. Deep learning techniques enable models to analyze vast amounts of agricultural data, such as field images, sensor readings, and environmental conditions, to provide real-time and data-driven insights. These insights allow for more precise and informed decision-making across various farming operations, including crop health monitoring, disease detection, weed management, and yield prediction. In addition to reviewing the current state-of-the-art technologies, this paper discusses the challenges that limit the widespread adoption of AI in agriculture. These challenges include the scarcity of labeled and high-quality datasets, difficulties in generalizing AI models to diverse environmental conditions, and computational limitations in deploying AI-powered systems in low-resource settings. Furthermore, we discuss potential solutions to these issues, including data augmentation, domain adaptation, and model compression techniques, which aim to make deep learning models more accessible and scalable in real-world agricultural environments.},
  keywords={Deep learning;Smart agriculture;Computer vision;Explainable AI;Computational modeling;Biological system modeling;Data models;Real-time systems;Artificial intelligence;Farming;agriculture automation;computer vision;deep learning;machine learning;smart agriculture;vision transformers},
  doi={10.1109/ICICT64582.2025.00066},
  ISSN={2769-4542},
  month={March},}
