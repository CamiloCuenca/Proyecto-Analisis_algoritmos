@ARTICLE{9631205,
  author={Dalvi, Chirag and Rathod, Manish and Patil, Shruti and Gite, Shilpa and Kotecha, Ketan},
  journal={IEEE Access}, 
  title={A Survey of AI-Based Facial Emotion Recognition: Features, ML & DL Techniques, Age-Wise Datasets and Future Directions}, 
  year={2021},
  volume={9},
  number={},
  pages={165806-165840},
  abstract={Facial expressions are mirrors of human thoughts and feelings. It provides a wealth of social cues to the viewer, including the focus of attention, intention, motivation, and emotion. It is regarded as a potent tool of silent communication. Analysis of these expressions gives a significantly more profound insight into human behavior. AI-based Facial Expression Recognition (FER) has become one of the crucial research topics in recent years, with applications in dynamic analysis, pattern recognition, interpersonal interaction, mental health monitoring, and many more. However, with the global push towards online platforms due to the Covid-19 pandemic, there has been a pressing need to innovate and offer a new FER analysis framework with the increasing visual data generated by videos and photographs.Furthermore, the emotion-wise facial expressions of kids, adults, and senior citizens vary, which must also be considered in the FER research. Lots of research work has been done in this area. However, it lacks a comprehensive overview of the literature that showcases the past work done and provides the aligned future directions. In this paper, the authors have provided a comprehensive evaluation of AI-based FER methodologies, including datasets, feature extraction techniques, algorithms, and the recent breakthroughs with their applications in facial expression identification. To the best of the author’s knowledge, this is the only review paper stating all aspects of FER for various age brackets and would significantly impact the research community in the coming years.},
  keywords={Emotion recognition;Psychology;Face recognition;Videos;Symbiosis;Market research;Facial emotion recognition (FER);feature extraction;machine learning;facial expressions},
  doi={10.1109/ACCESS.2021.3131733},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8628777,
  author={Leangarun, Teema and Tangamchit, Poj and Thajchayapong, Suttipong},
  booktitle={2018 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Stock Price Manipulation Detection using Generative Adversarial Networks}, 
  year={2018},
  volume={},
  number={},
  pages={2104-2111},
  abstract={We implemented Generative Adversarial Networks (GANs) for detecting abnormal trading behaviors caused by stock price manipulations. Long short-term memory (LSTM) was used as a base structure of our GANs, which learned normal market behaviors in an unsupervised way. After the training, the discriminator network of GANs was used as a detector to discriminate between normal and manipulative trading. Our work is different from the previous work in that we did not use manipulation cases to train the neural networks. Instead, we used normal data to train them, and simulated manipulation cases were only used for testing purposes. The detection system was tested with the trading data from the Stock Exchange of Thailand (SET). It can achieve 68.1% accuracy in detecting pump-and-dump manipulations in unseen market data.},
  keywords={Gallium nitride;Stock markets;Manipulators;Anomaly detection;Data models;Training;Adaptation models;stock price manipulation;generative adversarial networks;anomaly detection},
  doi={10.1109/SSCI.2018.8628777},
  ISSN={},
  month={Nov},}@ARTICLE{10089190,
  author={Du, Changde and Fu, Kaicheng and Li, Jinpeng and He, Huiguang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features}, 
  year={2023},
  volume={45},
  number={9},
  pages={10760-10777},
  abstract={Decoding human visual neural representations is a challenging task with great scientific significance in revealing vision-processing mechanisms and developing brain-like intelligent machines. Most existing methods are difficult to generalize to novel categories that have no corresponding neural data for training. The two main reasons are 1) the under-exploitation of the multimodal semantic knowledge underlying the neural data and 2) the small number of paired (stimuli-responses) training data. To overcome these limitations, this paper presents a generic neural decoding method called BraVL that uses multimodal learning of brain-visual-linguistic features. We focus on modeling the relationships between brain, visual and linguistic features via multimodal deep generative models. Specifically, we leverage the mixture-of-product-of-experts formulation to infer a latent code that enables a coherent joint generation of all three modalities. To learn a more consistent joint representation and improve the data efficiency in the case of limited brain activity data, we exploit both intra- and inter-modality mutual information maximization regularization terms. In particular, our BraVL model can be trained under various semi-supervised scenarios to incorporate the visual and textual features obtained from the extra categories. Finally, we construct three trimodal matching datasets, and the extensive experiments lead to some interesting conclusions and cognitive insights: 1) decoding novel visual categories from human brain activity is practically possible with good accuracy; 2) decoding models using the combination of visual and linguistic features perform much better than those using either of them alone; 3) visual perception may be accompanied by linguistic influences to represent the semantics of visual stimuli.},
  keywords={Visualization;Decoding;Semantics;Brain modeling;Linguistics;Online services;Internet;Brain-visual-linguistic embedding;generic neural decoding;multimodal learning;mutual information maximization},
  doi={10.1109/TPAMI.2023.3263181},
  ISSN={1939-3539},
  month={Sep.},}@ARTICLE{10329993,
  author={Zou, Bangli and Wang, Yaojun and Chen, Ting and Liang, Jiandong and Yu, Gang and Hu, Guangmin},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={The Domain Adversarial and Spatial Fusion Semi-Supervised Seismic Impedance Inversion}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={The application of artificial intelligence in seismic impedance inversion makes the prediction of stratigraphic information more efficient. Semi-supervised framework for impedance inversion is the latest breakthrough method in this field. However, the 1-D semi-supervised methods now in use are unable to extract the spatiotemporal properties of the data solely through the network itself. Moreover, the initial model, a critical input for this method, is typically derived through extrapolation and interpolation of well log data. This can lead to significant errors, especially when the well data are sparse and the subsurface structures are complex. Well data only provide information for a limited section of the reservoir, thereby making it challenging to capture the overall behavior accurately. As a result, the creation of an accurate initial model is often fraught with errors. A more desirable approach is to use seismic attribute-guided methods, such as neural networks, which incorporate both seismic and well log data, leading to a more accurate low-frequency model with lateral variations. In this article, we develop a semi-supervised domain adversarial and spatial fusion (DASF) inversion framework. This method uses a 1-D convolutional neural network (CNN)-based global spatiotemporal analysis module and a 2-D CNN-based local spatiotemporal analysis module to complete the inversion and forward task simultaneously. Multiple spatiotemporal characteristics from two submodules can be successfully fused using an adaptive fusion approach. In this network, the step of extracting the initial model is incorporated into the learning process. Moreover, we adopt adversarial learning in the impedance domain to guide the training process, thereby reducing the network’s dependence on labels. The experiments on the synthetic and field dataset show that the proposed method can efficiently improve the prediction accuracy of the inversion results compared with conventional methods. Meanwhile, the local spatiotemporal analysis module can be used to create a more trustworthy initial model that incorporates the characteristic of seismic and well-logging data.},
  keywords={Impedance;Spatiotemporal phenomena;Feature extraction;Generative adversarial networks;Data models;Training;Pattern analysis;Generative adversarial network (GAN);seismic impedance inversion;semi-supervised learning},
  doi={10.1109/TGRS.2023.3336392},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9182381,
  author={Qianqian, Zhang and Bo, Su and Chao, Wang},
  booktitle={2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, 
  title={Computational holographic imaging through scattering media using deep learning}, 
  year={2020},
  volume={},
  number={},
  pages={258-260},
  abstract={It is difficult to restore the phase-only computer-generated hologram (CGH) through scattering medium since wavefront of optical beam is distorted by the speckles caused by random medium. In this article we presented Generative Adversarial architecture (GAN) used to learn functional relation between phase map imaging and ground truth imaging and then reconstruct CGH through scattering medium. Our experiments proved that GAN has learned the scattering characteristics of the scattering medium and used to restore CGH through scattering medium.},
  keywords={Scattering;Imaging;Image restoration;Data models;Gallium nitride;Training;Speckle;CGH;GAN;scattering media;speckle},
  doi={10.1109/ICAICA50127.2020.9182381},
  ISSN={},
  month={June},}@INPROCEEDINGS{10224759,
  author={Kumi, Sandra and Hilton, Maxwell and Snow, Charles and Lomotey, Richard K. and Deters, Ralph},
  booktitle={2023 IEEE International Conference on Digital Health (ICDH)}, 
  title={SleepSynth: Evaluating the use of Synthetic Data in Health Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={121-130},
  abstract={Health Digital Twins (HDTs) are virtual replicas of a patient’s physical/actual data. The major setbacks for applying Machine Learning (ML) in HDTs are the lack of availability of patients’ data due to privacy concerns and Artificial Intelligence (AI) bias. Given these shortcomings, synthetic data has been leveraged to solve privacy issues and increase diversity in datasets. In this paper, we evaluate four synthetic data generation models namely, Gaussian Copula, Conditional Tabular Generative Adversarial Network (CTGAN), CopulaGAN, and Tabular Variational Autoencoder (TVAE) which are used to generate synthetic data for actual sleep data retrieved from a wearable device. Gaussian Copula performed best in capturing the correlation between the variables with the real data with a quality score of approximately 96%. Additionally, we evaluate the efficacy of the synthetic generation models by training five well-known ML models on the generated synthetic data. Our experimental results show that the ML models trained on the synthetic data achieve an MAE (Mean Absolute Error) of less than 10% in the prediction of sleep quality score. The results from this work indicate that synthetic data could be used for ML tasks while preserving the privacy of data subjects.},
  keywords={Training;Performance evaluation;Data privacy;Wearable computers;Predictive models;Generative adversarial networks;Data models;Digital Twin;Privacy;Synthetic Data;Wearable Data;Machine Learning;Copulas;Generative Adversarial Network (GAN)},
  doi={10.1109/ICDH60066.2023.00027},
  ISSN={},
  month={July},}@INPROCEEDINGS{9579703,
  author={Namboodiri, Rahul and Singla, Kanishka and Kulkarni, Vaishali},
  booktitle={2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={GAN Based Try-On System: Improving CAGAN Towards Commercial Viability}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={In the era of disruptive innovations where technological leaps in internet-based applications along with advanced Artificial intelligence and Computing are combining to produce consumer-based applications, there is a notable shift of the fashion retail industry towards e-commerce. However, it still falls behind the traditional experience of shopping which offers customers not just a more visual and tactile stimuli, but also assurance of their purchase. This gap in experience can therefore be attributed to the lack of online try on solutions with online retail industry giants. This generates a disconnect between the retailers and consumers, making them hesitant and hence, skeptical about ordering via e-commerce. Further, consumer misjudgment of products results in returns, which are undesirable for online retailers with thin margins. We in this paper propose a prototypical solution to this problem by introducing a photorealistic virtual try-on solution while ensuring optimization and minimalistic use of resources as required by industrial standards. We present a Generative Adversarial Network (GAN) built upon the existing CAGAN architecture, with an improved generator based on a reworked U-Net generator architecture and a combination of instance and batch normalization for generator and discriminator respectively. Further, a combination of ReLu and Leaky ReLu activation functions have been utilized for the generator along with hyperparameter optimization of the network. In our visual analysis of the generated images, comparing with the CAGAN approach on Zalando dataset, we have achieved a superior quality in shorter training duration of 13 hours on a smaller dataset of 10,167 images.},
  keywords={Training;Industries;Visualization;Image color analysis;Graphics processing units;Computer architecture;Generative adversarial networks;Generative Adversarial Network;virtual trial room;try-on;deep learning;GAN application;CAGAN;Pix2Pix},
  doi={10.1109/ICCCNT51525.2021.9579703},
  ISSN={},
  month={July},}@INPROCEEDINGS{10777200,
  author={Mhawi, Mays Y. and Abdullah, Hikmat N. and Sikora, Axel},
  booktitle={2024 1st International Conference on Emerging Technologies for Dependable Internet of Things (ICETI)}, 
  title={The Influence of Hyperparameters on GANs Performance for Medical Image Transformation}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Generative Adversarial Networks (GANs) have accomplished compelling performance in many fields such as image-to-image transformation, image data generation, translation of image information into text information, and many more. In GANs, two neural networks, a generator and discriminator, compete with each other in an adversarial manner, and due to their robust generating ability, GANs can produce high-quality images for human reference. Thus, GANs are a very promising approach artificial intelligence field. In this work, GANs with optimized hyperparameters selection are proposed to generate skin lesion medical images that are indistinguishable from the real images and analyze the quality of generated images using image quality metrics including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Fréchet Inception Distance (FID). Additionally, the impact of hyperparameters including learning rate, batch size, latent space size, and number of epochs is analyzed against GANs performance. The experimental results indicate that the learning rate is the most effective hyperparameter for GAN stability and performance. Moreover, latent dimension, batch size, and number of epochs have a relatively small impact when paired with an appropriate learning rate thus good learning rate provides flexibility in optimizing these hyperparameters.},
  keywords={PSNR;Stability criteria;Neural networks;Network architecture;Generative adversarial networks;Skin;Lesions;Medical diagnosis;Internet of Things;Medical diagnostic imaging;Generative Adversarial Networks;Generator model;Discriminator model;Hyperparameters},
  doi={10.1109/ICETI63946.2024.10777200},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10885562,
  author={C, Soundarya B and L, Gururaj H},
  booktitle={2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS)}, 
  title={Temporal Deepfake Detection using CNN with Spatio-Temporal Features}, 
  year={2025},
  volume={},
  number={},
  pages={790-792},
  abstract={With the advancement of Artificial Intelligence (AI), facial recognition has become a crucial biometric feature. Deepfake technology leverages AI and can create hyper-realistic digitally manipulated images and videos of people appearing to say or do things that never occurred. The emergence of Generative Adversarial Networks (GANs) in 2014 has further enabled the creation of fake visual content. This technology has diverse applications, such as in the film industry, where it allows for video recreation without reshooting, creating awareness videos, restoring the voices of those who have lost them, and updating movie scenes at low cost. However, video-based manipulations pose significant challenges to detection systems. While most deepfake detectors focus on spatial anomalies in individual frames, temporal inconsistencies across frames can offer crucial clues. This paper presents a novel approach to video-based deepfake detection using Dense Swin Transformer, which leverages spatio-temporal feature extraction. Our proposed method, trained on the DFDC dataset, demonstrates improved accuracy in detecting deepfakes, achieving 98.25% accuracy with low computational cost.},
  keywords={Deepfakes;Visualization;Accuracy;Computational modeling;Image edge detection;Feature extraction;Transformers;Motion pictures;Image restoration;Artificial intelligence;deepfake;videoe;GenAI;Biometric},
  doi={10.1109/COMSNETS63942.2025.10885562},
  ISSN={2155-2509},
  month={Jan},}@INPROCEEDINGS{10980892,
  author={Munia, Nusrat and Al Zubaer Imran, Abdullah},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Artificial Intelligence (AI), specifically deep learning has made significant advancements in skin disease diagnoses. However, a major concern with deep learning-based models is the biased performance across subgroups, particularly regarding sensitive attributes like skin color. Toward mitigating such diagnosis biases, we propose a novel generative AI-based framework, namely Dermatology Diffusion Transformer (DermDiT). DermDiT leverages text prompts generated via large vision-language models and multimodal text-image learning to generate new dermoscopic images. Through an effective prompting, DermDiT can generate realistic synthetic images leading to improved representation of underrepresented groups in highly imbalanced datasets for clinical diagnoses. Extensive experimentation showcases that our innovative prompting in DermDiT provides more insightful representations to generate high-quality and useful images. Our code is available at https://github.com/Munia03/DermDiT.},
  keywords={Deep learning;Image synthesis;Image color analysis;Dermatology;Transformers;Skin;Robustness;Artificial intelligence;Medical diagnostic imaging;Diseases;Dermatology;Vision-Language Model;Diffusion Transformer;Image Generation;Diagnosis Bias},
  doi={10.1109/ISBI60581.2025.10980892},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{10864578,
  author={Huang, Xinyue and Zhang, Yong and Liu, Chang and Liu, Qingjie and Duan, Yaqiong and Lin, Yi},
  booktitle={2024 China Automation Congress (CAC)}, 
  title={Digital Twin Based Fault Diagnosis of Motor with Spatio-Temporal Graph Convolutional Networks}, 
  year={2024},
  volume={},
  number={},
  pages={4526-4531},
  abstract={Artificial intelligence (AI)-driven fault diagnosis is essential for the safe and efficient operation of motor. However, in practical industrial settings, it is challenging to get high-quality labeled datasets for model training. This paper proposes a fault diagnosis method using digital twin and graph convolutional networks, consisting of a virtual-real fusion module and a spatio-temporal graph convolutional networks module. The fusion module uses a Wasserstein generative adversarial network to improve virtual data quality, addressing the disparity between virtual and physical spaces. The spatio-temporal graph convo-lutional networks module, built with CNN and GCN, captures spatio-temporal features. Experiments show improved diagnostic performance with limited physical fault data, advancing AI engineering applications.},
  keywords={Fault diagnosis;Training;Graph convolutional networks;Data integrity;Virtual environments;Motors;Data models;Digital twins;Artificial intelligence;Context modeling;Digital twin;Multivariate time series;High-fidelity virtual model;Graph convolutional networks;Fault Diagnosis},
  doi={10.1109/CAC63892.2024.10864578},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10617314,
  author={Joshi, Ankita and Bansal, Saloni and Gard, Gopal Krishan and Al-Farouni, Mohammed and Srinivasan, S and Shetty, Surendra},
  booktitle={2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={They use of NLP in Association with AI Technology for the Purpose of Optimization in Various Fields}, 
  year={2024},
  volume={},
  number={},
  pages={896-900},
  abstract={In the world of Natural Language Processing (NLP), developing structures that not simplest carry out nicely but additionally offer apparent reasoning for their selections is paramount. Traditional gadget mastering fashions, in particular the ones based on deep gaining knowledge of, have excelled in diverse NLP duties. However, the ones models frequently act as “black containers,” providing little to no perception into their selection-making processes. This opacity can undermine the don’t forget and reliability perceived with the useful resource of customers. To deal with this trouble, we advocate a unique generative rationalization framework that endeavors to bridge the distance between high overall performance and explainability in NLP systems. This frameworks designed to concurrently make kind picks and generate unique, terrific-grained causes for the ones choices, integrating an explainable difficulty and minimum chance schooling. By providing human-readable justifications for its predictions, our framework enhances the interpretability and trustworthiness of NLP fashions. Experimental validation on two newly curated datasets demonstrates the framework’s capacity to outperform existing baselines, significantly advancing the sector of explainable AI inside NLP.},
  keywords={Bridges;Explainable AI;Containers;Natural language processing;Cognition;Reliability;Optimization;Natural Language Processing;Explainable AI (XAI);Text Classification;Generative Explanation Framework},
  doi={10.1109/ICACITE60783.2024.10617314},
  ISSN={},
  month={May},}@ARTICLE{9789138,
  author={Lou, Yunxia and Kumar, Anil and Xiang, Jiawei},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Machinery Fault Diagnosis Based on Domain Adaptation to Bridge the Gap Between Simulation and Measured Signals}, 
  year={2022},
  volume={71},
  number={},
  pages={1-9},
  abstract={In intelligent fault diagnosis, the success of artificial intelligence (AI) models is highly dependent on labeled training samples, which may not be obtained in real-world applications. Recently, a finite element method (FEM) simulation-based personalized diagnosis method was developed to overcome the problems of insufficient and incomplete labeled training samples. However, the simulation signals obtained using the FEM and measured signals actually have a certain deviation. To supplement the FEM simulation-based personalized diagnosis method, a fault diagnosis method using domain adaptation (DA) is proposed to bridge the gap between simulation signals and measured signals. First, the FEM is adopted to obtain sufficient and complete simulation samples of all the fault categories as the original fault samples in the source domain. Second, the original simulation fault samples are adjusted using a generative adversarial network (GAN)-based DA network to make them similar to the measured samples through the adversarial training of the refiner and domain discriminator. Last, credible adjustment fault samples and measured fault samples obtained in machinery are applied to a convolutional neural network (CNN) for training and testing to complete the fault classification. The data obtained from rolling element bearing and gear test rigs are utilized to explore the feasibility of the proposed method, and the classification accuracies reach 99.44% and 99.58%, respectively. The comparison investigations using experimental data of gears and bearings indicate that the present method can accurately classify faults in machinery.},
  keywords={Finite element analysis;Adaptation models;Training;Data models;Mechanical systems;Artificial intelligence;Fault diagnosis;Adversarial transfer learning;convolutional neural network (CNN);domain adaptation (DA);fault diagnosis;finite element method (FEM)},
  doi={10.1109/TIM.2022.3180416},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{8460982,
  author={Wulfmeier, Markus and Bewley, Alex and Posner, Ingmar},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Incremental Adversarial Domain Adaptation for Continually Changing Environments}, 
  year={2018},
  volume={},
  number={},
  pages={4489-4495},
  abstract={Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.},
  keywords={Training;Task analysis;Adaptation models;Robots;Gallium nitride;Mathematical model;Lighting},
  doi={10.1109/ICRA.2018.8460982},
  ISSN={2577-087X},
  month={May},}@INPROCEEDINGS{10658320,
  author={Geng, Zigang and Yang, Binxin and Hang, Tiankai and Li, Chen and Gu, Shuyang and Zhang, Ting and Bao, Jianmin and Zhang, Zheng and Li, Houqiang and Hu, Han and Chen, Dong and Guo, Baining},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={InstructDiffusion: A Generalist Modeling Interface for Vision Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={12709-12720},
  abstract={We present InstructDiffusion, a unified and generic framework for aligning computer vision tasks with hu-man instructions. Unlike existing approaches that integrate prior knowledge and pre-define the output space (e.g., categories and coordinates) for each vision task, we cast diverse vision tasks into a human-intuitive image-manipulating pro-cess whose output space is a flexible and interactive pixel space. Concretely, the model is built upon the diffusion process and is trained to predict pixels according to user instructions, such as encircling the man's left shoulder in red or applying a blue mask to the left car. InstructDiffusion could handle a variety of vision tasks, including understanding tasks (such as segmentation and keypoint de-tection) and generative tasks (such as editing and enhance-ment) and outperforms prior methods on novel datasets. This represents a solid step towards a generalist modeling interface for vision tasks, advancing artificial general intelligence in the field of computer vision.},
  keywords={Training;Computer vision;Solid modeling;Computational modeling;Artificial general intelligence;Shoulder;Predictive models},
  doi={10.1109/CVPR52733.2024.01208},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9710664,
  author={He, Sen and Liao, Wentong and Yang, Michael Ying and Song, Yi-Zhe and Rosenhahn, Bodo and Xiang, Tao},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Disentangled Lifespan Face Synthesis}, 
  year={2021},
  volume={},
  number={},
  pages={3857-3866},
  abstract={A lifespan face synthesis (LFS) model aims to generate a set of photo-realistic face images of a person’s whole life, given only one snapshot as reference. The generated face image given a target age code is expected to be age-sensitive reflected by bio-plausible transformations of shape and texture, while being identity preserving. This is extremely challenging because the shape and texture characteristics of a face undergo separate and highly nonlinear transformations w.r.t. age. Most recent LFS models are based on generative adversarial networks (GANs) whereby age code conditional transformations are applied to a latent face representation. They benefit greatly from the recent advancements of GANs. However, without explicitly disentangling their latent representations into the texture, shape and identity factors, they are fundamentally limited in modeling the nonlinear age-related transformation on texture and shape whilst preserving identity. In this work, a novel LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively. This is achieved by extracting shape, texture and identity features separately from an encoder. Critically, two transformation modules, one conditional convolution based and the other channel attention based, are designed for modeling the nonlinear shape and texture feature transformations respectively. This is to accommodate their rather distinct aging processes and ensure that our synthesized images are both age-sensitive and identity preserving. Extensive experiments show that our LFS model is clearly superior to the state-of-the-art alternatives. Codes and demo are available on our project website: https://senhe.github.io/projects/iccv_2021_lifespan_face.},
  keywords={Computer vision;Codes;Shape;Convolution;Biological system modeling;Computational modeling;Aging;Faces;Image and video manipulation detection and integrity methods;Image and video synthesis},
  doi={10.1109/ICCV48922.2021.00385},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{9250803,
  author={Patel, Mohil and Gupta, Aaryan and Tanwar, Sudeep and Obaidat, M. S.},
  booktitle={2020 IEEE 5th International Conference on Computing Communication and Automation (ICCCA)}, 
  title={Trans-DF: A Transfer Learning-based end-to-end Deepfake Detector}, 
  year={2020},
  volume={},
  number={},
  pages={796-801},
  abstract={With the advent of information and communication technologies, there have been breakthrough developments in the field of Artificial Intelligence (AI). Moreover, increasing computation power and decreasing processing times, new applications are being developed at great speeds. One such application is Deepfakes, which tackles the increased manipulated and forged media content. But these fake images and videos hamper the security and privacy of individuals and can have large-scale religious, communal, or political implications that may prove to be catastrophic for a nation. The face swapped content at times can be identified by human observation, but with the use of Generative adversarial networks (GANs), such forged content can be developed with is hard to be identified even by humans. Hence, detecting such videos and images is a challenging task for researchers. Motivated from these gaps, in this paper, we propose a pipeline for detecting and extracting human faces from videos, process them to extract features from them, and then classify them as real or fake. The results of the proposed model achieved an accuracy of 90.2% for classifying fake images from real ones.},
  keywords={Computational modeling;Detectors;Feature extraction;Artificial intelligence;Faces;Videos;Information integrity;Deepfakes;Classification;Feature Extraction;Random Forest;VGG;ResNet;Inception;MobileNet;DenseNet},
  doi={10.1109/ICCCA49541.2020.9250803},
  ISSN={2642-7354},
  month={Oct},}@INPROCEEDINGS{10606651,
  author={Wang, Li and Qu, Qilin and Wang, Yanwei and Wong, David Shan-Hill and Zheng, Ying},
  booktitle={2024 IEEE 13th Data Driven Control and Learning Systems Conference (DDCLS)}, 
  title={Data Augmentation Integrated with Feature-Enhanced Convolutional Neural Network for Imbalanced Fault Diagnosis in Rolling Bearings}, 
  year={2024},
  volume={},
  number={},
  pages={1204-1209},
  abstract={Imbalance in fault samples is a key problem with most rolling bearing datasets in real industrial processes, limiting the performance of fault detection. In addition, existing fault diagnosis methods for rolling bearings do not adopt a modular division between the initial layer and the classification layer. This lack of modularization inhibits the full activation of the network's feature extraction capabilities and impedes subsequent model improvements. In this paper, a novel fault diagnosis method based on data augmentation integrated with a feature-enhanced convolutional neural network (DA-FECNN) is proposed. First, Variational Autoencoder-Generative Adversarial Network (VAE-GAN) is utilized to expand the number of fault samples and improve the diversity of few-shot. Second, a uniform feature-enhanced structure is designed with a feature extraction module, a multi-scale classification module, and a reconstruction module. The network's ability on feature extraction is improved by focusing on high quality features. Thereby, accurate fault classification is realized simultaneously. Experiments on real world data show the effectiveness of the proposed method on fault diagnosis task.},
  keywords={Fault diagnosis;Learning systems;Accuracy;Limiting;Rolling bearings;Feature extraction;Data augmentation;Rolling bearing;Sample imbalance;Fault diagnosis;VAE-GAN;DA-FECNN},
  doi={10.1109/DDCLS61622.2024.10606651},
  ISSN={2767-9861},
  month={May},}@INBOOK{10952643,
  author={Jay, Rabi},
  booktitle={Enterprise AI in the Cloud: A Practical Guide to Deploying End-to-End Machine Learning and ChatGPT Solutions}, 
  title={Choosing Your AI/ML Algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={268-314},
  abstract={Summary <p>Choosing the right artificial intelligence (AI)/machine learning (ML) algorithm is like choosing the right tool for a job. This chapter covers different ML algorithms and explores aspects such as how they work, when to use them, and what use cases they can be employed for. It explores different categories of ML, such as supervised learning, unsupervised learning, and deep learning. The chapter presents a plethora of algorithms such as linear regression, decision trees, neural networks, and more. It discusses factors to consider when choosing an algorithm, ensuring that we have a robust methodology to align our choice with the problem at hand. Generative adversarial networks (GANs) are a type of machine learning model that is used to generate new data like the data that it is trained on. GANs have been used to create audio, text, and images that look realistic but do not exist in real life.</p>},
  keywords={Machine learning algorithms;Artificial intelligence;Machine learning;Prediction algorithms;Data models;Predictive models;Automobiles;Deep learning;Classification algorithms;Scalability},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394213078},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952643},}@ARTICLE{10891716,
  author={Li, Zhuangzhuang and Zhao, Kun and Chen, Pindong and Wang, Dawei and Yao, Hongxiang and Zhou, Bo and Lu, Jie and Wang, Pan and Zhang, Xi and Han, Ying and Liu, Yong},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Disentangled Representation Learning for Capturing Individualized Brain Atrophy via Pseudo-Healthy Synthesis}, 
  year={2025},
  volume={29},
  number={7},
  pages={5056-5068},
  abstract={Brain atrophy emerges as a distinctive hallmark in various neurodegenerative diseases, demonstrating a progressive trajectory across diverse disease stages and concurrently manifesting in tandem with a discernible decline in cognitive abilities. Understanding the individualized patterns of brain atrophy is critical for precision medicine and the prognosis of neurodegenerative diseases. However, it is difficult to obtain longitudinal data to compare changes before and after the onset of diseases. In this study, we present a deep disentangled generative model (DDGM) for capturing individualized atrophy patterns via disentangling patient images into “realistic” healthy counterfactual images and abnormal residual maps. The proposed DDGM consists of four modules: normal MRI synthesis, residual map synthesis, input reconstruction module, and mutual information neural estimator (MINE). The MINE and adversarial learning strategy together ensure independence between disease-related features and features shared by both disease and healthy controls. In addition, we proposed a comprehensive evaluation of the effectiveness of synthetic pseudo-healthy images, focusing on both their healthiness and subject identity. The results indicated that the proposed DDGM effectively preserves these characteristics in the synthesized pseudo-healthy images, outperforming existing methods. The proposed method demonstrates robust generalization capabilities across two independent datasets from different races and sites. Analysis of the disease residual/saliency maps revealed specific atrophy patterns associated with Alzheimer's disease (AD), particularly in the hippocampus and amygdala regions. These accurate individualized atrophy patterns enhance the performance of AD classification tasks, resulting in an improvement in classification accuracy to 92.50 $\pm$ 2.70%.},
  keywords={Diseases;Atrophy;Pathology;Translation;Grey matter;Accuracy;Magnetic resonance imaging;Generators;Electronic mail;Brain modeling;Biological validity;brain atrophy;disentangled representation learning;pseudo-healthy synthesis},
  doi={10.1109/JBHI.2025.3543218},
  ISSN={2168-2208},
  month={July},}@INPROCEEDINGS{10938417,
  author={Sawyer, Scott M.},
  booktitle={2024 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Privacy-Preserving AI for Document Understanding with Controlled Unclassified Information}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In manufacturing, manual exchange and review of engineering drawings constrains the supply chain. AI techniques have the potential to streamline these processes. This paper presents two important use cases and proposes solutions powered by generative AI models, including text and multimodal Large Language Models. The solutions preserve the privacy of documents used for training and inference and are thus suitable for controlled data. Performance is measured and discussed in the context of the applications, including technical and other practical considerations. Results indicate generative AI is comparable or better than previous solutions.},
  keywords={Training;Data privacy;Generative AI;Engineering drawings;Supply chains;Training data;Data science;Manufacturing;Artificial intelligence;Context modeling},
  doi={10.1109/HPEC62836.2024.10938417},
  ISSN={2643-1971},
  month={Sep.},}@ARTICLE{10789626,
  author={Rodrigues Perche Mahlow, Felipe and Zanella, André Felipe and Cruz Castañeda, William Alberto and Aparecida Sarzi-Ribeiro, Regilene},
  journal={IEEE Latin America Transactions}, 
  title={Illustrating Classic Brazilian Books using a Text-To-Image Diffusion Model}, 
  year={2024},
  volume={22},
  number={12},
  pages={1000-1008},
  abstract={In recent years, Generative Artificial Intelligence (GenAI) has undergone a profound transformation in addressing intricate tasks involving diverse modalities such as textual, auditory, visual, and pictorial generation. Within this spectrum, text-to-image (TTI) models have emerged as a formidable approach to generating varied and aesthetically appealing compositions, spanning applications from artistic creation to realistic facial synthesis, and demonstrating significant advancements in computer vision, image processing, and multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a paradigm shift in the domain of AI capabilities. This article delves into the feasibility of employing the Stable Diffusion LDM to illustrate literary works. For this exploration, seven classic Brazilian books have been selected as case studies. The objective is to ascertain the practicality of this endeavor and to evaluate the potential of Stable Diffusion in producing illustrations that augment and enrich the reader's experience. We will outline the beneficial aspects, such as the capacity to generate distinctive and contextually pertinent images, as well as the drawbacks, including any shortcomings in faithfully capturing the essence of intricate literary depictions. Through this study, we aim to provide a comprehensive assessment of the viability and efficacy of utilizing AI-generated illustrations in literary contexts, elucidating both the prospects and challenges encountered in this pioneering application of technology.},
  keywords={Artificial intelligence;Image synthesis;Diffusion models;Training;Visualization;Text to image;Noise reduction;Computational modeling;Refining;Ethics;image generation;diffusion models;text-to-image;illustration},
  doi={10.1109/TLA.2024.10789626},
  ISSN={1548-0992},
  month={Dec},}@ARTICLE{11031156,
  author={Nadeem, Mohammad and Sohail, Shahab Saquib and Cambria, Erik and Schuller, Björn W. and Hussain, Amir},
  journal={IEEE Intelligent Systems}, 
  title={Gender Bias in Text-to-Video Generation Models: A Case Study of Sora}, 
  year={2025},
  volume={40},
  number={3},
  pages={10-15},
  abstract={The advent of text-to-video generation models has revolutionized content creation as it produces high-quality videos from textual prompts. However, concerns regarding inherent biases in such models have prompted scrutiny, particularly regarding gender representation. Our study investigates the presence of gender bias in OpenAI’s Sora, a state-of-the-art text-to-video generation model. We uncover significant evidence of bias by analyzing the generated videos from a diverse set of gender-neutral and stereotypical prompts. The results indicate that Sora disproportionately associates specific genders with stereotypical behaviors and professions, which reflects societal prejudices embedded in its training data.},
  keywords={Analytical models;Leadership;Ethics;Generative AI;Prevention and mitigation;Training data;Cultural differences;Text to video;Intelligent systems;Content management;Gender issues},
  doi={10.1109/MIS.2025.3561475},
  ISSN={1941-1294},
  month={May},}@ARTICLE{10896807,
  author={Hu, Xiyuan and Qu, Jinglei and Chen, Chen},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Facilitating Continuous Facial Aging Through Latent Age Attribute Modulation}, 
  year={2025},
  volume={6},
  number={8},
  pages={2163-2177},
  abstract={In recent years, facial aging has attracted significant research interest due to its broad applications and potential benefits. While generative adversarial networks (GANs) have achieved notable progress in synthesizing realistic facial images, many GAN-based facial aging methods struggle to accurately capture the continuous progression of age-related changes over time. In this article, we propose an innovative framework featuring the latent age attribute module (LAAM), which maps age attributes to a structured latent space that facilitates efficient sampling for precise age attribute modeling. We further introduce the age-AdaIN fusion module (AFM), which seamlessly integrates age features from LAAM with facial content features, enabling the generation of images that exhibit smooth, continuous age transitions. This framework excels in capturing fine-grained aging details, particularly for elderly individuals. Quantitative and qualitative evaluations on benchmark datasets demonstrate the effectiveness of our approach in generating realistic age-progressed facial images, with a notable improvement in elderly aging accuracy and detail.},
  keywords={Aging;Faces;Convolution;Face recognition;Generators;Training;Modulation;Feature extraction;Couplings;Codes;Age-AdaIN fusion (AFM);continuous face aging;facial image generation;normalizing flows},
  doi={10.1109/TAI.2025.3543811},
  ISSN={2691-4581},
  month={Aug},}@INPROCEEDINGS{11092086,
  author={Qiu, Ziying and Peng, Shiqing and Lu, Jiachao},
  booktitle={2025 7th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={A Study on Undergraduate Students' Willingness to Use GAI Based on the Technology Acceptance Model 3}, 
  year={2025},
  volume={},
  number={},
  pages={1065-1070},
  abstract={The rapid development and widespread application of generative artificial intelligence (GAI) have provided university students with personalized learning opportunities. This study offers a new perspective on the application of GAI in the educational field by analyzing undergraduate students' technology acceptance of generative artificial intelligence. Using the Technology Acceptance Model (TAM3), this study examines students' usage behaviors of generative AI. Data were collected through a survey, and structural equation modeling was employed for systematic analysis. The findings reveal that university students demonstrate a high level of acceptance of generative artificial intelligence. This study provides new insights into the application of generative AI in education and holds significant theoretical and practical implications for promoting the use of generative AI among undergraduate students.},
  keywords={Surveys;Analytical models;Technology acceptance model;Uncertainty;Systematics;Generative AI;Education;Psychology;Mathematical models;Social factors;Generative AI;Undergraduate students;Technology acceptance model 3 (TAM3);Willingness to use GAI},
  doi={10.1109/CSTE64638.2025.11092086},
  ISSN={},
  month={April},}@ARTICLE{11153779,
  author={Ayobo-Abongo, Damos and Jaafar, Wael and Langar, Rami},
  journal={IEEE Communications Magazine}, 
  title={Generative AI-assisted Digital Twin for Power Asset Management}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Power asset management is a critical public safety service to ensure electrical networks' reliability, efficiency, and resilience. Traditional approaches, relying on periodic inspections and reactive maintenance, often leave the grid vulnerable to failures, costly outages, and operational inefficiencies. Adopting digital twin (DT) technology has introduced a paradigm shift by enabling real-time monitoring, predictive maintenance, and network optimization through virtual replicas of physical assets. However, the effectiveness of DTs is highly dependent on the timely availability of high-quality data, which remains a major challenge. In parallel, recent generative artificial intelligence (GenAI) advancements have demonstrated significant potential in data generation, predictive modeling, and decision support. This article proposes a novel framework integrating GenAI with DT technology to enhance power asset management. By leveraging GenAI techniques, such as generative adversarial networks (GANs) for data augmentation, vision-language models (VLMs) for automated asset analysis, and large language models (LLMs) for decision support, the proposed framework aims to improve asset performance monitoring, failure prediction, and maintenance planning. This integration enables a transition toward a proactive and intelligence-driven approach, thus contributing to a more robust, adaptive, and sustainable power infrastructure.},
  keywords={Asset management;Monitoring;Maintenance;Real-time systems;Sensors;Reliability;Vegetation mapping;Predictive models;Meteorology;Inspection},
  doi={10.1109/MCOM.001.2500155},
  ISSN={1558-1896},
  month={},}@ARTICLE{10967549,
  author={Yun, Qing},
  journal={IEEE Access}, 
  title={Vision Transformers (ViTs) for Feature Extraction and Classification of AI-Generated Visual Designs}, 
  year={2025},
  volume={13},
  number={},
  pages={69459-69477},
  abstract={Deep learning has become a cornerstone of modern Artificial Intelligence (AI), enabling machines to process and interpret complex visual information with unprecedented accuracy. As AI-generated content becomes more realistic, the ability to distinguish between machine-created and human-created images is increasingly important. This challenge extends beyond technical concerns, influencing digital media credibility, intellectual property rights, and the integrity of visual communication. Developing robust classification models to accurately attribute image origins is crucial for ensuring transparency, preventing misinformation, and upholding artistic authenticity in an era of rapidly evolving generative AI technologies. This study addresses the critical need to differentiate between AI-generated and human-generated aesthetic images through the application of advanced deep learning models. We investigate the effectiveness of advanced deep learning architectures including High Resolution Networks (HRNet), and Vision Transformers (ViT) which are generally accurate when used to infer the creative characteristics of human visual art. The proposed model ViT, employing a mechanism of self-attention to process images as sequences of patches for feature extraction, is examined for its potential to capture global contextual relationships within images, which is essential for recognizing the nuanced differences between AI and human artistry. ViT achieves 97% accuracy shows that superior performance validates its ability, using its transformer structure, to analyze and learn about the complex features of images which disclose their origin as compared to HRNet model of 95%. This research highlights the potential of using sophisticated deep learning techniques to address the challenges of content authenticity in digital media. By leveraging the unique strengths of each model, we provide insights into their applicability and effectiveness in distinguishing between different forms of digital creation, marking a significant step forward in the field of digital forensics and content verification.},
  keywords={Artificial intelligence;Art;Visualization;Deep learning;Transformers;Accuracy;Data models;Computer vision;Feature extraction;Computational modeling;Artificial intelligence;convolutional neural network;deep learning;vision transformer;visual aesthetics},
  doi={10.1109/ACCESS.2025.3562130},
  ISSN={2169-3536},
  month={},}@ARTICLE{11104959,
  author={Dou, Hongkun and Du, Jinyang and Jiang, Xingyu and Li, Hongjue and Yao, Wen and Deng, Yue},
  journal={IEEE Transactions on Image Processing}, 
  title={Image-to-Image Bayesian Flow Networks With Structurally Informative Priors}, 
  year={2025},
  volume={34},
  number={},
  pages={4968-4982},
  abstract={Generative models represented by diffusion models have recently shown great potential in image generation. They usually use a reverse iteration process to map noise into the data. However, for many real-world applications such as image restoration and translation, the model input comes from a distribution that is not random noise, making it difficult for these models to adapt directly to these tasks. In this paper, we introduce Image-to-Image Bayesian Flow Networks (I2I-BFNs), a novel framework for general-purpose image-to-image translation (I2I) that operates within the parameter space of distributions. This method upholds Gaussian distributions over pixel intensities, refining distribution parameters through closed-form Bayesian inference, steered by the network’s predictions for the target image. An essential aspect of our approach is the utilization of the conditional image as a robust prior parameter, initializing the translation process from a deterministic, clean image to reduce variance and produce interpretable generation. Additionally, we introduce a skip sampling technique that enhances the efficiency of I2I-BFNs, facilitating rapid translation in diverse image restoration and general I2I tasks. Our experimental evaluations showcase the model’s competitive edge in various settings, underscoring its efficacy and adaptability. This work contributes new insights and opportunities for the large-scale development of efficient conditional generation systems.},
  keywords={Bayes methods;Translation;Training;Image restoration;Noise;Diffusion models;Bridges;Noise measurement;Artificial intelligence;Adaptation models;Conditional generative models;image-to-image translation;Bayesian flow networks},
  doi={10.1109/TIP.2025.3592546},
  ISSN={1941-0042},
  month={},}@ARTICLE{10964554,
  author={Wang, Qinghe and Li, Baolu and Li, Xiaomin and Cao, Bing and Ma, Liqian and Lu, Huchuan and Jia, Xu},
  journal={IEEE Transactions on Image Processing}, 
  title={CharacterFactory: Sampling Consistent Characters With GANs for Diffusion Models}, 
  year={2025},
  volume={34},
  number={},
  pages={2544-2559},
  abstract={Recent advances in text-to-image models have opened new frontiers in human-centric generation. However, these models cannot be directly employed to generate images with consistent newly coined identities. In this work, we propose CharacterFactory, a framework that allows sampling new characters with consistent identities in the latent space of GANs for diffusion models. More specifically, we consider the word embeddings of celeb names as ground truths for the identity-consistent generation task and train a GAN model to learn the mapping from a latent space to the celeb embedding space. In addition, we design a context-consistent loss to ensure that the generated identity embeddings can produce identity-consistent images in various contexts. Remarkably, the whole model only takes 10 minutes for training, and can sample infinite characters end-to-end during inference. Extensive experiments demonstrate excellent performance of the proposed CharacterFactory on character creation in terms of identity consistency and editability. Furthermore, the generated characters can be seamlessly combined with the off-the-shelf image/video/3D diffusion models. We believe that the proposed CharacterFactory is an important step for identity-consistent character generation. Code and Gradio demo are available at: https://qinghew.github.io/CharacterFactory/},
  keywords={Diffusion models;Training;Text to image;Optimization;Noise reduction;Electronic mail;Character generation;Image synthesis;Data models;Data mining;Generative adversarial network;diffusion models;identity-consistent generation;character creation},
  doi={10.1109/TIP.2025.3558668},
  ISSN={1941-0042},
  month={},}@ARTICLE{10419041,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  abstract={Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474},
  ISSN={1558-2191},
  month={July},}@INPROCEEDINGS{8742317,
  author={Liu, Shuanglong and Zeng, Chenglong and Fan, Hongxiang and Ng, Ho-Cheung and Meng, Jiuxi and Que, Zhiqiang and Niu, Xinyu and Luk, Wayne},
  booktitle={2018 International Conference on Field-Programmable Technology (FPT)}, 
  title={Memory-Efficient Architecture for Accelerating Generative Networks on FPGA}, 
  year={2018},
  volume={},
  number={},
  pages={30-37},
  abstract={Generative adversarial networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks: a generative network (generator) and a discriminative network (discriminator). These two networks compete with each other to perform better at their respective tasks. The generator is typically a deconvolutional neural network and the discriminator is a convolutional neural network (CNN). Deconvolution performs a fundamentally new type of mathematical operation which differs from convolution. While the FPGA-based CNN accelerators have been widely studied in prior work, the acceleration of deconvolutional networks on FPGA is rarely explored. This paper proposes a novel parametrized deconvolutional architecture based on an FPGA-friendly method, in contrast to the transposed convolution implementation in CPUs and GPUs. Hardware design templates which map this architecture to FPGAs are provided with configurable deconvolutional layer parameters. Furthermore, a memory-efficient architecture with a new tiling method is proposed to accelerate the generator of GANs, by storing all intermediate data in on-chip memories and significantly reducing off-chip data transfers. The performance of the proposed accelerator is evaluated using a variety of GANs on a Xilinx Zynq 706 board, which shows 2.3x higher speed and 8.2x off-chip memory access reduction than an optimized Vanilla FPGA design. Compared to the respective implementations on CPUs and GPUs, the achieved improvements are in the range of 30x-92x in speed over an Intel 8-core i7-950 CPU, and 8x-108x in terms of Performance-per-Watt over an NVIDIA Titan X GPU.},
  keywords={Generators;Field programmable gate arrays;Computer architecture;Hardware;Convolution;Gallium nitride;Deconvolution;Generative adversarial networks (GANs);Field Programmable Gate Array;Hardware Acceleration;Deconvolution},
  doi={10.1109/FPT.2018.00016},
  ISSN={},
  month={Dec},}@ARTICLE{10638123,
  author={Wen, Jinbo and Nie, Jiangtian and Zhong, Yue and Yi, Changyan and Li, Xiaohuan and Jin, Jiangming and Zhang, Yang and Niyato, Dusit},
  journal={IEEE Internet of Things Journal}, 
  title={Diffusion-Model-Based Incentive Mechanism With Prospect Theory for Edge AIGC Services in 6G IoT}, 
  year={2024},
  volume={11},
  number={21},
  pages={34187-34201},
  abstract={The fusion of the Internet of Things (IoT) with sixth-generation (6G) technology has significant potential to revolutionize the IoT landscape. With the ultrareliable and low-latency communication capabilities of 6G, 6G-IoT networks can transmit high-quality and diverse data to enhance edge learning. Artificial intelligence-generated content (AIGC) harnesses advanced artificial intelligence (AI) algorithms to automatically generate various types of content. The emergence of edge AIGC integrates with edge networks, facilitating real-time provision of customized AIGC services by deploying AIGC models on edge devices. However, the current practice of edge devices as AIGC service providers (ASPs) lacks incentives, hindering the sustainable provision of high-quality edge AIGC services amidst information asymmetry. In this article, we develop a user-centric incentive mechanism framework for edge AIGC services in 6G-IoT networks. Specifically, we first propose a contract theory model for incentivizing ASPs to provide AIGC services to clients. Recognizing the irrationality of clients toward personalized AIGC services, we utilize prospect theory (PT) to capture their subjective utility better. Furthermore, we adopt the diffusion-based soft actor-critic algorithm to generate the optimal contract design under PT, outperforming traditional deep reinforcement learning algorithms. Our numerical results demonstrate the effectiveness of the proposed scheme.},
  keywords={Internet of Things;6G mobile communication;Contracts;Computational modeling;Biological system modeling;Artificial intelligence;Wireless communication;Contract theory;edge artificial intelligence-generated content (AIGC);generative diffusion models (GDMs);prospect theory (PT);sixth-generation (6G)-Internet of Things (IoT) networks},
  doi={10.1109/JIOT.2024.3445171},
  ISSN={2327-4662},
  month={Nov},}@INPROCEEDINGS{11058609,
  author={Ciabattini, Leonardo and Sciullo, Luca and de Marchi, Alberto and Gigli, Lorenzo and Bononi, Luciano and Di Felice, Marco},
  booktitle={2025 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
  title={Generative Digital Twin for Predictive Modeling in Dynamic IoT Scenarios}, 
  year={2025},
  volume={},
  number={},
  pages={98-105},
  abstract={Digital Twins (DTs) have emerged as a powerful tool for simulating, predicting, and optimizing the behavior of complex real-world systems. In many cases, DTs rely on real-world data generated by Internet of Things (IoT) systems and leverage Deep Learning (DL) techniques for predictive future system states. However, DTs often struggle to adapt to dynamic scenarios where additional loT devices and new configurations are introduced, requiring costly data collection and retraining. In this paper, we address this challenge by exploring the use of Generative Deep Learning (GDL) to build robust DTs capable of handling sparse, noisy, and biased sensor data streams. We propose a transformer-based GDL model that encodes variable-length collections of loT devices while supporting predictive capabilities under novel device configurations and the addition of new loT devices. We evaluate our proposed GDL model in two loT scenarios: smart homes and smart hydraulics systems, where physical models are used to generate the ground truth data. Experimental results demonstrate our approach's adaptability without retraining, achieving an $R^{2} =99.5\%$ when adding a temperature sensor in the smart home scenario (only 0.3% degradation), and $R^{2}=83.8 \%$ with 5 valves in the smart hydraulics scenario, comparing favorably to VARX despite VARX being retrained on each dataset.},
  keywords={Performance evaluation;Deep learning;Adaptation models;Smart homes;Hydraulic systems;Predictive models;Transformers;Data models;Digital twins;Internet of Things;Digital Twin;Generative Artificial Intelligence;Internet of Things;Simulation;Performance Evaluation},
  doi={10.1109/SMARTCOMP65954.2025.00071},
  ISSN={2693-8340},
  month={June},}@INPROCEEDINGS{10917672,
  author={Ge, Yinchi and Zhang, Hui and Sun, Haohang and Zhai, Xuyao and Wu, Mingxin},
  booktitle={2024 International Conference on Identification, Information and Knowledge in the Internet of Things (IIKI)}, 
  title={Privacy-Preserving Blockchain Transactions Synthesis with Diffusion Generative Models}, 
  year={2024},
  volume={},
  number={},
  pages={92-98},
  abstract={Blockchain transactions are increasingly vulnerable to sophisticated attacks, necessitating detection mechanisms that safeguard user privacy. Traditional methods, such as CTGAN and PATE-GAN, often sacrifice privacy to enhance detection accuracy, or compromise detection capabilities to maintain privacy, struggling to effectively balance both. This paper introduces BTGM (Blockchain Transactions Generative Models), an advanced approach that ingeniously combines differential privacy with a two-stage diffusion process, and incorporates Row Modeling based on VGM (Variable Gaussian Mixture) encoding. This hybrid model not only ensures the robust anonymization of transaction data through differential privacy but also effectively learns the distribution of data features via its innovative use of Row Modeling and diffusion processes. Validated on the Elliptic++ dataset, BTGM achieves 0.96 accuracy and 0.86 PRAUC, outperforming SOTA models like CTGAN and STaSy. It effectively thwarts membership inference attacks, maintaining a strong balance between privacy protection and attack detection.},
  keywords={Privacy;Differential privacy;Accuracy;Diffusion processes;Data models;Blockchains;Information filtering;Internet of Things;Protection;Information integrity;Blockchain Transactions;Privacy;Generative Models;Artificial Intelligence;Diffusion Models;Differential Privacy},
  doi={10.1109/IIKI65561.2024.00026},
  ISSN={},
  month={Dec},}@ARTICLE{9905589,
  author={Natarajan, B. and Rajalakshmi, E. and Elakkiya, R. and Kotecha, Ketan and Abraham, Ajith and Gabralla, Lubna Abdelkareim and Subramaniyaswamy, V.},
  journal={IEEE Access}, 
  title={Development of an End-to-End Deep Learning Framework for Sign Language Recognition, Translation, and Video Generation}, 
  year={2022},
  volume={10},
  number={},
  pages={104358-104374},
  abstract={The recent developments in deep learning techniques evolved to new heights in various domains and applications. The recognition, translation, and video generation of Sign Language (SL) still face huge challenges from the development perspective. Although numerous advancements have been made in earlier approaches, the model performance still lacks recognition accuracy and visual quality. In this paper, we introduce novel approaches for developing the complete framework for handling SL recognition, translation, and production tasks in real-time cases. To achieve higher recognition accuracy, we use the MediaPipe library and a hybrid Convolutional Neural Network + Bi-directional Long Short Term Memory (CNN + Bi-LSTM) model for pose details extraction and text generation. On the other hand, the production of sign gesture videos for given spoken sentences is implemented using a hybrid Neural Machine Translation (NMT) + MediaPipe + Dynamic Generative Adversarial Network (GAN) model. The proposed model addresses the various complexities present in the existing approaches and achieves above 95% classification accuracy. In addition to that, the model performance is tested in various phases of development, and the evaluation metrics show noticeable improvements in our model. The model has been experimented with using different multilingual benchmark sign corpus and produces greater results in terms of recognition accuracy and visual quality. The proposed model has secured a 38.06 average Bilingual Evaluation Understudy (BLEU) score, remarkable human evaluation scores, 3.46 average Fréchet Inception Distance to videos (FID2vid) score, 0.921 average Structural Similarity Index Measure (SSIM) values, 8.4 average Inception Score, 29.73 average Peak Signal-to-Noise Ratio (PSNR) score, 14.06 average Fréchet Inception Distance (FID) score, and an average 0.715 Temporal Consistency Metric (TCM) Score which is evidence of the proposed work.},
  keywords={Assistive technologies;Gesture recognition;Deep learning;Generative adversarial networks;Real-time systems;Streaming media;Visualization;Sign language;Video recording;Deep learning;generative adversarial networks;sign language recognition;sign language translation;video generation},
  doi={10.1109/ACCESS.2022.3210543},
  ISSN={2169-3536},
  month={},}@ARTICLE{9756847,
  author={Chen, Mu-Yen and Chiang, Hsiu-Sen and Huang, Wei-Kai},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Efficient Generative Adversarial Networks for Imbalanced Traffic Collision Datasets}, 
  year={2022},
  volume={23},
  number={10},
  pages={19864-19873},
  abstract={Rapid breakthroughs in information technologies have driven substantial developments in artificial intelligence applications, particularly the widespread use of deep learning techniques in domains such as speech, image and text recognition. However, real world data distribution applications suffer from significant problems including data imbalance which can easily lead to machine learning biased towards the side with more data, resulting in inaccurate classification or prediction results. Therefore, effectively addressing data imbalance is a pressing research topic. Generative Adversarial Networks (GAN) addresses data imbalance, but is prone to vanishing gradients. Recent work has thus focused on improving the GAN architecture to resolve this problem. The present research extends these efforts, applying C4.5, Random Forest, Support Vector Machine, K-Nearest Neighbor and Naïve Bayes classification algorithms to a single imbalanced traffic collision dataset to identify methods for improving prediction results. Experimental results show that classification performance significantly improves after data augmentation using Synthetic Minority Oversampling Technique, GAN, Conditional GAN, and Gaussian Discriminant Analysis GAN as compared with the non-augmented dataset. In addition, the Gaussian Discriminant Analysis GAN with Naïve Bayes classifier produces a dataset that optimizes classification performance for traffic accident prediction at highway intersections.},
  keywords={Generative adversarial networks;Generators;Training;Data models;Probability distribution;Prediction algorithms;Deep learning;Data imbalanced;traffic collision;data augmentation;GAN;GDAGAN},
  doi={10.1109/TITS.2022.3162395},
  ISSN={1558-0016},
  month={Oct},}@ARTICLE{9334417,
  author={Wang, Zi-Ming and Li, Meng-Han and Xia, Gui-Song},
  journal={IEEE Transactions on Image Processing}, 
  title={Conditional Generative ConvNets for Exemplar-Based Texture Synthesis}, 
  year={2021},
  volume={30},
  number={},
  pages={2461-2475},
  abstract={The goal of exemplar-based texture synthesis is to generate texture images that are visually similar to a given exemplar. Recently, promising results have been reported by methods relying on convolutional neural networks (ConvNets) pretrained on large-scale image datasets. However, these methods have difficulties in synthesizing image textures with non-local structures and extending to dynamic or sound textures. In this article, we present a conditional generative ConvNet (cgCNN) model which combines deep statistics and the probabilistic framework of generative ConvNet (gCNN) model. Given a texture exemplar, cgCNN defines a conditional distribution using deep statistics of a ConvNet, and synthesizes new textures by sampling from the conditional distribution. In contrast to previous deep texture models, the proposed cgCNN does not rely on pre-trained ConvNets but learns the weights of ConvNets for each input exemplar instead. As a result, cgCNN can synthesize high quality dynamic, sound and image textures in a unified manner. We also explore the theoretical connections between our model and other texture models. Further investigations show that the cgCNN model can be easily generalized to texture expansion and inpainting. Extensive experiments demonstrate that our model can achieve better or at least comparable results than the state-of-the-art methods.},
  keywords={Adaptation models;Image texture;Heuristic algorithms;Probabilistic logic;Maximum likelihood estimation;Generators;Feature extraction;Texture synthesis;deep neural networks;generative model},
  doi={10.1109/TIP.2021.3052075},
  ISSN={1941-0042},
  month={},}@ARTICLE{10236563,
  author={Wang, Zhen and Zhang, Yang and Pang, Yan and Wang, Nannan and Bah, Mohamed Jaward and Li, Ke and Zhang, Ji},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Toward Learning Joint Inference Tasks for IASS-MTS Using Dual Attention Memory With Stochastic Generative Imputation}, 
  year={2024},
  volume={35},
  number={12},
  pages={17504-17518},
  abstract={Irregularly, asynchronously and sparsely sampled multivariate time series (IASS-MTS) are characterized by sparse and uneven time intervals and nonsynchronous sampling rates, posing significant challenges for machine learning models to learn complex relationships within and beyond IASS-MTS to support various inference tasks. The existing methods typically either focus solely on single-task forecasting or simply concatenate them through a separate preprocessing imputation procedure for the subsequent classification application. However, these methods often ignore valuable annotated labels or fail to discover meaningful patterns from unlabeled data. Moreover, the approach of separate prefilling may introduce errors due to the noise in raw records, and thus degrade the downstream prediction performance. To overcome these challenges, we propose the time-aware dual attention and memory-augmented network (DAMA) with stochastic generative imputation (SGI). Our model constructs a joint task learning architecture that unifies imputation and classification tasks collaboratively. First, we design a new time-aware DAMA that accounts for irregular sampling rates, inherent data nonalignment, and sparse values in IASS-MTS data. The proposed network integrates both attention and memory to effectively analyze complex interactions within and across IASS-MTS for the classification task. Second, we develop the stochastic generative imputation (SGI) network that uses auxiliary information from sequence data for inferring the time series missing observations. By balancing joint tasks, our model facilitates interaction between them, leading to improved performance on both classification and imputation tasks. Third, we evaluate our model on real-world datasets and demonstrate its superior performance in terms of imputation accuracy and classification results, outperforming the baselines.},
  keywords={Time series analysis;Task analysis;Interpolation;Stochastic processes;Learning systems;Data models;Encoding;Deep generative model;dual attention memory;irregularly sampled mutivariate time series;joint task learning;stochastic imputation},
  doi={10.1109/TNNLS.2023.3305542},
  ISSN={2162-2388},
  month={Dec},}@INPROCEEDINGS{10207938,
  author={Wang, Alex X. and Chukova, Stefanka S. and Simpson, Colin R. and Nguyen, Binh P.},
  booktitle={2023 IEEE Statistical Signal Processing Workshop (SSP)}, 
  title={Data-centric AI to Improve Early Detection of Mental Illness}, 
  year={2023},
  volume={},
  number={},
  pages={369-373},
  abstract={The growth of information technology and advancements in artificial intelligence (AI) have made data creation and usage more prevalent. AI research can be grouped into two categories: model-centric and data-centric. Model-centric AI focuses on using the same data and making changes to model hyper-parameters, architectures, and other configurations. Data-centric AI, on the other hand, prioritizes improving existing data or incorporating new data to improve the performance of machine learning (ML) algorithms. Data-centric AI can greatly improve the performance of machine learning models by improving data quality, increasing data diversity, and using advanced data augmentation methods. The use of ML for early detection of mental health issues is vital due to its ability to identify issues early, provide personalized treatments, detect patterns, and increase accessibility to mental health services. While there have been numerous mental illness detection studies using model-centric approaches, there is a lack of research from a data-centric AI perspective. This study aims to address this gap by comparing established tabular data synthesis methods to explore the impact of synthetic data and data-centric AI on the early detection of mental health issues.},
  keywords={Surveys;Machine learning algorithms;Government;Signal processing algorithms;Mental health;Signal processing;Data augmentation;data-centric AI;synthetic data;mental illness;early detection;machine learning},
  doi={10.1109/SSP53291.2023.10207938},
  ISSN={2693-3551},
  month={July},}@ARTICLE{10612835,
  author={Xu, Kaiwen and Hu, Xiyuan and Zhou, Xiaokang and Xu, Xiaolong and Qi, Lianyong and Chen, Chen},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={RLGC: Reconstruction Learning Fusing Gradient and Content Features for Efficient Deepfake Detection}, 
  year={2024},
  volume={70},
  number={3},
  pages={6084-6094},
  abstract={Current deepfake detection methods, which utilize noise features, localized textures, or frequency statistics, may perform well in special domains or forgery methods. But the generalization performance of these methods is often unsatisfactory because of the ignorance of mining intrinsic facial features. To address this problem, we re-evaluated the fusion of image gradient features in neural networks and delved deeper into the intrinsic structure of input images. Consequently, we propose a reconstruction-classification network that initially learns face content and gradient separately from a reconstruction perspective and then detects forged faces by fusing them together. This paper introduces three well-designed components: 1) a dual-branch feature extraction module to excite distributional inconsistencies between real and forged faces; 2) a content-gradient feature fusion module to investigate the relationship between face content and image gradient; 3) a reconstruction disparity based Bi-Directional attention module that guides the model in efficiently categorizing the fused features. Extensive experiments on large-scale benchmark datasets demonstrate that our method significantly enhances performance, especially for generalization ability, compared to state-of-the-art methods.},
  keywords={Image reconstruction;Feature extraction;Forgery;Faces;Face recognition;Deepfakes;Generative adversarial networks;Deepfake detection;deep generative model;multi-scale feature fusion;reconstruction learning},
  doi={10.1109/TCE.2024.3435032},
  ISSN={1558-4127},
  month={Aug},}@ARTICLE{10570156,
  author={Huang, Xu and Ye, Yunming and Zhang, Bowen and Lin, Huiwei and Sun, Yuxi and Li, Xutao and Luo, Chuyao},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Toward a Variation-Aware and Interpretable Model for Radar Image Sequence Prediction}, 
  year={2024},
  volume={20},
  number={10},
  pages={11532-11541},
  abstract={Radar image sequence prediction (RISP) aims to predict future radar images based on historical observations. In the past few years, neural network-based methods have shown impressive performance for RISP. However, two limitations stills exist. 1) They fail to exploit variation information when capturing spatial dependencies. 2) They neglect to analyze and interpret the model. In this article, we propose a variation-aware prediction model for the first limitation, and develop a relevance propagation technique for the second one. Specifically, 1) we recustomize the vanilla convolution by introducing a variation-aware item. The new convolution unit yields two advantages when capturing spatial dependencies, i.e., exploiting variation information and offering spatially-varying kernels. As a result, it can learn the diverse and complex radar echo patterns. By equipping the unit into a typical network (PredRNN), we propose a novel prediction model, dubbed as VA-PredRNN. 2) As for analyzing our model, we propagate the output backward layer by layer till the input. Hence, we can reveal the relevance between the output and the intermediate states. To the best of the authors' knowledge, this is the first work to study the interpretability of a multilayer RISP model. We conduct extensive experiments on two datasets, and the results demonstrate the effectiveness of our VA-PredRNN. We also carry out a series of analyses using the proposed relevance propagation technique. According to the results, we discover the importance of different states.},
  keywords={Convolutional neural networks;Predictive models;Task analysis;Spatiotemporal phenomena;Radar;Image sequences;Explainable artificial intelligence (XAI);neural networks;radar image sequence prediction (RISP);relevance propagation},
  doi={10.1109/TII.2024.3399401},
  ISSN={1941-0050},
  month={Oct},}@INPROCEEDINGS{11052930,
  author={S, Prema and Apparna, C. and Gayathri, M.},
  booktitle={2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)}, 
  title={AI-Enabled Wildfire Risk Mapping & Emergency Navigation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Wildfires significantly threaten ecosystems, human settlements, and infrastructure, requiring advanced technologies for risk assessment and emergency response. This project introduces an AI-enabled Wildfire Risk Mapping and Emergency Navigation system that uses deep learning techniques to improve wildfire prediction, risk assessment, and real-time emergency response. The system utilizes Convolutional Neural Networks (CNNs) to analyze satellite imagery and climate data and identify high-risk zones. NLP-powered chatbots process real-time reports from social media, emergency broadcasts, and citizen input to improve situational awareness. In addition, Generative AI models simulate potential wildfire scenarios and suggest optimal evacuation strategies. By leveraging AI-driven risk mapping, real-time data analysis, and generative simulations, this solution improves disaster resilience, enhances early warnings, and optimizes evacuation strategies, ultimately minimizing loss of life and property.},
  keywords={Wildfires;Generative AI;Disasters;Emergency services;Satellite navigation systems;Real-time systems;Satellite images;Risk management;Resilience;Meteorology;Artificial Intelligence;Deep Learning;Convolutional Neural Networks (CNNs);Natural Language Processing (NLP);Generative AI;Satellite Imagery Analysis;Climate Data;Real-time Data Processing;Risk Assessment;Disaster Resilience;Emergency Response;Evacuation Strategies},
  doi={10.1109/ICCRTEE64519.2025.11052930},
  ISSN={},
  month={May},}@ARTICLE{9131701,
  author={Kang, Jae-Mo and Chun, Chang-Jae and Kim, Il-Min},
  journal={IEEE Access}, 
  title={Deep Learning Based Channel Estimation for MIMO Systems With Received SNR Feedback}, 
  year={2020},
  volume={8},
  number={},
  pages={121162-121181},
  abstract={Channel estimation with received signal-to-noise ratio (SNR) feedback is promising and effective for practical wireless multiple-input multiple-output (MIMO) systems. In this paper, we investigate the channel estimation problem for the MIMO system with received SNR feedback, of which goal is to estimate the MIMO channel coefficients at a transmitter based on the received SNR feedback information from a receiver in the sense of minimizing the mean square error (MSE) of the channel estimation. For analysis, we consider two very common and widely adopted scenarios of fading: (i) quasi-static block fading and (ii) time-varying fading. In both fading scenarios, it is generally challenging to analytically tackle the channel estimation problem due to its nonlinearity and nonconvexity. To intelligently and effectively address this issue, deep learning is exploited in this paper. First, in the quasi-static block fading scenario, we propose a novel learning scheme for joint channel estimation and pilot signal design by constructing a deep autoencoder via a convolutional neural network (CNN). Also, in the time-varying fading scenario, a novel channel estimation scheme is developed by connecting a recurrent neural network (RNN) to a CNN. Moreover, in both fading scenarios, we present new and effective ways to train the proposed schemes using generative adversarial networks (GANs) to address the practical issue of a limited number of actual channel samples (i.e., real-world data) required for training. Through extensive numerical simulations, we demonstrate effectiveness and superior performance of the proposed schemes.},
  keywords={Channel estimation;Signal to noise ratio;MIMO communication;Transmitters;Fading channels;Deep learning;Feature extraction;Autoencoder;channel estimation;convolutional neural network (CNN);deep learning;generative adversarial network (GAN);recurrent neural network (RNN);received SNR feedback;pilot signal design},
  doi={10.1109/ACCESS.2020.3006518},
  ISSN={2169-3536},
  month={},}@ARTICLE{10459195,
  author={Neifar, Nour and Ben-Hamadou, Achraf and Mdhaffar, Afef and Jmaiel, Mohamed and Freisleben, Bernd},
  journal={IEEE Access}, 
  title={Leveraging Statistical Shape Priors in GAN-Based ECG Synthesis}, 
  year={2024},
  volume={12},
  number={},
  pages={36002-36015},
  abstract={Electrocardiogram (ECG) data collection during emergency situations is challenging, making ECG data generation an efficient solution for dealing with highly imbalanced ECG training datasets. In this paper, we propose a novel approach for ECG signal generation using Generative Adversarial Networks (GANs) and statistical ECG data modeling. Our approach leverages prior knowledge about ECG dynamics to synthesize realistic signals, addressing the complex dynamics of ECG signals. To validate our approach, we conducted experiments using ECG signals from the MIT-BIH arrhythmia database. Our results demonstrate that our approach, which models temporal and amplitude variations of ECG signals as 2-D shapes, generates more realistic signals compared to state-of-the-art GAN based generation baselines. Our proposed approach has significant implications for improving the quality of ECG training datasets, which can ultimately lead to better performance of ECG classification algorithms. This research contributes to the development of more efficient and accurate methods for ECG analysis, which can aid in the diagnosis and treatment of cardiac diseases.},
  keywords={Electrocardiography;Training;Generative adversarial networks;Solid modeling;Heuristic algorithms;Deep learning;Physiology;Emergency services;Shape control;Statistical analysis;Data models;Cardiovascular diseases;GAN;deep learning;ECG;time series;physiological signals},
  doi={10.1109/ACCESS.2024.3373724},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9070241,
  author={Lee, Seyun and Kim, Ji-Hwan and Heo, Jae-Pil},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Super-Resolution of License Plate Images via Character-Based Perceptual Loss}, 
  year={2020},
  volume={},
  number={},
  pages={560-563},
  abstract={License Plate Recognition (LPR) is an highly influential problem in computer vision. In this paper, we present a super-resolution model specialized for the license plate images, CSRGAN, trained with a novel character-based perceptual loss. Specifically, we focus on the character-level recognizability of the super-resolved images rather than the pixel-level reconstruction. Experimental results validate the benefits of our proposed method in both quantitative and qualitative aspects. In particular, our method achieves a higher character-level recognition accuracy over the state-of-the-art image super-resolution techniques.},
  keywords={Image resolution;License plate recognition;Character recognition;Image recognition;Optical character recognition software;Feature extraction;Generators;Super-Resolution;License Plate Recognition;Generative},
  doi={10.1109/BigComp48618.2020.000-1},
  ISSN={2375-9356},
  month={Feb},}@ARTICLE{9109281,
  author={Sun, Liang and Song, Junjie and Wang, Ye and Li, Baoyu},
  journal={IEEE Access}, 
  title={Cooperative Coupled Generative Networks for Generalized Zero-Shot Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={119287-119299},
  abstract={Compared with zero-shot learning (ZSL), the generalized zero-shot learning (GZSL) is more challenging since its test samples are taken from both seen and unseen classes. Most previous mapping-based methods perform well on ZSL, while their performance degrades on GZSL. To solve this problem, inspired by the ensemble learning, this paper proposes a model with cooperative coupled generative networks (CCGN). Firstly, to alleviate the hubness problem, the reverse visual feature space is taken as the embedding space, with the mapping achieved by a visual feature center generation network. To learn a proper visual representation of each class, we propose a coupled of generative networks, which cooperate with each other to synthesize a visual feature center template of the class. Secondly, to improve the generative ability of the coupled networks, we further employ a deeper network to generate. Meanwhile, to alleviate loss semantic information problem caused by multiple network layers, a residual module is employed. Thirdly, to mitigate overfitting and to increase scalability, an adversarial network is introduced to discriminate the generation of visual feature centers. Finally, a reconstruction network, which reverses the generation process, is employed to restrict the structural correlation between the generated visual feature center and the original semantic representation of each class. Extensive experiments on five benchmark datasets (AWA1, AWA2, CUB, SUN, APY) demonstrate that the proposed algorithm yields satisfactory results, as compared with the state-of-the-art methods.},
  keywords={Visualization;Semantics;Generative adversarial networks;Neural networks;Correlation;Training;Task analysis;Zero-shot learning;generalized zero-shot learning;generative adversarial network;neural network;residual module},
  doi={10.1109/ACCESS.2020.3000347},
  ISSN={2169-3536},
  month={},}@ARTICLE{10121681,
  author={Way, Der-Lor and Lo, Chang-Hao and Wei, Yu-Hsien and Shih, Zen-Chung},
  journal={IEEE Access}, 
  title={TwinGAN: Twin Generative Adversarial Network for Chinese Landscape Painting Style Transfer}, 
  year={2023},
  volume={11},
  number={},
  pages={60844-60852},
  abstract={Recently, style transfers have received considerable attention. However, most of these studies were suitable for Western paintings. In this paper, a deep learning method is proposed to imitate multiple styles of Chinese landscape paintings. Twin generative adversarial network style transfer was proposed based on the characteristics of Chinese landscape ink paintings. SketchGAN and renderGAN were performed using generative models based on generative adversarial networks. The SketchGAN involves determining the structure and simplifying the content of an input image. RenderGAN involves transferring the results of sketchGAN into the final stylized image. Moreover, a loss function was designed to maintain the shape of the input content image. Finally, the proposed TwinGAN was successfully used to imitate five styles of Chinese landscape ink paintings. This study also provided ablation studies and comparisons with previous works. The experimental results show that our algorithm synthesizes Chinese landscape stylized paintings that are higher in quality than those produced by previous algorithms.},
  keywords={Painting;Generative adversarial networks;Ink;Generators;Feature extraction;Deep learning;Neural networks;Image processing;Deep neural networks;style transfer;generative adversarial network (GAN);loss function;Chinese landscape painting},
  doi={10.1109/ACCESS.2023.3274666},
  ISSN={2169-3536},
  month={},}@ARTICLE{10539967,
  author={Zhang, Rui and Mariano, Vladimir Y.},
  journal={IEEE Access}, 
  title={Integration of Emotional Factors With GAN Algorithm in Stock Price Prediction Method Research}, 
  year={2024},
  volume={12},
  number={},
  pages={77368-77378},
  abstract={Stock prices are characterized by non-stationarity and volatility, and investors are easily influenced by their own emotions, and their investment decision-making behavior is characterized by irrationality, so stock prices are difficult to predict. This paper proposes a stock price prediction method based on a generative adversarial network combining sentiment factors with financial data. The method is based on the GAN algorithm, which better matches the logic behind the operation of the stock market, and introduces the text branch to realize the sentiment analysis of the text of the research report, which provides the public opinion influence factor for the algorithmic model, and proposes the TK-GAN model. The model combines financial data with sentiment factors to reduce the error of model training, and introduces Soft Attention to improve the learning ability of stock-related data features. In addition, the TK-GAN model utilizes BERT for financial domain fine-tuning in the text branch to increase the model’s suitability for specific financial domains. Based on the TK-GAN model, AdamK, a learning rate adaptive optimization algorithm, which is more suitable for this research, is proposed. The experimental results of the model on the Kweichow Moutai stock dataset show that the MAE and MSE values are as low as 0.01949 and 0.00091 respectively.},
  keywords={Generative adversarial networks;Predictive models;Generators;Data models;Prediction algorithms;Long short term memory;Training;Stock markets;Investment;Stock prediction;sentiment factors;GAN;soft attention;optimizer},
  doi={10.1109/ACCESS.2024.3406223},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9288276,
  author={McGonigle, Daniel and Wang, Tianyang and Yuan, Juefei and He, Kai and Li, Bo},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={I2S2: Image-to-Scene Sketch Translation Using Conditional Input and Adversarial Networks}, 
  year={2020},
  volume={},
  number={},
  pages={773-778},
  abstract={Image generation from sketch is a popular and well-studied computer vision problem. However, the inverse problem image-to-sketch (I2S) synthesis still remains open and challenging, let alone image-to-scene sketch (I2S2) synthesis, especially when full-scene sketch generations are highly desired. In this paper, we propose a framework for generating full-scene sketch representations from natural scene images, aiming to generate outputs that approximate hand-drawn scene sketches. Specifically, we exploit generative adversarial models to produce full-scene sketches given arbitrary input images that are actually conditions which are incorporated to guide the distribution mapping in the context of adversarial learning. To advance the use of such conditions, we further investigate edge detection solutions and propose to utilize Holistically-nested Edge Detection (HED) maps to condition the generative model. We conduct extensive experiments to validate the proposed framework and provide detailed quantitative and qualitative evaluations to demonstrate its effectiveness. In addition, we also demonstrate the flexibility of the proposed framework by using different conditional inputs, such as the Canny edge detector.},
  keywords={Inverse problems;Image synthesis;Image edge detection;Conferences;Detectors;Tools;Context modeling;image generation;scene sketch;image-to-scene sketch translation;conditional input;generative adversarial networks;edge map},
  doi={10.1109/ICTAI50040.2020.00123},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{9913966,
  author={Liu, Hangyu and Liu, Qicheng},
  journal={IEEE Access}, 
  title={Image Creation Based on Transformer and Generative Adversarial Networks}, 
  year={2022},
  volume={10},
  number={},
  pages={108296-108306},
  abstract={To address the problem of low authenticity of generated images in existing generative models, the transformer super-resolution generative adversarial network(TransSRGAN) model based on the generative adversarial network is proposed. The generator of the model uses the transformer encoder sub-module as the basic module. The features of the input vector are extracted. low-definition images are generated through the transformer encoder submodule, and the low-definition image is up-sampled by the convolutional neural network to complete the image generation. The discriminator of this model uses the convolutional neural network as the basic module. To discriminate the real samples from the generated fake samples, the discriminator extracts the image features by the convolutional neural network. The experimental results show that the TransSRGAN model brings the distribution of the generated samples closer to the training samples, effectively raises the quality of the generated samples, improves the authenticity of the generated samples, and enriches the diversity of the generated samples. During the training process, there was no mode collapse or instability.},
  keywords={Generative adversarial networks;Transformers;Feature extraction;Training data;Convolutional neural networks;Image generation;Image synthesis;Superresolution;Encoding;Image generation;generative adversarial network;transformer;self-attention},
  doi={10.1109/ACCESS.2022.3213079},
  ISSN={2169-3536},
  month={},}@ARTICLE{10005123,
  author={You, Jiawei and Huang, Ganyu and Han, Tianyuan and Yang, Haoze and Shen, Liping},
  journal={IEEE Access}, 
  title={A Unified Framework From Face Image Restoration to Data Augmentation Using Generative Prior}, 
  year={2023},
  volume={11},
  number={},
  pages={2907-2919},
  abstract={In the domain of data enhancement, image restoration and data augmentation are two tasks gaining increasing attention. Current image restoration models focus on improving clarity using pre-trained generative models, and data augmentation methods try to generate new samples with the help of generative models. These two related topics have long been studied completely separately. We propose a downstream-friendly restoration framework based on pre-trained generative models with the capability of data augmentation for face images. We carefully design our framework to achieve high fidelity when inheriting the generation ability from the pre-trained generator. To achieve this goal, we use a modified U-Net to predict the biases of latent codes and feature maps to guide the generator. We further propose to adopt linear interpolation as an approach to enriching the datasets for downstream tasks, especially for class-imbalanced tasks. Effectiveness of our method is demonstrated through experiments on three datasets and one downstream task.},
  keywords={Image restoration;Generators;Facial recognition;Task analysis;Data models;Codes;Interpolation;Generative adversarial networks;Interpolation;Data augmentation;Face image restoration;data augmentation;generative adversarial network;linear interpolation;latent inversion},
  doi={10.1109/ACCESS.2022.3233868},
  ISSN={2169-3536},
  month={},}@ARTICLE{9954016,
  author={AlBasiouny, Eman R. and Heliel, Abdel-Fattah Attia and Abdelmunim, Hossam E. and Abbas, Hazem M.},
  journal={IEEE Access}, 
  title={Multilayer Perceptron Generative Model via Adversarial Learning for Robust Visual Tracking}, 
  year={2022},
  volume={10},
  number={},
  pages={121230-121248},
  abstract={Visual tracking is an open and exciting field of research. The researchers introduced great efforts to be close to the ideal state of stable tracking of objects regardless of different appearances or circumstances. Owing to the attractive advantages of generative adversarial networks (GANs), they have been a promising area of research in many fields. However, GAN network architecture has not been thoroughly investigated in the visual tracking research community. Inspired by visual tracking via adversarial learning (VITAL), we present a novel network to generate randomly initialized masks for building augmented feature maps using multilayer perceptron (MLP) generative models. To obtain more robust tracking these augmented masks can extract robust features that do not change over a long temporal span. Some models such as deep convolutional generative adversarial networks (DCGANs) have been proposed to obtain powerful generator architectures by eliminating or minimizing the use of fully connected layers. This study demonstrates that the use of MLP architecture for the generator is more robust and efficient than the convolution-only architecture. Also, to realize better performance, we used one-sided label smoothing to regularize the discriminator in the training stage and the label smoothing regularization (LSR) method to reduce the overfitting of the classifier in the online tracking stage. The experiments show that the proposed model is more robust than the DCGAN model and offers satisfactory performance compared with the state-of-the-art deep visual trackers on OTB-100, VOT2019 and LaSOT datasets.},
  keywords={Feature extraction;Target tracking;Visualization;Generators;Adversarial machine learning;Generative adversarial networks;Deep learning;Deep learning;generative adversarial network;multilayer perceptron;visual tracking},
  doi={10.1109/ACCESS.2022.3222867},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9994087,
  author={Al-Taani, Ahmad T. and Al-Rababaah, Batool},
  booktitle={2022 International Arab Conference on Information Technology (ACIT)}, 
  title={Detection of Covid-19 Virus using Supervised Machine Learning Algorithms}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Due to the continuous increase of Covid-19 infections as a global pandemic, it became necessary to detect it to avoid the damage caused by the spread of the infection. Artificial Intelligence (AI) techniques such as machine learning and deep learning have an important and effective role in the medical field applications like the classification of medical images and the detection of many diseases. In this article, we propose the use of several supervised machine learning classifiers for Covid-19 virus detection using chest x-ray (CXR) images. Five supervised classifiers are used: Support Vector Machines (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN), Logistic Regression (LR) and Artificial Neural Network (ANN). A standard dataset of 1824 CXR images are used for training and testing; 70% for training and 30% for testing. Four image embedders including Vgg16, Vgg19, SqueezeNet, and Inception-v3 are used in the experiments. Experiment results showed that most of these models achieved promising accuracy, precision, recall, and F1-scores. KNN, ANN, and LR classifiers have achieved highest classification accuracies using SqueezeNet image embedder.},
  keywords={COVID-19;Training;Support vector machines;Machine learning algorithms;Pandemics;Artificial neural networks;X-ray imaging;Supervised machine learning;x-ray images;image embedder;Covid-19;CNN models;deep learning},
  doi={10.1109/ACIT57182.2022.9994087},
  ISSN={2831-4948},
  month={Nov},}@INPROCEEDINGS{10632662,
  author={Zhang, Jie-Long and Zhu, Qun-Xiong and Song, Xiao-Lu and He, Yan-Lin and Xu, Yuan},
  booktitle={2024 5th International Conference on Industrial Engineering and Artificial Intelligence (IEAI)}, 
  title={Virtual Sample Generation Using Conditional Adversarial Network with Latent Spaces as Noise Inputs}, 
  year={2024},
  volume={},
  number={},
  pages={47-52},
  abstract={Data-driven soft sensors are widely applied in the chemical industry. However, due to the production conditions and resource limitations, the soft sensor models faced the challenge of poor prediction effect caused by the small sample. To solve this challenge, this paper puts forward a novel virtual sample generation (VSG) method based on conditional generative adversarial networks with latent spaces as noise inputs (CGAN-LSNI). In the CGAN-LSNI, we use the latent space of the Variational Auto-Encoder (VAE) as the noise input to the CGAN. In addition, utilizing the samples generated by VAE and the original samples to co-train the conditional generative adversarial nets (CGAN) by semi-supervised learning method. This method aims to improve generalization ability of the CGAN predictions. To better apply the numerical data obtained from sampling in the chemical industry, we incorporate the mean square error into the existing loss function of CGAN. The benchmark function and industrial purified terephthalic acid (PTA) data are used to verify the performance of the proposed CGAN-LSNI. The results show that the CGAN-LSNI has better prediction accuracy compared to the related methods of generating virtual samples.},
  keywords={Chemical industry;Soft sensors;Noise;Production;Mean square error methods;Semisupervised learning;Predictive models;Small sample;Soft sensor;Virtual sample generation;Generative Adversarial Nets;Variational AutoEncoder},
  doi={10.1109/IEAI62569.2024.00017},
  ISSN={},
  month={April},}@INPROCEEDINGS{10147972,
  author={Senarathna, Danushka and Tragoudas, Spyros and Gowda, Kiriti Nagesh and Schmit, Mike},
  booktitle={2023 IEEE 24th International Conference on High Performance Switching and Routing (HPSR)}, 
  title={Detection and Quantization of Data Drift in Image Classification Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={38-42},
  abstract={An unforeseen change in the input data is called drift and may impact the accuracy of machine-learning models. A novel scheme for diagnosing data drift in the input stream of image classification neural networks is presented. The proposed drift detection and quantization method uses a threshold dictionary for the prediction probabilities of each class in the neural network model. The method is applicable to any drift type in images such as noise, and weather effects, among others. Experimental results on various datasets, drift types, and neural network models show that the proposed method estimates the drift magnitude with high accuracy, especially when the level of drift impacts the model's performance significantly.},
  keywords={Quantization (signal);Statistical analysis;Neural networks;Estimation;Switches;Streaming media;Routing;Classification Neural Networks;Artificial Intelligence;Machine Learning;Drift Data Detection},
  doi={10.1109/HPSR57248.2023.10147972},
  ISSN={2325-5609},
  month={June},}@INPROCEEDINGS{9695748,
  author={Sheng, Yiwei},
  booktitle={2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)}, 
  title={Asymmetric CycleGAN for Unpaired Image-to-Image Translation Based on Dual Attention Module}, 
  year={2021},
  volume={},
  number={},
  pages={726-730},
  abstract={Numerous studies have been conducted on image style transfer. With the advancement of artificial intelligence, the application of convolutional neural networks and Generative Adversarial Networks (GAN) has garnered considerable attention in this field. Traditional CycleGAN is limited in extracting the features of different images well and effectively for asymmetric translation due to its own completely symmetric structure. In this paper, we propose an asymmetric CycleGAN model for spring and winter image style transfer. Our method modifies one of the generators by adding dual attention module, which can capture global features through spatial and channel dimensions so as to obtain rich information of images more deeply. The model is experimentally validated to be more capable of color recognition than the original CycleGAN.},
  keywords={Technological innovation;Image recognition;Image color analysis;Generative adversarial networks;Feature extraction;Transformers;Generators;Asymmetric CycleGAN;Dual Attention Module;Image style transfer},
  doi={10.1109/IAECST54258.2021.9695748},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10960089,
  author={Chen, Di and Xu, Xinyue and Wang, Zhifeng and Shen, Jialiang},
  booktitle={2024 International Conference on Intelligent Education and Intelligent Research (IEIR)}, 
  title={Personalized Art Image Generation Model for Smart Art Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={With the rise of the wave of smart education, art education is gradually moving towards the transformation stage of personalization and intelligence. Thanks to the rapid development of artificial intelligence technology, the field of education has ushered in unprecedented technological empowerment, opening up new development paths for art education. Personalized art image generation technology can generate customized art images according to students’ learning situation and interest preferences, thus improving the teaching effect and learning experience. However, the existing mainstream technology has problems such as high demand for computing resources, style distortion, and lack of detail expression, so there is still room for improvement in artistic expression. Aiming at the above problems, this paper proposes a personalized art image generation method based on diffusion model. The method uses a stabilized diffusion model as the backbone network, and the model is fine-tuned using LoRA and Dreambooth, aiming to improve the artistic style recognition and detail richness of the generated images. Experimental results show that the method performs well on the task of personalized art image generation, with excellent performance in terms of realism and diversity.},
  keywords={Training;Visualization;Image texture;Art;Image synthesis;Education;Diffusion models;Distortion;Tuning;Optimization;art image generation;diffusion model;parameter efficient fine tuning;deep learning},
  doi={10.1109/IEIR62538.2024.10960089},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10090088,
  author={Temgoua Nanfack, Pelagie Flore and Tagne Fute, Elie and Abiama Ele, Patrice},
  booktitle={2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)}, 
  title={Pycaret for the evaluation of classification methods in order to set up a decision making system for the early diagnosis of schizophrenia by EEG}, 
  year={2022},
  volume={},
  number={},
  pages={335-340},
  abstract={According to the World Health Organization, approximately 24 million people suffer from schizophrenia worldwide. However, there is no clinical examination to diagnose this pathology. Specialists rely on symptoms and signs. Which can be problematic. The aim of this work is to set up a Decision-Making System allowing the early diagnosis of schizophrenia by EEG. Pycaret is the classification tool used throughout this work. A python code was previously written for the preprocessing and analysis of EEGs. This last task led to obtaining an Excel file which is passed as input to pycaret. This method led to an overall accuracy of 60%. The confusion matrix obtained showed that approximately 67% of people who are actually ill are classified as such and 63% of those who are not ill are classified as not ill. However, this work can be improved by using ensemble methods and graph-based techniques for example.},
  keywords={Pathology;Codes;Mental disorders;Decision making;Organizations;Electroencephalography;Ensemble learning;schizophrenia;electroencephalogram;artificial intelligence;decision making system},
  doi={10.1109/SITIS57111.2022.00057},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11105312,
  author={Usman, Yusuf and Vitashkevich, Daniel and Muriuki-Mureithi, Caleb A. and Akl, Robert and Chataut, Robin},
  booktitle={2025 IEEE World AI IoT Congress (AIIoT)}, 
  title={AI-Enhanced Deep Neural Network Architecture for Accurate Channel Estimation in 6G Networks}, 
  year={2025},
  volume={},
  number={},
  pages={0732-0736},
  abstract={The evolution of 5G and 6G networks is accelerating the need for highly accurate and efficient channel estimation in mmWave massive MIMO systems. However, channel estimation and beamforming optimization remain critical challenges in current networks due to its computational complexity and accuracy concerns. This research explores deep learning-based methods for mmWave massive MIMO channel estimation and beamforming, leveraging ray-tracing datasets from DeepMIMO to train neural network models. Our deep learning model achieves 99.54% channel estimation accuracy, significantly outperforming traditional methods. The results indicate that simpler deep learning architectures reduce overfitting while maintaining high precision, making them well-suited for real-time communication scenarios. These findings are particularly relevant for 6G-enabled applications, including mmWave communications, satellite-assisted, and AI-driven mobility-aware beamforming. The proposed approach contributes to the advancement of intelligent vehicular networks, ensuring reliable, adaptive, and efficient wireless communication in next-generation transportation systems. The proposed approach advances intelligent wireless communication through AI-driven optimization strategies in 6G networks.},
  keywords={6G mobile communication;Wireless communication;Deep learning;Accuracy;Array signal processing;Computational modeling;Channel estimation;Massive MIMO;Millimeter wave communication;Optimization;Artificial Intelligence;Large Language Model;Massive MIMO;Channel Estimation;Beamforming;Deep Learning},
  doi={10.1109/AIIoT65859.2025.11105312},
  ISSN={},
  month={May},}@INPROCEEDINGS{10603551,
  author={Zong, Xing and Li, Qiang and Zhang, Qin and Xia, Sai},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Context-based Python Code Recommendation}, 
  year={2024},
  volume={},
  number={},
  pages={498-502},
  abstract={Code recommendation plays a crucial role in programming, assisting programmers in improving code quality, efficiency, and maintainability, thereby better addressing complex software development tasks. In recent years, artificial intelligence has been widely applied in software engineering domains such as code recommendation, particularly in the field of static languages, where significant advancements have been made in intelligent code recommendation. However, for dynamic languages like Python, intelligent code recommendation faces numerous challenges, such as the difficulties in dynamic analysis and code data imbalance. In light of this, building upon previous work, this paper achieves more accurate Python code recommendation by representing code context with semantically richer graphs and employing conditional generative adversarial networks for data augmentation of code semantic graphs, mitigating the impact of data imbalance. Experimental results demonstrate that the proposed code recommendation method, GraphPyRec, can effectively recommend Python code, and the data augmentation method, GraphGAN, can significantly optimize the recommendation model.},
  keywords={Codes;Accuracy;Semantics;Programming;Data augmentation;Generative adversarial networks;Task analysis;Software development;Software engineering;Code recommendation;Code characterisation;Data Augmentation},
  doi={10.1109/ICCEA62105.2024.10603551},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{9577939,
  author={Li, Linguo and Wang, Minsi and Ni, Bingbing and Wang, Hang and Yang, Jiancheng and Zhang, Wenjun},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={3D Human Action Representation Learning via Cross-View Consistency Pursuit}, 
  year={2021},
  volume={},
  number={},
  pages={4739-4748},
  abstract={In this work, we propose a Cross-view Contrastive Learning framework for unsupervised 3D skeleton-based action Representation (CrosSCLR), by leveraging multi-view complementary supervision signal. CrosSCLR consists of both single-view contrastive learning (Skeleton-CLR) and cross-view consistent knowledge mining (CVC-KM) modules, integrated in a collaborative learning manner. It is noted that CVC-KM works in such a way that high-confidence positive/negative samples and their distributions are exchanged among views according to their embedding similarity, ensuring cross-view consistency in terms of contrastive context, i.e., similar distributions. Extensive experiments show that CrosSCLR achieves remarkable action recognition results on NTU-60 and NTU-120 datasets under unsupervised settings, with observed higher-quality action representations. Our code is available at https://github.com/LinguoLi/CrosSCLR.},
  keywords={Computer vision;Three-dimensional displays;Codes;Collaborative work;Pattern recognition},
  doi={10.1109/CVPR46437.2021.00471},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9139368,
  author={Liu, Wei and Hou, Xianxu and Duan, Jiang and Qiu, Guoping},
  journal={IEEE Transactions on Image Processing}, 
  title={End-to-End Single Image Fog Removal Using Enhanced Cycle Consistent Adversarial Networks}, 
  year={2020},
  volume={29},
  number={},
  pages={7819-7833},
  abstract={Single image defogging is a classical and challenging problem in computer vision. Existing methods towards this problem mainly include handcrafted priors based methods that rely on the use of the atmospheric degradation model and learning-based approaches that require paired fog-fogfree training example images. In practice, however, prior-based methods are prone to failure due to their own limitations and paired training data are extremely difficult to acquire. Moreover, there are few studies on the unpaired trainable defogging network in this field. Thus, inspired by the principle of CycleGAN network, we have developed an end-to-end learning system that uses unpaired fog and fogfree training images, adversarial discriminators and cycle consistency losses to automatically construct a fog removal system. Similar to CycleGAN, our system has two transformation paths; one maps fog images to a fogfree image domain and the other maps fogfree images to a fog image domain. Instead of one stage mapping, our system uses a two stage mapping strategy in each transformation path to enhance the effectiveness of fog removal. Furthermore, we make explicit use of prior knowledge in the networks by embedding the atmospheric degradation principle and a sky prior for mapping fogfree images to the fog images domain. In addition, we also contribute the first real world nature fog-fogfree image dataset for defogging research. Our multiple real fog images dataset (MRFID) contains images of 200 natural outdoor scenes. For each scene, there is one clear image and corresponding four foggy images of different fog densities manually selected from a sequence of images taken by a fixed camera over the course of one year. Qualitative and quantitative comparison against several state-of-the-art methods on both synthetic and real world images demonstrate that our approach is effective and performs favorably for recovering a clear image from a foggy image.},
  keywords={Atmospheric modeling;Image color analysis;Training;Degradation;Learning systems;Training data;Mathematical model;Single image defogging;cycleGAN;unpaired training;image restoration},
  doi={10.1109/TIP.2020.3007844},
  ISSN={1941-0042},
  month={},}@ARTICLE{10489990,
  author={Noman, Mubashir and Fiaz, Mustansar and Cholakkal, Hisham and Narayan, Sanath and Muhammad Anwer, Rao and Khan, Salman and Shahbaz Khan, Fahad},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Remote Sensing Change Detection With Transformers Trained From Scratch}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  abstract={Current transformer-based change detection (CD) approaches either employ a pretrained model trained on a large-scale image classification ImageNet dataset or rely on first pretraining on another CD dataset and then fine-tuning on the target benchmark. This current strategy is driven by the fact that transformers typically require a large amount of training data to learn inductive biases, which is insufficient in standard CD datasets due to their small size. We develop an end-to-end CD approach with transformers that is trained from scratch and yet achieves state-of-the-art performance on five benchmarks. Instead of using conventional self-attention that struggles to capture inductive biases when trained from scratch, our architecture utilizes a shuffled sparse-attention operation that focuses on selected sparse informative regions to capture the inherent characteristics of the CD data. Moreover, we introduce a change-enhanced feature fusion (CEFF) module to fuse the features from input image pairs by performing a per-channel re-weighting. Our CEFF module aids in enhancing the relevant semantic changes while suppressing the noisy ones. Extensive experiments on five CD datasets reveal the merits of the proposed contributions, achieving gains as high as 1.35% in intersection over union (IoU) score, compared to the best-published results in the literature. The code is available at https://github.com/mustansarfiaz/ScratchFormer.},
  keywords={Feature extraction;Transformers;Task analysis;Convolutional neural networks;Semantics;Noise measurement;Training;Change detection (CD);remote sensing;transformers},
  doi={10.1109/TGRS.2024.3383800},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9880433,
  author={Zhu, Xiaopei and Hu, Zhanhao and Huang, Siyuan and Li, Jianmin and Hu, Xiaolin},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Infrared Invisible Clothing: Hiding from Infrared Detectors at Multiple Angles in Real World}, 
  year={2022},
  volume={},
  number={},
  pages={13307-13316},
  abstract={Thermal infrared imaging is widely used in body temperature measurement, security monitoring, and so on, but its safety research attracted attention only in recent years. We proposed the infrared adversarial clothing, which could fool infrared pedestrian detectors at different angles. We simulated the process from cloth to clothing in the digital world and then designed the adversarial “QR code” pattern. The core of our method is to design a basic pattern that can be expanded periodically, and make the pattern after random cropping and deformation still have an adversarial effect, then we can process the flat cloth with an adversarial pattern into any 3D clothes. The results showed that the optimized “QR code” pattern lowered the Average Precision (AP) of YOLOv3 by 87.7%, while the random “QR code” pattern and blank pattern lowered the AP of YOLOv3 by 57.9% and 30.1%, respectively, in the digital world. We then manufactured an adversarial shirt with a new material: aerogel. Physical-world experiments showed that the adversarial “QR code” pattern clothing lowered the AP of YOLOv3 by 64.6%, while the random “QR code” pattern clothing and fully heat-insulated clothing lowered the AP of YOLOv3 by 28.3% and 22.8%, respectively. We used the model ensemble technique to improve the attack transferability to unseen models.},
  keywords={Temperature measurement;Temperature sensors;Three-dimensional displays;Computational modeling;Clothing;QR codes;Safety;Adversarial attack and defense; Recognition: detection;categorization;retrieval},
  doi={10.1109/CVPR52688.2022.01296},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9729452,
  author={Li, Shaojie and Lin, Mingbao and Wang, Yan and Wu, Yongjian and Tian, Yonghong and Shao, Ling and Ji, Rongrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Distilling a Powerful Student Model via Online Knowledge Distillation}, 
  year={2023},
  volume={34},
  number={11},
  pages={8743-8752},
  abstract={Existing online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students’ information, while the latter increases the computational complexity during deployment. In this article, we propose a novel method for online knowledge distillation, termed feature fusion and self-distillation (FFSD), which comprises two key components: FFSD, toward solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student set and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD.},
  keywords={Training;Computational modeling;Knowledge engineering;Informatics;Optimization;Message passing;Memory management;Feature fusion;knowledge distillation;online distillation;self-distillation},
  doi={10.1109/TNNLS.2022.3152732},
  ISSN={2162-2388},
  month={Nov},}@ARTICLE{9335499,
  author={Liu, Feng and Liu, Haozhe and Zhang, Wentian and Liu, Guojie and Shen, Linlin},
  journal={IEEE Transactions on Image Processing}, 
  title={One-Class Fingerprint Presentation Attack Detection Using Auto-Encoder Network}, 
  year={2021},
  volume={30},
  number={},
  pages={2394-2407},
  abstract={Automated Fingerprint Recognition Systems (AFRSs) have been threatened by Presentation Attack (PA) since its existence. It is thus desirable to develop effective presentation attack detection (PAD) methods. However, the unpredictable PAs make PAD be a challenging problem. This paper proposes a novel One-Class PAD (OCPAD) method for Optical Coherence Technology (OCT) images based fingerprint PA detection. The proposed OCPAD model is learned from a training set only consists of Bonafides (i.e. real fingerprints). The reconstruction error and latent code obtained from the trained auto-encoder network in the proposed model is taken as the basis for the following spoofness score calculation. To get more accurate reconstruction error, we propose an activation map based weighting model to further refine the accuracy of reconstruction error. We test different statistics and distance measures and finally use a decision level fusion to make the final prediction. Our experiments are performed using a dataset with 93200 bonafide scans and 48400 PA scans. The results show that the proposed OCPAD can achieve a True Positive Rate (TPR) of 99.43% when the False Positive Rate (FPR) equals to 10% and a TPR of 96.59% when FPR=5%, which significantly outperformed a feature based approach and a supervised learning based model requiring PAs for training.},
  keywords={Training;Silicon compounds;Feature extraction;Detectors;Data models;Sensors;Supervised learning;Presentation attack detection;one-class;optical coherence technology;unsupervised learning system},
  doi={10.1109/TIP.2021.3052341},
  ISSN={1941-0042},
  month={},}@ARTICLE{10103922,
  author={Chai, Zehua and Ling, Yongguo and Luo, Zhiming and Lin, Dazhen and Jiang, Min and Li, Shaozi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dual-Stream Transformer With Distribution Alignment for Visible-Infrared Person Re-Identification}, 
  year={2023},
  volume={33},
  number={11},
  pages={6764-6776},
  abstract={Visible-infrared person re-identification(VI-ReID) aims to match the person images captured by visible and infrared cameras and suffers from severe cross-modality discrepancy and intra-modality variations. Existing approaches mainly use convolution neural network (CNN)-based architectures to extract pedestrian features, which fail to capture the long-range dependencies within an image. In addition, previous works usually attempt to bridge the modality gap by using adversarial learning to generate style-consistent images or designing different feature-level metric learning constraints. However, few works consider the cross-modality disparity from the perspective of assessing overall distance distribution discrepancy. To address these problems, we design a pure Transformer-based Visible-Infrared (TransVI) network with a conventional two-stream structure, which can explicitly capture modality-specific representations and learn multi-modality sharable knowledge. TransVI can efficiently address the lack of global dependency in CNN-based architectures due to the multi-head self-attention modules in the transformer, which allows us to capture the long-range dependencies of pedestrian images. Furthermore, we introduce the Cross-Modality Dissimilarity-based Maximum Mean Discrepancy (CMD-MMD) constraint to handle the cross-modality discrepancy at the distance distribution level. Specifically, CMD-MMD leverages intra-modality distribution separability to guide inter-modality distribution separability learning, aligning pair-wise distance distributions of intra- and inter-modality for within-class and between-class, respectively. In this way, the distance distributions of intra- and inter-modality become more similar, significantly mitigating the cross-modality discrepancy and learning more modality invariant representations. Extensive experimental results on two public VI-ReID datasets confirm that our proposed framework can achieve state-of-the-art performance.},
  keywords={Transformers;Feature extraction;Task analysis;Training;Computer architecture;Measurement;Data mining;Person re-identification;visible-infrared;distribution alignment;cross-modality;dissimilarity space},
  doi={10.1109/TCSVT.2023.3268080},
  ISSN={1558-2205},
  month={Nov},}@INPROCEEDINGS{10203554,
  author={Wanyan, Yuyang and Yang, Xiaoshan and Chen, Chaofan and Xu, Changsheng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={6492-6502},
  abstract={Recently, few-shot action recognition receives increasing attention and achieves remarkable progress. However, previous methods mainly rely on limited unimodal data (e.g., RGB frames) while the multimodal information remains relatively underexplored. In this paper, we propose a novel Active Multimodal Few-shot Action Recognition (AMFAR) framework, which can actively find the reliable modality for each sample based on task-dependent context information to improve few-shot reasoning procedure. In meta-training, we design an Active Sample Selection (ASS) module to organize query samples with large differences in the reliability of modalities into different groups based on modality-specific posterior distributions. In addition, we design an Active Mutual Distillation (AMD) to capture discriminative task-specific knowledge from the reliable modality to improve the representation learning of unreliable modality by bidirectional knowledge distillation. In meta-test, we adopt Adaptive Multimodal Inference (AMI) to adaptively fuse the modality-specific posterior distributions with a larger weight on the reliable modality. Extensive experimental results on four public benchmarks demonstrate that our model achieves significant improvements over existing unimodal and multimodal methods.},
  keywords={Representation learning;Computer vision;Adaptation models;Fuses;Benchmark testing;Reliability engineering;Cognition;Video: Action and event understanding},
  doi={10.1109/CVPR52729.2023.00628},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9997549,
  author={Zhang, Yi-Fan and Zhang, Zhang and Li, Da and Jia, Zhen and Wang, Liang and Tan, Tieniu},
  journal={IEEE Transactions on Image Processing}, 
  title={Learning Domain Invariant Representations for Generalizable Person Re-Identification}, 
  year={2023},
  volume={32},
  number={},
  pages={509-523},
  abstract={Generalizable person Re-Identification (ReID) aims to learn ready-to-use cross-domain representations for direct cross-data evaluation, which has attracted growing attention in the recent computer vision (CV) community. In this work, we construct a structural causal model (SCM) among identity labels, identity-specific factors (clothing/shoes color etc.), and domain-specific factors (background, viewpoints etc.). According to the causal analysis, we propose a novel Domain Invariant Representation Learning for generalizable person Re-Identification (DIR-ReID) framework. Specifically, we propose to disentangle the identity-specific and domain-specific factors into two independent feature spaces, based on which an effective backdoor adjustment approximate implementation is proposed for serving as a causal intervention towards the SCM. Extensive experiments have been conducted, showing that DIR-ReID outperforms state-of-the-art (SOTA) methods on large-scale domain generalization (DG) ReID benchmarks.},
  keywords={Data models;Correlation;Adaptation models;Training;Feature extraction;Analytical models;Representation learning;Generalizable person re-Identification;disentanglement;backdoor adjustment},
  doi={10.1109/TIP.2022.3229621},
  ISSN={1941-0042},
  month={},}@INPROCEEDINGS{10205147,
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Gu, Haiyun and Zhao, Chaoyang and Tang, Ming and Wang, Jinqiao},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ZBS: Zero-Shot Background Subtraction via Instance-Level Background Modeling and Foreground Selection}, 
  year={2023},
  volume={},
  number={},
  pages={6355-6364},
  abstract={Background subtraction (BGS) aims to extract all moving objects in the video frames to obtain binary foreground segmentation masks. Deep learning has been widely used in this field. Compared with supervised-based BGS methods, unsupervised methods have better generalization. However, previous unsupervised deep learning BGS algorithms perform poorly in sophisticated scenarios such as shadows or night lights, and they cannot detect objects outside the pre-defined categories. In this work, we propose an unsuper-vised BGS algorithm based on zero-shot object detection called Zero-shot Background Subtraction (ZBS). The proposed method fully utilizes the advantages of zero-shot object detection to build the open-vocabulary instance-level background model. Based on it, the foreground can be effectively extracted by comparing the detection results of new frames with the background model. ZBS performs well for sophisticated scenarios, and it has rich and extensible categories. Furthermore, our method can easily generalize to other tasks, such as abandoned object detection in unseen environments. We experimentally show that ZBS surpasses state-of-the-art unsupervised BGS methods by 4.70% F-Measure on the CDnet 2014 dataset. The code is released at https://github.com/CASIA-IVA-Lab/ZBS.},
  keywords={Deep learning;Computer vision;Codes;Image edge detection;Object detection;Detectors;Pattern recognition;Video: Low-level analysis;motion;and tracking},
  doi={10.1109/CVPR52729.2023.00615},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10085957,
  author={Sun, Tong and Wang, Chuang and Dong, Hongli and Zhou, Yina and Guan, Chuang},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={A Novel Parameter-Optimized Recurrent Attention Network for Pipeline Leakage Detection}, 
  year={2023},
  volume={10},
  number={4},
  pages={1064-1076},
  abstract={Accurate detection of pipeline leakage is essential to maintain the safety of pipeline transportation. Recently, deep learning (DL) has emerged as a promising tool for pipeline leakage detection (PLD). However, most existing DL methods have difficulty in achieving good performance in identifying leakage types due to the complex time dynamics of pipeline data. On the other hand, the initial parameter selection in the detection model is generally random, which may lead to unstable recognition performance. For this reason, a hybrid DL framework referred to as parameter-optimized recurrent attention network (PRAN) is presented in this paper to improve the accuracy of PLD. First, a parameter-optimized long short-term memory (LSTM) network is introduced to extract effective and robust features, which exploits a particle swarm optimization (PSO) algorithm with cross-entropy fitness function to search for globally optimal parameters. With this framework, the learning representation capability of the model is improved and the convergence rate is accelerated. Moreover, an anomaly-attention mechanism (AM) is proposed to discover class discriminative information by weighting the hidden states, which contributes to amplifying the normal-abnormal distinguishable discrepancy, further improving the accuracy of PLD. After that, the proposed PRAN not only implements the adaptive optimization of network parameters, but also enlarges the contribution of normal-abnormal discrepancy, thereby overcoming the drawbacks of instability and poor generalization. Finally, the experimental results demonstrate the effectiveness and superiority of the proposed PRAN for PLD.},
  keywords={Adaptation models;Adaptive systems;Pipelines;Signal processing algorithms;Transportation;Feature extraction;Classification algorithms;Anomaly-attention mechanism (AM);long short-term memory (LSTM);parameter-optimized recurrent attention network (PRAN);particle swarm optimization (PSO);pipeline leakage detection (PLD)},
  doi={10.1109/JAS.2023.123180},
  ISSN={2329-9274},
  month={April},}@ARTICLE{10443664,
  author={Chen, Yaxiong and Huang, Jirui and Xiong, Shengwu and Lu, Xiaoqiang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Integrating Multisubspace Joint Learning With Multilevel Guidance for Cross-Modal Retrieval of Remote Sensing Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-17},
  abstract={In recent years, with the continuous advancement of remote sensing (RS) technology and text processing techniques, there has been a growing abundance of RS images and associated textual data. Combining RS images with their corresponding textual data allows for integrated analysis and retrieval, which holds significant practical implications across multiple application domains, including geographic information systems (GIS), environmental monitoring, and agricultural management. RS images have the characteristics of multitargets and multiscales, and the textual descriptions of these targets are not fully utilized, leading to a decrease in retrieval accuracy. Previous methods have struggled to balance intermodality information interaction and intramodality feature fusion, and they have paid little attention to the consistency of distribution within modalities. In light of this, this article proposes a symmetric multilevel guidance network (SMLGN) for cross-modal retrieval in RS. SMLGN first introduces fusion guidance between local and global within modalities and fine-grained bidirectional guidance between modalities, allowing for the learning of a common semantic space. Furthermore, to address the distribution differences of different modalities within the common semantic space, we design an adversarial joint learning framework and a multiobjective loss function to optimize the SMLGN method and achieve consistency in data distribution. The experimental results demonstrate that the SMLGN method performs well in the task of cross-modal retrieval between RS images and textual data. It effectively integrates the information from both modalities, improving the accuracy and reliability of the retrieval process.},
  keywords={Remote sensing;Semantics;Task analysis;Green buildings;Roads;Adversarial machine learning;Sensors;Adversarial learning;feature fusion;modality alignment;multisubspace joint learning;remote sensing (RS) image-text (I2T) retrieval},
  doi={10.1109/TGRS.2024.3369042},
  ISSN={1558-0644},
  month={},}@ARTICLE{10197333,
  author={Deng, Ye and Hui, Siqi and Zhou, Sanping and Huang, Wenli and Wang, Jinjun},
  journal={IEEE Transactions on Image Processing}, 
  title={Context Adaptive Network for Image Inpainting}, 
  year={2023},
  volume={32},
  number={},
  pages={6332-6345},
  abstract={In a typical image inpainting task, the location and shape of the damaged or masked area is often random and irregular. The vanilla convolutions widely used in learning-based inpainting models treat all spatial features as valid and share parameters across regions, making it difficult for them to cope with those irregular damages, and models tend to produce inpainting results with color discrepancy and blurriness. In this paper, we propose a novel Context Adaptive Network (CANet) to address this issue. The main idea of the proposed CANet is able to generate different weights depending on the miscellaneous input, which may help to complement images with multiple broken forms in a flexible way. Specifically, the proposed CANet has two novel context adaptive modules, namely, the context adaptive block (CAB) and the cross-scale contextual attention (CSCA), which utilize attention mechanisms to cope with diverse content breakdowns. The proposed CAB, during the forward propagation, uses an adaptive term to determine the importance between adaptive term and convolution kernel, so as to dynamically balance features based on the degree of breakage (confidence level or soft mask), and the overall calculation is formulated as a classic convolution implementation with an additional attention term to describe local structure. Besides, the proposed CSCA, not only takes advantage of the contextual attention module, but also considers cross-scale information transfer to generate reasonable features for damaged areas, thus alleviating the inefficiency of the long-range modeling capability of convolutional neural networks. Qualitative and quantitative experiments show that our method performs better than state-of-the-arts, producing clearer, more coherent and visually plausible inpainting results. The code can be found at github.com/dengyecode/CANet_image_inpainting},
  keywords={Convolutional codes;Adaptive systems;Adaptation models;Context modeling;Task analysis;Semantics;Generators;Image inpainting;deep learning;attention},
  doi={10.1109/TIP.2023.3298560},
  ISSN={1941-0042},
  month={},}@ARTICLE{9963913,
  author={Lu, Ting and Hu, Yaochen and Fu, Wei and Ding, Kexin and Bai, Beifang and Fang, Leyuan},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={SCL-Net: An End-to-End Supervised Contrastive Learning Network for Hyperspectral Image Classification}, 
  year={2022},
  volume={60},
  number={},
  pages={1-12},
  abstract={In recent years, deep learning (DL) presents a promising performance in hyperspectral image (HSI) classification, due to the powerful capability of automatically learning deep semantic characteristics of images. However, it is still difficult to learn highly discriminative features when limited samples are available for training a deep network. Focused on this issue, a novel end-to-end supervised contrastive learning network (SCL-Net) for spectral–spatial classification is proposed, in this article. Instead of learning features of the individual sample, the supervised contrastive learning is introduced to capture the similarity and dissimilarity distribution properties of sample pairs in a feature representation space. In this way, the need for plenty of training samples will be alleviated while an effective network training mechanism is provided for learning highly separative features. Here, SCL-Net mainly consists of one pairwise contrastive learning (PCL) subnetwork and one multilevel spectral–spatial information fusion (MLSIF) subnetwork. For the PCL subnetwork, spectral vectors are projected into deep spectral features based on convolutional operators, which are then followed by distance evaluation between “positive” pairs of similar samples and “negative” pairs of dissimilar ones. Then, a spectral distance matrix is constructed to push the network to gradually learn better features of higher intraclass compactness and interclass dispersion. For the MLSIF subnetwork, a hybrid feature-decision fusion strategy is designed, where spatial and spectral features are jointly exploited to further boost the classification performance. In specific, feature fusion is conducted by connecting low/mid/high-level spectral and spatial features via weighting, while multiple class estimations based on multilevel fusion features are adaptively integrated via probabilistic decision fusion. Overall, these two subnetworks are collaboratively trained in one framework, by optimizing a defined joint loss function consisting of a contrastive loss and a cross-entropy loss. Compared with several state-of-the-art methods, the proposed method yields a superior classification performance in terms of both objective metrics and visual performance.},
  keywords={Feature extraction;Hyperspectral imaging;Training;Deep learning;Task analysis;Data mining;Convolutional neural networks;Contrastive learning;convolutional neural networks (CNNs);hyperspectral image (HSI);spectral–spatial classification},
  doi={10.1109/TGRS.2022.3223664},
  ISSN={1558-0644},
  month={},}@ARTICLE{10195178,
  author={Zhao, Dongjie and Chen, Yaxiong and Xiong, Shengwu},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Multiscale Context Deep Hashing for Remote Sensing Image Retrieval}, 
  year={2023},
  volume={16},
  number={},
  pages={7163-7172},
  abstract={With the advancement of remote sensing satellites and sensor technology, the quantity and diversity of remote sensing imagery have exhibited a sustained trend of growth. Remote sensing image retrieval has gained significant attention in the realm of remote sensing. Hashing methods have been widely applied in remote sensing image retrieval due to their high computational efficiency, low storage cost, and effective performance. However, existing remote sensing image retrieval methods often struggle to accurately capture the intricate information of remote sensing images. They often lack high attention to key features. The neglect of multiscale and saliency information in remote sensing images can result in feature loss and difficulties in maintaining the balance of hash codes. In response to the issues, we introduce a multiscale context deep hashing network (MSCDH). First, we can obtain finer-grained multi-scale features and achieve a larger receptive field by incorporating the proposed multiscale residual blocks. Then, the proposed multicontext attention modules can increase the perceptual field and suppress the interference from irrelevant information by aggregating contextual information along channels and spatial dimensions. The experimental results on the UCMerced dataset and WHU-RS dataset demonstrate that the proposed method achieves state-of-the-art retrieval performance.},
  keywords={Remote sensing;Image retrieval;Feature extraction;Task analysis;Sensors;Visualization;Semantics;Attention mechanism;deep hash;multiscale context information},
  doi={10.1109/JSTARS.2023.3298990},
  ISSN={2151-1535},
  month={},}@ARTICLE{10269000,
  author={Li, Lulu and Zhu, Ruijie and Wu, Shuning and Ding, Wenting and Xu, Mingliang and Lu, Jiwen},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Adaptive Multi-Agent Deep Mixed Reinforcement Learning for Traffic Light Control}, 
  year={2024},
  volume={73},
  number={2},
  pages={1803-1816},
  abstract={Despite significant advancements in Multi-Agent Deep Reinforcement Learning (MADRL) approaches for Traffic Light Control (TLC), effectively coordinating agents in diverse traffic environments remains a challenge. Studies in MADRL for TLC often focus on repeatedly constructing the same intersection models with sparse experience. However, real road networks comprise Multi-Type of Intersections (MTIs) rather than being limited to intersections with four directions. In the scenario with MTIs, each type of intersection exhibits a distinctive topology structure and phase set, leading to disparities in the spaces of state and action. This article introduces Adaptive Multi-agent Deep Mixed Reinforcement Learning (AMDMRL) for addressing tasks with multiple types of intersections in TLC. AMDMRL adopts a two-level hierarchy, where high-level proxies guide low-level agents in decision-making and updating. All proxies are updated by value decomposition to obtain the globally optimal policy. Moreover, the AMDMRL approach incorporates a mixed cooperative mechanism to enhance cooperation among agents, which adopts a mixed encoder to aggregate the information from correlated agents. We conduct comparative experiments involving four traditional and four DRL-based approaches, utilizing three training and four testing datasets. The results indicate that the AMDMRL approach achieves average reductions of 41% than traditional approaches, and 16% compared to DRL-based approaches in traveling time on three training datasets. During testing, the AMDMRL approach exhibits a 37% improvement in reward compared to the MADRL-based approaches.},
  keywords={Training;Topology;Reinforcement learning;Decision making;Computational modeling;Adaptation models;Testing;Traffic light control (TLC);multi-type of intersections (MTIs);value decomposition;multi-agent deep reinforcement learning (MADRL)},
  doi={10.1109/TVT.2023.3319698},
  ISSN={1939-9359},
  month={Feb},}@ARTICLE{10246362,
  author={Hu, Yongli and Feng, Lincong and Jiang, Huajie and Liu, Mengting and Yin, Baocai},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Domain-Aware Prototype Network for Generalized Zero-Shot Learning}, 
  year={2024},
  volume={34},
  number={5},
  pages={3180-3191},
  abstract={Generalized zero-shot learning(GZSL) aims to recognize images from seen and unseen classes with side information, such as manually annotated attribute vectors. Traditional methods focus on mapping images and semantics into a common latent space, thus achieving the visual-semantics alignment. Since the unseen classes are unavailable during training, there is a serious problem of recognition bias, which will tend to recognize unseen classes as seen classes. To solve this problem, we propose a Domain-aware Prototype Network(DPN), which splits the GZSL problem into the seen class recognition and unseen class recognition problem. For the seen classes, we design a domain-aware prototype learning branch with a dual attention feature encoder to capture the essential visual information, which aims to recognize the seen classes and discriminate the novel categories. To further recognize the fine-grained unseen classes, a visual-semantic embedding branch is designed, which aims to align the visual and semantic information for unseen-class recognition. Through the multi-task learning of the prototype learning branch and visual-semantic embedding branch, our model can achieve excellent performance on three popular GZSL datasets.},
  keywords={Visualization;Prototypes;Semantics;Transformers;Image recognition;Feature extraction;Task analysis;Generalized zero-shot learning;transformer-based dual attention;domain detection},
  doi={10.1109/TCSVT.2023.3313727},
  ISSN={1558-2205},
  month={May},}@ARTICLE{10531693,
  author={Zhu, Ruijie and Wu, Shuning and Li, Lulu and Ding, Wenting and Lv, Ping and Sui, Luyao},
  journal={IEEE Internet of Things Journal}, 
  title={Adaptive Broad Deep Reinforcement Learning for Intelligent Traffic Light Control}, 
  year={2024},
  volume={11},
  number={17},
  pages={28496-28507},
  abstract={Deep reinforcement learning (DRL) has superior autonomous decision-making capabilities, combining deep learning and reinforcement learning (RL). Unlike DRL employs deep neural networks (DNNs), broad RL (BRL) adopts the broad learning system (BLS) that is established with flat networks to generate the strategy. This article proposes the multiagent adaptive broad-DRL (ABDRL) approach for traffic light control (TLC), which combines the broad network with the deep network structure. Specifically, the structure of ABDRL first expands in the form of flatted broad networks. Then, the feature representation module that contains DNNs is employed to extract the critical traffic information. In addition, experiences sampled randomly by the experience replay mechanism cannot reflect the current training status of the agent effectively. In order to alleviate the impacts caused by random sampling, the forgetful experience mechanism (FEM) is incorporated into ABDRL. The FEM enables the agent to discriminate the importance of experiences stored in the experience reply buffer to improve robustness and adaptability. We validate the effectiveness of ABDRL in TLC, and the results illustrate the optimality and robustness of ABDRL over the state-of-the-art multiagent DRL (MADRL) algorithms.},
  keywords={Feature extraction;Training;Robustness;Optimization;Internet of Things;Finite element analysis;Deep reinforcement learning;Broad learning system (BLS);broad reinforcement learning (BRL);deep reinforcement learning (DRL);multiagent DRL (MADRL);traffic light control (TLC)},
  doi={10.1109/JIOT.2024.3401829},
  ISSN={2327-4662},
  month={Sep.},}@ARTICLE{10330731,
  author={Zhang, Zeyang and Li, Hui and Xu, Tianyang and Wu, Xiao-Jun and Fu, Yu},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={GuideFuse: A Novel Guided Auto-Encoder Fusion Network for Infrared and Visible Images}, 
  year={2024},
  volume={73},
  number={},
  pages={1-11},
  abstract={Although the deep network has rich semantic expression ability, the details of the source image will inevitably be lost due to the increase of model depth. Thus, how to introduce the image details into a deep network is a key problem for image fusion tasks. To solve this problem, in this article, we propose a novel gradient-based auto-encoder fusion network for infrared and visible images, termed GuideFuse. We calculate the gradient map of the source image, and a special GuideValue (GV) cooperating with it to guide the Decoder to reconstruct the image. Then, a novel fusion strategy based on the calculated GV is proposed. The whole training process can be described as training an auto-encoder, in which the Encoder is responsible for extracting the features of the source image as much as possible. At the same time, the decoder reconstructs the source image according to the extracted features. Experimental results show that the proposed fusion network achieves the best fusion effect compared with the existing fusion methods.},
  keywords={Feature extraction;Image reconstruction;Training;Semantics;Image fusion;Decoding;Autoencoders;Deep learning;Auto-encoder;deep learning;image fusion;Laplacian},
  doi={10.1109/TIM.2023.3306537},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10131036,
  author={Stoean, Catalin and Bacanin, Nebojsa and Stoean, Ruxandra and Ionescu, Leonard and Alecsa, Cristian and Hotoleanu, Mircea and Atencia, Miguel and Joya, Gonzalo},
  booktitle={2022 24th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={On Using Perceptual Loss within the U-Net Architecture for the Semantic Inpainting of Textile Artefacts with Traditional Motifs}, 
  year={2022},
  volume={},
  number={},
  pages={276-283},
  abstract={It is impressive when one gets to see a hundreds or thousands years old artefact exhibited in the museum, whose appearance seems to have been untouched by centuries. Its restoration had been in the hands of a multidisciplinary team of experts and it had undergone a series of complex procedures. To this end, computational approaches that can support in deciding the most visually appropriate inpainting for very degraded historical items would be helpful as a second objective opinion for the restorers. The present paper thus attempts to put forward a U-Net approach with a perceptual loss for the semantic inpainting of traditional Romanian vests. Images taken of pieces from the collection of the Oltenia Museum in Craiova, along with such images with garments from the Internet, have been given to the deep learning model. The resulting numerical error for inpainting the corrupted parts is adequately low, however the visual similarity still has to be improved by considering further possibilities for finer tuning.},
  keywords={Deep learning;Visualization;Scientific computing;Semantics;Museums;Image restoration;Numerical models;semantic inpainting;deep learning;cultural heritage;textile artefacts;traditional motifs},
  doi={10.1109/SYNASC57785.2022.00051},
  ISSN={2470-881X},
  month={Sep.},}@INPROCEEDINGS{10203906,
  author={Ren, Xingyu and Deng, Jiankang and Ma, Chao and Yan, Yichao and Yang, Xiaokang},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues}, 
  year={2023},
  volume={},
  number={},
  pages={4511-4520},
  abstract={Recent 3D face reconstruction methods have made significant advances in geometry prediction, yet further cosmetic improvements are limited by lagged albedo because inferring albedo from appearance is an ill-posed problem. Although some existing methods consider prior knowledge from illumination to improve albedo estimation, they still produce a light-skin bias due to racially biased albedo models and limited light constraints. In this paper, we reconsider the relationship between albedo and face attributes and propose a ID2Albedo to directly estimate albedo without constraining illumination. Our key insight is that intrinsic semantic attributes such as race, skin color, and age can be used to constrain the albedo map. We first introduce visual-textual cues and design a semantic loss to supervise facial albedo estimation. Specifically, we pre-define text labels such as race, skin color, age, and wrinkles. Then, we employ the text-image model (CLIP) to compute the similarity between the text and the input image, and assign a pseudo-label to each facial image. We constrain generated albedos in the training phase to have the same attributes as the inputs. In addition, we train a high-quality, unbiased facial albedo generator and utilize the semantic loss to learn the mapping from illumination-robust identity features to the albedo latent codes. Finally, our ID2Albedo is trained in a self-supervised way and outperforms state-of-the-art albedo estimation methods in terms of accuracy and fidelity. It is worth mentioning that our approach has excellent generalizability and fairness, especially on in-the- wild data.},
  keywords={Training;Three-dimensional displays;Image color analysis;Semantics;Estimation;Lighting;Reconstruction algorithms;Humans: Face;body;pose;gesture;movement},
  doi={10.1109/CVPR52729.2023.00438},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10204817,
  author={Yang, Siqi and Cui, Xuanning and Zhu, Yongjie and Tang, Jiajun and Li, Si and Yu, Zhaofei and Shi, Boxin},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Complementary Intrinsics from Neural Radiance Fields and CNNs for Outdoor Scene Relighting}, 
  year={2023},
  volume={},
  number={},
  pages={16600-16609},
  abstract={Relighting an outdoor scene is challenging due to the diverse illuminations and salient cast shadows. Intrinsic image decomposition on outdoor photo collections could partly solve this problem by weakly supervised labels with albedo and normal consistency from multiview stereo. With neural radiance fields (NeRF), editing the appearance code could produce more realistic results without interpreting the outdoor scene image formation explicitly. This paper proposes to complement the intrinsic estimation from volume rendering using NeRF and from inversing the photometric image formation model using convolutional neural networks (CNNs). The former produces richer and more reliable pseudo labels (cast shadows and sky appearances in addition to albedo and normal) for training the latter to predict interpretable and editable lighting parameters via a single-image prediction pipeline. We demonstrate the advantages of our method for both intrinsic image decomposition and relighting for various real outdoor scenes.},
  keywords={Training;Solid modeling;Pipelines;Lighting;Estimation;Rendering (computer graphics);Image decomposition;Physics-based vision and shape-from-X},
  doi={10.1109/CVPR52729.2023.01593},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10008098,
  author={Zhou, Sanping and Wang, Jinjun and Wang, Le and Wan, Xingyu and Hui, Siqi and Zheng, Nanning},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Inverse Adversarial Diversity Learning for Network Ensemble}, 
  year={2024},
  volume={35},
  number={6},
  pages={7923-7935},
  abstract={Network ensemble aims to obtain better results by aggregating the predictions of multiple weak networks, in which how to keep the diversity of different networks plays a critical role in the training process. Many existing approaches keep this kind of diversity either by simply using different network initializations or data partitions, which often requires repeated attempts to pursue a relatively high performance. In this article, we propose a novel inverse adversarial diversity learning (IADL) method to learn a simple yet effective ensemble regime, which can be easily implemented in the following two steps. First, we take each weak network as a generator and design a discriminator to judge the difference between the features extracted by different weak networks. Second, we present an inverse adversarial diversity constraint to push the discriminator to cheat generators that all the resulting features of the same image are too similar to distinguish each other. As a result, diverse features will be extracted by these weak networks through a min–max optimization. What is more, our method can be applied to a variety of tasks, such as image classification and image retrieval, by applying a multitask learning objective function to train all these weak networks in an end-to-end manner. We conduct extensive experiments on the CIFAR-10, CIFAR-100, CUB200-2011, and CARS196 datasets, in which the results show that our method significantly outperforms most of the state-of-the-art approaches.},
  keywords={Training;Feature extraction;Generators;Adversarial machine learning;Task analysis;Diversity methods;Computer vision;Adversarial learning;deep neural network;diversity constraint;network ensemble},
  doi={10.1109/TNNLS.2022.3222263},
  ISSN={2162-2388},
  month={June},}@INPROCEEDINGS{9987791,
  author={Alshammari, Alanoud and Alshammari, Reem and Altalak, Maha and Alshammari, Khulud and Alhakamy, A'aeshah},
  booktitle={2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={Credit-card Fraud Detection System using Big Data Analytics}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Due to the rapid emergence and evolution of online transactions, credit cards have become the most popular payment method. Credit card fraud involves using fake credit cards to purchase goods without paying. On the other hand, researchers have proposed a wide range of anti-fraud systems with quick research and development around information technology and data mining including neural networks and decision trees, to advanced machine learning and deep learning methods. This paper presents a framework that combines the capabilities of Apache Spark and machine learning to analyze and monitor a large amount of data. It allows users to perform credit-card fraud detection and improve the classification performance on a real credit-card dataset. The obtained results showed an enhancement in the classification accuracy over the existing results.},
  keywords={Deep learning;Mechatronics;Neural networks;Credit cards;Fraud;Sparks;Decision trees;Fraud detection;Machine Learning;Big Data;Apache Spark},
  doi={10.1109/ICECCME55909.2022.9987791},
  ISSN={},
  month={Nov},}@ARTICLE{10670577,
  author={Gao, Junyu and Yang, Hao and Zhang, Da and Yuan, Yuan and Li, Xuelong},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={Imbalanced Aircraft Data Anomaly Detection}, 
  year={2025},
  volume={61},
  number={2},
  pages={1422-1432},
  abstract={Anomaly detection in temporal data from sensors under aviation scenarios is a practical but challenging task. First, long temporal data are difficult to extract contextual information with temporal correlation, and second, the anomalous data are rare in time series, causing normal/abnormal imbalance in anomaly detection, making the detector classification degenerate or even fail. To remedy the aforementioned problems, we propose a graphical temporal data analysis framework. It consists of three modules, named series-to-image (S2I), cluster-based resampling approach using Euclidean distance (CRD), and variance-based loss (VBL). Specifically, to better extract global information in temporal data from sensors, S2I converts the data to curve images to demonstrate abnormalities in data changes. CRD and VBL balance the classification to mitigate the unequal distribution of classes. CRD extracts minority samples with similar features to majority samples by clustering and oversamples them. And VBL fine-tunes the decision boundary by balancing the fitting degree of the network to each class. Ablation experiments on the Flights dataset indicate the effectiveness of CRD and VBL on precision and recall, respectively. Extensive experiments demonstrate the synergistic advantages of CRD and VBL on F1-score on Flights and three other temporal datasets.},
  keywords={Anomaly detection;Sensors;Feature extraction;Time series analysis;Intelligent sensors;Aircraft;Data models;Anomaly detection;imbalanced learning;temporal data analysis},
  doi={10.1109/TAES.2024.3456748},
  ISSN={1557-9603},
  month={April},}@ARTICLE{10445290,
  author={Yang, Yuqun and Tang, Xu and Ma, Jingjing and Zhang, Xiangrong and Pei, Shiji and Jiao, Licheng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={ECPS: Cross Pseudo Supervision Based on Ensemble Learning for Semi-Supervised Remote Sensing Change Detection}, 
  year={2024},
  volume={62},
  number={},
  pages={1-17},
  abstract={Semi-supervised learning (SSL) aims to exploit the potential of unlabeled data to enhance model performance, which makes it suitable for addressing the challenge of limited labeled data. As a popular technology, pseudo-label is widely applied in many semi-supervised remote sensing (RS) change detection methods. However, when facing limited labeled data, abundant low-quality pseudo-labels from a poorly performing model hinder the effective enhancement of model performance. To address this issue, we propose a novel semi-supervised strategy, named ensemble cross pseudo supervision (ECPS). The utilization of ensemble learning to merge outputs from several change detection models enhances pseudo-label quality, leading to more accurate change information and a significant boost in model performance, even with limited labeled data. In this method, adopting crosswise supervision ensures that no additional inference costs caused by ensemble learning are consumed. This provides both high efficiency and effectiveness for identifying land-cover changes. On the other hand, a simple yet effective ensemble strategy is proposed, which allows to manually adjust the model’s tendency toward higher precision or recall for satisfying practical requirements. We conduct extensive experiments on four public RS change detection datasets, and the promising results demonstrate the superiority of the proposed method across various numbers of labeled samples. Our source codes are available at https://github.com/TangXu-Group/ECPS.},
  keywords={Data models;Task analysis;Training;Feature extraction;Ensemble learning;Costs;Remote sensing;Change detection;ensemble learning;pseudo-label;remote sensing (RS);semi-supervised learning (SSL)},
  doi={10.1109/TGRS.2024.3370236},
  ISSN={1558-0644},
  month={},}@ARTICLE{10752514,
  author={Liu, Zili and Chen, Hao and Bai, Lei and Li, Wenyuan and Ouyang, Wanli and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={MambaDS: Near-Surface Meteorological Field Downscaling With Topography Constrained Selective State-Space Modeling}, 
  year={2024},
  volume={62},
  number={},
  pages={1-15},
  abstract={In an era of frequent extreme weather and global warming, obtaining precise, fine-grained near-surface weather forecasts is increasingly essential for human activities. Downscaling (DS), a crucial task in meteorological forecasting and remote sensing, enables the reconstruction of high-resolution meteorological states for target regions from global-scale forecast results. Previous downscaling methods, inspired by convolutional neural network (CNN) and Transformer-based super-resolution (SR) models, lacked tailored designs for meteorology and encountered structural limitations. Notably, they failed to efficiently integrate topography, a crucial prior to the downscaling process. In this article, we address these limitations by pioneering the selective state-space model (SSM) into the meteorological field downscaling and propose a novel model called MambaDS. This model retains the advantages of Mamba in long-range dependency modeling and linear computational complexity while enhancing the learning ability of multivariate correlation. In addition, by designing an efficient topography constraint layer, this prior information can be used more efficiently than ever before. Through extensive experiments in both China mainland and the continental United States (CONUS), we validated that our proposed MambaDS achieves state-of-the-art (SOTA) results in three different types of meteorological field downscaling settings.},
  keywords={Climate change;Meteorology;Weather forecasting;State-space methods;Superresolution;Remote sensing;Surface treatment;Globalization;Meteorological field downscaling (DS);remote sensing;state-space model (SSM);super-resolution (SR);weather forecasting},
  doi={10.1109/TGRS.2024.3496895},
  ISSN={1558-0644},
  month={},}@ARTICLE{10897691,
  author={Wu, Zhongqi and Guo, Jianwei and Zhuang, Chuanqing and Xiao, Jun and Yan, Dong-Ming and Zhang, Xiaopeng},
  journal={Computational Visual Media}, 
  title={Joint specular highlight detection and removal in single images via Unet-Transformer}, 
  year={2023},
  volume={9},
  number={1},
  pages={141-154},
  abstract={Specular highlight detection and removal is a fundamental problem in computer vision and image processing. In this paper, we present an efficient end-to-end deep learning model for automatically detecting and removing specular highlights in a single image. In particular, an encoder-decoder network is utilized to detect specular highlights, and then a novel Unet-Ttansformer network performs highlight removal; we append transformer modules instead of feature maps in the Unet architecture. We also introduce a highlight detection module as a mask to guide the removal task. Thus, these two networks can be jointly trained in an effective manner. Thanks to the hierarchical and global properties of the transformer mechanism, our framework is able to establish relationships between continuous self-attention layers, making it possible to directly model the mapping between the diffuse area and the specular highlight area, and reduce indeterminacy within areas containing strong specular highlight reflection. Experiments on public benchmark and real-world images demonstrate that our approach outperforms state-of-the-art methods for both highlight detection and removal tasks.},
  keywords={Image color analysis;Transformers;Lighting;Feature extraction;Convolution;Decoding;Computer vision;Synthetic data;Sparse matrices;Robustness;specular highlight detection;specular highlight removal;Unet-Transformer},
  doi={10.1007/s41095-022-0273-9},
  ISSN={2096-0662},
  month={March},}@ARTICLE{9928427,
  author={Zhang, Ruohan and Jiao, Licheng and Li, Lingling and Liu, Fang and Liu, Xu and Yang, Shuyuan},
  journal={IEEE Transactions on Cybernetics}, 
  title={Evolutionary Dual-Stream Transformer}, 
  year={2024},
  volume={54},
  number={4},
  pages={2166-2178},
  abstract={Vision transformers (ViTs) are rapidly evolving and are widely used in computer vision. However, high-performance ViTs require many computations, which limit their further development in the vision field. In this article, a novel evolutionary dual-stream transformer (E-DST) model is proposed to alleviate the computational resource demand problem. A hybrid attention mechanism structure is proposed for a DST model. The DST model uses a dual-branch structure to fuse convolutional and transformer features. Combining the features learned by the transformer and convolution effectively saves model computational resources. In addition, an evolutionary optimizer is proposed to optimize the parameters of the model. The excellent search ability of the evolutionary algorithm is utilized to optimize the transformer model parameters. The convergence of the evolutionary optimizer is proved in this article. In addition, the proposed E-DST model is experimentally compared with a variety of classic models and their deformations based on three datasets. And, the evolutionary optimizer proves its generality in convolutional and recurrent neural networks. The experimental results show that the E-DST model can effectively reduce computational resources and that the evolutionary optimizer can solve large-scale optimization problems. In conclusion, our proposed method is feasible and effective.},
  keywords={Transformers;Computational modeling;Optimization;Feature extraction;Adaptation models;Training;Deep learning;evolutionary optimization;image classification;transformer},
  doi={10.1109/TCYB.2022.3213537},
  ISSN={2168-2275},
  month={April},}@INPROCEEDINGS{10054829,
  author={Lu, Bo and Huang, Min and Li, Xi and Nie, Yunhao and Miao, Qinghai and Lv, Yisheng},
  booktitle={2022 China Automation Congress (CAC)}, 
  title={Pedestrian Detection for Autonomous Vehicles Using Virtual-to-Real Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={3652-3657},
  abstract={Annotated data are essential to the success of training deep neural models for autonomous driving. Practically, it is both expensive and time consuming to collect and annotate plenty of data driving cross the city. It is even harder to cover the corner cases involving rare pedestrian poses with occlusions. To tackle this problem, we propose a data augmentation method for pedestrian detection. First, we take a SMPL model to generate pedestrians with a variety of poses and textures. Second, we insert the rendered pedestrians into images with real traffic background according to vanishing point. Occlusions can also be set easily by taking use of segmentation annotations of original real dataset. We evaluated the effectiveness of the generated synthetic dataset using multiple detectors. Results of three experiments shown that detectors trained with mixture of synthetic and real data outperform those trained on only real datasets.},
  keywords={Training;Image segmentation;Urban areas;Neural networks;Detectors;Data models;Labeling;data augmentation;corner case;traffic scene;computer vision;deep learning},
  doi={10.1109/CAC57257.2022.10054829},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{9880250,
  author={Kan, Ge and Lü, Jinhu and Wang, Tian and Zhang, Baochang and Zhu, Aichun and Huang, Lei and Guo, Guodong and Snoussi, Hichem},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Bi-level Doubly Variational Learning for Energy-based Latent Variable Models}, 
  year={2022},
  volume={},
  number={},
  pages={18439-18448},
  abstract={Energy-based latent variable models (EBLVMs) are more expressive than conventional energy-based models. However, its potential on visual tasks are limited by its training process based on maximum likelihood estimate that requires sampling from two intractable distributions. In this paper, we propose Bi-level doubly variational learning (BiDVL), which is based on a new bi-level optimization framework and two tractable variational distributions to facilitate learning EBLVMs. Particularly, we lead a decoupled EBLVM consisting of a marginal energy-based distribution and a structural posterior to handle the difficulties when learning deep EBLVMs on images. By choosing a symmetric KL divergence in the lower level of our framework, a compact BiDVL for visual tasks can be obtained. Our model achieves impressive image generation performance over related works. It also demonstrates the significant capacity of testing image reconstruction and out-of-distribution detection.},
  keywords={Training;Visualization;Maximum likelihood estimation;Maximum likelihood detection;Stacking;Lead;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01791},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10542521,
  author={Sun, Xiaoni and Zhang, Yong and Piao, Xinglin and Wu, Jiayi and Jing, Guodong and Yin, Baocai},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={PN-HGNN: Precipitation Nowcasting Network Via Hypergraph Neural Networks}, 
  year={2024},
  volume={62},
  number={},
  pages={1-12},
  abstract={Precipitation nowcasting within 2 h is an important and hard issue in the weather research area. Benefiting from the outstanding nonlinear relationship modeling capability, methods based on deep learning (DL) have achieved significant success in the task of precipitation nowcasting compared to the others. However, existing DL-based methods always disregard the intricate high-order correlations and lack substantial connections with the evolution of the precipitation system, which would lead to blurred forecasts and implausible predictions. To address these issues, we proposed a new Precipitation Nowcasting Network within a 2-h model based on the Hypergraph Neural Network (PN-HGNN). In this work, a Hypergraph Neural Network is first adopted for extracting spatiotemporal dynamic echo features. Second, regulation evolution is in charge of capturing the memory features to guide the extrapolation. Finally, we design a dual branch module to extrapolate the radar echoes. The proposed model has been assessed on the dataset HKO-7. The experimental results demonstrate that PN-HGNN achieved better prediction performance than the six representative echo extrapolation models.},
  keywords={Precipitation;Radar;Task analysis;Extrapolation;Neural networks;Meteorology;Predictive models;Hypergraph neural network (HGNN);MotionRNN;precipitation nowcasting;radar echo extrapolation},
  doi={10.1109/TGRS.2024.3407157},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{8959926,
  author={Chin, Chiun-Li and Lin, Jo-Wei and Wei, Chia-Shin and Hsu, Ming-Chen},
  booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)}, 
  title={Dentition Labeling And Root Canal Recognition Using Ganand Rule-Based System}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Dentition identification and root canal are very common in dentistry. Dentition identification helps dentists to diagnose the teeth conditions of patient and discuss about their treatments, while root canal is one of the common treatments. The goal of this paper is to label tooth dentition and identify root canal via GAN and rule-based algorithm. Before inputting images into GAN, the original images were sharpened to make the learning task easier. Next, we use the rule-based algorithm to identify root canal. For evaluating proposed method, we invite three dentists to evaluate the accuracy of the results, and they consider that the results are reliable. We also discuss the impact that iterations have on accuracy. The model becomes stable when iteration comes to 12000, and the accuracy reaches 93.7%.},
  keywords={Dentistry;Teeth;Irrigation;Gallium nitride;Deep learning;Image segmentation;Image color analysis;tooth dentition;root canal;GAN;rule-based algorithm},
  doi={10.1109/TAAI48200.2019.8959926},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{9561379,
  author={Jian, Pingcheng and Yang, Chao and Guo, Di and Liu, Huaping and Sun, Fuchun},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Adversarial Skill Learning for Robust Manipulation}, 
  year={2021},
  volume={},
  number={},
  pages={2555-2561},
  abstract={Deep reinforcement learning has made significant progress in robotic manipulation tasks and it works well in the ideal disturbance-free environment. However, in a real-world environment, both internal and external disturbances are inevitable, thus the performance of the trained policy will dramatically drop. To improve the robustness of the policy, we introduce the adversarial training mechanism to the robotic manipulation tasks in this paper, and an adversarial skill learning algorithm based on soft actor-critic (SAC) is proposed for robust manipulation. Extensive experiments are conducted to demonstrate that the learned policy is robust to internal and external disturbances. Additionally, the proposed algorithm is evaluated in both the simulation environment and on the real robotic platform.},
  keywords={Training;Automation;Conferences;Reinforcement learning;Robustness;Task analysis;Robots},
  doi={10.1109/ICRA48506.2021.9561379},
  ISSN={2577-087X},
  month={May},}@INPROCEEDINGS{10724511,
  author={Kanimozhi Selvi, C.S. and Kalaivani, K.S. and Srigha, S. and Ramesh, Hariesh and Kalki Kartik, S.L.},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Yoga Pose Recognition and Correction Using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Yoga is an ancient practice that holds contemporary significance in promoting holistic well- being. This study focuses on yoga pose recognition via deep learning-based methodology, utilizing Media Pipe for pose estimation. Specifically, the study concentrates on estimating poses for beginners. The yoga pose images were manually categorized into three levels: Beginners, Intermediate, and Advanced. The estimated yoga pose data for beginners was used to train several types of deep learning models: an Advanced Convolutional Neural Network (CNN), a hybrid model combining Convolutional Neural Network with Long Short-Term Memory, ResNet50, VGG16. All the models have demonstrated promising accuracy. The CNN model achieved a training accuracy of $\mathbf{9 8. 8 9 \%}$ and a validation accuracy of $83.33 \%$. In contrast, the CNN- LSTM model achieved a training accuracy of $94.06 \%$, resulting in a marginal increase in validation accuracy to $84.79 \%$. Furthermore, with an accuracy of $92 \%$, the VGG16 model showed excellent performance, while the ResNet50 model attained an astounding $\mathbf{9 3. 6 8 \%}$ accuracy. The primary objective is for the model to accurately identify beginners’ yoga poses and provide recommendations for adjusting alignment to achieve the perfect Yoga Pose.},
  keywords={Training;Deep learning;Solid modeling;Accuracy;Computational modeling;Media;Data models;Convolutional neural networks;Long short term memory;Residual neural networks;Deep Learning;Media Pipe;Yoga;Pose recognition;Convolutional Neural Network (CNN);Long Short-Term Memory (LSTM);ResNet50;VGG16},
  doi={10.1109/ICCCNT61001.2024.10724511},
  ISSN={2473-7674},
  month={June},}@ARTICLE{9944792,
  author={Liang, Xuefeng and Liang, Zhihui and Shi, Huiwen and Zhang, Xiaosong and Zhou, Ying and Ma, Yifan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multipattern Mining Using Pattern-Level Contrastive Learning and Multipattern Activation Map}, 
  year={2024},
  volume={35},
  number={7},
  pages={9080-9094},
  abstract={Visual patterns are basic elements in images and represent the discernible regularity in the visual world. Thus, mining visual patterns is a fundamental task in computer vision. Most previous studies consider that only one visual pattern exists in a category, and then builds up a one-to-one mapping using category label. In reality, however, many categories include multiple patterns, which are many-to-one mappings. Without knowing the information of patterns, few existing pattern mining methods can discover and distinguish varied patterns in a category. To tackle this problem, we propose a novel framework, PaclMap, which learns medium-grained features to represent patterns. It includes an unsupervised pattern-level contrastive learning and a multipattern activation map. Their joint optimization encourages the network to mine both discriminative and frequent patterns in a category. Extensive experiments conducted on four benchmark datasets (Place-20, imagenet large scale visual recognition challenge (ILSVRC)-20, visual object classes (VOC), and Travel) demonstrate that PaclMap outperforms six state-of-the-art methods with average improvements of 2.9% on accuracy and 12.3% on frequency, respectively.},
  keywords={Visualization;Feature extraction;Task analysis;Semantics;Convolutional neural networks;Optimization;Transforms;Joint optimization;multipattern activation map;multipattern mining;pattern-level contrastive learning},
  doi={10.1109/TNNLS.2022.3218073},
  ISSN={2162-2388},
  month={July},}@ARTICLE{9991048,
  author={Liu, Zhi and Yang, Shuyuan and Feng, Zhixi and Wang, Min and Yu, Zhifan},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Deep Compressive Imaging With Meta-Learning}, 
  year={2023},
  volume={72},
  number={},
  pages={1-9},
  abstract={Recently, deep learning-based compressive synthetic aperture radar (SAR) imaging has received increasing interests. However, its performances rely heavily on the training data and could not well adapt to new observations. To solve it, in this article a robust compressive SAR imaging method is proposed under the paradigm of meta-learning. First, a Deep Skip-connected Convolutional Network (DSCN) is constructed, to learn the mapping from Low-Resolution (LR) radar returns to High-Resolution (HR) radar image of the same scene. Then a set of meta tasks are sampled from one or multiple radar systems, to guide the search of DSCN and knowledge transfer to new compressive imaging tasks. This method is non-iterative and does not require any information of radar system. Moreover, it can work for compressive imaging in both range and azimuth. Extensive experiments are taken and the results show that the proposed meta-learning-based DSCN (ML-DSCN) can achieve accurate and rapid resolution enhancement, and is superior to its counterparts in terms of reconstruction speed, robustness, and generalization.},
  keywords={Radar imaging;Radar;Radar polarimetry;Imaging;Task analysis;Image coding;Signal resolution;Compressive imaging;deep skip-connected convolutional network (DSCN);meta-learning (ML);synthetic aperture radar (SAR)},
  doi={10.1109/TIM.2022.3228011},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{9922118,
  author={Chen, Weihuang and Zheng, Fang and Shi, Liushuai and Zhu, Yongdong and Sun, Hongbin and Zheng, Nanning},
  booktitle={2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Multiple Goals Network for Pedestrian Trajectory Prediction in Autonomous Driving}, 
  year={2022},
  volume={},
  number={},
  pages={717-722},
  abstract={As the most vulnerable traffic participants, pedestrians have always received considerable attention from autonomous driving. However, predicting the future behavior of pedestrians is challenging due to the intentions of pedestrian are potentially stochastic and difficult to be captured accurately through only a single trajectory. In order to solve these problems, we propose a multiple goals network (MGNet) for pedestrian trajectory prediction to generate a set of plausible trajectories in the crowds. The multimodality is achieved by sampling various goals from the parametric distribution which can sufficiently represent the stochastic intentions of pedestrian. The parametric distribution is obtained from the observations by a simple and effective multilayer perceptrons module. Finally, the whole future trajectories are generated by a Transformer-based encoder-decoder module with a new goal-visible masking mechanism. Experimental results on the most widely used datasets, i.e., the ETH-UCY datasets, demonstrate that MGNet is capable of achieving competitive performance compared with state-of-the-art methods.},
  keywords={Human-robot interaction;Multilayer perceptrons;Benchmark testing;Transformers;Prediction algorithms;Generators;Trajectory},
  doi={10.1109/ITSC55140.2022.9922118},
  ISSN={},
  month={Oct},}@ARTICLE{10223416,
  author={Wang, Shaofan and Wang, Weixing and Huang, Shiyu and Han, Yuwei and Wei, Fuhao and Yin, Baocai},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Nowcasting the Vehicular Control Delay From Low-Ping Frequency Trajectories via Incremental Hypergraph Learning}, 
  year={2024},
  volume={73},
  number={1},
  pages={185-199},
  abstract={Nowcasting the vehicular delay at intersections of road networks not only optimizes the signal timing at the intersections, but also alleviates traffic congestion effectively. Existing research work on the vehicular delay nowcasting involves two issues: low effectiveness on low-ping frequency trajectory data, and low efficiency for the nowcasting task. Inspired by recent works on hypergraphs which explore the high-order relationship of trajectory points, we propose an incremental hypergraph learning framework for nowcasting the control delay of vehicles from low-ping frequency trajectories. The framework characterizes the relationship among trajectory points using multi-kernel learning of multiple attributes of trajectory points. Then, it predicts the unknown trajectory points by incrementally constructing hypergraphs of both observed and unknown points and examining the total similarities of hyperedges associated with all the points. Finally, it evaluates the control delay of each trajectory precisely and efficiently based on the timestamp difference of critical points. We conduct experiments on the Didi-Chengdu dataset with 10-second ping frequency. Our framework outperforms state-of-the-art methods in both the accuracy and efficiency (with 6 seconds at each intersection averagely) for the control delay nowcasting task. That facilitates our framework for many real-world traffic scenarios.},
  keywords={Trajectory planning;Delays;Predictive models;Hidden Markov models;Frequency control;Markov processes;Control delay nowcasting;incremental hypergraph learning;multi-kernel affinity learning;trajectory prediction},
  doi={10.1109/TVT.2023.3306158},
  ISSN={1939-9359},
  month={Jan},}@ARTICLE{10540389,
  author={Zang, Shizhe and Zhang, Yikun and Hu, Dianlin and Mao, Weilong and Fei, Xuanjia and Ji, Xu and Yao, Yi and Yang, Chunfeng and Coatrieux, Gouenou and Chen, Yang},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Coarse-to-Fine Learning for Planning CT-Enhanced Limited-Angle CBCT Reconstruction}, 
  year={2024},
  volume={73},
  number={},
  pages={1-15},
  abstract={In the image-guided radiation therapy, the on-board cone-beam computed tomography (CBCT) is usually used for volumetric imaging, and the limited-angle scanning protocol is often adopted to avoid the possible collisions of the moving gantry with patients and devices. However, images directly reconstructed using incomplete projection data may suffer from severe artifacts, which cannot provide precise guidance for the following therapeutic procedures. Compared with CBCT, the planning computed tomography (pCT) acquired for the treatment plan can provide high-quality images of the same patient, showing the potential to improve the limited-angle CBCT. In this article, we propose a multidimensional joint cascaded network (MJCNet), which can exploit the prior information from pCT. MJCNet improves the imaging quality of limited-angle CBCT through a coarse-to-fine strategy. In the coarse restoration stage, a 2-D network with two encoders that could extract and exploit the information of pCT is used to remove limited-angle artifacts slice-by-slice. In the fine-tuning stage, a 3-D network with a dense attention mechanism is employed to further improve the image details and remove the interslice artifacts. The real data from different parts of human bodies are collected to evaluate the proposed method. Experimental results demonstrate the promising performance of MJCNet in reducing wedge artifacts, restoring image structures, and correcting hounsfield unit (HU) numbers.},
  keywords={Computed tomography;Image reconstruction;Planning;Three-dimensional displays;Image restoration;Radiation therapy;Optimization;Coarse-to-fine learning;limited angle;planning computed tomography (pCT) enhanced cone-beam computed tomography (CBCT)},
  doi={10.1109/TIM.2024.3406810},
  ISSN={1557-9662},
  month={},}
