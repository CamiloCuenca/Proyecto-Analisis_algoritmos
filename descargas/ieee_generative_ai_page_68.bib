@INBOOK{10952605,
  author={Vayadande, Kuldeep and Bohri, Mustansir and Chawala, Mohit and Kulkarni, Ashutosh M. and Mursal, Asif},
  booktitle={How Machine Learning is Innovating Today's World: A Concise Technical Guide}, 
  title={The Rise of AI&#x2010;Generated News Videos}, 
  year={2024},
  volume={},
  number={},
  pages={423-451},
  abstract={Summary <p>The rapid advancements in Artificial Intelligence (AI) have given rise to the possibility of automating news video creation. AI&#x2010;powered news videos will offer a fresh and dynamic perspective on the day's top stories, delivering the content people need in a way that is easy to consume. AI&#x2010;generated news videos are the next evolution in journalism, providing a fast and accurate way to consume news content that is both informative and visually stunning. In this review paper, we explore the process of converting news articles into AI&#x2010;generated videos that can be published on platforms like YouTube. The process involves web scraping of text and images, news authentication, image searching, voice&#x2010;over creation, video generation, thumbnail creation, and YouTube video upload. We have reviewed several research papers related to each of these steps and highlighted their applications in news video creation. We have identified the challenges involved in each step, such as the authenticity of news articles, relevance of images, and the need for high&#x2010;quality voice&#x2010;overs. We have discussed proposed solutions for these challenges and the potential of AI&#x2010;generated news videos in revolutionizing the news industry. Our review paper also highlights the research gaps in this field, such as the need for more advanced image and voice recognition technology and the potential ethical concerns of using AI&#x2010;generated content.</p>},
  keywords={Videos;Artificial intelligence;Media;Reviews;Face recognition;Data mining;Visualization;Text recognition;Ontologies;Industries},
  doi={10.1002/9781394214167.ch25},
  ISSN={},
  publisher={Wiley},
  isbn={9781394214150},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10952605},}@INPROCEEDINGS{11068398,
  author={Ha, Seongjun and Lercel, Damon and Nanda, Gaurav},
  booktitle={2025 IEEE Aerospace Conference}, 
  title={A Lightweight Anomaly Detection Model in Aero Turbine Borescope Using Unsupervised Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={Visual inspection is a critical task typically performed by humans, but human decisions can be inconsistent due to various factors. While many studies have introduced artificial intelligence (AI) to assist aircraft technicians, supervised approaches often encounter limitations, such as the need for human intervention and a lack of sufficient data. By understanding these challenges, researchers developed a lightweight anomaly detection (AD) model in aero turbine borescope inspection using unsupervised deep learning. Considering the utilization of the borescope AD model, two usability perspectives are considered, 1) lightweight operation and 2) input image resolution. The selected AD algorithm is Patch Distribution Model (PaDiM). This algorithm is trained with normal data and tested with normal and abnormal data. One of the key benefits of the PaDiM model is that it reduces the need for human intervention during turbine borescope inspections. It requires only normal data for training, provides anomaly scores, and can localize potential defect areas when abnormal data is input To operate this model for light operations, MobilNetV3 was selected for the backbone, with layers selectively adjusted based on different settings and each model's performance using matrices such as AUROC, F1 score, training time, and prediction time. When compared to the original PaDiM Models, this research found the performance of the new model to be relatively consistent across different settings while requiring less time to train and predict, and that higher resolution images can better detect small defects than lower resolution images. The proposed unsupervised deep learning based AD models can aid aircraft technicians in improving the performance of borescope inspection.},
  keywords={Training;Deep learning;Visualization;Image resolution;Atmospheric modeling;Inspection;Predictive models;Aircraft;Artificial intelligence;Turbines},
  doi={10.1109/AERO63441.2025.11068398},
  ISSN={2996-2358},
  month={March},}@INPROCEEDINGS{11025504,
  author={Song, Xue},
  booktitle={2024 5th International Conference on Information Science and Education (ICISE-IE)}, 
  title={Research on a personalised education recommendation system based on deep learning: from learning data to customised learning paths}, 
  year={2024},
  volume={},
  number={},
  pages={147-152},
  abstract={With the rapid application of artificial intelligence, especially deep learning technology, in the field of education, personalised education recommendation systems have become an important means of enhancing learning outcomes and teaching quality. The deep learning-based personalised education recommendation system can not only effectively handle large-scale, multi-dimensional student data, but also intelligently generate customised learning paths based on students' learning status and behaviour. However, there are still many areas in the optimisation of learning pathways and the enhancement of student learning outcomes that remain to be fully explored. This article aims to propose cutting-edge topics that have not been sufficiently researched from the perspective of personalised recommendation systems, including personalised emotional modelling for students, the recommendation of long-tail learning resources, and the impact of multi-source data fusion on the effectiveness of personalised recommendations. To this end, this article also designs a practical experimental plan aimed at verifying the research value of these emerging topics and demonstrating the broad prospects and practical significance of deep learning-based recommendation systems in the field of education.},
  keywords={Deep learning;Information science;Heavily-tailed distribution;Education;Data integration;Learning (artificial intelligence);Data models;Recommender systems;Optimization;Faces;component;Deep learning;personalised education;emotional modelling;Multi-source data fusion},
  doi={10.1109/ICISE-IE64355.2024.11025504},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11034964,
  author={Wang, Dan},
  booktitle={2025 4th International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE)}, 
  title={Intelligent Dance Teaching Support System based on 2D Convolutional Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The development of artificial intelligence has transformed a number of industries, including the performing arts and education. This project investigates an AI-based intelligent dance teaching support system that combines 2D convolutional neural networks and few-shot learning. The technology is intended to assess and analyse dance moves while giving students immediate feedback. Few-shot learning makes the model extremely effective for dancing forms by allowing it to adjust to different dance styles with little training data. To provide accurate posture estimation and movement recognition, features are extracted from motion sequences using the 2D CNN architecture. The proposed method dramatically increases learning accuracy and efficiency in dance training, according to experimental results. Through intelligent feedback and adaptive learning methods, the study demonstrates how AI technologies may improve traditional dance education. The recommended approach achieves the maximum accuracy in the Intelligent Dance Teaching Support System, according to the performance metrics, with an accuracy score of 98.2%, a precision score of 98.12%, a recall score of 98.11%, and an F1 score of 98.12%. The study highlights how crucial this framework is for creating intelligent dance teaching systems and differentiating dancing styles.},
  keywords={Training;Humanities;Accuracy;Education;Training data;Feature extraction;Real-time systems;Convolutional neural networks;Few shot learning;Artificial intelligence;few-shot learning;2D convolutional neural networks;intelligent dance teaching;posture estimation and movement recognition},
  doi={10.1109/ICDCECE65353.2025.11034964},
  ISSN={},
  month={April},}@INPROCEEDINGS{9347100,
  author={Zhang, Pei and Wu, Shengyu and Li, Jiateng and Bu, Siqi},
  booktitle={2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)}, 
  title={Application of Emerging Information Technologies in Modern Energy Strategy Planning}, 
  year={2020},
  volume={},
  number={},
  pages={3540-3545},
  abstract={The emerging information technologies provides an opportunity for supporting the planning of energy industry development. This paper presents an overview on the application of the novel information technologies including big data, artificial intelligence (AI), cloud computing and virtual reality (VR) in the modern energy strategy planning (ESP). Firstly, the concept and current challenges of ESP is briefly reviewed and some typical information technologies which might be suitable for modern ESP are summarized. Secondly, the major technical needs of modern ESP are investigated. On this basis, the application scenarios of these emerging information technologies in modern ESP are extensively discussed. It can be foreseen that an efficient and economic ESP can be achieved by implementing these new technologies for the benefit of both policy makers and system planners.},
  keywords={Industries;Cloud computing;Virtual reality;Big Data;Planning;Information technology;Artificial intelligence;Energy Strategy Planning;Energy Transition;Information Technologies;Policy Maker;System Planner},
  doi={10.1109/EI250167.2020.9347100},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10826894,
  author={Ahakonye, Love Allen Chijioke and Nwakanma, Cosmas Ifeanyi and Kim, Dong-Seong},
  booktitle={2024 15th International Conference on Information and Communication Technology Convergence (ICTC)}, 
  title={Roadmap for Integrating Open RAN and AI for Intrusion Detection Systems in 6G Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1586-1591},
  abstract={This work presents a roadmap for integrating Open Radio Access Networks (O-RAN) and Artificial Intelligence (AI) into 6G networks, focusing on intrusion and fault detection. O-RAN introduces flexibility in network design by enabling disaggregated and vendor-neutral components, but it also poses security challenges due to open interfaces and multi-vendor integration. This study proposes strategically deploying an intrusion detection system (IDS) within the O-RAN framework to enhance security. The IDS structure suggests specific placement of sensors at critical locations such as Near-Real-Time RAN Intelligent Controllers (RICs), Open Distributed Units (ODU), and Open Fronthaul interfaces, among others. This deployment is complemented by a federated learning approach for IDS model training, which ensures data privacy and resilience by keeping sensitive data local while improving model performance through aggregated updates. The paper emphasizes the importance of such a framework for the secure and efficient operation of 6G networks, addressing the challenges posed by the increasing complexity and potential vulnerabilities of O-RAN. The proposed roadmap is intended to guide stakeholders in mitigating risks and optimizing the integration of AI in future 6G networks.},
  keywords={6G mobile communication;Data privacy;Accuracy;Federated learning;Image edge detection;Intrusion detection;Open RAN;Data models;Security;Artificial intelligence;AI;O-RAN;IDS;6G},
  doi={10.1109/ICTC62082.2024.10826894},
  ISSN={2162-1241},
  month={Oct},}@INPROCEEDINGS{11032058,
  author={C H, Muhammed Mishab and P, Thiyagarajan and S, Vigneshwari and Abdurrahman, Usama},
  booktitle={2025 International Conference on Data Science and Business Systems (ICDSBS)}, 
  title={Comparative Study on the Performance of Auto Encoders in Detecting Deep Fake Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Due to the fast progress in deepfake technology, the world is experiencing fundamental shifts in how digital content is produced and consumed. Deepfakes, which rely on Artificial Intelligence (AI) for the creation of realistic but fake digital content, have considerable applications in different fields like entertainment, security, and social media platforms such as Facebook and Instagram. But it can be used in negative way also, recently it is used more for abuse and other illegal activities, so it is very important to differentiate which is fake and real. In this research, five different types of autoencoders namely, Convolutional autoencoders, Variational autoencoders, Hybrid Convolutional autoencoders, Contractive autoencoders and Vanilla autoencoder are compared on two different data sets namely “140k real and fake faces”, and “deepfake and real images”. These all models are part of the autoencoder family and follow same basic structure by using encoder and decoder. Encoder compress the input data into lower dimensional latent representation and decoder reconstruct the original data from the latent representation. Among the autoencoders which is implemented on the dataset “140k real and fake faces”, convolutional autoencoder achieve impressive accuracy of 98%, hybrid convolutional autoencoder achieves 97% and contractive autoencoder achieves 96% accuracy on classifying deepfake images from real images. In the dataset “deepfake and real images” the convolutional autoencoder achieve impressive accuracy of 90%, hybrid convolutional autoencoder achieves 93 %, and contractive autoencoder achieves 93 % accuracy on classifying deepfake images from real images.},
  keywords={Deepfakes;Accuracy;Social networking (online);Autoencoders;Decoding;Web sites;Security;Multimedia communication;Artificial intelligence;Faces;Deep fake;Images;Auto encoders;Deep Learning},
  doi={10.1109/ICDSBS63635.2025.11032058},
  ISSN={},
  month={April},}@INPROCEEDINGS{10407412,
  author={R, Anitha and N, Kishore and Vijay Anand, M.},
  booktitle={2023 9th International Conference on Smart Structures and Systems (ICSSS)}, 
  title={NextGen Dynamic Video Generator using AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={In an era marked by remarkable technological advancements, the way we create and share information has undergone a profound transformation. This paradigm shift is epitomized by the NextGen Dynamic Video Generator using AI, a cutting-edge tool that seamlessly integrates artificial intelligence with content creation. What sets this tool apart is the customizability, which allows the users to fine-tune every aspect of the tutorials. Users could tune creative elements like humor, the depth of explanation, character appearance, and voice to tailor tutorials to precise specifications. The core functionality of the tool revolves around script generation, the Cohere's language, laying the groundwork for the tutorial's content. Furthermore, seamless integration with Edge TTS ensures that the generated scripts are delivered with utmost clarity and engagement, enhancing the overall learning experience. Character animation is powered by SadTalker, adding a dynamic and captivating dimension to these tutorials. This animated character serves as a relatable guide, facilitating a deeper connection between the content and the audience. The tool also seamlessly integrates relevant and eye-catching images from Google, which are incorporated into the presentation slides. The workflow is a well-orchestrated process involving script generation, audio dialogue creation, image retrieval, video generation, and the seamless fusion of character animations and slides. The resulting video tutorials are not only comprehensive but also engaging and ready to be shared as valuable educational resources.},
  keywords={Productivity;Noise reduction;Tutorials;Linguistics;Animation;Generators;Artificial intelligence;Cohere language model;Edge TTS;SadTalker;script generation;audio dialogue creation;humor;image retrieval;next-gen;character animation;google;slides},
  doi={10.1109/ICSSS58085.2023.10407412},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10927317,
  author={Ojo, Olakunle L. and Ajiboye, Yinka and Afolalu, Adeniran S. and Morebise, Ademola T. and Abe, Oladipo E. and Omoyajowo, Ifeoluwa M. and Olumodimu, Olugbenga and Adeyeye, David S. and Olawunmi, Oyelami},
  booktitle={2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)}, 
  title={AI for Ionospheric Disturbance Analysis Using GNSS TEC Data}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The ionosphere, a dynamic layer of Earth’s atmosphere, significantly impacts radio wave propagation and GNSS signal integrity, influencing communication, navigation, and space weather systems. This review explores recent advancements in artificial intelligence (AI) for analyzing ionospheric disturbances using Global Navigation Satellite System (GNSS) Total Electron Content (TEC) data. Machine learning algorithms—including supervised, unsupervised, and reinforcement learning—are highlighted for their roles in pattern detection, classification, and prediction of ionospheric anomalies. Deep learning architectures, such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, demonstrate exceptional capability in processing large-scale TEC datasets, enabling high-resolution ionospheric mapping and temporal forecasting. Hybrid models that integrate AI with traditional ionospheric frameworks further enhance predictive accuracy and real-time adaptability. These advancements not only deepen our understanding of ionospheric dynamics but also provide actionable tools to mitigate disruptions in critical technologies.},
  keywords={Global navigation satellite system;Heuristic algorithms;Predictive models;Prediction algorithms;Real-time systems;Data models;Classification algorithms;Artificial intelligence;Forecasting;Long short term memory},
  doi={10.1109/NIGERCON62786.2024.10927317},
  ISSN={2377-2697},
  month={Nov},}@INPROCEEDINGS{11088542,
  author={S, Saranya and Vinayagam, P and S, Nagarajan and B, Yuvasri and M, Ezhilvendan and R, Siva Subramanian},
  booktitle={2025 11th International Conference on Communication and Signal Processing (ICCSP)}, 
  title={Revolutionizing Technology: Advancements, Challenges, and Future of Computer Vision}, 
  year={2025},
  volume={},
  number={},
  pages={13-18},
  abstract={Computer vision is an active subfield of AI that empowers machines to analyse graphic information from the physical environment. It starts from simple image processing up to modern-day AI enabled deep learning systems. This field is widely used in healthcare (tumor detection, medical imaging), automobile industry (object and lane detection), and retail industry (visual search, quality inspection). The paradigms of Vision Transformers (ViTs) and self-supervised learning are shifting, as new approaches to scalability and performance issues arise. There are still considerable challenges, such as, data collection, computational resources and issues related to bias and privacy. Techniques such as model pruning, federated learning, explainable artificial intelligence (XAI) are being worked out to improve Fairness. The emerging trends for further developments are MIMAC, Real time edge computing, and Ethical AI to promote the proper implementation of the system. With the integration of other fields in AI and by solving current issues, computer vision is ready to revolutionize industries and people’s lives.},
  keywords={Industries;Deep learning;Computer vision;Ethics;Computational modeling;Self-supervised learning;Medical services;Transformers;Real-time systems;Artificial intelligence;Computer Vision;Deep Learning;Vision Transformers;Autonomous Vehicles;Ethical AI;Self-Supervised Learning},
  doi={10.1109/ICCSP64183.2025.11088542},
  ISSN={2836-1873},
  month={June},}@ARTICLE{8917633,
  author={Jiao, Licheng and Zhao, Jin},
  journal={IEEE Access}, 
  title={A Survey on the New Generation of Deep Learning in Image Processing}, 
  year={2019},
  volume={7},
  number={},
  pages={172231-172263},
  abstract={During the past decade, deep learning is one of the essential breakthroughs made in artificial intelligence. In particular, it has achieved great success in image processing. Correspondingly, various applications related to image processing are also promoting the rapid development of deep learning in all aspects of network structure, layer designing, and training tricks. However, the deeper structure makes the back-propagation algorithm more difficult. At the same time, the scale of training images without labels is also rapidly increasing, and class imbalance severely affects the performance of deep learning, these urgently require more novelty deep models and new parallel computing system to more effectively interpret the content of the image and form a suitable analysis mechanism. In this context, this survey provides four deep learning model series, which includes CNN series, GAN series, ELM-RVFL series, and other series, for comprehensive understanding towards the analytical techniques of image processing field, clarify the most important advancements and shed some light on future studies. By further studying the relationship between deep learning and image processing tasks, which can not only help us understand the reasons for the success of deep learning but also inspires new deep models and training methods. More importantly, this survey aims to improve or arouse other researchers to catch a glimpse of the state-of-the-art deep learning methods in the field of image processing and facilitate the applications of these deep learning technologies in their research tasks. Besides, we discuss the open issues and the promising directions of future research in image processing using the new generation of deep learning.},
  keywords={Machine learning;Task analysis;Generative adversarial networks;Convolutional neural networks;Image resolution;Mathematical model;Image processing;deep learning;convolutional neural network;generative adversarial network;extreme learning machine;deep forest;capsule networks;ADMM-Net;image classification;style transfer;object detection;super-resolution},
  doi={10.1109/ACCESS.2019.2956508},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10724532,
  author={Ram Prasath, K and Sowmya, V. and Deepak, K. and Premjith, B. and Lal, G. Jyothish},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Appearance-Trajectory Network for Video Anomaly Detection System}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Artificial intelligence in the field of security and surveillance is evolving so rapidly and video anomaly detection (VAD) is an essential activity that seeks to discover anomalous events in large-scale video streams. In this study, a memory module-enhanced convolution neural network (CNN)-based unsupervised anomaly detection technique is presented. Improving anomaly detection’s generalization is the main goal, with a special emphasis on human-centric events like fights and robberies. Our method adds a new updating methodology to the memory adaptors to extract representative patterns of normal data, and then enhances discriminating power with specially designed losses for these memory adaptors like compactness and separateness losses. Using an end-to-end attention-based (memory adaptors mimics attention) and a encoder-decoder architecture, Further we expand our approach to include skeletal sequences, enabling the collaborative learning of hidden representations of occluded regions. The efficacy of our proposed method is confirmed by trail evaluations carried out on benchmark datasets, such as Avenue and ShanghaiTech, which produce Area Under the Curve (AUC) scores of ${8 2 \%}$ and ${7 3 \%}$ for appearance based anomaly detection module and ${8 9 \%}$ and ${7 8 \%}$ for Skeleton Trajectories. Finally, handle human-centric anomaly occurrences by smoothly integrating our VAD system with a video captioning model, improving interpretability and providing explanation captions for anomalies that are discovered. Code and features are released at GitHub ramk06122000/VAD.},
  keywords={Surveillance;Neural networks;Streaming media;Transformers;Skeleton;Trajectory;Security;Anomaly detection;Streams;Software development management;Video Anomaly Detection (VAD);Memory-augmented convolutional neural network (CNN);Skeleton Trajectories;Spatial-Temporal Transformers},
  doi={10.1109/ICCCNT61001.2024.10724532},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10540566,
  author={Dietz, Katharina and Mühlhauser, Michael and Kögel, Jochen and Schwinger, Stephan and Sichermann, Marleen and Seufert, Michael and Herrmann, Dominik and Hoßfeld, Tobias},
  journal={IEEE Access}, 
  title={The Missing Link in Network Intrusion Detection: Taking AI/ML Research Efforts to Users}, 
  year={2024},
  volume={12},
  number={},
  pages={79815-79837},
  abstract={Intrusion Detection Systems (IDS) tackle the challenging task of detecting network attacks as fast as possible. As this is getting more complex in modern enterprise networks, Artificial Intelligence (AI) and Machine Learning (ML) have gained substantial popularity in research. However, their adoption into real-world IDS solutions remains poor. Academic research often overlooks the interconnection of users and technical aspects. This leads to less explainable AI/ML models that hinder trust among AI/ML non-experts. Additionally, research often neglects secondary concerns such as usability and privacy. If IDS approaches conflict with current regulations or if administrators cannot deal with attacks more effectively, enterprises will not adopt the IDS in practice. To identify those problems systematically, our literature survey takes a user-centric approach; we examine IDS research from the perspective of stakeholders by applying the concept of personas. Further, we investigate multiple factors limiting the adoption of AI/ML in security and suggest technical, non-technical, and user-related considerations to enhance the adoption in practice. Our key contributions are threefold. (i) We derive personas from realistic enterprise scenarios, (ii) we provide a set of relevant hypotheses in the form of a review template, and (iii), based on our reviews, we derive design guidelines for practical implementations. To the best of our knowledge, this is the first paper that analyzes practical adoption barriers of AI/ML-based intrusion detection solutions concerning appropriateness of data, reproducibility, explainability, practicability, usability, and privacy. Our guidelines may help researchers to holistically evaluate their AI/ML-based IDS approaches to increase practical adoption.},
  keywords={Surveys;Security;Monitoring;Network intrusion detection;Usability;Privacy;Anomaly detection;Artificial intelligence;Intrusion detection;Machine learning;Anomaly detection;artificial intelligence;intrusion detection;machine learning;network monitoring;privacy;security;usability},
  doi={10.1109/ACCESS.2024.3406939},
  ISSN={2169-3536},
  month={},}@ARTICLE{10415427,
  author={Alzahrani, Ahmad A.},
  journal={IEEE Access}, 
  title={Bioinspired Image Processing Enabled Facial Emotion Recognition Using Equilibrium Optimizer With a Hybrid Deep Learning Model}, 
  year={2024},
  volume={12},
  number={},
  pages={22219-22229},
  abstract={Owing to the unpredictable nature of human facial expression, Facial emotion recognition (FER) from facial images becomes a tedious process. FER is a field within artificial intelligence (AI) and computer vision (CV) that concentrates on developing algorithms and technologies to interpret and analyze emotional expressions shown on human faces. The major intention of this area of research is to enable computers and machines to automatically classify and detect emotions, including sadness, happiness, anger, etc., based on facial expressions and cues. Deep learning (DL) systems namely recurrent neural network (RNN) and convolutional neural network (CNN) can be used for the task of automatically classifying and detecting emotions from human facial expressions. Furthermore, Bioinspired Image Processing Enabled FER combines principles from biological systems with image processing techniques to better the robustness and accuracy of FER. This technique is inspired by natural processes to enhance the interpretation and understanding of human emotion conveyed by facial expressions. This study designs a new Bioinspired Image Processing Enabled Facial Emotion Recognition using an Equilibrium Optimizer with Hybrid Deep Learning (BIPFER-EOHDL) model. The primary objectives of the BIPFER-EOHDL technique lie in the effectual and automated identification of facial expressions using a hyperparameter-tuned DL algorithm. In the presented BIPFER-EOHDL technique, the median filtering (MF) approach can be applied for image pre-processing. Besides, the BIPFER-EOHDL method applies the EfficientNetB7 model for the process of feature extraction. Meanwhile, the hyperparameter selection of the EfficientNetB7 model takes place by the use of the EO algorithm. Finally, the multi-head attention bi-directional long short-term memory (MA-BLSTM) model is exploited for the recognition and classification of facial emotions. A wide-ranging simulation analysis was performed to validate the higher FER outcomes of the BIPFER-EOHDL methodology. The simulation results stated that the BIPFER-EOHDL technique accomplishes better FER results than other recent approaches.},
  keywords={Feature extraction;Emotion recognition;Brain modeling;Classification algorithms;Image processing;Tuning;Deep learning;Artificial intelligence;Non-verbal communication;facial emotion recognition;deep learning;equilibrium optimizer;artificial intelligence},
  doi={10.1109/ACCESS.2024.3359436},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9601604,
  author={Xie, Chenxi and Meng, Zhongvi and Song, Bo and Jiang, Guoping and Song, Yurong},
  booktitle={2021 33rd Chinese Control and Decision Conference (CCDC)}, 
  title={A Neural Network Based on WXLNet and Multi-Task Lable Embedding for Sentiment Analysis}, 
  year={2021},
  volume={},
  number={},
  pages={2359-2366},
  abstract={Sentiment analysis is an important field in natural language processing, the development of which has been greatly promoted by deep learning recently. However, previous studies focused on the structure of the model, and did not make full use of text semantics and label information. Moreover, the traditional model does not perform well in complex fine-grained sentiment classification. In this paper, we propose a new deep neural network model WXLNet-MTLE. By modifying XLNet language model, the utilization of text information and the language expression ability have been greatly improved. At the same time, we add Multi-Task Lable Embedding to improve the generalization ability of our model in its downstream sentiment analysis tasks. Comparative analysis are carried out on the data sets of 3 different tasks and 8 different scenarios. The experimental results show that WXLNet-MTLE performs better in sentiment analysis of multiple scenes than the other models.},
  keywords={Deep learning;Sentiment analysis;Analytical models;Neural networks;Semantics;Process control;Generative adversarial networks;Natural Language Processing;Sentiment Analysis;WXLNet;MTLE;Deep Learning;Language Model},
  doi={10.1109/CCDC52312.2021.9601604},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{11134877,
  author={Garjola, Riya and Birkhani, Priyanshu and Bameta, Pratyush and Biswas, Shiva},
  booktitle={2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)}, 
  title={Design and Training of GANs: A Case Study on Digit Image Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={160-167},
  abstract={A potent type of deep learning models known as Generative Adversarial Networks (GANs) is able to produce realistic-looking synthetic data. There is a gap for researchers and students utilizing the MATLAB environment since MATLAB-based implementations of GANs are still scarcely studied, whereas the majority of GAN implementations concentrate on Python-based templates. In this work, a full end-to-end GAN implementation in MATLAB is presented for the purpose of creating synthetic images using the Digit Dataset. Adversarial training was conducted over 200 epochs for the generator and discriminator architectures, which were meticulously constructed using transposed and standard convolutional layers, respectively. Both visual and statistical assessments, such as pixel intensity distribution and standard deviation to evaluate image variety, were used to assess the quality of generated images, while loss functions were used to track the training process. The GAN's capacity to learn and replicate the data distribution, and generate realistic and diverse images, is evident in the output. Training was reasonably balanced, evident in the final discriminator and generator loss of 1.4851 and 1.3285, respectively. For anyone interested in trying or teaching GANs using MATLAB, the research is a suitable point of reference in that it also presents architectural images and concise performance metrics.},
  keywords={Training;Deep learning;Convolutional codes;Visualization;Image synthesis;Generative adversarial networks;Generators;Data models;MATLAB;Standards;Generative Adversarial Network (GAN);Image Generation;MATLAB;Deep Learning;Discriminator;Generator;Neural Network;Pixel Intensity Distribution;Image Diversity},
  doi={10.1109/ICDICI66477.2025.11134877},
  ISSN={},
  month={July},}@INPROCEEDINGS{11010407,
  author={Meyer, Bruno H. and Pozo, Aurora T. R. and Nogueira, Michele and Zola, Wagner M. Nunan},
  booktitle={2025 IEEE Symposium on Computational Intelligence in Security, Defence and Biometrics (CISDB)}, 
  title={Enhancing Intrusion Detection Systems with representation methods: A comparative study}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper presents a comparative analysis of three data representation methods for improving Intrusion Detection Systems (IDS). The methods compared are autoencoders, Generative Adversarial Networks (GANs), and contrastive learning. Additionally, a baseline approach using raw input data is evaluated. The study is conducted on three well-known IDS datasets: NSL-KDD, Ton-IoT, and Bot-IoT, each with distinct characteristics. Our results demonstrate that representational methods significantly enhance classification performance, particularly when ample unlabeled data is available. Among the methods, GANs achieved the highest f1-score improvements in the Ton-IoT dataset, while contrastive learning excelled in the Bot-IoT dataset. The experiments also reveal that the choice of classifier impacts performance, with Random Forest performing best on raw data and Multi-Layer Perceptrons (MLP) excelling with transformed data. The study highlights the importance of selecting appropriate representation learning techniques and classifiers based on dataset characteristics. It emphasizes the potential of unsupervised learning methods to utilize large volumes of unlabeled data, a common scenario in real-world cybersecurity applications. The findings provide a foundation for future research in leveraging unsupervised learning for IDS and other cybersecurity challenges.},
  keywords={Representation learning;Autoencoders;Intrusion detection;Data visualization;Contrastive learning;NSL-KDD;Generative adversarial networks;Computer security;Unsupervised learning;Random forests;unsupervised learning;intrusion detection systems;autoencoders;generative adversarial networks;contrastive learning},
  doi={10.1109/CISDB64969.2025.11010407},
  ISSN={},
  month={March},}@INBOOK{11062541,
  author={Kumar, Rishi},
  booktitle={Winning the AI Arms Race: Defeating China and Russia, Re-establishing American Superpower for Global Prosperity and the Greater Good with Artificial Intelligence}, 
  title={17 The Data Dilemma: Security and Privacy}, 
  year={2025},
  volume={},
  number={},
  pages={149-158},
  abstract={Rishi Kumar offers an insightful and compelling exploration of how artificial intelligence is set to shape America&#x2019;s future and its standing on the global stage with "Winning the AI Arms Race &#x2013; Defeating China and Russia, Re-establishing American Superpower for Global Prosperity and the Greater Good with Artificial Intelligence." With his extensive experience as an award-winning Silicon Valley C-suite executive, a former congressional candidate, an executive board member of the state party, and an elected leader in his city, Kumar brings a visionary yet grounded perspective on leveraging AI&#x2019;s transformative potential. His unique expertise in technology, public policy, and public service allows him to present strategies that could significantly influence national and global advancements in AI. The book is structured around three pivotal themes: strengthening and safeguarding America&#x2019;s superpower status, countering the threats posed by malicious actors, and harnessing AI for the greater global good. This book is essential reading for policy makers navigating the complexities of AI&#x2019;s future and business leaders aiming to position themselves for success in the AI-driven world. It&#x2019;s an indispensable resource for anyone looking to understand and influence the future of AI.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788743800880},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/11062541},}@ARTICLE{8930499,
  author={Yuan, Kangle and Wei, Jianguo and Lu, Wenhuan and Xiong, Naixue},
  journal={IEEE Access}, 
  title={Single Image Dehazing via NIN-DehazeNet}, 
  year={2019},
  volume={7},
  number={},
  pages={181348-181356},
  abstract={Single image dehazing has always been a challenging problem in the field of computer vision. Traditional image defogging methods use manual features. With the development of artificial intelligence, the defogging method based on deep learning has developed rapidly. In this paper, we propose a novel image defogging approach called NIN-DehazeNet for single image. This method estimates the transmission map by NIN-DehazeNet combining Network-in-Network with MSCNN(Single Image Dehazing via Multi-Scale Convolutional Neural Networks). In the test stage, we estimate the transmission map of the input hazy image based on the trained model, and then generate the dehazed image using the estimated atmospheric light and computed transmission map. Extensive experiments have shown that the proposed algorithm overperformance traditional methods.},
  keywords={Atmospheric modeling;Feature extraction;Convolution;Convolutional neural networks;Scattering;Image color analysis;Manuals;Single image dehazing;manual features;deep learning;NIN-DehazeNet;Network-in-Network;multi-scale convolutional neural networks,atmospheric scattering model},
  doi={10.1109/ACCESS.2019.2958607},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10386858,
  author={Coles, Patrick J. and Szczepanski, Collin and Melanson, Denis and Donatella, Kaelan and Martinez, Antonio J. and Sbahi, Faris},
  booktitle={2023 IEEE International Conference on Rebooting Computing (ICRC)}, 
  title={Thermodynamic AI and the Fluctuation Frontier}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Many Artificial Intelligence (AI) algorithms are inspired by physics and employ stochastic fluctuations. We connect these physics-inspired AI algorithms by unifying them under a single mathematical framework that we call Thermodynamic AI, including: (1) Generative diffusion models, (2) Bayesian neural networks, (3) Monte Carlo sampling and (4) Simulated annealing. Such Thermodynamic AI algorithms are currently run on digital hardware, ultimately limiting their scalability and overall potential. Stochastic fluctuations naturally occur in physical thermodynamic systems, and such fluctuations can be viewed as a computational resource. Hence, we propose a novel computing device, called Thermodynamic AI hardware, that could accelerate such algorithms. We contrast Thermodynamic AI hardware with quantum computing where noise is a roadblock rather than a resource. Thermodynamic AI hardware can be viewed as a novel form of computing, since it uses a novel fundamental building block. We identify stochastic units (s-units) as the building blocks for Thermodynamic AI hardware. In addition to these s-units, Thermodynamic AI hardware employs a Maxwell’s demon device that guides the system to produce non-trivial states. We provide a few simple physical architectures for building these devices.},
  keywords={Thermodynamics;Fluctuations;Quantum computing;Scalability;Neural networks;Stochastic processes;Simulated annealing},
  doi={10.1109/ICRC60800.2023.10386858},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9637396,
  author={Jennings, Madeleine and Sandoval, Jorge and Sanders, Jeanne and Koro, Mirka and Kellam, Nadia and Jayasuriya, Suren},
  booktitle={2021 IEEE Frontiers in Education Conference (FIE)}, 
  title={Use of AI-Generated Visual Media in Interviews to Understand Power Differentials in Gender, Romantic, and Sexual Minority Students}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={This work-in-progress briefly outlines the theoretical background, methods, and preliminary results of a qualitative study conducted with gender, romantic, and sexual minority (GRSM) students immersed in higher education spaces. We elaborate on the efficacy of our innovative qualitative methodologies through the use of AI-human art-making interactions during our interviews, which helped to produce richer qualitative data from our participants. Our methodology was constructed using a Foucauldian theoretical framework to inform the framework of this study, focusing explicitly on GRSM students' experiences with power in higher education and when using technology, as well as the ways in which they resist power through the use of technology and AI -generated visual media.},
  keywords={Visualization;Conferences;Education;Focusing;Resists;Media;Interviews;queer;qualitative research;AI-human interaction;GAN;artmaking},
  doi={10.1109/FIE49875.2021.9637396},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10661085,
  author={Williams, Randi and Ali, Safinah and Alcantara, Raúl and Burghleh, Tasneem and Alghowinem, Sharifa and Breazeal, Cynthia},
  booktitle={2024 19th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Doodlebot: An Educational Robot for Creativity and AI Literacy}, 
  year={2024},
  volume={},
  number={},
  pages={772-780},
  abstract={Today, Artificial Intelligence (AI) is prevalent in everyday life, with emerging technologies like AI companions, autonomous vehicles, and AI art tools poised to significantly transform the future. The development of AI curricula that shows people how AI works and what they can do with it is a powerful way to prepare everyone, and especially young learners, for an increasingly AI-driven world. Educators often employ robotic toolkits in the classroom to boost engagement and learning. However, these platforms are generally unsuitable for young learners and learners without programming expertise. Moreover, these platforms often serve as either programmable artifacts or pedagogical agents, rarely capitalizing on the opportunity to support students in both capacities. We designed Doodlebot, a mobile social robot for hands-on AI education to address these gaps. Doodlebot is an effective tool for exploring AI with grade school (K-12) students, promoting their understanding of AI concepts such as perception, representation, reasoning and generation. We begin by elaborating Doodlebot’s design, highlighting its reliability, user-friendliness, and versatility. Then, we demonstrate Doodlebot’s versatility through example curricula about AI character design, autonomous robotics, and generative AI accessible to young learners. Finally, we share the results of a preliminary user study with elementary school youth where we found that the physical Doodlebot platform was as effective and user-friendly as the virtual version. This work offers insights into designing interactive educational robots that can inform future AI curricula and tools.CCS CONCEPTS• Human-centered computing → Collaborative and social computing devices.},
  keywords={Social computing;Educational robots;Statistical analysis;Social robots;Prototypes;Transforms;Reliability engineering;Social robots;education;creativity;collaboration},
  doi={},
  ISSN={2167-2121},
  month={March},}@INPROCEEDINGS{10933330,
  author={Lanka, Surekha and Moodhitaporn, Taipida},
  booktitle={2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)}, 
  title={IoT Security Enhancements in Smart Healthcare Using Federated Learning}, 
  year={2025},
  volume={},
  number={},
  pages={263-267},
  abstract={Federated learning (FL) is a distributive machine learning (ML) approach that makes use of a centralised server to assist several Internet of Things (IoT) devices in cooperatively training an ML model. IoT device local data is safeguarded since it never leaves the device in FL. Since distributed IoT devices in FL often gather their local data on their own, each device's data set may naturally constitute a unique source domain. In this research work, there will be maintaining the security enhancements for smart healthcare using federated learning. There will be securing the data of IoT -based sensors which may lose local data while using the smart systems. Removing this issue will give importance to local data also using the FL method. Thus, this research involves considering different research work and gives the research methodology for the security-enhancing process using FL in IoT enhanced smart healthcare.},
  keywords={Training;Sentiment analysis;Federated learning;Distributed databases;Medical services;Network security;Smart systems;Sensor systems;Internet of Things;Servers;Federated Learning (FL);Internet of Things (IoT);security;smart healthcare;data;Machine Learning (ML)},
  doi={10.1109/ICSADL65848.2025.10933330},
  ISSN={},
  month={Feb},}@INBOOK{10710608,
  author={Briggs, Caleb and Briggs, Rex},
  booktitle={The AI Conundrum: Harnessing the Power of AI for Your Organization—Profitably and Safely}, 
  title={5 Precision, Input Control, and the Rationale for Decisions}, 
  year={2024},
  volume={},
  number={},
  pages={81-95},
  abstract={There are many cases where precision is not absolutely necessary to convey meaning to a human. Researchers have found that the human brain can make sense of words and sentences with the letters jumbled as long as the first and last letters are correct and a few other caveats are observed (the length of the word is maintained, the use of capitalization is not jumbled, double letters are kept together, and the jumbled word does not translate into another word). The research traces back to Graham Rawlinson's PhD thesis on the topic in 1976, where he conducted 16 experiments that laid the foundation for a popular meme about the ability to make sense of jumbled text, shown in figure 5.1.},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262378963},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10710608},}@INPROCEEDINGS{11135539,
  author={Khan, Aijaz Ahmed and Prakash, Vivek and Jaiswal, Kamal},
  booktitle={2025 4th International Conference on Advances in Computing, Communication, Embedded and Secure Systems (ACCESS)}, 
  title={Aircraft Surface Crack Detection Using an AI-Powered UAV System}, 
  year={2025},
  volume={},
  number={},
  pages={50-54},
  abstract={The safety of an aircraft relies heavily on the structure and frame, making the early detection of cracks a priority in aircraft maintenance. This paper present an automated crack detection approach that employs state-of-the-art object identification utilizing the YOLO algorithms for automatic detection and classification of cracks on any aircraft surfaces. The technology involves capturing images through an Unmanned Aerial Vehicles (UAV) of any aircraft surface and conducting post-analysis on the ground to detect cracks. This proposed setup uses an Nvidia Jetson board integrated with a high-definition camera and YOLO algorithm to enhance general safety, reduce human errors, and increase the accuracy of inspection. Initial tests show that detection accuracy and efficiency are potentially high, which will permit incorporation into current aircraft maintenance practices. Future work will refine the model for different aircraft materials and settings.},
  keywords={YOLO;Accuracy;Inspection;Autonomous aerial vehicles;Safety;Maintenance;Classification algorithms;Aircraft;Surface cracks;Surface treatment;Aircraft Crack Detection;Automated Inspection;Computer Vision;Image Processing;Structural Integrity;UAV Technology;YOLOv8},
  doi={10.1109/ACCESS65134.2025.11135539},
  ISSN={},
  month={June},}@INPROCEEDINGS{9907426,
  author={Zhang, Kexin and Dou, Xiaobo and Yu, Hang and Hu, Yongming},
  booktitle={2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)}, 
  title={Data processing technology of distribution station area combined with convolutional neural network}, 
  year={2022},
  volume={},
  number={},
  pages={866-870},
  abstract={As the end of the distribution network closest to the user, the station area is very important to ensure the stability and safety of the station area. But now the station area has more problems in data collection and storage than the Central Asian side. This makes it more difficult to optimize the regulation of the Taiwan area. Aiming at the problems of abnormal sampling period, abnormal sampling reference time, data noise, and missing data in the data of the station area, this paper proposes a data processing method combining convolutional neural network and mathematical methods to solve these problems in the station area, and constructs a station The accurate time series of regional data was obtained, and the feasibility of the method was verified by simulation.},
  keywords={Low voltage;Time series analysis;Power system stability;Data models;Regulation;Mathematical models;Stability analysis},
  doi={10.1109/CYBER55403.2022.9907426},
  ISSN={2642-6633},
  month={July},}@INPROCEEDINGS{11166954,
  author={Sathya, J. and Chitra, M. P.},
  booktitle={2025 3rd International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)}, 
  title={Frequency-Selective Adversarial Attack in WSN Detected using Deep Learning Algorithms}, 
  year={2025},
  volume={},
  number={},
  pages={1870-1874},
  abstract={Adversarial attacks in Wireless Sensor Networks (WSNs), such as jamming and spoofing, threaten the reliability and security of communication systems. This paper presents a novel adversarial detection framework based on sub-band energy levels using a deep learning (DL) architecture with multi-task learning (MTL). The detection system, implemented on a Jetson Nano platform, processes incoming modulation signals susceptible to attack. The signals are first preprocessed using the Transverse Dyadic Wavelet Transform (TyDWT), which decomposes them into frequency sub-bands to eliminate noise and extract critical features. Subsequently, the Rational Dilation Wavelet Transform (RADWT) and Tunable Q-factor Wavelet Transform (TQWT) are applied to preserve and adaptively analyze the signal's full frequency spectrum, focusing on identifying abnormal oscillations linked to adversarial interference. The processed signal features are then analyzed by a DL model that classifies normal and adversarial signals. By utilizing multi-task learning, the model efficiently handles multiple tasks, such as anomaly detection, signal classification, and attack identification. The proposed system demonstrates a robust capability to detect adversarial activities and ensure secure communication in WSNs.},
  keywords={Wavelet transforms;Deep learning;Wireless sensor networks;Interference;Wavelet analysis;Multitasking;Feature extraction;Security;Jamming;Anomaly detection;Adversarial attacks;Attack identification;Deep learning (DL);Jamming;Jetson Nano;Modulation signals;Rational Dilation Wavelet Transform (RADWT);Wireless Sensor Networks (WSNs)},
  doi={10.1109/ICSCDS65426.2025.11166954},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10089273,
  author={Chao, James and Chao, William S. and Lange, Douglas S.},
  booktitle={2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)}, 
  title={Neural Symbolic AI For POMDP Games}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper demonstrates the difficulties of solving partially observable Markov decision process (POMDP) games with pure deep reinforcement learning. And shows that combining the strengths of deep reinforcement learning and symbolic reasoning can achieve improved results with less training time. Deep reinforcement learning excels at exploring states and optimizing actions based on those observations, but struggle to reason when facing uncertainty. So we propose an agent to use deep reinforcement learning to force exploration on the game states and its corresponding values, and then use reasoning on the given observations to deduce winning actions when arriving at high-value states, to enhance the performance compared to a pure deep reinforcement learning agent.},
  keywords={Deep learning;Training;Uncertainty;Reinforcement learning;Games;Programming;Probabilistic logic;deep reinforcement learning;first order logic;POMDP games},
  doi={10.1109/CSDE56538.2022.10089273},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10371420,
  author={Arakawa, Sora and Fukuta, Naoki},
  booktitle={2023 14th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)}, 
  title={A Preliminary Implementation for an Ontology-based Explainable Layout Recommendation Engine for Graphics and Web-Designing Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={679-682},
  abstract={In this paper, we show an overview of a prototype engine to support layout decisions in graphic design tasks such as designing posters and presentation materials and in website design. e engine we show here has preliminary for suggesting the location and color scheme of each element, such as texts and images, while the design work will be done by human workers. In addition, by conceptualizing elements and layouts to be used in the design tasks into ontologies, we aim to implement a function that outputs the explanations of generated design proposals as sentences.},
  keywords={Image color analysis;Layout;Prototypes;Machine learning;Ontologies;Proposals;Task analysis;ontology;layout recommendation;XAI},
  doi={10.1109/IIAI-AAI59060.2023.00133},
  ISSN={2472-0070},
  month={July},}@ARTICLE{9896140,
  author={Shimizu, Yuki and Morimoto, Shigeo and Sanada, Masayuki and Inoue, Yukinori},
  journal={IEEE Transactions on Energy Conversion}, 
  title={Automatic Design System With Generative Adversarial Network and Convolutional Neural Network for Optimization Design of Interior Permanent Magnet Synchronous Motor}, 
  year={2023},
  volume={38},
  number={1},
  pages={724-734},
  abstract={The optimal design of interior permanent magnet synchronous motors requires a long time because finite element analysis (FEA) is performed repeatedly. To solve this problem, many researchers have used artificial intelligence to construct a prediction model that can replace FEA. However, because the training data are generated by FEA, it takes a very long time to obtain a sufficient amount of data, making it impossible to train a large-scale prediction model. Here, we propose a method for generating a large amount of data from a small number of FEA results using machine learning. An automatic design system with a deep generative model and a convolutional neural network is then constructed. With its sufficient data, the proposed system can handle three topologies and three motor parameters in a wide range of current vector regions. The proposed system was applied to multi-objective optimization design, with the optimization completed in 13–15 seconds.},
  keywords={Topology;Predictive models;Optimization;Rotors;Shape;Data models;Training data;Convolutional neural network;design optimization;generative adversarial network;permanent magnet motor;semisupervised learning},
  doi={10.1109/TEC.2022.3208129},
  ISSN={1558-0059},
  month={March},}@INPROCEEDINGS{9616079,
  author={Banerjee, Rohan and Ghose, Avik},
  booktitle={2021 29th European Signal Processing Conference (EUSIPCO)}, 
  title={Synthesis of Realistic ECG Waveforms Using a Composite Generative Adversarial Network for Classification of Atrial Fibrillation}, 
  year={2021},
  volume={},
  number={},
  pages={1145-1149},
  abstract={In recent days, computer-aided diagnosis systems powered by artificial intelligence and machine learning have become an important part of medicine for assisting the doctors in critical decision making. They are popularly deployed in cardiology for early and automatic detection of various life-threatening diseases. However, a machine learning algorithm requires a large volume of training data to create the learning model which is an empirical problem in medical domain. Generating synthetic patient data has emerged as an important area of research to solve the issue. In this paper, we propose a novel Generative Adversarial Network (GAN) architecture using medical domain knowledge to create realistic Electrocardiogram (ECG) waveforms containing the signature of Atrial Fibrillation (AF), a common type of arrhythmia. Our composite architecture consists of a pair of GANs to simulate the disease-specific Heart Rate Variability (HRV) pattern and the unique signal morphology in the generated waveforms. The proposed architecture is applied on two public datasets for synthesis of AF-specific ECG to mitigate the class imbalance issue. Results show that the performance of existing AF classifiers significantly improves on both datasets by adding the synthetic data to the training set.},
  keywords={Training;Solid modeling;Supervised learning;Atrial fibrillation;Training data;Computer architecture;Electrocardiography;ECG;Deep learning;Data augmentation;Generative Adversarial Network},
  doi={10.23919/EUSIPCO54536.2021.9616079},
  ISSN={2076-1465},
  month={Aug},}@ARTICLE{8778792,
  author={Karavolos, Daniel and Liapis, Antonios and Yannakakis, Georgios},
  journal={IEEE Transactions on Games}, 
  title={A Multifaceted Surrogate Model for Search-Based Procedural Content Generation}, 
  year={2021},
  volume={13},
  number={1},
  pages={11-22},
  abstract={This paper proposes a framework for the procedural generation of level and ruleset components of games via a surrogate model that assesses their quality and complementarity. The surrogate model combines level and ruleset elements as input and gameplay outcomes as output, thus constructing a mapping between three different facets of games. Using this model as a surrogate for expensive gameplay simulations, a search-based generator can adapt content toward a target gameplay outcome. Using a shooter game as the target domain, this paper explores how parameters of the players' character classes can be mapped to both the level's representation and the gameplay outcomes of balance and match duration. The surrogate model is built on a deep learning architecture, trained on a large corpus of randomly generated sets of levels, classes, and simulations from game playing agents. Results show that a search-based generative approach can adapt character classes, levels, or both toward designer-specified targets. The model can thus act as a design assistant or be integrated in a mixed-initiative tool. Most importantly, the combination of three game facets into the model allows it to identify the synergies between levels, rules, and gameplay and orchestrate the generation of the former two toward desired outcomes.},
  keywords={Games;Adaptation models;Generators;Computational modeling;Deep learning;Weapons;Measurement;Deep learning;procedural content generation (PCG);search-based PCG;shooter games;surrogate model},
  doi={10.1109/TG.2019.2931044},
  ISSN={2475-1510},
  month={March},}@INPROCEEDINGS{9194930,
  author={Liu, Yichang and Reynolds, Mark and Huynh, Du and Hassan, Ghulam},
  booktitle={2020 IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS)}, 
  title={Study of Accurate and Fast Estimation Method of Vehicle Length Based on YOLOs}, 
  year={2020},
  volume={},
  number={},
  pages={118-121},
  abstract={With the development of AIoT, Smart City Traffic Management System based on artificial intelligence and big data has gradually become an effective urban management system. Instead of raw data from sensors and cameras, some specific information, such as the length of vehicles are more expected to be gained in Smart City. In this paper, a vehicle length estimation method is proposed based on the Convolutional Neural Networks (CNN) and image processing. The vehicles will be detected by YOLOs, a CNN model for object detection. Then the approximate length of vehicles will be estimated. Experiment results verify that the vehicle length estimation based on YOLOs approach a high accuracy at low time consumption.},
  keywords={Three-dimensional displays;Estimation;Roads;Solid modeling;Cameras;Australia;Object detection;Deep Learning;YOLOs;Image Processing;Vehicle Length Estimation},
  doi={10.1109/ICAIIS49377.2020.9194930},
  ISSN={},
  month={March},}@ARTICLE{11010862,
  author={Adam, Abuzar B. M. and Kamal, Tahir and Elhassan, Mohammed A. M. and Alshahrani, Abdullah and Alsamhi, Saeed Hamood and Aziz, Ahmed},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Multi-Scale Generative Transformer-Based Primal-Dual PPO Framework for AAV-Aided Intelligent Transportation Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-16},
  abstract={Intelligent transportation networks are increasingly integrating autonomous aerial vehicles (AAVs) to enable services essential for modernizing industries like transportation, logistics, and search and rescue. A significant beneficiary is the Internet of Connected Vehicles (IoCVs), where AAVs play a transformative role in supporting next-generation cellular networks. This paper addresses the challenge of minimizing the cost of delivering content to vehicles on road segments with congested traffic or insufficient infrastructure. Vehicles entering these segments request content from a AAV-hosted library that updates dynamically based on item popularity. Each vehicle submits a request, requiring the AAV to compute an optimal trajectory to maximize operational utility. Given the AAV’s limited energy, we aim to develop an energy-efficient solution. The problem is framed as a joint optimization of caching decisions, AAV trajectory planning, and radio resource allocation, formulated as a mixed integer non-linear programming (MINLP) problem. The environment’s complexity, with random vehicle arrivals and fluctuating content, makes traditional optimization techniques inadequate. To address this, we reformulate the problem as a constrained Markov decision process (CMDP) and employ a primal-dual proximal policy optimization (PPO) algorithm, enhanced by a multi-scale generative transformer (MGFormer) for improved speed and accuracy over traditional deep neural networks (DNNs). Simulation results demonstrate that the proposed framework reduces service costs by up to 30% and achieves robust performance even under high-density traffic and Doppler shifts, validating its superiority over traditional Primal-dual PPO.},
  keywords={Autonomous aerial vehicles;Transformers;Vehicle dynamics;Optimization;Transportation;Costs;Accuracy;Energy efficiency;Doppler shift;Connected vehicles;Intelligent transportation network;autonomous aerial vehicle (AAV);internet of connected vehicles (IoCVs);content caching;multi-scale generative transformer;primal-dual proximal policy gradient (Primal-dual PPO)},
  doi={10.1109/TITS.2025.3569621},
  ISSN={1558-0016},
  month={},}@ARTICLE{9068414,
  author={Zhang, Chao and Yang, Zichao and He, Xiaodong and Deng, Li},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Multimodal Intelligence: Representation Learning, Information Fusion, and Applications}, 
  year={2020},
  volume={14},
  number={3},
  pages={478-493},
  abstract={Deep learning methods haverevolutionized speech recognition, image recognition, and natural language processing since 2010. Each of these tasks involves a single modality in their input signals. However, many applications in the artificial intelligence field involve multiple modalities. Therefore, it is of broad interest to study the more difficult and complex problem of modeling and learning across multiple modalities. In this paper, we provide a technical review of available models and learning methods for multimodal intelligence. The main focus of this review is the combination of vision and natural language modalities, which has become an important topic in both the computer vision and natural language processing research communities. This review provides a comprehensive analysis of recent works on multimodal deep learning from three perspectives: learning multimodal representations, fusing multimodal signals at various levels, and multimodal applications. Regarding multimodal representation learning, we review the key concepts of embedding, which unify multimodal signals into a single vector space and thereby enable cross-modality signal processing. We also review the properties of many types of embeddings that are constructed and learned for general downstream tasks. Regarding multimodal fusion, this review focuses on special architectures for the integration of representations of unimodal signals for a particular task. Regarding applications, selected areas of a broad interest in the current literature are covered, including image-to-text caption generation, text-to-image generation, and visual question answering. We believe that this review will facilitate future studies in the emerging field of multimodal intelligence for related communities.},
  keywords={Task analysis;Visualization;Machine learning;Training;Semantics;Natural language processing;Multimodality;representation;multimodal fusion;deep learning;embedding;speech;vision;natural language;caption generation;text-to-image generation;visual question answering;visual reasoning},
  doi={10.1109/JSTSP.2020.2987728},
  ISSN={1941-0484},
  month={March},}@INBOOK{10954935,
  author={Soon, Debbie},
  booktitle={Digital Mavericks: A Guide to Web3, NFTS, and Becoming the Main Character of the Next Internet Revolution}, 
  title={The Robots Are Coming}, 
  year={2025},
  volume={},
  number={},
  pages={151-164},
  abstract={Summary <p>Today, it is still mind&#x2010;boggling what generative artificial intelligence (AI) can accomplish. With some thoughtfully put together text prompts, people are creating personalized nutrition plans, websites from napkin sketches, mental health chat bots, and more. After all, Digital Mavericks is meant first and foremost to serve as a guide for anyone interested in navigating and entering the world of Web3. In other words, blockchain can be the trust layer for AI, providing an immutable time&#x2010;stamped record of who created what using AI, and when. Another challenge that AI faces is that of bias. Conversely, AI has already made its way into the non&#x2010;fungible tokens space. One of the most exciting things about being a digital maverick is that we are early enough to truly shape the future of the increasingly digital world we will be living in.</p>},
  keywords={Artificial intelligence;Blockchains;Generators;Chatbots;Robots;Business;Biological system modeling;Art;Web sites;Navigation},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394220908},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10954935},}@INPROCEEDINGS{10141005,
  author={Thiruthuvaraj, Rajasekhar and Jo, Ashly Ann and Raj, Ebin Deni},
  booktitle={2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Explainability to Business: Demystify Transformer Models with Attention-based Explanations}, 
  year={2023},
  volume={},
  number={},
  pages={680-686},
  abstract={Recently, many companies are relying on Natural Language Processing (NLP) techniques to understand the text data generated daily. It has become very critical to deal with this data because finding the sentiments of text and summarizing them will help the company understand the pain points of the customers posting reviews on social media or understand the experience of the customer. These requirements have increasingly demanded many advanced algorithms to deal the text data. The introduction of Transformers led to businesses adopting NLP methods more and more to keep up with their needs. Models like Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT), state-of-the-art results were achieved with billions of parameters learned. Although these advancements improved the accuracy and expanded the use of algorithms to a wide range of NLP tasks like language translation, text summarization, and language modeling. Businesses are more interested in the Explainability of the model compared to its accuracy. Explainable Artificial Intelligence (XAI) plays an important role to comprehend the complexities of the model as well as the influence of weights on predictions. In this paper, the complexities of the transformer model are unraveled by presenting a straightforward method for computing explainable predictions. The DistilBERT model is chosen as an example to implement the explainable system due to its lighter nature. Combining the strengths of a Posthoc expla-nation with those of a self-learning neural network, the method makes it simple to scale it to other algorithms to implement. With technologies like python, PyTorch, and Hugging Face, a detailed step-by-step algorithmic computation is demonstrated to explain the predictions from the attention-based explanations.},
  keywords={Computational modeling;Companies;Predictive models;Transformers;Prediction algorithms;Natural language processing;Complexity theory;Explainability;Explainable Artificial Intelli-gence (XAI);Transformers;DistilBERT;Attention-Based;Accuracy},
  doi={10.1109/ICAAIC56838.2023.10141005},
  ISSN={},
  month={May},}@INPROCEEDINGS{10425071,
  author={Kumar, R V B S Prasanth and Srinivas, Kethavath and J, Amudha},
  booktitle={2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)}, 
  title={Autonomous Vehicle Along with Surrounding Agent's Path Prediction Using CNN-Based Models}, 
  year={2023},
  volume={},
  number={},
  pages={39-45},
  abstract={In the modern era, Autonomous Vehicles (AVs) are pivotal within the transportation system, presenting notable engineering challenges in the development of self-driving systems, particularly in agent detection such as pedestrians, surrounding vehicles, traffic lights, bicycles, etc. The dataset utilized comprises level 5 data, encompassing three key tables: scenes, time frames, and traffic agents. Specifically, the traffic agent table contains Zarr compressed files providing information about aerial maps, velocities, and coordinates of agents. The primary objective of this paper revolves around constructing a Convolution Neural Network (CNN) model to discern the motion trajectories of agents in relation to the AV, a demanding and high-priority task in building an advanced self-driving system. To predict these motion trajectories, baseline CNN models like ResNet34 and ResNet50 were employed, compared to determine the optimal model. The results indicate a significant enhancement in Negative Multi Log Likelihood when compared with non-frozen ResNet models. Our future research aims to establish a pipeline for object classification around AV and implement Generative Adversarial Network (GAN) on noise data to project future trajectories.},
  keywords={Computational modeling;Transportation;Predictive models;Generative adversarial networks;Trajectory;Task analysis;Autonomous vehicles;ResNet50;ResNet34;Negative Multi Log Likelihood;GAN},
  doi={10.1109/ICCCIS60361.2023.10425071},
  ISSN={},
  month={Nov},}@ARTICLE{10945752,
  author={Pandey, Chandrasen and Tiwari, Vaibhav and Francis, Sharmila A. J. and Pal, Vipin and Sinha Roy, Diptendu},
  journal={IEEE Internet of Things Journal}, 
  title={MF-CGAN: Multifeature Conditional GAN for Synthetic Data Generation in Internet of Medical Things}, 
  year={2025},
  volume={12},
  number={10},
  pages={13469-13476},
  abstract={Synthetic data generation via generative artificial intelligence (GenAI) is essential for enhancing cybersecurity and safeguarding privacy in the Internet of Medical Things (IoMT) and healthcare. We introduce multifeature conditional generative adversarial network (GAN) (MF-CGAN), a Conditional GAN with multifeature integration, that generates realistic synthetic data using the WUSTL EHMS 2020 dataset of network traffic and health metrics. MF-CGAN’s architecture integrates conditional variables such as cyberattack types, traffic direction, and status flags, preserving their intricate interdependencies. The generator employs dense layers with batch normalization and Leaky ReLU activations to produce conditional synthetic data; the discriminator evaluates authenticity using the same conditional inputs. Experimental results show that MF-CGAN produces high-fidelity synthetic data, closely mirroring real data characteristics. In addition, we quantitatively demonstrate (MAE  ${=} 0.012$ , RMSE  ${=} 0.035$ , and classification accuracy up to 99%) that MF-CGAN-generated data effectively supports cybersecurity tasks, indicating its potential to significantly enhance data availability for IoMT.},
  keywords={Synthetic data;Medical services;Generative adversarial networks;Data privacy;Computer security;Training;Data models;Telecommunication traffic;Generators;Computer science;Generative adversarial network (GAN);healthcare cybersecurity;Internet of Medical Things (IoMT);multifeature conditional GAN (MF-CGAN);network traffic analysis;sustainable development goals (SDG);synthetic data generation},
  doi={10.1109/JIOT.2025.3554207},
  ISSN={2327-4662},
  month={May},}@ARTICLE{11048711,
  author={Wang, Haiteng and Ren, Lei and Li, Yikang and Wang, Yuqing},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={MetaIndux-TS: Frequency-Aware AIGC Foundation Model for Industrial Time Series}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Implementing advanced AI techniques in industrial manufacturing requires large volumes of annotated sensor data. Unfortunately, collecting such data is often impractical due to extreme environments and the manual burden of expert annotation. Recent advancements in artificial intelligence generated content (AIGC) have inspired the exploration of industrial time-series generation to mitigate data shortages. However, existing AIGC models encounter difficulties in generating industrial time series due to their complex temporal dynamics, multichannel intercolumn correlations, and diverse frequency characteristics. To address these challenges, we propose MetaIndux-TS, a frequency-informed AIGC foundation model based on diffusion model frameworks. This model is designed to generate industrial time-series data under a variety of working conditions, across different types of equipment, and with variable lengths. Specifically, MetaIndux-TS integrates dual-frequency cross-attention networks, transforming time series into the frequency domain to model multivariate dependencies and capture intricate temporal details. In addition, the contrastive synthesis layer is constructed to generate high-fidelity time series by comparing periodic and long-term trends with initial noisy sequences. Comprehensive experiments show that MetaIndux-TS outperforms state-of-the-art models (SSSD, Dit, and TabDDPM), achieving a 57.5% improvement in fidelity and 20.4% in predictive score. MetaIndux-TS exhibits zero-shot generation capabilities for samples under unseen conditions, offering the potential to address data collection challenges in extreme environments. Codes are available at: https://github.com/Dolphin-wang/MetaIndux},
  keywords={Time series analysis;Time-frequency analysis;Diffusion models;Foundation models;Data models;Training;Computational modeling;Frequency diversity;Correlation;Predictive models;Artificial intelligence generated content (AIGC);diffusion model;foundation model;generative model;industrial time series},
  doi={10.1109/TNNLS.2025.3577203},
  ISSN={2162-2388},
  month={},}@INPROCEEDINGS{10212565,
  author={Ahamed, Shamim and Al Amin, Abdullah and Ahsan, Sk. Md. Masudul},
  booktitle={2023 International Conference on Next-Generation Computing, IoT and Machine Learning (NCIM)}, 
  title={Synthesizing Realistic Images from Textual Descriptions: A Transformer-Based GAN Approach}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The ability to automatically generate realistic images from textual input is a challenging and important goal in artificial intelligence. In this research, a novel approach is represented that combines RoBERTa a transformer-based language model, with Generative Adversarial Networks (GANs) to synthesize high-quality images from textual description. The proposed architecture uses RoBERTa model to encode the text input and Generative Adversarial Network that relates it to each pixel to produce an image that closely represents the input. The quality of the synthesized images is measured using Fréchet Inception Distance (FID) and Inception Score (IS) metrics. Three variants of the architecture have been proposed and demonstrated that this approach can produce realistic images. The results indicate that transformer-based language models can effectively be used with GANs for image synthesis, thus paving the way for further research in this area.},
  keywords={Measurement;Image synthesis;Computer architecture;Machine learning;Generative adversarial networks;Transformers;Solids;Text- to-image synthesis;Generative Adversarial Networks;Computer Vision;Natural Language Processing;Transformer-based Language Models;Image Quality Metrics},
  doi={10.1109/NCIM59001.2023.10212565},
  ISSN={},
  month={June},}@ARTICLE{10838708,
  author={Wang, Yifan and Sun, Shu and Liu, Na and Xu, Lianming and Wang, Li},
  journal={IEEE Wireless Communications Letters}, 
  title={Two-Stage Radio Map Construction With Real Environments and Sparse Measurements}, 
  year={2025},
  volume={14},
  number={4},
  pages={969-973},
  abstract={Radio map construction based on extensive measurements is accurate but expensive and time-consuming, while environment-aware radio map estimation reduces the costs at the expense of low accuracy. Considering accuracy and costs, a first-predict-then-correct (FPTC) method is proposed by leveraging generative adversarial networks (GANs). A primary radio map is first predicted by a radio map prediction GAN (RMP-GAN) taking environmental information as input. Then, the prediction result is corrected by a radio map correction GAN (RMC-GAN) with sparse measurements as guidelines. Specifically, the self-attention mechanism and residual-connection blocks are introduced to RMP-GAN and RMC-GAN to improve the accuracy, respectively. Experimental results validate that the proposed FPTC-GANs method achieves the best radio map construction performance, compared with the state-of-the-art methods.},
  keywords={Accuracy;Radio transmitters;Generative adversarial networks;Receivers;Costs;Wireless communication;Position measurement;Meteorology;Interpolation;Electronic mail;Channel modeling;radio map construction;generative artificial intelligence;GANs},
  doi={10.1109/LWC.2025.3528512},
  ISSN={2162-2345},
  month={April},}@INPROCEEDINGS{10724626,
  author={Roy, Sutirtha and Nath, Sayan and Chowdhury, Moshfiq-Us-Saleheen and Pratishtha},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={FTASD - A Fine Tuning Approach for Stable Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Image generation is one of the critical tasks performed in today’s world for the expansion of research domains like computer vision and Generative Artificial Intelligence (Generative AI). Therefore, there is an ultimate need for 2-D image synthesis models, which are already introduced by the researchers in the form of stable diffusion models. Recently, various companies like OpenAI and Google AI introduced such models in the computer vision industry. The fundamental approach is to generate target images through a diffusion process. The various applications of stable diffusion include text-to-image generation, image restoration, image-to-image generation, video generation, and facial restoration. In recent years, multiple development plans have been incorporated for improving the image generation models. The improvements have been done in the form of improving the loss function, architecture designs and optimization method. In this paper, we propose a fine-tuning method for improving the performance of various image generation models using Stable Diffusion (SD). Our major focus is on various stable diffusion models which involve text (prompt) to image generation methodology for generating various synthetic images. In our fine tuning process, we leveraged the KerasCV and trained the pre-trained Stable Diffusion Model on a diversified POKEMON (BLIP caption generated) dataset fetched from the Hugging Face database. Our model outperformed the existing KerasCV stable diffusion model which is responsible for text to image generation. We also fine-tuned the model using our self collected Keji National Forest dataset and it produced outstanding application specific results.},
  keywords={Computer vision;Image synthesis;Generative AI;Computational modeling;Text to image;Optimization methods;Diffusion models;Image restoration;Internet;Tuning;Generative-AI;SD;Fine-Tuning;Computer Vision},
  doi={10.1109/ICCCNT61001.2024.10724626},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{11086178,
  author={T, Kumaresan and G, Rajeshkumar and Kumar, T.Suresh and T S, Vishnu Priya and C, Kotteeswari and Kavitha, V},
  booktitle={2025 Second International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS)}, 
  title={Optimized Credit Card Fraud Detection Using Conditional GAN}, 
  year={2025},
  volume={},
  number={},
  pages={692-698},
  abstract={In this digitally evolving era, there is increasing existence of the credit card fraud and has exhibited the important tasks to financial institutions. This also prompts the need for effective and reliable fraud detection systems. The emergence of the artificial intelligence provided advanced techniques and new opportunities to hold these kind of tasks. Here, this research paper familiarizes an innovative approach by leveraging the Conditional Generative Adversarial Networks (CGAN), a type of Generative AI, in combination with the Graph Convolutional Networks (GCNs). One of the main and critical issue is the class imbalance, where the fraudulent transactions are very less than the legitimate ones, also leading to the partial models. The proposed approach solves this issue by generating synthetic data using the CGANs, it balances the dataset. Additionally, the feature selection is a stern phase for improving model accuracy and interpretability, attention mechanisms is employed to mainly emphasis on the best relevant features while discarding noise. To enhance the robustness further, GCNs is employed for the classification technique, taking use of their ability to capture the complex relationships between features in the data. Along with GCNs, Transfer Learning is applied to generalize across different types of datasets. Furthermore, the over fitting issue is addressed by regularization method and guaranteeing that the model generalizes well to hidden data. Finally, the performance is estimated using the key metrics like accuracy, recall, precision and F1-score with the outcomes that demonstrates its control over the existing methods in detecting the fraudulent credit card transactions.},
  keywords={Measurement;Attention mechanisms;Accuracy;Graph convolutional networks;Transfer learning;Credit cards;Generative adversarial networks;Feature extraction;Data models;Fraud;Credit Card Fraud;Conditional Generative Adversarial Networks;Attention Mechanisms;Graph Convolutional Networks;Regularization},
  doi={10.1109/ICC-ROBINS64345.2025.11086178},
  ISSN={},
  month={June},}@INPROCEEDINGS{11101869,
  author={Patias, Ioannis},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Foundational Models as General-Purpose Technology: A Guide to Corporate Transformation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The emergence of foundational models (FMs), including large language models (LLMs), and generative AI marks a transformative era in technology, comparable to the impact of historical general-purpose technologies (GPTs) like electricity and the steam engine. This paper explores the essential conditions and policy frameworks that companies must adopt to effectively leverage these powerful tools. FMs are distinguished by their broad applicability across diverse sectors, driving innovation and efficiency. LLMs, a subset of FMs, excel in natural language processing and content generation, revolutionizing communication and information dissemination. To successfully integrate these technologies, organizations must prioritize strategic investments in research and development, enhance their data infrastructure, and foster a culture of continuous learning. Additionally, robust regulatory frameworks are crucial to address ethical considerations and ensure equitable access to these transformative tools. By understanding the historical context of GPTs and implementing supportive policies, companies can position themselves at the forefront of technological advancement, driving economic growth and societal progress.},
  keywords={Economics;Technological innovation;Ethics;Frequency modulation;Generative AI;Biological system modeling;Large language models;Companies;Research and development;Investment;Artificial Intelligence (AI);Large Language Models (LLMs);Generative AI (GenAI);Foundational Models (FMs);General-Purpose Technology (GPT)},
  doi={10.1109/ISAS66241.2025.11101869},
  ISSN={},
  month={June},}@INPROCEEDINGS{11125750,
  author={Pradhan, Dipen and Arora, Lakshit and Shetgaonkar, Ankit and Girija, Sanjay Surendranath and Kapoor, Shashank and Raj, Aman},
  booktitle={2025 IEEE International Conference on Omni-layer Intelligent Systems (COINS)}, 
  title={Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The proliferation of IoT in cities, combined with Digital Twins, creates a rich data foundation for Smart Cities aimed at improving urban life and operations. Generative AI (GenAI) significantly enhances this potential, moving beyond traditional AI analytics and predictions by processing multimodal content and generating novel outputs like text and simulations. Using specialized or foundational models, GenAI's natural language abilities such as Natural Language Understanding (NLU) and Natural Language Generation (NLG) can power tailored applications and unified interfaces, dramatically lowering barriers for users interacting with complex smart city systems. In this paper, we focus on GenAI applications based on conversational interfaces within the context of three critical user archetypes in a Smart City - Citizens, Operators and Planners. We identify and review GenAI models and techniques that have been proposed or deployed for various urban subsystems in the contexts of these user archetypes. We also consider how GenAI can be built on the existing data foundation of official city records, IoT data streams and Urban Digital Twins. We believe this work represents the first comprehensive summarization of GenAI techniques for Smart Cities from the lens of the critical users in a Smart City.},
  keywords={Surveys;Smart cities;Generative AI;Reviews;Natural language generation;Urban planning;Digital twins;Stakeholders;Research and development;Synthetic data;Generative Artificial Intelligence;Internet-of-Things;Urban Digital Twin;Smart City;Large Language Models;Synthetic Data Generation},
  doi={10.1109/COINS65080.2025.11125750},
  ISSN={2996-5330},
  month={Aug},}@INPROCEEDINGS{10213134,
  author={Pan, Fei and Wang, DeJun and Hu, ZongHua and Yu, LongYang},
  booktitle={2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)}, 
  title={Generating Talking Facial Videos Driven by Speech Using 3D Model and Motion Model}, 
  year={2023},
  volume={},
  number={},
  pages={669-676},
  abstract={Facial expression is one of the most important features of a face. In previous works on generating talking facial videos driven by speech without additional driving information, existing models struggled to directly learn the mapping from speech to facial features, resulting in poor quality of generated facial expressions. In this paper, we propose a method for generating speech-driven facial videos using 3D models and motion capture. This method demonstrates good performance in terms of model robustness, adaptation to large head poses, and improvement of fine-grained facial expression details. We learn features from speech, reconstruct the face by fitting 3DMM coefficients using speech, and employ a motion-captured based generative adversarial network to ensure clear facial texture details in the generated faces. On the publicly available dataset VoxCeleb2, our method achieves scores of 31.22 in PSNR, 0.89 in SSIM, 19.4 in FID, and 1.96 in F_LMD, outperforming other methods. On the MEAD dataset, our method achieves scores of 30.65 in PSNR, 0.68 in SSIM, 20.5 in FID, 2.36 in SyncNet, and 2.45 in F_LMD, outperforming other methods. Experimental results demonstrate that our method effectively enhances the model robustness for speech-driven facial video generation without additional driving information.},
  keywords={Solid modeling;Adaptation models;Three-dimensional displays;Fitting;Generative adversarial networks;Robustness;Faces;component;Artificial Intelligence;Talking Facial Video Generation;Speech-Driven;3DMM;Generative Adversarial Network;Motion Model;Attention Mechanism},
  doi={10.1109/ISCTIS58954.2023.10213134},
  ISSN={},
  month={July},}@INPROCEEDINGS{11149861,
  author={Gong, Minliang and Qin, Yan},
  booktitle={2025 40th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={Image classification of industrial parts based on image generation}, 
  year={2025},
  volume={},
  number={},
  pages={273-279},
  abstract={The application of computer vision in industrial parts image classification has become more and more widespread. In order to solve the problem of accurately classifying industrial precision parts in industrial automation, a classification method based on image generation technology is proposed. First, different types of part image data are collected, the image data is expanded through personalized generation models, and then classifier filtering is used to construct a new data training set. This paper proposes a method for data enhancement and expansion of small sample data sets, which can effectively increase the number of training samples in the classifier model and improve the quality of model training. The research results show that compared with the method that does not use image generation data as a training set, the overall accuracy rate increases from 51.46% to 59.06%, and the part with the lowest accuracy rate increases from 38.75% to 44.375%. When classifying small samples of industrial precision parts images, the method of using the generative model proposed in this paper to expand image data can significantly improve the recognition accuracy of the classifier model.},
  keywords={Training;Accuracy;Automation;Image recognition;Image synthesis;Generative AI;Filtering;Data models;Manufacturing;Image classification;industrial parts;image classification;data augmentation;generative artificial intelligence},
  doi={10.1109/YAC66630.2025.11149861},
  ISSN={2837-8601},
  month={May},}@ARTICLE{9991053,
  author={Rautela, Mahindra and Senthilnath, J. and Huber, Armin and Gopalakrishnan, S.},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Toward Deep Generation of Guided Wave Representations for Composite Materials}, 
  year={2024},
  volume={5},
  number={3},
  pages={1102-1109},
  abstract={Laminated composite materials are widely used in most fields of engineering. Wave propagation analysis plays an essential role in understanding the short-duration transient response of composite structures. The forward physics-based models are utilized to map from elastic properties space to wave propagation behavior in a laminated composite material. Due to the high-frequency, multimodal, and dispersive nature of the guided waves, the physics-based simulations are computationally demanding. It makes property prediction, generation, and material design problems more challenging. In this work, a forward physics-based simulator, such as the stiffness matrix method is utilized to collect group velocities of guided waves for a set of composite materials. A variational autoencoder (VAE)-based deep generative model is proposed for the generation of new and realistic polar group velocity representations. It is observed that the deep generator is able to reconstruct unseen representations with very low mean square reconstruction error. Global Monte Carlo and directional equally spaced samplers are used to sample the continuous, complete, and organized low-dimensional latent space of VAE. The sampled point is fed into the trained decoder to generate new polar representations. The network has shown exceptional generation capabilities. It is also seen that the latent space forms a conceptual space where different directions and regions show inherent patterns related to the generated representations and their corresponding material properties.},
  keywords={Computational modeling;Composite materials;Propagation;Predictive models;Mathematical models;Material properties;Generators;Composite materials;deep generative model;variational autoencoder (VAE);wave propagation},
  doi={10.1109/TAI.2022.3229653},
  ISSN={2691-4581},
  month={March},}@INPROCEEDINGS{10574927,
  author={Vishal, Yekbote and Bhaskar, J Uday and Yaswanthreddy, Reddem and Vyshnavi, Cheenepalli and Shanti, S},
  booktitle={2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={A Novel Approach for Inappropriate Content Detection and Classification of Youtube Videos using Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={539-545},
  abstract={The rapid expansion of YouTube videos has attracted billions of viewers, the majority of whom are young. Nevertheless, fraudulent uploaders also take advantage of this site to disseminate disturbing videos to everyone, especially kids, like sharing indecent content through cartoons and movies but within all of this content comes unsuitable content that can be dangerous to viewers, particularly for young viewers. The appearance of such unsuitable content poses serious concerns, particularly for young viewers. Thus, it is highly recommended that social platforms incorporate real-time video content filtering. This study has proposed a new method which is an innovative deep learning architecture for identifying and categorizing objectionable video content is presented in this proposal. In particular, it extracts video descriptors by using Generative Adversarial Networks and Our method improves inappropriate video content detection and classification for child safety by better capturing the contextual information of video descriptors, as confirmed by comparisons with existing methodologies. Besides technological innovation, our principal objective is to contribute to the development of safer online environments. We hope to strengthen moderation of content by leveraging deep learning, resulting in a safer online landscape for people all around the world.},
  keywords={Deep learning;Technological innovation;Video on demand;Motion pictures;Real-time systems;Safety;Web sites;Generative Adversarial Networks(GAN);Content Detection and Classification},
  doi={10.1109/ICAAIC60222.2024.10574927},
  ISSN={},
  month={June},}@INPROCEEDINGS{10551051,
  author={Lu, Yan and Yang, Qiufen and Lu, Huadeng},
  booktitle={2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Research on Deep Learning-based Network Intrusion Detection Model}, 
  year={2023},
  volume={},
  number={},
  pages={600-606},
  abstract={This paper investigates the deep learning-based network intrusion detection model to address the problem of recognizing unknown attacks. By analyzing existing models, the potential of applying deep learning in network intrusion detection is explored, with a particular focus on the capabilities of discriminative variational autoencoders and generative adversarial networks in extracting and recognizing network traffic and attack features. We construct a deep learning-based network intrusion detection model with the aim of improving the recognition rate of new attacks and reducing false positives. By evaluating and comparing performance metrics, the advantages of the proposed model in recognizing unknown attacks are validated. Experimental results demonstrate that the model achieves higher recognition rates and accuracy for unknown attacks, outperforming traditional models in intrusion detection performance. This research provides important theoretical and practical guidance for the development and application of network intrusion detection.},
  keywords={Measurement;Deep learning;Computer hacking;Computational modeling;Network intrusion detection;Telecommunication traffic;Big Data;Deep Learning;Network Intrusion Detection;Discriminative Variational Autoencoders;Generative Adversarial Networks;Unknown Attacks},
  doi={10.1109/ICCBD-AI62252.2023.00110},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10490461,
  author={Chen, Zhaoyang and Liu, Tao and Wang, Zhenya and Fan, Xiaoyu and Wang, Yanan},
  booktitle={2023 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Application of VAE-WGAN-GP in Imbalanced Bearing Data Fault Diagnosis}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The imbalance of bearing fault samples can bring about the problems of the unstable learning process of the classification model and low classification accuracy. A Wasserstein generative adversarial network model (V AE-WGAN-GP) that fuses a variational auto-encoder and data with Gradient Penalty (GP) is proposed in this work. First, the structure of the generator is improved to extract the hidden variables by feature coding through the encoding-decoding structure to extract the latent information; Then, the training process adopts the Wasserstein distance to measure the difference between the generated samples and the real samples score, and introduces the GP term so as to improve the stability of the fault diagnosis model; Finally, the fake samples with real features are generated by a game between the generator and the discriminator. The experimental results show that the proposed method can generate high-quality bearing fault samples and improve the fault diagnosis accuracy under imbalance conditions.},
  keywords={Fault diagnosis;Training;Analytical models;Transfer learning;Feature extraction;Data models;Generators;rolling bearing;variational auto-encoder (V AE);Wasserstein generative adversarial network(WGAN);gradient penalized;fault diagnosis},
  doi={10.1109/ICSMD60522.2023.10490461},
  ISSN={},
  month={Nov},}@ARTICLE{9598877,
  author={Montenegro, Helena and Silva, Wilson and Cardoso, Jaime S.},
  journal={IEEE Access}, 
  title={Privacy-Preserving Generative Adversarial Network for Case-Based Explainability in Medical Image Analysis}, 
  year={2021},
  volume={9},
  number={},
  pages={148037-148047},
  abstract={Although Deep Learning models have achieved incredible results in medical image classification tasks, their lack of interpretability hinders their deployment in the clinical context. Case-based interpretability provides intuitive explanations, as it is a much more human-like approach than saliency-map-based interpretability. Nonetheless, since one is dealing with sensitive visual data, there is a high risk of exposing personal identity, threatening the individuals’ privacy. In this work, we propose a privacy-preserving generative adversarial network for the privatization of case-based explanations. We address the weaknesses of current privacy-preserving methods for visual data from three perspectives: realism, privacy, and explanatory value. We also introduce a counterfactual module in our Generative Adversarial Network that provides counterfactual case-based explanations in addition to standard factual explanations. Experiments were performed in a biometric and medical dataset, demonstrating the network’s potential to preserve the privacy of all subjects and keep its explanatory evidence while also maintaining a decent level of intelligibility.},
  keywords={Biomedical imaging;Data models;Privacy;Data privacy;Task analysis;Deep learning;Privatization;Case-based interpretability;deep learning;generative adversarial networks;privacy-preserving machine learning;medical image analysis},
  doi={10.1109/ACCESS.2021.3124844},
  ISSN={2169-3536},
  month={},}@ARTICLE{10772069,
  author={Rijlaarsdam, David and Hendrix, Tom and González, Pablo Tomás Toledano and Velasco-Mata, Alberto and Buckley, Léonie and Miquel, Juan Puig and Casaled, Oriol Aragon and Dunne, Aubrey},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={The Next Era for Earth Observation Spacecraft: An Overview of CogniSAT-6}, 
  year={2025},
  volume={18},
  number={},
  pages={2450-2463},
  abstract={Earth Observation (EO) spacecraft plays a pivotal role in various critical applications impacting life on Earth. Historically, these systems have adhered to conventional operational paradigms, namely the “mow-the-lawn” and “bent pipe” approaches. In these paradigms, operational schedules are formulated on the ground and subsequently uploaded to the spacecraft for execution. Execution involves either systematically acquiring vast amounts of data (mow-the-lawn) or targeting specific areas of interest as defined by end users or operators. We aim to depart from these traditional methodologies by integrating onboard artificial intelligence, near real-time communication, and new observing strategies in one system called CogniSAT-6. These innovations will amplify the amount, speed, and quality of the information yielded by such a system by up to an order of magnitude. This article provides an overview of the current state of the art in autonomous EO spacecraft and the application of onboard processing in EO spacecraft. An overview is given of the CogniSAT-6 mission, its concept of operations, system architecture, and data processing design. In addition, the first results of our in-orbit functional tests are presented. Since we believe that the technology presented here will have a significant impact on society, an ethical framework for such systems is presented. Finally, the benefits of the technology and implications for EO systems going forward are discussed.},
  keywords={Space vehicles;Real-time systems;Payloads;CubeSat;Artificial intelligence;Hyperspectral imaging;Earth;Systems architecture;Ethics;Pipelines;Artificial intelligence (AI);Earth Observation (EO);new observing strategies (NOS);onboard processing;real-time insights},
  doi={10.1109/JSTARS.2024.3509734},
  ISSN={2151-1535},
  month={},}@ARTICLE{10949740,
  author={Jiang, Yuhua and Gao, Feifei and Jin, Shi},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Electromagnetic Property Sensing and Channel Reconstruction Based on Diffusion Schrödinger Bridge in ISAC}, 
  year={2025},
  volume={24},
  number={8},
  pages={6737-6752},
  abstract={Integrated sensing and communications (ISAC) has emerged as a transformative paradigm for next-generation wireless systems. In this paper, we present a novel ISAC scheme that leverages the diffusion Schrödinger bridge (DSB) to realize the sensing of electromagnetic (EM) property of a target as well as the reconstruction of the wireless channel. The DSB framework connects EM property sensing and channel reconstruction by establishing a bidirectional process: the forward process transforms the distribution of EM property into the channel distribution, while the reverse process reconstructs the EM property from the channel. To handle the difference in dimensionality between the high-dimensional sensing channel and the lower-dimensional EM property, we generate latent representations using an autoencoder network. The autoencoder compresses the sensing channel into a latent space that retains essential features, which incorporates positional embeddings to process spatial context. The simulation results demonstrate the effectiveness of the proposed DSB framework, which achieves superior reconstruction of the target’s shape, relative permittivity, and conductivity. Moreover, the proposed method can also realize accurate channel reconstruction given the EM property of the target. The dual capability of accurately sensing the EM property and reconstructing the channel across various positions within the sensing area underscores the versatility and potential of the proposed approach for broad application in future ISAC systems.},
  keywords={Image reconstruction;Integrated sensing and communication;Wireless communication;Vectors;Permittivity;OFDM;Bridges;Accuracy;Wireless sensor networks;Conductivity;Electromagnetic (EM) property sensing;channel reconstruction;integrated sensing and communications (ISAC);diffusion Schrödinger bridge (DSB);generative artificial intelligence (GAI)},
  doi={10.1109/TWC.2025.3555685},
  ISSN={1558-2248},
  month={Aug},}@INPROCEEDINGS{10350188,
  author={Naritomi, Yusuke and Adachi, Takanori},
  booktitle={2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={Stock Market Simulation by Micro-Macro GAN}, 
  year={2023},
  volume={},
  number={},
  pages={600-607},
  abstract={In recent years, research on the fusion of computer simulations with real data, as seen in data assimilation, has been active. In addition, with the development of machine learning, especially deep learning, computer simulations are increasingly using deep learning models in addition to physical models. In the field of finance, research has also been conducted to use machine learning models when simulating markets, for example, to emulate synthetic orders placed by virtual traders. However, it is fundamentally difficult to generate realistic macro dynamics such as the price from micro order dynamics. In this study, we propose a new market simulation method by machine learning model to generate macro dynamics from micro order dynamics. The market simulator built by the machine learning model, which is called Micro-Macro GAN, is trained by coupling two mechanisms. The first mechanism generates micro order dynamics and trains a generator of micro order dynamics to distinguish the real from the synthetic order data using a Wasserstein Generative Adversarial Network (WGAN), which is called the Micro GAN. The second mechanism generates macro dynamics from the micro order dynamics generated by the Micro GAN and trains a generator of macro dynamics to distinguish the real from the synthetic macro data using a Sig-W GAN, which is called the Macro GAN. The macro dynamics data is converted to a signature, and finally, the Sig- W metric is computed as a loss function. As a demonstration, training was performed on Toyota Motor Corporation taken from the Flex Full data provided by the Japan Exchange Group (JPX). The order dynamics data generated from the Micro GAN and the Micro-Macro GAN were compared to the real order dynamics data. The results showed that the Micro-Macro GAN results were more similar to the real dynamics data than the Micro GAN results.},
  keywords={Deep learning;Training;Flexible printed circuits;Computational modeling;Computer simulation;Time series analysis;Generative adversarial networks;Generative model;Data-driven simulation;Agent simulation;Artificial stock market;GAN},
  doi={10.1109/WI-IAT59888.2023.00099},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9915885,
  author={Chauhan, Surendra Singh and Jain, Nitin and Pandey, Satish Chandra and Chabaque, Aakash},
  booktitle={2022 IEEE International Conference on Data Science and Information System (ICDSIS)}, 
  title={Deepfake Detection in Videos and Picture: Analysis of Deep Learning Models and Dataset}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Deepfake detection is the concept of distinguishing a computer manipulated graphic from a real recorded graphic. The technology used for this purpose is deep learning. It is a sub branch of artificial intelligence. With technology becoming more readily available, deepfakes are also increasing in use in recent years. It becomes evident that a system is needed that detects deepfakes and prevents its use in suspicious activities. Development of a deepfake detection technology becomes evident to avoid the use of deepfakes in such activities. For this purpose, many tech giants have assimilated huge datasets which consist of videos that were made using deepfakes already available. To detect a deepfake, one requires an equally capable or even better algorithm and detection technique. Generative Adversarial Nets, GANs, is one such technique that might be able to rival other deepfake techniques. This paper will discuss various methods to apply to detect deep fakes along with the process, libraries used, dataset liabilities and limitations, analysis and efficiency. Since Deep Learning technology is evolving each day with new innovations, this paper provides a comparative study about methods that have already been tested and their limitations with respective models and how to possibly make them more efficient.},
  keywords={Deep learning;Graphics;Deepfakes;Technological innovation;Analytical models;Data science;Libraries;Generative Adversarial Nets;Deepfakes;Deep Learning.},
  doi={10.1109/ICDSIS55133.2022.9915885},
  ISSN={},
  month={July},}@INPROCEEDINGS{10608315,
  author={Clark, Autumn and Igbokwe, Daniel and Ross, Samantha and Zibran, Minhaz F.},
  booktitle={2024 7th International Conference on Software and System Engineering (ICoSSE)}, 
  title={A Quantitative Analysis of Quality and Consistency in AI-generated Code}, 
  year={2024},
  volume={},
  number={},
  pages={37-41},
  abstract={With the recent emergence of generative AI (Artificial intelligence), Large Language Model (LLM) based tools such as ChatGPT have become popular assistants to humans in diverse tasks. ChatGPT has also been widely adopted for solving programming problems and for generating source code in software development. This research investigates both the code quality and the consistency of code quality over iterative prompts in 625 ChatGPT-generated Python code samples in the DevGPT dataset and the corresponding code snippets regenerated by manually prompting ChatGPT. Code samples are measured in terms of seven Halstead complexity metrics. We also assess how consistent they are across code snippets generated by different versions of ChatGPT. It was found that while ChatGPT generates good quality code across iterative prompts, it does generate semifrequent bugs, similar to how humans do, necessitating code review before integration. These traits also remain consistent across code snippets generated by subsequent releases of ChatGPT. These results suggest using AI-generated source code in software development will not hinder the process.},
  keywords={Measurement;Codes;Statistical analysis;Source coding;Computer bugs;Chatbots;Complexity theory;ChatGPT;Generative AI;Code;Quality;Complexity;Consistency;Python;Program;Analysis},
  doi={10.1109/ICoSSE62619.2024.00014},
  ISSN={},
  month={April},}@INPROCEEDINGS{10431492,
  author={Elloumi, Nabila and Mbarki, Zouhaier and Seddik, Hassene},
  booktitle={2023 IEEE Afro-Mediterranean Conference on Artificial Intelligence (AMCAI)}, 
  title={3D Medical Images Segmentation and Securing Based GAN Architecture and Watermarking Algorithm Using Schur Decomposition}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Recently, due to the exponentially evolution of medical imaging system, 3D medical image segmentation be a useful tool for cancer detection and localization. In fact, several convolution neural networks with different architectures are used for this purpose. Those techniques significantly improved the efficiency and the performance of automatic 3D medical image segmentation with high computation and memory requirements. Meanwhile, since the performance using a computer system, the privacy and the security of patient data are vulnerable to cyber-attacks. For this, DICOM data must be a primary concern in medical applications among hospitals and clinics. In this work, we proposed a hybrid technique for 3D image segmentation and protection. In fact, this approach we propose two steps. The first one is a 3D segmentation operation, which performed with the GAN neural network architecture, and the second one is a patient data protection technique using an appropriate watermarking algorithm. The simulation results on real application prove the efficiency of this method. In fact, the obtained findings are interesting that combine the method of deep learning the GAN in segmentation of medical images and securing them with the appropriate algorithm.},
  keywords={Image segmentation;Three-dimensional displays;Computed tomography;Neural networks;Watermarking;Generative adversarial networks;Biomedical imaging;3D images segmentation;GAN neural network;cyber security;watermarking algorithm},
  doi={10.1109/AMCAI59331.2023.10431492},
  ISSN={},
  month={Dec},}@ARTICLE{10065433,
  author={Triantafyllopoulos, Andreas and Schuller, Björn W. and İymen, Gökçe and Sezgin, Metin and He, Xiangheng and Yang, Zijiang and Tzirakis, Panagiotis and Liu, Shuo and Mertes, Silvan and André, Elisabeth and Fu, Ruibo and Tao, Jianhua},
  journal={Proceedings of the IEEE}, 
  title={An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era}, 
  year={2023},
  volume={111},
  number={10},
  pages={1355-1381},
  abstract={Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human–computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. However, the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions—aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesized utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology that underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In this overview, we outline ongoing trends and summarize state-of-the-art approaches in an attempt to provide a broad overview of this exciting field.},
  keywords={Speech synthesis;Deep learning;Computational modeling;Affective computing;Appraisal;Task analysis;Mood;Emotion recognition;Affective computing;deep learning;emotional voice conversion (EVC);speech synthesis},
  doi={10.1109/JPROC.2023.3250266},
  ISSN={1558-2256},
  month={Oct},}@INPROCEEDINGS{9578909,
  author={Zhu, Fei and Zhang, Xu-Yao and Wang, Chuang and Yin, Fei and Liu, Cheng-Lin},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Prototype Augmentation and Self-Supervision for Incremental Learning}, 
  year={2021},
  volume={},
  number={},
  pages={5867-5876},
  abstract={Despite the impressive performance in many individual tasks, deep neural networks suffer from catastrophic forgetting when learning new tasks incrementally. Recently, various incremental learning methods have been proposed, and some approaches achieved acceptable performance relying on stored data or complex generative models. However, storing data from previous tasks is limited by memory or privacy issues, and generative models are usually unstable and inefficient in training. In this paper, we propose a simple non-exemplar based method named PASS, to address the catastrophic forgetting problem in incremental learning. On the one hand, we propose to memorize one class-representative prototype for each old class and adopt prototype augmentation (protoAug) in the deep feature space to maintain the decision boundary of previous tasks. On the other hand, we employ self-supervised learning (SSL) to learn more generalizable and transferable features for other tasks, which demonstrates the effectiveness of SSL in incremental learning. Experimental results on benchmark datasets show that our approach significantly outperforms non-exemplar based methods, and achieves comparable performance compared to exemplar based approaches.},
  keywords={Training;Learning systems;Deep learning;Data privacy;Computer vision;Computational modeling;Prototypes},
  doi={10.1109/CVPR46437.2021.00581},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10879571,
  author={Wang, Bin and Wu, Shuangrui},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={HiFE-GAN: Semantic Synthesis of GPR B-Scan Images by High-Frequency Encoding GAN}, 
  year={2025},
  volume={63},
  number={},
  pages={1-13},
  abstract={As a common infrastructure, roads, subject to heavy traffic in modern cities, are progressively becoming dangerous since they are under the threat of subsurface disasters caused by geological structure changes, groundwater overutilization, etc. Benefiting from the superiority in nondestructivity as well as efficiency, ground-penetrating radar (GPR) has been widely applied in the detection of underground disasters; however, manual interpretation of GPR images is labor-intensive and the intelligentization of GPR is impeded by the lack of labeled images. Considering that the texture pattern of echo waves in real b-scan images is the key clue for engineers to classify underground disasters, we proposed a semantic synthesis network with a high-frequency structure, i.e., HiFE-GAN, to augment GPR b-scan images for training detection networks, by which one can customize the position, scale, and size of underground disasters in the simulated b-scan images. The provided experiments show that our network can generate b-scan images more similar to the real ones compared with several current generation networks; meanwhile, mixing the simulated b-scan images synthesized by HiFE-GAN with the real ones to train detection networks can dramatically improve the detection performance for underground disasters.},
  keywords={Disasters;Generative adversarial networks;Training;Generators;Convolutional neural networks;Roads;Feature extraction;Artificial intelligence;Urban areas;Translation;Generative adversarial network;ground-penetrating radar (GPR);image augmentation;image synthesis;underground disaster},
  doi={10.1109/TGRS.2025.3540974},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{11024741,
  author={Wang, Yi and Wang, Zhoubo and Yang, Jiahao},
  booktitle={2024 5th International Conference on Intelligent Design (ICID)}, 
  title={Research on AIGC-Assisted Industrial Equipment Modeling Design Under Complex Structural Constraints}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={With the continuous development of artificial intelligence technology-AIGC (AI Generated Content) technology has been widely used. However, when using AIGC technology to generate industrial equipment shapes under complex structural constraints, there are still problems of blurred recognition and low efficiency. This article proposes an intelligent styling design method based on the modular design concept, using AIGC technology to collaborate with designers to design complex industrial equipment shapes, in order to optimize the design process of such products and improve the output efficiency of designers'creative solutions. First, the equipment is deconstructed according to the product functional modules, the module morphological characteristics are extracted, and the relevant industrial equipment image production database is collected; then the massive image data is deeply studied to obtain the required Lora model and a preliminary plan is generated; finally, the designer relies on his own experience Integrate and optimize the intelligent generation plan to arrive at the final design plan. Experiments show that AIGC technology assists designers in generating industrial equipment styling design plans, which has significant advantages in design innovation and design efficiency.},
  keywords={Technological innovation;Costs;Shape;Generative AI;Databases;Human-machine systems;Production;Product design;Data models;Data mining;AIGC;industrial equipment;styling design;modular design;human-machine collaborative design},
  doi={10.1109/ICID64166.2024.11024741},
  ISSN={},
  month={Oct},}@ARTICLE{11007262,
  author={Zhu, Hongbo and Liu, Bowen and Wei, Xiaotong and Han, Guangjie and Zhang, Wenbo and Ma, Yue and Darwesh, Aso},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Synthesis Image Editing for Attribute Evolution in the Pseudo-Temporal Sequence of Pulmonary Nodule Growth}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Medical Mixed Reality (MR) has made significant progress in virtual surgery simulation and clinical oncology education. This paper proposes a framework for pulmonary nodule attribute editing based on image feature consistency, achieving spatial alignment of multi-stage case data. To address the limitations of traditional time-image reconstruction, we design an adversarial siamese model architecture capable of synthesizing missing nodule images, completing temporal data, and fine-grained modeling of nodule growth. To tackle challenges such as deformation, background inconsistency, and attribute uncertainty in generated samples, we introduce a Denoising Diffusion Implicit Model (DDIM) and construct an attribute vector space for pathological feature editing. Additionally, we propose a separable image reconstruction strategy to enhance local feature stability. Extensive validation on the lung-specific LIDC-IDRI dataset demonstrates superior performance with SSIM of 97.5% and LPIPS of 0.036. To further verify generalization capability, cross-organ testing on the liver-focused LiTS dataset achieves competitive results with SSIM of 85.0% and LPIPS of 0.128. These outcomes provide strong technical support for high-fidelity virtual surgery and VR-based clinical teaching in oncology.},
  keywords={Generative adversarial networks;Virtual reality;Medical diagnostic imaging;Image reconstruction;Medical services;Lungs;Training;Artificial intelligence;Computed tomography;Lesions;Attribute Editing;Pulmonary Nodule;Image Reconstruction;Denoising Diffusion Implicit Models;Generative Adversarial Networks},
  doi={10.1109/JBHI.2025.3571812},
  ISSN={2168-2208},
  month={},}@INPROCEEDINGS{698672,
  author={De Bonet, J.S. and Viola, P.},
  booktitle={Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)}, 
  title={Texture recognition using a non-parametric multi-scale statistical model}, 
  year={1998},
  volume={},
  number={},
  pages={641-647},
  abstract={We describe a technique for using the joint occurrence of local features at multiple resolutions to measure the similarity between texture images. Though superficially similar to a number of "Gabor" style techniques, which recognize textures through the extraction of multi-scale feature vectors, our approach is derived from an accurate generative model of texture, which is explicitly multiscale and non-parametric. The resulting recognition procedure is similarly non-parametric, and can model complex non-homogeneous textures. We report results on publicly available texture databases. In addition, experiments indicate that this approach may have sufficient discrimination power to perform target detection in synthetic aperture radar images (SAR).},
  keywords={Frequency;Pixel;Learning;Artificial intelligence;Joints;Image resolution;Feature extraction;Image databases;Spatial databases;Object detection},
  doi={10.1109/CVPR.1998.698672},
  ISSN={1063-6919},
  month={June},}@ARTICLE{8467323,
  author={Li, Na and Zheng, Ziqiang and Zhang, Shaoyong and Yu, Zhibin and Zheng, Haiyong and Zheng, Bing},
  journal={IEEE Access}, 
  title={The Synthesis of Unpaired Underwater Images Using a Multistyle Generative Adversarial Network}, 
  year={2018},
  volume={6},
  number={},
  pages={54241-54257},
  abstract={Underwater image datasets are crucial in underwater vision research. Because of the strong absorption and scattering effects that occur underwater, some ground truth such as the depth map, which can be easily collected in-air, becomes a great challenge in underwater environments. To solve the issues associated with the lack of underwater ground truth, we propose a trainable end-to-end system of an underwater multistyle generative adversarial network (UMGAN) that takes advantage of a cycle-consistent adversarial network (CycleGAN) and conditional generative adversarial networks. This system can generate multiple realistic underwater images from in-air images using a hybrid adversarial system and an unpaired method. Moreover, our model can translate in-air images to underwater images that retain the main content and structural information of the in-air images under specified turbidities or water styles through a style classifier and a conditional vector. Furthermore, we define the color loss and include the structural similarity index measure loss for the system to preserve the content and structure of original in-air images while transferring the backgrounds of the images from air to water. Using UMGAN, we can take advantage of the in-air ground truth and convert the corresponding in-air images into an underwater dataset with multiple water color styles. Our experiments demonstrate that our synthesized underwater images have a high score on image assessment against CycleGAN, WaterGAN, StarGAN, AdaIN, and other state-of-the-art methods. We also show that our synthesized underwater images with in-air depths can be applied to real underwater image depth map estimation.},
  keywords={Estimation;Image color analysis;Gallium nitride;Loss measurement;Task analysis;Generative adversarial networks;Neural networks;Underwater technology;artificial neural networks;image processing},
  doi={10.1109/ACCESS.2018.2870854},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{4632339,
  author={Michaelsen, Eckart and Jaeger, Klaus},
  booktitle={2008 11th International Conference on Information Fusion}, 
  title={Evidence fusion using the GESTALT-system}, 
  year={2008},
  volume={},
  number={},
  pages={1-7},
  abstract={A production system is presented that can help coding knowledge about recognizing structured objects in a declarative way. Together with a respective interpreter it forms the GESTALT-system. This system is particularly suited for the fusion of evidence from different sensors giving data from the same scene, for the inclusion of prior knowledge - such as GIS or digital maps - and for the treatment of large numbers of primitive and non-primitive objects. In the last two decades the underlying system structure has been used for diverse applications. There have been several re-implementation endeavors. Three quite different examples of applications are presented.},
  keywords={Hands;Production systems;Symbols;Target recognition;Image segmentation;Bridges;Syntactics;Sensor systems;Sensor phenomena and characterization;Radar imaging;Artificial intelligence;production system;evidence fusion;object recognition and localization},
  doi={},
  ISSN={},
  month={June},}@INPROCEEDINGS{10893115,
  author={Kalaigian, Maddy and Thompson, Michael S. and VanLone, Janet and Nickel, Robert},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={Using Generative AI to Implement UDL Principles in Traditional STEM Classrooms}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={This innovative practice full paper presents a guided approach to integrating universal design for learning (UDL) principles into an Explicit Instruction classroom. To address the common challenges of scope and additional time requirements of integrating UDL, we are using generative artificial intelligence (GAI) tools like ChatGPT because they have reached a level of functionality that allows a GAI tool to replace multiple specialized tools. This paper provides specific guidelines of where UDL interventions can be included in an explicit instruction lesson and how to use GAI to support them. This paper has multiple goals. First, we are focused on an approach that can help embed modern, best practices in traditional, higher ed STEM classes that use an approach that is typically close to the explicit instruction model. Second, we want to show how GAI can be used to implement UDL principles in classes in a way that does not require much additional instructor time and can help UDL adoption scale to larger classes. Example GAI prompts and “real” responses are provided for those who are unfamiliar with GAI tools, in order to help demonstrate the capabilities of the tools.},
  keywords={Generative AI;Chatbots;Best practices;Guidelines},
  doi={10.1109/FIE61694.2024.10893115},
  ISSN={2377-634X},
  month={Oct},}@ARTICLE{9272282,
  author={Haque, Kazi Nazmul and Rana, Rajib and Schuller, Björn W.},
  journal={IEEE Access}, 
  title={High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder}, 
  year={2020},
  volume={8},
  number={},
  pages={223509-223528},
  abstract={Generating high-fidelity conditional audio samples and learning representation from unlabelled audio data are two challenging problems in machine learning research. Recent advances in the Generative Adversarial Neural Networks (GAN) architectures show great promise in addressing these challenges. To learn powerful representation using GAN architecture, it requires superior sample generation quality, which requires an enormous amount of labelled data. In this paper, we address this issue by proposing Guided Adversarial Autoencoder (GAAE), which can generate superior conditional audio samples from unlabelled audio data using a small percentage of labelled data as guidance. Representation learned from unlabelled data without any supervision does not guarantee its' usability for any downstream task. On the other hand, during the representation learning, if the model is highly biased towards the downstream task, it losses its generalisation capability. This makes the learned representation hardly useful for any other tasks that are not related to that downstream task. The proposed GAAE model also address these issues. Using this superior conditional generation, GAAE can learn representation specific to the downstream task. Furthermore, GAAE learns another type of representation capturing the general attributes of the data, which is independent of the downstream task at hand. Experimental results involving the S09 and the NSynth dataset attest the superior performance of GAAE compared to the state-of-the-art alternatives.},
  keywords={Task analysis;Gallium nitride;Generative adversarial networks;Data models;Training;Neural networks;Speech recognition;Audio generation;representation learning;generative adversarial neural network;guided generative adversarial autoencoder},
  doi={10.1109/ACCESS.2020.3040797},
  ISSN={2169-3536},
  month={},}@ARTICLE{11150468,
  author={Shi, Kaijie and Lu, Wanglong and Zhao, Hanli and Prado da Fonseca, Vinicius and Zou, Ting and Jiang, Xianta},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Toward Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning}, 
  year={2025},
  volume={33},
  number={},
  pages={3544-3554},
  abstract={Limb loss affects millions globally, impairing physical function and reducing quality of life. Most traditional surface electromyographic (sEMG) and semi-autonomous methods require users to generate myoelectric signals for each control, imposing physically and mentally taxing demands. This study aims to develop a fully autonomous control system that enables a prosthetic hand to automatically grasp and release objects of various shapes using only a camera attached to the wrist. By placing the hand near an object, the system will automatically execute grasping actions with a proper grip force in response to the hand’s movements and the environment. To release the object being grasped, just naturally place the object close to the table and the system will automatically open the hand. Such a system would provide individuals with limb loss with a very easy-to-use prosthetic control interface and may help reduce mental effort while using. To achieve this goal, we developed a teleoperation system to collect human demonstration data for training the prosthetic hand control model using imitation learning, which mimics the prosthetic hand actions from human. By training the model on data from a limited set of objects collected from a single participant’s demonstration, we showed that the imitation learning algorithm can achieve high success rates and generalize effectively to new users and previously unseen objects with varying weights. The demonstrations are available at https://sites.google.com/view/autonomous-prosthetic-hand},
  keywords={Prosthetic hand;Hands;Grasping;Cameras;Imitation learning;Wrist;Training;Muscles;Computer science;Autoencoders;Prosthetic hand control;computer vision;imitation learning;generative models},
  doi={10.1109/TNSRE.2025.3605579},
  ISSN={1558-0210},
  month={},}@ARTICLE{11164902,
  author={Qiu, Liuxiang and Chen, Si and Xue, Jing-Hao and Wang, Da-Han and Zhu, Shunzhi and Yan, Yan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={HOH-Net: High-Order Hierarchical Middle-Feature Learning Network for Visible-Infrared Person Re-Identification}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Visible-infrared person re-identification (VI-ReID) is a cross-modality retrieval task that aims to match images of the same person across visible (VIS) and infrared (IR) modalities. Existing VI-ReID methods ignore high-order structure information of features and struggle to learn a reliable common feature space due to the modality discrepancy between VIS and IR images. To alleviate the above issues, we propose a novel high-order hierarchical middle-feature learning network (HOH-Net) for VI-ReID. We introduce a high-order structure learning (HSL) module to explore the high-order relationships of short- and long-range feature nodes, for significantly mitigating model collapse and effectively obtaining discriminative features. We further develop a fine-coarse graph attention alignment (FCGA) module, which efficiently aligns multi-modality feature nodes from node-level and region-level perspectives, ensuring reliable middle-feature representations. Moreover, we exploit a hierarchical middle-feature agent learning (HMAL) loss to hierarchically reduce the modality discrepancy at each stage of the network by using the agents of middle features. The proposed HMAL loss also exchanges detailed and semantic information between low- and high-stage networks. Finally, we introduce a modality-range identity-center contrastive (MRIC) loss to minimize the distances between VIS, IR, and middle features. Extensive experiments demonstrate that the proposed HOH-Net yields state-of-the-art performance on the image-based and video-based VI-ReID datasets. The code is available at: https://github.com/Jaulaucoeng/HOS-Net.},
  keywords={Feature extraction;Bidirectional control;Semantics;Pedestrians;Identification of persons;Generative adversarial networks;Training;Silicon;Reliability engineering;Videos;Visible-infrared person re-identification;high-order structure;middle-feature learning},
  doi={10.1109/TCSVT.2025.3609840},
  ISSN={1558-2205},
  month={},}@ARTICLE{4588975,
  author={Gianniotis, Nikolaos and Tino, Peter},
  journal={IEEE Transactions on Neural Networks}, 
  title={Visualization of Tree-Structured Data Through Generative Topographic Mapping}, 
  year={2008},
  volume={19},
  number={8},
  pages={1468-1493},
  abstract={ In this paper, we present a probabilistic generative approach for constructing topographic maps of tree-structured data. Our model defines a low-dimensional manifold of local noise models, namely, (hidden) Markov tree models, induced by a smooth mapping from low-dimensional latent space. We contrast our approach with that of topographic map formation using recursive neural-based techniques, namely, the self-organizing map for structured data (SOMSD) (Hagenbuchner , 2003). The probabilistic nature of our model brings a number of benefits: 1) naturally defined cost function that drives the model optimization; 2) principled model comparison and testing for overfitting; 3) a potential for transparent interpretation of the map by inspecting the underlying local noise models; 4) natural accommodation of alternative local noise models implicitly expressing different notions of structured data similarity. Furthermore, in contrast with the recursive neural-based approaches, the smooth nature of the mapping from the latent space to the local model space allows for calculation of magnification factors—a useful tool for the detection of data clusters. We demonstrate our approach on three data sets: a toy data set, an artificially generated data set, and on a data set of images represented as quadtrees. },
  keywords={Data visualization;Neurons;Hidden Markov models;Lattices;Gaussian processes;Cost function;Computer science;Training data;Testing;Neural networks;Hidden Markov tree model (HMTM);structured data;topographic mapping},
  doi={10.1109/TNN.2008.2001000},
  ISSN={1941-0093},
  month={Aug},}@ARTICLE{10474179,
  author={Bai, Shurui and Gonda, Donn Emmanuel and Hew, Khe Foon},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Write-Curate-Verify: A Case Study of Leveraging Generative AI for Scenario Writing in Scenario-Based Learning}, 
  year={2024},
  volume={17},
  number={},
  pages={1301-1312},
  abstract={This case study explored the use of generative artificial intelligence (GenAI), specifically chat generative pretraining transformer (ChatGPT), in writing scenarios for scenario-based learning (SBL). Our research addressed three key questions: 1) how do teachers leverage GenAI to write scenarios for SBL purposes? 2) what is the quality of GenAI-generated SBL scenarios and tasks? and 3) how does GenAI-supported SBL affect students’ motivation, learning performance, and learning perceptions? A three-step prompting engineering process (write the prompts, curate the output, and verify the output, WCV) was established during the teacher interaction with GenAI in the scenario writing. Findings revealed that by using the WCV approach, ChatGPT enabled the efficient creation of quality scenarios for SBL purposes in a short timeframe. Moreover, students exhibited increased intrinsic motivation, learning performance, and positive attitudes toward GenAI-supported scenarios. We also suggest guidelines for using the WCV prompt engineering process in scenario writing.},
  keywords={Writing;Chatbots;Task analysis;Visualization;Education;Encoding;Testing;Generative artificial intelligence (GenAI);intrinsic motivation;prompt engineering;scenario-based learning (SBL)},
  doi={10.1109/TLT.2024.3378306},
  ISSN={1939-1382},
  month={},}@ARTICLE{10577669,
  author={Zhang, Renlong and Chen, Junxin and Fang, Bo and Zhang, Li-Bo and Singh, Amit Kumar and Lyu, Zhihan},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Artificial Intelligence Generated Data Augmentation for Abdominal Multi-Organ Segmentation}, 
  year={2024},
  volume={70},
  number={3},
  pages={6031-6041},
  abstract={Artificial intelligence (AI) generation in medical image synthesis provides a more accurate and efficient method for medical image analysis. Medical image segmentation can assist doctors in identifying and locating lesions, organs, and tissues to improve the accuracy of disease diagnosis. However, medical images have issues such as scarcity, fuzzy boundaries and intra-class heterogeneity. In this paper, we propose a multi-organ segmentation algorithm for abdominal magnetic resonance imaging (MRI) images based on AI generation. The algorithm consists of a data augmentation module and a segmentation network. In the data augmentation part, traditional methods and an improved Pix2pix synthesize training images using the sample labels. The generated images and original samples are then put into the improved U-Net for training the segmentation model. Comparative experiments demonstrate the effectiveness and advantages of the proposed algorithm.},
  keywords={Image segmentation;Data augmentation;Training;Medical diagnostic imaging;Data models;Magnetic resonance imaging;Computational modeling;Multi-organ segmentation;AI generated Images;data augmentation;smart healthcare;affine transformations},
  doi={10.1109/TCE.2024.3421266},
  ISSN={1558-4127},
  month={Aug},}@INPROCEEDINGS{10511301,
  author={Kumar, Shashank and Tiwari, Sneha and Prasad, Rishabh and Rana, Abhay and Arti, M.K},
  booktitle={2024 11th International Conference on Signal Processing and Integrated Networks (SPIN)}, 
  title={Comparative Analysis of Human and AI generated Text}, 
  year={2024},
  volume={},
  number={},
  pages={168-173},
  abstract={AI (Artificial Intelligence) has emerged as a transformative tool that has revolutionized the things that we do daily. However, this rapid advancement in AI technology also raises ethical concerns that need to be addressed. This includes biases in the response of generative AI, interventions affecting the privacy of individuals, etc. Also, with easy access to emerging technologies like ChatGPT, Bard, DALL-E, etc., there has been a significant rise in the generation of fake content like fake images and deep-fake videos. Proper measures for the identification and validation of AI-generated content need to be established to minimize the circulation of false or fabricated information. Government regulations and public awareness are needed to ensure strict ethical practices in the content generated by AI. In this paper, a survey is conducted to collect human responses to a set of questions. The same questions are fed to AI tools to generate responses. These responses are then analyzed by various machine learning algorithms and studied on several parameters, like vocabulary richness, spelling errors, etc., to help detect whether the content is generated by AI or humans.},
  keywords={Surveys;Ethics;Vocabulary;Privacy;Machine learning algorithms;Government;Signal processing;ARTIFICIAL INTELLIGENCE;REAL VS FAKE;GENERATIVE AI},
  doi={10.1109/SPIN60856.2024.10511301},
  ISSN={2688-769X},
  month={March},}@INPROCEEDINGS{10435600,
  author={Chen, Dingxin and Wei, Xuancheng and Ni, Chenglan},
  booktitle={2023 3rd International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Machine Learning in Wireless Communication}, 
  year={2023},
  volume={},
  number={},
  pages={243-247},
  abstract={In recent years, the technology of wireless communication has developed rapidly. For instance, 5G and 6G are a next-generation communication technology. Because of the potential of 5G and 6 G, large quantities of data can be generated, which can be utilized to enhance the performance of neural networks. Recently, convolutional neural networks have achieved remarkable performance in various applications, including voice recognition, channel prediction, and so on. Recently, there have been a lot of studies in this field, including physical, radio, and wireless. Recently, great progress has been made in the area of deep learning in the physical layer. This research covers a broad spectrum of topics. A novel method for automatic modulation recognition is proposed in this paper. This paper gives a comparative analysis of traditional AMR and DL-based AMR, and also describes CNN arithmetic. In addition to CNN-based 2D AMRs, they are also included in Star Map, Time Rate Entry, Eye Graph, etc. In the following section, we will show you an AMR algorithm for RNNs. Finally, this paper sums up and forecasts its future.},
  keywords={Wireless communication;Deep learning;6G mobile communication;Visualization;5G mobile communication;Hardware;Convolutional neural networks;Automatic Modulation Recognition Recurrent Neural Network Convolutional Neural Network},
  doi={10.1109/EIECS59936.2023.10435600},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9891965,
  author={Robinson, Brian S. and Lau, Clare W. and New, Alexander and Nichols, Shane M. and Johnson, Erik C. and Wolmetz, Michael and Coon, William G.},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Continual learning benefits from multiple sleep stages: NREM, REM, and Synaptic Downscaling}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Learning new tasks and skills in succession without overwriting or interfering with prior learning (i.e., “catastrophic forgetting”) is a computational challenge for both artificial and biological neural networks, yet artificial systems struggle to achieve even rudimentary parity with the performance and functionality apparent in biology. One of the processes found in biology that can be adapted for use in artificial systems is sleep, in which the brain deploys numerous neural operations relevant to continual learning and ripe for artificial adaptation. Here, we investigate how modeling three distinct components of mammalian sleep together affects continual learning in artificial neural networks: (1) a veridical memory replay process observed during non-rapid eye movement (NREM) sleep; (2) a generative memory replay process linked to REM sleep; and (3) a synaptic downscaling process which has been proposed to tune signal-to-noise ratios and support neural upkeep. To create this tripartite artificial sleep, we modeled NREM veridical replay by training the network using intermediate representations of samples from the current task. We modeled REM by utilizing a generator network to create intermediate representations of samples from previous tasks for training. Synaptic downscaling, a novel con-tribution, is modeled utilizing a size-dependent downscaling of network weights. We find benefits from the inclusion of all three sleep components when evaluating performance on a continual learning CIFAR-100 image classification benchmark. Maximum accuracy improved during training and catastrophic forgetting was reduced during later tasks. While some catastrophic forget-ting persisted over the course of network training, higher levels of synaptic downscaling lead to better retention of early tasks and further facilitated the recovery of early task accuracy during subsequent training. One key takeaway is that there is a trade-off at hand when considering the level of synaptic downscaling to use - more aggressive downscaling better protects early tasks, but less downscaling enhances the ability to learn new tasks. Intermediate levels can strike a balance with the highest overall accuracies during training. Overall, our results both provide insight into how to adapt sleep components to enhance artificial continual learning systems and highlight areas for future neuroscientific sleep research to further such systems.},
  keywords={Training;Learning systems;Sleep;Learning (artificial intelligence);Biology;Generators;Rapid eye movement sleep;continual learning;memory replay;sleep;synaptic downscaling},
  doi={10.1109/IJCNN55064.2022.9891965},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9461370,
  author={Dilibal, Cinay and Davis, Brian L. and Chakraborty, Chinmay},
  booktitle={2021 3rd International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)}, 
  title={Generative Design Methodology for Internet of Medical Things (IoMT)-based Wearable Biomedical Devices}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={Application-driven generative design concept produces multiple solutions in varied fields in the industry. Generative design is an iterative design process that involves a machine learning algorithm that creates varied outputs that meet pre-determined constraints. This methodology provides an efficient tool for creating different design options for biomedical devices. Biomedical devices need different functionality and complexity. The generative design combined with additive manufacturing increases the functionality of the biomedical device with reduced material and total cost. Using artificial intelligence (AI) algorithm and the cloud, generative design enables to develop creative design options for the internet of medical things (IoMT)-based biomedical devices. In this study, the crucial role of generative design methodology is revealed for creating functional IoMT-based biomedical devices.},
  keywords={Industries;Human computer interaction;Machine learning algorithms;Design methodology;Tools;Three-dimensional printing;Iterative algorithms;Generative design;biomedical device;internet of medical things;telemedicine},
  doi={10.1109/HORA52670.2021.9461370},
  ISSN={},
  month={June},}@INPROCEEDINGS{10973536,
  author={Zhang, Xinsheng and Zhang, Yi},
  booktitle={2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={A Retrieval-Augmented Dialogue Framework for Multimodal Medical Consultation}, 
  year={2024},
  volume={},
  number={},
  pages={618-623},
  abstract={The medical consultation system based on artificial intelligence technology aims to discover users' illnesses and provide accurate treatment recommendations through interactive dialogue consultation, in order to timely and effectively solve users' problems. We propose a retrieval-augmented large language model framework for multimodal medical consultation. This framework combines generative large language model and information retrieval technique to enhance the responsiveness of language model by extracting relevant information from large-scale medical databases. Specifically, our framework can not only provide intelligent dialogue based on language models, but also integrate the latest medical knowledge to ensure the professionalism and reliability of consulting content by introducing a retrieval-augmented mechanism. The experimental results show that the framework is significantly superior to traditional methods in terms of accuracy and response time, demonstrating its enormous potential in practical medical applications.},
  keywords={Knowledge engineering;Biomedical equipment;Accuracy;Databases;Large language models;Medical services;Information retrieval;Time factors;Reliability;Intelligent agents;retrieval-augmented;dialogue;multimodal;medical consultation},
  doi={10.1109/WI-IAT62293.2024.00099},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10917571,
  author={Jyothirmai, D. and Surekha, Balla and Chandra, Bankula Sharath and Ramyasri, Bheemunipalli and Reddy, Chada Harshith and Pitchai, R.},
  booktitle={2024 4th International Conference on Soft Computing for Security Applications (ICSCSA)}, 
  title={A Framework for Automated Generation of Floor Plan Images through Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={228-234},
  abstract={This project aims to establish or develop a system for generating 2D floor plans based on user- provided requirements as text descriptions. It integrates with PyQt or PySide based user interface for taking the input, and then it pre-processes the textual data which will be suitable for deep learning model training. We utilize the LSTM networks to map descriptions to floor plans. Firstly we gathered a large dataset related to architectural designs .This dataset consists the data like images of buildings ,floor plans, and all the relevant data .Then the next step would be data preprocessing which is to preprocess the collected data that means it performs tasks such as image resizing ,normalization and data augmentation which means to create modified copies of a dataset using existing data .Then comes selecting appropriate model architecture for generating model architecture, now the training of deep learning model is done using the architectural data which is preprocessed . The deep learning model will be integrated with the architectural design software so that the architectural designs are developed based on the given user input. The generated floor plans are then displayed to the user through interface. The project follows an agile methodology, focusing on user centered design and continuous integration},
  keywords={Deep learning;Training;Solid modeling;Visualization;Computational modeling;Computer architecture;Continuous integration;Data models;Floors;Long short term memory;Data pre-processing;Dataset;Deep Learning;Agile methodology;Continuous Integration},
  doi={10.1109/ICSCSA64454.2024.00043},
  ISSN={},
  month={Sep.},}@ARTICLE{11174065,
  author={Lin, Leping and Gong, Liqin and Lu, Ritian and Ouyang, Ning},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Cross-scene Hyperspectral Image Classification Based on Frequency Domain Enhancement and Geometry Preservation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Cross-scene hyperspectral image classification (HSIC) is seriously affected by domain bias, which often leads to the decline of model generalization ability in new scenes. It has become a key challenge in this field. Existing methods mostly rely on cross-entropy loss and contrastive learning strategies. The former only focuses on the classification boundary and is difficult to ensure the domain invariance of features; the latter easily leads to the reduction of intra-class feature space and limits the generalization ability. To this end, this paper proposes a hyperspectral image domain generalization (DG) network that integrates frequency domain enhancement and geometry preservation. The Fourier-based frequency enhancement encoder (FBFE encoder) is designed to extract stable features by perturbing the amplitude and preserving the phase; the geometric divergence minimization constraint is introduced to enhance the local geometric consistency between the extended domain and the source domain; and the domain-adaptive contrastive loss (DAC loss) function is constructed to achieve the coordinated optimization of inter-domain alignment and intra-domain expansion. Experimental results on two real hyperspectral remote sensing datasets show that the proposed method outperforms existing advanced methods in classification accuracy and generalization performance.},
  keywords={Feature extraction;Frequency-domain analysis;Hyperspectral imaging;Training;Semantics;Laplace equations;Image classification;Generators;Vectors;Perturbation methods;cross-scene;contrastive learning;domain generalization (DG);Hyperspectral image classification (HSIC)},
  doi={10.1109/LGRS.2025.3611904},
  ISSN={1558-0571},
  month={},}@ARTICLE{8478237,
  author={Shin, Younghak and Qadir, Hemin Ali and Balasingham, Ilangko},
  journal={IEEE Access}, 
  title={Abnormal Colon Polyp Image Synthesis Using Conditional Adversarial Networks for Improved Detection Performance}, 
  year={2018},
  volume={6},
  number={},
  pages={56007-56017},
  abstract={One of the major obstacles in automatic polyp detection during colonoscopy is the lack of labeled polyp training images. In this paper, we propose a framework of conditional adversarial networks to increase the number of training samples by generating synthetic polyp images. Using a normal binary form of polyp mask which represents only the polyp position as an input conditioned image, realistic polyp image generation is a difficult task in a generative adversarial networks approach. We propose an edge filtering-based combined input conditioned image to train our proposed networks. This enables realistic polyp image generations while maintaining the original structures of the colonoscopy image frames. More importantly, our proposed framework generates synthetic polyp images from normal colonoscopy images which have the advantage of being relatively easy to obtain. The network architecture is based on the use of multiple dilated convolutions in each encoding part of our generator network to consider large receptive fields and avoid much contractions of a feature map size. An image resizing with convolution for upsampling in the decoding layers is considered to prevent artifacts on generated images. We show that the generated polyp images are not only qualitatively realistic, but also help to improve polyp detection performance.},
  keywords={Generators;Generative adversarial networks;Gallium nitride;Colonoscopy;Image generation;Training;Decoding;Colonoscopy;convolutional neural network;dilated convolution;generative adversarial networks;polyp detection},
  doi={10.1109/ACCESS.2018.2872717},
  ISSN={2169-3536},
  month={},}@ARTICLE{10057379,
  author={Chauhan, Karansingh and Patel, Shail Nimish and Kumhar, Malaram and Bhatia, Jitendra and Tanwar, Sudeep and Davidson, Innocent Ewean and Mazibuko, Thokozile F. and Sharma, Ravi},
  journal={IEEE Access}, 
  title={Deep Learning-Based Single-Image Super-Resolution: A Comprehensive Review}, 
  year={2023},
  volume={11},
  number={},
  pages={21811-21830},
  abstract={High-fidelity information, such as 4K quality videos and photographs, is increasing as high-speed internet access becomes more widespread and less expensive. Even though camera sensors’ performance is constantly improving, artificially enhanced photos and videos created by intelligent image processing algorithms have significantly improved image fidelity in recent years. Single image super-resolution is a class of algorithms that produces a high-resolution image from a given low-resolution image. Since the advent of deep learning a decade ago, this field has made significant strides. This paper presents a comprehensive review of the deep learning assisted single image super-resolution domain including generative adversarial network (GAN) models that discusses the prominent architectures, models used, and their merits and demerits. The reason behind covering the GAN models is that it is been known to perform better than the conventional deep learning methods given the resources and the time. For real-world applications with noise and other issues that can cause low-fidelity super resolution (SR) images, we examine another solution based on GAN model. This GAN model-based technique popularly known as blind super resolution is more resilient. We examined the various super-resolution techniques by varying image scaling factors (i.e., 2x, 3x, 4x) to measure PSNR and SSIM metrics for the different datasets. PSNR across the different datasets covered in the experimental Section shows an average of 14–17 % decrease in the score as we move up the image resolution scale from 2x to 4x. This is observed across all the datasets and for every model mentioned in the experimental Section of the paper. The results also show that blind super-resolution outperforms the conventional deep learning methods and the more complex GAN models. GAN models are complex and preferred when the upscale factor is high, while residual and dense models are recommended for smaller upscaling factors. This paper also discusses the applications of image super-resolution, and finally, the paper is concluded with challenges and future directions.},
  keywords={Superresolution;Deep learning;Generative adversarial networks;Interpolation;Mathematical models;Videos;Feature extraction;Image super-resolution;deep learning;convolutional neural network;generative adversarial network},
  doi={10.1109/ACCESS.2023.3251396},
  ISSN={2169-3536},
  month={},}@ARTICLE{8669749,
  author={Chae, Dong-Kyu and Shin, Jung Ah and Kim, Sang-Wook},
  journal={IEEE Access}, 
  title={Collaborative Adversarial Autoencoders: An Effective Collaborative Filtering Model Under the GAN Framework}, 
  year={2019},
  volume={7},
  number={},
  pages={37650-37663},
  abstract={Recently, deep learning has become a preferred choice for performing tasks in diverse application domains such as computer vision, natural language processing, sensor data analytics for healthcare, and collaborative filtering for personalized item recommendation. In addition, the Generative Adversarial Networks (GAN) has become one of the most popular frameworks for training machine learning models. Motivated by the huge success of GAN and deep learning on a wide range of fields, this paper explores an effective way to exploit both techniques into the collaborative filtering task for the accurate recommendation. We have noticed that the IRGAN and GraphGAN are pioneering methods that successfully apply GAN to recommender systems. However, we point out an issue regarding the employment of standard matrix factorization (MF) as their basic model, which is linear and unable to capture the non-linear, subtle latent factors underlying user-item interactions. Our proposed recommendation framework, named Collaborative Adversarial Autoencoders (CAAE), significantly extends the conventional IRGAN and GraphGAN as summarized below: 1) we use Autoencoder, which is one of the most successful deep neural networks, as our generator, instead of using the MF model; 2) we employ Bayesian personalized ranking (BPR) as our discriminative model; and 3) we incorporate another generator model into our framework that focuses on generating negative items, which are items that a given user may not be interested in. We empirically test our framework using three real-life datasets along with four evaluation metrics. Owing to those extensions, our proposed framework not only produces considerably higher recommendation accuracy than the conventional GAN-based recommenders (i.e., IRGAN and GraphGAN), but also outperforms the other state-of-the-art top-N recommenders (i.e., BPR, PureSVD, and FISM).},
  keywords={Gallium nitride;Generative adversarial networks;Deep learning;Generators;Task analysis;Collaboration;Training;Collaborative filtering;deep learning;generative adversarial networks;recommender systems},
  doi={10.1109/ACCESS.2019.2905876},
  ISSN={2169-3536},
  month={},}@ARTICLE{9133574,
  author={Fathi-Kazerooni, Sina and Rojas-Cessa, Roberto},
  journal={IEEE Access}, 
  title={GAN Tunnel: Network Traffic Steganography by Using GANs to Counter Internet Traffic Classifiers}, 
  year={2020},
  volume={8},
  number={},
  pages={125345-125359},
  abstract={In this paper, we introduce a novel traffic masking method, called Generative Adversarial Network (GAN) tunnel, to protect the identity of applications that generate network traffic from classification by adversarial Internet traffic classifiers (ITCs). Such ITCs have been used in the past for website fingerprinting and detection of network protocols. Their use is becoming more ubiquitous than before for inferring user information. ITCs based on machine learning can identify user applications by analyzing the statistical features of encrypted packets. Our proposed GAN tunnel generates traffic that mimics a decoy application and encapsulates actual user traffic in the GAN-generated traffic to prevent classification from adversarial ITCs. We show that the statistical distributions of the generated traffic features closely resemble those of the actual network traffic. Therefore, the actual user applications and information associated with the user remain anonymous. We test the GAN tunnel traffic against high-performing ITCs, such as Random Forest and eXtreme Gradient Boosting (XGBoost), and we show that the GAN tunnel protects the identity of the source applications effectively.},
  keywords={Gallium nitride;Generative adversarial networks;Generators;Neural networks;Servers;Telecommunication traffic;Internet;Generative adversarial networks;GAN;Internet packet classification;machine learning;online privacy;statistical traffic properties;WGAN},
  doi={10.1109/ACCESS.2020.3007577},
  ISSN={2169-3536},
  month={},}@ARTICLE{9893790,
  author={Tiago, Cristiana and Gilbert, Andrew and Beela, Ahmed Salem and Aase, Svein Arne and Snare, Sten Roar and Šprem, Jurica and McLeod, Kristin},
  journal={IEEE Access}, 
  title={A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images Using a GAN}, 
  year={2022},
  volume={10},
  number={},
  pages={98803-98815},
  abstract={Due to privacy issues and limited amount of publicly available labeled datasets in the domain of medical imaging, we propose an image generation pipeline to synthesize 3D echocardiographic images with corresponding ground truth labels, to alleviate the need for data collection and for laborious and error-prone human labeling of images for subsequent Deep Learning (DL) tasks. The proposed method utilizes detailed anatomical segmentations of the heart as ground truth label sources. This initial dataset is combined with a second dataset made up of real 3D echocardiographic images to train a Generative Adversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound images paired with ground truth labels. To generate the synthetic 3D dataset, the trained GAN uses high resolution anatomical models from Computed Tomography (CT) as input. A qualitative analysis of the synthesized images showed that the main structures of the heart are well delineated and closely follow the labels obtained from the anatomical models. To assess the usability of these synthetic images for DL tasks, segmentation algorithms were trained to delineate the left ventricle, left atrium, and myocardium. A quantitative analysis of the 3D segmentations given by the models trained with the synthetic images indicated the potential use of this GAN approach to generate 3D synthetic data, use the data to train DL models for different clinical tasks, and therefore tackle the problem of scarcity of 3D labeled echocardiography datasets.},
  keywords={Three-dimensional displays;Solid modeling;Echocardiography;Biomedical imaging;Image segmentation;Biomedical imaging;Generative adversarial networks;3-D image generation;data augmentation;deep learning;echocardiography;generative adversarial networks;segmentation},
  doi={10.1109/ACCESS.2022.3207177},
  ISSN={2169-3536},
  month={},}@ARTICLE{10288435,
  author={Irfan, Muhammad and Mushtaq, Zohaib and Khan, Nabeel Ahmed and Althobiani, Faisal and Mursal, Salim Nasar Faraj and Rahman, Saifur and Magzoub, Muawia Abdelkafi and Latif, Muhammad Armghan and Yousufzai, Imran Khan},
  journal={IEEE Access}, 
  title={Improving Bearing Fault Identification by Using Novel Hybrid Involution-Convolution Feature Extraction With Adversarial Noise Injection in Conditional GANs}, 
  year={2023},
  volume={11},
  number={},
  pages={118253-118267},
  abstract={Bearing faults are critical in machinery; their early detection is vital to prevent costly breakdowns and ensure operational safety. This study presents a pioneering take on addressing the challenges of imbalanced datasets in bearing fault diagnosis. By mitigating issues such as mode collapse and vanishing gradients, our novel method employs Conditional Generative Adversarial Networks (CGANs) with spectral normalization and adaptive adversarial noise injection to generate high-quality bearing fault samples. This enhances generalization and robustness against noisy data, significantly improving the stability of CGAN training. To extract meaningful features from grayscale bearing fault images, we introduce a novel combination of involution and convolution feature extraction method named I-C FFN. This innovative feature extraction method captures both local and global information, making it capable of handling various types of features, including channel-agnostic, spatial-specific, spatial-agnostic, and channel-specific characteristics. Our proposed oversampling methodology helped enhance the performance of proposed classification scheme as well as of benchmark transfer learning models. Having accuracy values between 99.40% to 99.61% for 0 and 1 HP imbalanced dataset respectively, our models outperformed standard transfer learning methodologies. Furthermore, by the inclusion of our proposed Adaptive Adversarial Class- Conditional GAN (AAC-cGAN), the samples quality and the robustness to noise was significantly increased as demonstrated by the quantitative assessment through various Evaluation Metrics employed in this paper. Lastly, the performance of each combination of both the up-sampled and under-sampled methodologies were assessed through multiple metrics to determine the effectiveness of our proposed approach in addressing class imbalance in bearing fault diagnosis.},
  keywords={Generative adversarial networks;Feature extraction;Training;Noise measurement;Fault diagnosis;Generators;Adaptive systems;Neural networks;Gradient methods;Conditional generative adversarial networks;involutional neural networks;convolutional neural networks;mode collapse;vanishing gradients;channel-agnostic;spatial-specific;feature extraction;deep learning},
  doi={10.1109/ACCESS.2023.3326367},
  ISSN={2169-3536},
  month={},}@ARTICLE{10034761,
  author={Malakshan, Sahar Rahimi and Saadabadi, Mohammad Saeed Ebrahimi and Mostofa, Moktari and Soleymani, Sobhan and Nasrabadi, Nasser M.},
  journal={IEEE Access}, 
  title={Joint Super-Resolution and Head Pose Estimation for Extreme Low-Resolution Faces}, 
  year={2023},
  volume={11},
  number={},
  pages={11238-11253},
  abstract={State-of-the-art deep learning-based Head Pose Estimation (HPE) techniques have reached spectacular performance on High-Resolution (HR) face images. However, they still fail to achieve expected performance on low-resolution images at large scales. This work presents an end-to-end HPE framework assisted by a Face Super-Resolution (FSR) algorithm. The proposed FSR model is specifically guided to enhance the HPE performance rather than considering FSR as an independent task. To this end, we utilized a Multi-Stage Generative Adversarial Network (MSGAN) which benefit from a pose-aware adversarial loss and head pose estimation feedback to generate super-resolved images that are properly aligned for HPE. Also, we propose a degradation strategy rather than simple down-sampling approach to mimic the diverse properties of real-world Low-Resolution (LR) images. We evaluate the performance of our proposed method on both synthetic and real-world LR datasets and show the superiority of our approach in both visual and HPE metrics on the AFLW2000, BIWI, and WiderFace Datasets.},
  keywords={Face recognition;Task analysis;Generators;Pose estimation;Generative adversarial networks;Adaptation models;Head pose estimation (HPE);face super-resolution (FSR);multi-stage generative adversarial networks (MSGAN);low-resolution (LR) face images},
  doi={10.1109/ACCESS.2023.3241606},
  ISSN={2169-3536},
  month={},}@ARTICLE{8492399,
  author={Ge, Hongwei and Yao, Yao and Chen, Zheng and Sun, Liang},
  journal={IEEE Access}, 
  title={Unsupervised Transformation Network Based on GANs for Target-Domain Oriented Image Translation}, 
  year={2018},
  volume={6},
  number={},
  pages={61342-61350},
  abstract={Image-to-image translation usually refers to the task of translating an input image from the source domain to the target domain while preserving the structure in the source domain. Recently, generative adversarial networks (GANs) using paired images for this task have made great progress. However, paired training data will not be available for many tasks. In this paper, a GAN-based unsupervised transformation network (UTN-GAN) is proposed for image-to-image translation. Importantly, UTN-GAN employs hierarchical representations and weight-sharing mechanism to translate images from the source domain to the target domain without paired images. We employ two groups of unsupervised GANs to generate the images in different domains first, and then discriminate them. In UTN-GAN, an auto-encoder reconstruction network is designed to extract the hierarchical representations of the images in the source domain by minimizing the reconstruction loss. In particular, the high-level representations (semantics) are shared with a translation network to guarantee that the input image and the output image are paired up in the different domains. All network structures are trained together by using a joint loss function. The experimental studies in qualitative and quantitative aspects on several image translation tasks show that the proposed algorithm is effective and competitive compared with some state-of-the-art algorithms.},
  keywords={Gallium nitride;Task analysis;Generators;Image reconstruction;Generative adversarial networks;Semantics;Training;Generative adversarial networks;unsupervised learning;style transfer;image-to-image translation},
  doi={10.1109/ACCESS.2018.2876096},
  ISSN={2169-3536},
  month={},}@ARTICLE{10689398,
  author={Zhuang, Yanyu and Wei, Hua},
  journal={IEEE Access}, 
  title={Design of a Personal Credit Risk Prediction Model and Legal Prevention of Financial Risks}, 
  year={2024},
  volume={12},
  number={},
  pages={146244-146255},
  abstract={With the escalating demand for personal credit, banking financial institutions face the imperative to expand their user base and mitigate losses caused by non-performing loans (NPLs). This study addresses the necessity for accurate and efficient personal credit risk assessment systems, focusing on rectifying credit data imbalances and developing a high-performing credit risk prediction model. To tackle data imbalance, deep learning’s Generative Adversarial Network (GAN) is employed for oversampling, learning the distribution of default samples within high-risk financial groups. “New” default samples are then integrated into the original dataset to achieve a balanced representation of default and normal samples. In the era of big data, marked by a surge in personal credit data dimensions and sample numbers, an advanced ensemble learning model, the Light Gradient Boosting Machine (LightGBM), is combined with the GAN model for credit risk prediction. The study utilizes six different imbalance ratio datasets from the Knowledge Extraction based on Evolutionary Learning platform to experiment with the GAN model’s classification effectiveness. Additionally, consumer finance company credit data, consisting of 559 features, undergoes data preprocessing and feature extraction to validate the LightGBM-GAN model’s effectiveness for credit risk prediction. Experimental results reveal that: (1) GAN significantly improves risk information classification (achieving an 86.7% accuracy), outperforming traditional oversampling methods on multiple datasets; (2) The LightGBM-GAN model excels in Area Under the Receiver Operating Characteristic Curve and Kolmogorov-Smirnov statistic values (averaging 0.86 and 0.87, respectively), establishing itself as an effective credit risk prediction model; (3) Feature importance identified by the LightGBM model underscores the predictive potential of financial weakly correlated variables, such as consumer behavior data. This study strives to equip financial institutions with efficient and precise credit risk assessment tools, ensuring compliance with laws and regulations, averting legal risks, reducing NPL risks, and fortifying the stability of the financial system.},
  keywords={Predictive models;Generative adversarial networks;Data models;Accuracy;Training;Generators;Risk management;Gradient methods;Risk assessment;risk prediction;legal prevention;generative adversarial network;light gradient boosting machine},
  doi={10.1109/ACCESS.2024.3466192},
  ISSN={2169-3536},
  month={},}@ARTICLE{9804702,
  author={Sun, Qiushi and Guo, Jingtao and Liu, Yi},
  journal={IEEE Access}, 
  title={PattGAN: Pluralistic Facial Attribute Editing}, 
  year={2022},
  volume={10},
  number={},
  pages={68534-68544},
  abstract={Facial attribute editing has witnessed substantial progress due to the increasing power of generative adversarial networks. However, existing methods mainly focus on improving the quality of facial attribute editing but ignore the diversity of facial attribute editing, which can only generate a single editing result by binary attribute labels and cannot reveal the diversity of attribute styles. To overcome this limitation, we propose a novel pluralistic facial attribute editing approach termed PattGAN (pluralistic attribute GAN). Instead of directly using the binary attribute labels to guide facial attribute editing, PattGAN first learns the disentangled representation of facial attributes by the binary attribute labels and then uses the disentangled representation of attributes to guide facial attribute editing. To this end, an independent encoder called the “attribute encoder” is introduced to extract distributions of specific attributes from face images. Furthermore, a novel swapping strategy is designed to assist the attribute encoder in modeling the disentangled representation of facial attributes by enhancing the model’s ability to learn pluralistic attributes. Coupled with the classification loss, the attribute encoder can accurately separate attribute-related information from face images. Both qualitative and quantitative experiments on the CelebA datasets show that PattGAN can achieve diverse face editing results by different exemplars. In summary, rather than other state-of-the-art methods, PattGAN performs better in diversity facial attribute editing.},
  keywords={Facial features;Generators;Face recognition;Faces;Training;Generative adversarial networks;Image reconstruction;Facial attribute editing;generative adversarial networks;disentangled representation;attribute labels},
  doi={10.1109/ACCESS.2022.3185733},
  ISSN={2169-3536},
  month={},}@ARTICLE{10477991,
  author={Ku, Donghun and Pahk, Heui Jae},
  journal={IEEE Access}, 
  title={Enhancing Battery Exterior Defect Inspection Accuracy Through Defect-Background Separated GAN Development}, 
  year={2024},
  volume={12},
  number={},
  pages={44286-44305},
  abstract={This paper aims to develop a defect-background separated generative adversarial network (GAN) using deep learning and GAN to enhance the accuracy of battery exterior defect inspection. In actual battery production lines, the occurrence rates of defects vary by defect type, making it challenging to create a large, uniform defect dataset due to the time required for defect acquisition. This leads to a reduction in the accuracy of battery exterior defect inspection. To construct a large, uniform defect dataset, this paper proposes a defect-background separated GAN based on the principles of GANs. The defect-background separated GAN performs effective defect and background separation learning by referring to the segmentation labeling of defects. Through dataset augmentation using the defect-background separated GAN, the performance quality of newly generated synthetic defect images has improved, and accuracy in battery exterior defect inspection can be enhanced through extensive dataset training. Experimental results show the lowest Fréchet inception distance score among various other methods for the battery exterior defect dataset, and it generates clear synthetic defects perceptible to the human eye. By training defect segmentation on this large, uniform defect dataset, an accuracy of 96.1% and an intersection over union value of 0.71 were achieved. Ultimately, applying this defect inspection network to the actual production line demonstrated a 72% improvement in time efficiency. This demonstrates the stability and robustness of the large, uniform defect dataset generated through the defect-background separated GAN.},
  keywords={Inspection;Generative adversarial networks;Deep learning;Batteries;Training;Data models;Production;Defect detection;Synthetic data;Detection algorithms;Battery exterior defect inspection;deep learning;defect segmentation;generative adversarial network;synthetic defect},
  doi={10.1109/ACCESS.2024.3380618},
  ISSN={2169-3536},
  month={},}@ARTICLE{9810260,
  author={Zakia, Umme and Barua, Arnab and Jiang, Xianta and Menon, Carlo},
  journal={IEEE Access}, 
  title={Unsupervised, Semi-Supervised Interactive Force Estimations During pHRI via Generated Synthetic Force Myography Signals}, 
  year={2022},
  volume={10},
  number={},
  pages={69910-69921},
  abstract={Recognizing applied hand forces using force myography (FMG) biosignals requires adequate training data to facilitate physical human-robot interactions (pHRI). But in practice, data is often scarce, and labels are usually unavailable or time consuming to generate. Synthesizing FMG biosignals can be a viable solution. Therefore, in this paper, we propose for the first time a dual-phased algorithm based on semi-supervised adversarial learning utilizing fewer labeled real FMG data with generated unlabeled synthetic FMG data. We conducted a pilot study to test this algorithm in estimating applied forces during interactions with a Kuka robot in 1D-X, Y, Z directions. Initially, an unsupervised FMG-based deep convolutional generative adversarial network (FMG-DCGAN) model was employed to generate real-like synthetic FMG data. A variety of transformation functions were used to observe domain randomization for increasing data variability and for representing authentic physiological, environmental changes. Cosine similarity score and generated-to-input-data ratio were used as decision criteria minimizing the reality gap between real and synthetic data and helped avoid risks associated with wrong predictions. Finally, the FMG-DCGAN model was pretrained to generate pseudo-labels for unlabeled real and synthetic data, further retrained using all labeled and pseudo-labeled data and was termed as the self-trained FMG-DCGAN model. Lastly, this model was evaluated on unseen real test data and achieved accuracies of 85%>R2 > 77% in force estimation compared to the corresponding supervised baseline model (89%>R2 > 78%). Therefore, the proposed method can be more practical for use in FMG-based HRI, rehabilitation, and prosthetic control for daily, repetitive usage even with few labeled data.},
  keywords={Data models;Generative adversarial networks;Force;Dynamics;Training data;Training;Robot sensing systems;FMG signal;applied force estimation;unsupervised learning;generative adversarial networks;domain randomization;transformation functions;similarity score;semi-supervised learning;self-training;transfer learning},
  doi={10.1109/ACCESS.2022.3187115},
  ISSN={2169-3536},
  month={},}@ARTICLE{9684408,
  author={Zhao, Ming and Zhang, Yinglong and Xia, Xuewen and Xu, Xing},
  journal={IEEE Access}, 
  title={Motif-Aware Adversarial Graph Representation Learning}, 
  year={2022},
  volume={10},
  number={},
  pages={8617-8626},
  abstract={Graph representation learning has been extensively studied in recent years. It has been proven effective in network analysis and mining tasks such as node classification and link prediction. Learning method based on neural network has become mainstream solution because it can efficiently preserve nonlinear characteristics of graph. However, more efficient ways to encode complex graph structures are still being explored. In this paper, for undirected graphs, we present a novel Motif-Aware Generative Adversarial Network (MotifGAN) model to learn graph representation based on a re-weighted graph, which unifies both the Motif higher order structure and the original lower order structure. Firstly, we obtain the motif adjacency matrix by motif mining, which encodes the Motif higher order structure. Then we couple the motif adjacency matrix with the adjacency matrix of the original graph to get a reweighted matrix named motif connectivity matrix. The motif connectivity distribution implicit in the motif connectivity matrix is the target structure information we need to preserve. How to preserve a wealth of structural information is a challenge. Inspired by related applications of GAN, we formulate a GAN model to generate embedding representations that satisfy the target structure conditions. The GAN model usually consists of two components: generator and discriminator. Our generator tries to approximate the motif connectivity distribution, while the discriminator detects whether the connectivity of two vertexes is from ground truth or generated by the generator. In adversarial training, both parts can alternately and iteratively boost their performance. Experimental results on public datasets show that MotifGAN has made substantial progress in various applications, including link prediction, node classification and visualization.},
  keywords={Generative adversarial networks;Representation learning;Matrix decomposition;Task analysis;Generators;Neural networks;Mutual information;Graph representation learning;higher order structure;motif connectivity;generative adversarial networks},
  doi={10.1109/ACCESS.2022.3144233},
  ISSN={2169-3536},
  month={},}@ARTICLE{10704635,
  author={Richter, Anna and Ijaradar, Jyotirmaya and Wetzker, Ulf and Jain, Vineeta and Frotzscher, Andreas},
  journal={IEEE Access}, 
  title={A Survey on Multivariate Time Series Imputation Using Adversarial Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={148167-148189},
  abstract={Multivariate time series (MTS) are captured in a great variety of real-world applications. However, analysing and modeling the data for classification and forecasting purposes can become very challenging if values are missing in the data set. The need for imputation methods, to fill the gaps in MTS, is well known. Thus, a great variaty of algorithms for solving this task has been proposed in the literature. However, the research community is constantly working on the development of advanced algorithms, that fulfill the special requirements of multidimensional temporal data, since most of the existing imputation methods treat MTS as ordinary structured data and fail to model the temporal relationships within and between sequences of observations. The main emphasis of MTS imputation research is currently put on deep learning (DL) models, especially those making use of generative adversarial networks (GANs). In our survey, we present our categorization of imputation algorithms, especially of GAN-models. We included eighteen different GAN-models designed for the MTS imputation task, which we introduce in detail. We provide a comparison of the models including their performance regarding MTS imputation, based on our findings in the literature. The following points can be considered the most important findings from our survey: The research on GAN-based imputation models for MTS has gained momentum in the last years across different domains, therewhile showing the effectiveness of these methods. The latest trend in the research area is the incorporation of attention mechanisms into the algorithms. Nevertheless, there are open research challenges, e.g. the transferability of models across data sets from different domains.},
  keywords={Imputation;Time series analysis;Surveys;Data models;Generative adversarial networks;Generators;Deep learning;Predictive models;Hybrid power systems;Forecasting;Deep learning;generative adversarial networks;hybrid GANs;imputation;missing values;multivariate time series},
  doi={10.1109/ACCESS.2024.3473540},
  ISSN={2169-3536},
  month={},}@ARTICLE{10750167,
  author={Narang, Naina and Lingam, Greeshma},
  journal={IEEE Access}, 
  title={Design of a Novel Explainable Adversarial Autoencoder Model for the Electromagnetic Analysis of Functional Materials Based on Physics-Informed Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={166044-166057},
  abstract={Machine learning models have found their applications in solving problems that can traditionally handle ordinary or partial differential equations. The full-wave simulations solve such equations for functional materials in the microwave regime. These simulations heavily rely on the electrical properties of the materials in predicting microwave characteristics. Currently, most researchers are searching for novel materials and their electrical properties using trial and error methods, for example, microwave absorbing materials, which are used in various civil and military applications. To replace this trial and error method, there is a crucial need for an intelligent system to assist material scientists in fabricating and testing functional materials. This paper proposes a modified Physics-Informed Neural Network (PINN) learning model using an autoencoder with Generative Adversarial Networks (AGAN) and SHapley Additive exPlanations (SHAP) model to assist the researchers in modeling and characterizing any electromagnetic material depending on the user-defined application in a scientifically learned manner with minimum trial and error in selecting the electrical properties of the material. The proposed eXaplainable autoencoder PINN (XA-PINN) algorithm provides a deeper understanding of the decision-making process of microwave absorption frequency bandwidth classification. A proof of concept is demonstrated for dielectric materials by training the model over frequencies from 0.5 to 18 GHz with different permittivity and permeability values. A custom loss is introduced in the proposed XA-PINN model based on the solution of the Riccati equation and mean squared error (MSE).},
  keywords={Mathematical models;Microwave theory and techniques;Electromagnetics;Absorption;Computational modeling;Bandwidth;Analytical models;Training;Numerical models;Neural networks;Generative adversarial networks;Dielectric measurements;electromagnetic materials;generative adversarial networks;physics-informed learning;reflection loss;Riccati},
  doi={10.1109/ACCESS.2024.3495732},
  ISSN={2169-3536},
  month={},}@ARTICLE{10812718,
  author={Palanisamy, Balamurugan and Chakrabarti, Arjab and Singh, Anushka and Hassija, Vikas and Chalapathi, G. S. S. and Singh, Amit},
  journal={IEEE Access}, 
  title={From Information Overload to Lucidity: A Survey on Leveraging GPTs for Systematic Summarization of Medical and Biomedical Artifacts}, 
  year={2025},
  volume={13},
  number={},
  pages={7902-7922},
  abstract={In medical research, the rapid proliferation of condition-specific studies has led to an information overload, making it challenging for researchers and practitioners to stay abreast of the latest findings. This paper presents a comprehensive survey on leveraging Generative Pretrained Transformers (GPTs) to summarize medical and biomedical artifacts systematically. We delve into the current applications of GPTs in this domain, discussing their role in understanding and summarizing research papers, medical dialogues, and medical records. Through a comparative analysis of recent studies and methodologies, we highlight the effectiveness of GPTs in distilling complex medical information into concise, understandable summaries. Our survey underscores the potential of GPTs as a tool for navigating the information overload in medical research and bringing clarity to healthcare professionals. This transformation will enhance patient care and outcomes, such as improving the accessibility and comprehensibility of medical research, assisting in rapid information retrieval, and facilitating the summarization of complex medical studies for broader audiences.},
  keywords={Medical diagnostic imaging;Transformers;Radiology;Medical services;Encoding;Generative adversarial networks;Chatbots;COVID-19;Bidirectional control;Optimization;Biomedical;ChatGPT;Generative Pretrained Transformers;healthcare;medical;natural language processing;summarization},
  doi={10.1109/ACCESS.2024.3521596},
  ISSN={2169-3536},
  month={},}@ARTICLE{11077152,
  author={Zheng, Yicen and Xie, Yu and Yao, Jiamin},
  journal={IEEE Access}, 
  title={Integrated Feature-Temporal GAN for Imbalanced Transaction Fraud Detection}, 
  year={2025},
  volume={13},
  number={},
  pages={122116-122131},
  abstract={Transaction fraud detection (TFD) poses a significant challenge due to the severe class imbalance, where fraudulent transactions, though rare, cause substantial financial losses. Existing methods often fail to adequately capture both the critical discriminative features of fraud and the temporal dependencies inherent in transaction sequences, leading to suboptimal detection performance. In this work, we introduce the Integrated Feature-Temporal-aware Generative Adversarial Network (IFT-GAN), a novel framework that simultaneously refines feature space and models temporal dynamics during the generation of synthetic samples. IFT-GAN introduces two key innovations: 1) A Feature-Aware Sample Selection (FASS) module that employs Fisher score-based feature ranking and weighted k-means clustering to create a balanced subset enriched with discriminative fraud signatures; and 2) A Temporal Dependency GAN (TD-GAN) architecture employing a novel Time-aware Gated Recurrent Unit (TaGRU) in both the generator and discriminator, explicitly modeling the temporal intervals between transactions through adaptive temporal gating. Distinct from existing methods, IFT-GAN establishes a unified framework where feature-space refinement and temporal sequence generation mutually reinforce each other. The FASS module effectively reduces the amplification of non-discriminative features, while the TD-GAN ensures temporal consistency through adversarial training. Extensive experiments on both real-world and publicly available datasets demonstrate that IFT-GAN outperforms its competitive peers across key detection metrics, indicating the effectiveness of IFT-GAN in addressing the class imbalance challenge while preserving the integrity of authentic fraud patterns.},
  keywords={Fraud;Feature extraction;Generative adversarial networks;Adaptation models;Generators;Training;Heuristic algorithms;Gated recurrent units;Technological innovation;Synthetic data;Transaction fraud detection;imbalanced classification;generative adversarial network},
  doi={10.1109/ACCESS.2025.3587793},
  ISSN={2169-3536},
  month={},}@ARTICLE{10910108,
  author={Esmaeili, Mona and Hemmady, Sameer D. and Noakoasteen, Oameed and Schamiloglu, Edl and Christodoulou, Christos and Zarkesh-Ha, Payman},
  journal={IEEE Access}, 
  title={Advancing EMC Analysis With GAN-Driven Signal Classification and Waveform Modulation}, 
  year={2025},
  volume={13},
  number={},
  pages={44789-44799},
  abstract={This study advances Electromagnetic Compatibility (EMC) by investigating how electromagnetic interference (EMI) from Radio Frequency (RF) sources affects digital interconnects. Unlike traditional analyses centered on Continuous Wave (CW) signals, we adopt an RF-focused approach using S-parameter data and consistent RF power to emphasize steady-state responses. This method eliminates the need for time-domain conversions, allowing for more accurate analysis. Our research introduces a novel image-based classification system that accurately assesses signal safety based on steady-state responses. By leveraging a Generative Adversarial Network (GAN) trained on ‘safe’ and ‘unsafe’ signal images, our system can effectively recognize and distinguish between these two states. The GAN’s ability to generate realistic signal patterns enhances classification accuracy, especially when empirical data is limited. This approach has been validated through multiple transformations to ensure robustness and reliability. The findings offer significant improvements in EMC analysis and provide practical guidelines for designing robust digital interconnects. These advancements contribute to enhancing the reliability and security of electronic devices in environments with high RF interference, making them better suited for real-world commercial applications where signal integrity is critical.},
  keywords={Electromagnetic interference;Modulation;Electromagnetic compatibility;Safety;Couplings;Training;Generative adversarial networks;Feature extraction;Convolutional neural networks;Convolution;Electromagnetic compatibility (EMC);electromagnetic interference (EMI);generative adversarial networks (GANs);image-based signal classification;digital electronic interconnects},
  doi={10.1109/ACCESS.2025.3548033},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10292495,
  author={Gregory, Jonathan and Liao, Qi},
  booktitle={2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)}, 
  title={Adversarial Spam Generation Using Adaptive Gradient-Based Word Embedding Perturbations}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In recent years, artificial intelligence (AI) and machine learning (ML) have become extremely promising in almost every aspect of our lives, including in cybersecurity. For instance, intrusion detection systems (IDS) and spam filters use machine learning algorithms to constantly monitor networks for abnormal behavior. However, the security of AI/ML-based solutions remains largely unknown and a cause for concerns. This study examines the possibility of cheating AI/ML-based cybersecurity solutions such as spam filters. In particular, we developed an Adaptive Gradient-based Word Embedding Perturbations (AG-WEP) framework for automatically generating adversarial spam examples. AGWEP smartly chooses the optimal perturbations across all features of the word vectors to minimize the degree of modifications to real spam messages. The experimental study suggests the adversarial model is effective to generate meaningful adversarial examples to fool a CNN-based spam classifier.},
  keywords={Adaptation models;Machine learning algorithms;Perturbation methods;Intrusion detection;Machine learning;Filtering algorithms;Internet of Things},
  doi={10.1109/AIBThings58340.2023.10292495},
  ISSN={},
  month={Sep.},}
