@ARTICLE{10083220,
  author={Wu, Huahua and Chen, Jiagui and Wang, Tianlei and Lai, Xiaoping and Cao, Jiuwen},
  journal={IEEE Signal Processing Letters}, 
  title={Ship License Plate Super-Resolution in the Wild}, 
  year={2023},
  volume={30},
  number={},
  pages={394-398},
  abstract={Ship license plate (SLP) recognition plays an important role in ship supervision and harbour management. Practically, low-resolution (LR) SLP images are illegible and challenging to SLP recognition. Most existing super-resolution (SR) methods are not suitable for real-world LR SLP images with over smoothed reconstructions. To alleviate these deficiencies, in this letter, we propose a parallel enhanced SR generative adversarial network (PESRGAN) for SLP images. A novel degradation model is developed to construct a more feasible LR dataset. A parallel SR convolutional neural network (SRCNN) module based on ESRGAN is proposed for feature extraction. To characterize the difference between text foreground and background, a new gradient loss is developed in PESRGAN to sharpen the character boundary. Comparisons to many state-of-the-art (SOTA) SR methods are presented to show the effectiveness of the proposed algorithm.},
  keywords={Feature extraction;Generators;Degradation;Marine vehicles;Convolution;Training;Signal processing algorithms;Ship license plate super-resolution;ESRGAN;parallel SRCNN;gradient loss},
  doi={10.1109/LSP.2023.3262418},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{10055781,
  author={Liu, Yu and Hu, Changhui and Xu, Lintao and Li, Fengyao},
  booktitle={2022 China Automation Congress (CAC)}, 
  title={Swin Transformer based Unsupervised Network for Low-Light Image Enhancement}, 
  year={2022},
  volume={},
  number={},
  pages={1838-1843},
  abstract={Images captured under poor illumination suffer from poor visibility and weak information. The majority of low-light image enhancement (LLIE) methods are based on convolutional neural networks (CNNs), which leads to their inability to establish long-range context interaction. In this paper, we take the advantages of the Swin Transformer and ResNet to develop the Swin Transformer-based unsupervised Generative Adversarial Network (STUN) for the LLIE task. The STUN contains one generator and one discriminator. The generator includes modules for feature extraction, deep feature processing and image reconstruction. In particular, we use Swin Transformer blocks and ResNet in the deep feature processing module alternately to calculate the global and local attention. The self-feature preserving loss and the spatial consistency loss are employed to constrain the unsupervised learning of STUN. Experimental results on several low-light datasets indicate that STUN can achieve great performance in both visual quality and evaluation metrics.},
  keywords={Measurement;Visualization;Lighting;Network architecture;Transformers;Feature extraction;Generators;Swin Transformer;low-light image enhancement;unsupervised learning},
  doi={10.1109/CAC57257.2022.10055781},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10387122,
  author={Peng, Yang and Chaorong, Li and Xudong, Ling and Fengqing, Qin and Yong, Zheng and Lihua, Qiu},
  booktitle={2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)}, 
  title={A Diffusion Probabilistic Model Based on Multi-Residualattention For Medical Image Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Diffusion models are currently among the most attention- grabbing generative models, achieving revolutionary applications across multiple domains and demonstrating exceptional performance. In the application of conditional diffusion models to alter specific regions of the output image to match desired features, most methods employ V AE encoders for feature extraction from conditional images. However, V AE encoders face challenges of difficult training and large parameter sizes; simultaneously, the denoising UNet in diffusion models often leads to blurred edges in sampled images. To effectively address these issues, We have introduced ResAtMedDiff, an innovative diffusion model specifically designed for the field of medical image processing, offering a novel approach to enhance the accuracy and efficiency of image analysis. The ResAtMedDiff model consists of two core training networks: the MEUNet conditional encoder and the RsAtUNet encoder-decoder. MEUNet is responsible for efficiently encoding the conditional images and then fusing these encodings with noise image features across multiple scales. This fusion strategy significantly enhances the ability of conditional features to guide the generation of noise images. The MEUN et encoder blends the features of the original conditional image with the Gaussian noise image features of the corresponding layers of the RsAtUN et encoder, followed by the RsAtUNet decoder decoding these fused features. This process achieves the generation of image segmentation masks guided by the features of the original image as medical prior conditions, As a result, this significantly enhances the precision of lesion area segmentation in medical imaging. The experimental findings reveal that the ResAtMedDiff model surpasses existing state-of-the-art segmentation techniques in its performance, particularly in the segmentation of thyroid nodules in ultrasound images and brain tumors in MRI scans, demonstrating its superior efficacy and precision.},
  keywords={Training;Image segmentation;Ultrasonic imaging;Image coding;Computational modeling;Magnetic resonance imaging;Brain modeling;Diffusion models;Conditional encoding;thyroid nodule over ultrasound images;brain tumor over MRI images},
  doi={10.1109/ICCWAMTIP60502.2023.10387122},
  ISSN={2576-8964},
  month={Dec},}@INPROCEEDINGS{10498818,
  author={Neravetla, Anuteja Reddy and Samreen, Sana and Mohammed, Abdul Sajid and Jiwani, Nasmin and Logeshwaran, J.},
  booktitle={2024 International Conference on Integrated Circuits and Communication Systems (ICICACS)}, 
  title={An Improved Lung Cancer Prediction Algorithm using Generative Adversarial Network in Modern Healthcare}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In current years, the usage of gadgets to benefit understanding of (ML) algorithms in healthcare has received plenty of attention due to its capacity to decorate diagnosis and treatment consequences. Lung cancers are a leading reason of morbidity and mortality globally, and early detection is critical for successful remedy. In this look, a new set of rules for lung cancer prediction is proposed for using the Generative unfavorable network (GAN). GANs are a deep getting-to-know version with proven promising outcomes in image generation obligations. The proposed regulations integrate statistics from numerous resources, including demographic information, medical facts, and imaging records, to generate realistic lung cancer pics. Those images are then used to train the GAN version, which successfully predicts the probability of lung cancer in sufferers. The outcomes of this statement show the capability of GAN to improve lung cancer prediction accuracy, probably mainly to provide extra accurate diagnoses. This approach combines data from various medical imaging techniques and uses synthetic images to train the algorithm, resulting in better performance compared to traditional methods. This has the potential to aid in early detection and treatment planning for lung cancer, improving patient outcomes and reducing mortality rates. Further research and implementation of this algorithm could greatly benefit modern healthcare. This set of rules can enhance modern-day healthcare systems and improve affected persons' outcomes within their destinies.},
  keywords={Training;Sociology;Lung cancer;Medical services;Probability;Prediction algorithms;Generative adversarial networks;Modern Healthcare;Artificial Intelligence;Medical Diagnosis;Early Detection;Machine Learning},
  doi={10.1109/ICICACS60521.2024.10498818},
  ISSN={},
  month={Feb},}@ARTICLE{10988740,
  author={Hang, Ching Nam and Yu, Pei-Duo and Tan, Chee Wei},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish “trumors”, which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.},
  keywords={Fake news;Knowledge graphs;Semantics;Accuracy;Public healthcare;Social networking (online);Artificial intelligence;COVID-19;Generative AI;Cognition;Graph-based retrieval-augmented generation;large language models;semantic reasoning;knowledge graph;fact-checking;health informatics},
  doi={10.1109/TAI.2025.3567369},
  ISSN={2691-4581},
  month={},}@INPROCEEDINGS{10384863,
  author={Osmëni, Tea and Ali, Maaruf},
  booktitle={2023 International Conference on Computing, Networking, Telecommunications & Engineering Sciences Applications (CoNTESA)}, 
  title={Generative AI: Impactful Considerations to Responsible Data Practices in Business Execution}, 
  year={2023},
  volume={},
  number={},
  pages={75-82},
  abstract={AI is shaping our society and changing what it means to be human. The future is getting frenetic since the adoption of Generative AI (GenAI) in our everyday life. Humanity is slowly transcending from a society of consumers to creators. We first began with Siri and Alexa; now it is ChatGPT, DALL-E, Stable Diffusion and several other tools powered by Artificial Intelligence (AI) and Machine Learning (ML). Audi as well has started the use of Generative Adversarial Networks (GANs) to get inspiration on wheel design. This paper will focus on: the advantages; the social and cultural impact that GenAI opposes; use cases on how GenAI can enhance innovation and drive business value from the technical prospect; the potential risks that might arise if GenAI is abused and conclude with how technology teams alongside the C-Suite and Board of Directors should take action. These risks include deepfakes, legal confusion, hiring biases and inaccurate chatbots. AI is in the service of humanity and it is a responsibility of ours to treat(/train) it with dignity and fairness.},
  keywords={Humanities;Deepfakes;Technological innovation;Generative AI;Law;Virtual assistants;Wheels;bias;chatbots;deepfakes;GANs;GenAI;Operator 4.0},
  doi={10.1109/CoNTESA61248.2023.10384863},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10622882,
  author={Abusitta, Adel and Halabi, Talal and Bataineh, Ahmed Saleh and Zulkernine, Mohammad},
  booktitle={ICC 2024 - IEEE International Conference on Communications}, 
  title={Generative Adversarial Networks for Robust Anomaly Detection in Noisy IoT Environments}, 
  year={2024},
  volume={},
  number={},
  pages={4644-4649},
  abstract={The Internet of Things (IoT) enables us to collect and process vast amounts of data in real time. However, the security of IoT devices and networks is highly susceptible to cyber attacks that threaten data integrity and service availability. Furthermore, due to the diverse nature of data collected from numerous nodes in IoT systems and the disturbances occurring within them, detecting anomalous activities and compromised nodes is considerably more challenging than in conventional computer systems. Therefore, it is crucial to develop robust and dependable anomaly detection methods to identify and remove malicious and/or unwanted data, which ensures their exclusion from IoT-powered applications and data analytics. To achieve this, this paper proposes a Generative Adverserial Networks (GAN)-based anomaly detection for IoT systems. The proposed model enables the autoencoder - using the adversarial training of GAN - to learn a better representation of IoT data, making it robust against noisy and changing environments. Based on experiments with real-world IoT datasets, the proposed framework has shown to improve the accuracy of detecting malicious traffic in IoT and surpass state-of-the-art anomaly detection models.},
  keywords={Training;Data analysis;Data integrity;Generative adversarial networks;Real-time systems;Data models;Internet of Things;Anomaly detection;IoT security;GANs;robustness;Artificial Intelligence},
  doi={10.1109/ICC51166.2024.10622882},
  ISSN={1938-1883},
  month={June},}@ARTICLE{10980385,
  author={Xie, Wenwen and Sun, Geng and Li, Jiahui and Wang, Jiacheng and Du, Hongyang and Niyato, Dusit and Dobre, Octavia A.},
  journal={IEEE Internet of Things Magazine}, 
  title={Generative AI for Energy Harvesting Internet of Things Network: Fundamental, Applications, and Opportunities}, 
  year={2025},
  volume={8},
  number={3},
  pages={72-80},
  abstract={Internet of Things (IoT) devices are typically powered by small-sized batteries with limited energy storage capacity, requiring regular replacement or recharging. To reduce costs and maintain connectivity in IoT networks, energy harvesting technologies are regarded as a promising solution. Notably, due to its robust analytical and generative capabilities, generative artificial intelligence (GenAl) has demonstrated significant potential in optimizing energy harvesting networks. Therefore, we discuss key applications of GenAl in improving energy harvesting IoT networks in this article. Specifically, we first review the key technologies of GenAI and the architecture of energy harvesting IoT networks. Then, we show how GenAI can address different problems to improve the performance of the energy harvesting IoT networks. Subsequently, we present a case study of unmanned aerial vehicle (UAV)-enabled data col-lection and energy transfer. The case study shows distinctively the necessity of energy harvesting technology and verify the effectiveness of GenAI-based methods. Finally, we discuss some import-ant open directions.},
  keywords={Costs;Generative AI;Reviews;Energy exchange;Data collection;Autonomous aerial vehicles;Batteries;Energy harvesting;Internet of Things;Battery management systems;Energy storage},
  doi={10.1109/IOTM.001.2400125},
  ISSN={2576-3199},
  month={May},}@INPROCEEDINGS{11166476,
  author={Guo, Jinghui and Akbar, Khandakar Ashrafi and Khan, Latifur and Thuraisingham, Bhavani and Irtiza, Saquib},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={Streamlining Research Complexities for AI Agents: Charting Pathways to Innovative Idea Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={The emergence of generative artificial intelligence in language processing has made it possible to automate complex human tasks, such as research. By breaking down these intricate tasks, large language models (LLMs) can gain a deeper understanding of how to tackle problems effectively. Emphasizing hierarchical information and contrasting perspectives is crucial for enhancing LLMs’ contextual awareness, leading to more informed and accurate outcomes. We propose an innovative approach to help LLMs generate novel research ideas and assess their quality across various dimensions. Our case studies and experiments show that simplifying research narratives enables LLMs to generate more concrete, high-quality ideas. We conduct extensive automated experiments on the generated novel ideas, evaluating them against various metrics and demonstrating their effectiveness based on different qualitative factors.},
  keywords={Measurement;Accuracy;Generative AI;Large language models;Retrieval augmented generation;Learning (artificial intelligence);Complexity theory;LLMs;Retrieval-Augmented Generation;In-Context Learning;Prompt-Engineering},
  doi={10.1109/ACDSA65407.2025.11166476},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11105940,
  author={Lee, John S. Y.},
  booktitle={2025 11th International Conference on Computing and Artificial Intelligence (ICCAI)}, 
  title={Student use of accurate and inaccurate chatbot content: an empirical study}, 
  year={2025},
  volume={},
  number={},
  pages={374-379},
  abstract={With the integration of generative artificial intelligence (AI) into education, high-quality materials generated by Large Language Models (LLMs) have been shown to bring pedagogical benefits. Although it is well known that these models can hallucinate, there has been relatively less empirical research on the impact of misinformation on students’ learning outcome. This paper investigates whether university students can engage critically with LLMs and, specifically, the extent to which they can both benefit from accurate LLM content and recognize inaccurate content. In our study, 144 students answered short questions that required them to compare or distinguish between two concepts, scores or corpus queries. The correct answer may be one of the two options, or “both”. The answers generated by a chatbot were also shown to the treatment group, but not to the control group. In questions where the chatbot was correct, the treatment group outperformed the control group. In questions where the chatbot was incorrect, student performance varied according to the content of the chatbot answer. When the answer should be “both” but the chatbot accepted only one of the two options, the treatment group was more likely than the control group to recognize the validity of both options. However, when the chatbot also argued that the other option was incorrect, the treatment group was more prone to agree with the chatbot. Educators may find these results helpful in preparing students for the use of chatbot in their studies.},
  keywords={Accuracy;Generative AI;Large language models;Education;Learning (artificial intelligence);Chatbots;Question answering (information retrieval);Fake news;Large Language Models;hallucination;pedagogy;question answering},
  doi={10.1109/ICCAI66501.2025.00065},
  ISSN={},
  month={March},}@INPROCEEDINGS{10901741,
  author={Jin, Zhenzhou and You, Li and Kwan Ng, Derrick Wing and Xia, Xiang-Gen and Gao, Xiqi},
  booktitle={GLOBECOM 2024 - 2024 IEEE Global Communications Conference}, 
  title={A Generative Denoising Approach for Near-Field XL-MIMO Channel Estimation}, 
  year={2024},
  volume={},
  number={},
  pages={3443-3448},
  abstract={In this paper, we investigate the near-field (NF) channel estimation (CE) for extremely large-scale multiple-input multiple-output (XL-MIMO) systems. Considering the pronounced NF effects in XL-MIMO communications, we first establish a joint angle-distance (AD) domain-based spherical-wavefront physical channel model that captures the inherent sparsity of XL-MIMO channels in the NF region. Leveraging the sparsity of the channel, the CE is approached as a task of reconstructing sparse signals. Anchored in this framework, we first propose a compressed sensing algorithm to acquire a preliminary channel estimation. Harnessing the powerful latent representation capability of generative artificial intelligence (GenAI), we further propose a GenAI-based approach to refine the estimated channel by employing advanced image denoising techniques. Specifically, we perceive the estimated channel as a noisy color image. Then, we derive the evidence lower bound (ELBO) of the design objective utilizing variational inference and reparameterization techniques, and propose a generative diffusion probabilistic model (GDM) dedicated to denoising. Experimental results indicate that the proposed GDM is capable of offering substantial performance gain in CE compared to existing benchmark approaches in NF XL-MIMO systems.},
  keywords={Lower bound;Generative AI;Noise reduction;Channel estimation;Performance gain;MIMO;Noise measurement;Global communication;Image reconstruction;Image denoising},
  doi={10.1109/GLOBECOM52923.2024.10901741},
  ISSN={2576-6813},
  month={Dec},}@ARTICLE{9229522,
  author={Chen, Xu and Chen, Siheng and Yao, Jiangchao and Zheng, Huangjie and Zhang, Ya and Tsang, Ivor W.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning on Attribute-Missing Graphs}, 
  year={2022},
  volume={44},
  number={2},
  pages={740-757},
  abstract={Graphs with complete node attributes have been widely explored recently. While in practice, there is a graph where attributes of only partial nodes could be available and those of the others might be entirely missing. This attribute-missing graph is related to numerous real-world applications and there are limited studies investigating the corresponding learning problems. Existing graph learning methods including the popular GNN cannot provide satisfied learning performance since they are not specified for attribute-missing graphs. Thereby, designing a new GNN for these graphs is a burning issue to the graph learning community. In this article, we make a shared-latent space assumption on graphs and develop a novel distribution matching-based GNN called structure-attribute transformer (SAT) for attribute-missing graphs. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling of structures and attributes by distribution matching techniques. It could not only perform the link prediction task but also the newly introduced node attribute completion task. Furthermore, practical measures are introduced to quantify the performance of node attribute completion. Extensive experiments on seven real-world datasets indicate SAT shows better performance than other methods on both link prediction and node attribute completion tasks.},
  keywords={Task analysis;Gallium nitride;Generative adversarial networks;Convolution;Recurrent neural networks;Linear programming;graph neural network;attribute-missing graphs;distribution matching;node attribute completion;node classification;link prediction},
  doi={10.1109/TPAMI.2020.3032189},
  ISSN={1939-3539},
  month={Feb},}@ARTICLE{9389779,
  author={Yang, Haixu and Liu, Jihong and Zhang, Lvheng and Li, Yan and Zhang, Henggui},
  journal={IEEE Access}, 
  title={ProEGAN-MS: A Progressive Growing Generative Adversarial Networks for Electrocardiogram Generation}, 
  year={2021},
  volume={9},
  number={},
  pages={52089-52100},
  abstract={Electrocardiogram (ECG) is a physiological signal widely used in monitoring heart health, which is of great significance to the detection and diagnosis of heart diseases. Because abnormal heart rhythms are very rare, most ECG datasets have data imbalance problems. At present, many algorithms for ECG anomaly automatic recognition are affected by data imbalance. Conventional data augmentation methods are not suitable for the augmentation of the ECG signal, because the ECG signal is one-dimensional and their morphology has physiological significances. In this paper, we propose a ProGAN based ECG sample generation model, called ProEGAN-MS, to solve the problem of data imbalance. The model can stably generate realistic ECG samples. We evaluate the fidelity and diversity of the data generated by the model and compare the data distribution of the original and generated data. In addition, in order to show the diversity of the generated ECG data more intuitively, we manually checked the diversity and calculate the statistics of the data. The results show that compared with other ECG augmentation methods based on GANs, the ECG data generated by our model has higher fidelity and diversity, and the distribution of generated samples is closer to the distribution of original data. Finally, we established neural network models for arrhythmia classification, and used them to evaluate the improvement of the classification model performance by ProEGAN-MS. The results show that augmented data by ProEGAN-MS can effectively improve the insufficient sensitivity and precision of the classification model.},
  keywords={Electrocardiography;Generative adversarial networks;Feature extraction;Training;Data models;Solid modeling;Mathematical model;Generative adversarial networks;data augmentation;electrocardiogram signals;ECG generation},
  doi={10.1109/ACCESS.2021.3069827},
  ISSN={2169-3536},
  month={},}@INBOOK{10955660,
  author={Siva Kumar, Ram Shankar and Anderson, Hyrum and Schneier, Bruce},
  booktitle={Not with a Bug, But with a Sticker: Attacks on Machine Learning Systems and What To Do About Them}, 
  title={Index}, 
  year={2023},
  volume={},
  number={},
  pages={189-202},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119884903},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10955660},}@ARTICLE{10230895,
  author={Zhan, Fangneng and Yu, Yingchen and Wu, Rongliang and Zhang, Jiahui and Lu, Shijian and Liu, Lingjie and Kortylewski, Adam and Theobalt, Christian and Xing, Eric},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multimodal Image Synthesis and Editing: The Generative AI Era}, 
  year={2023},
  volume={45},
  number={12},
  pages={15098-15119},
  abstract={As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an introduction to different guidance modalities in image synthesis and editing, and then describe multimodal image synthesis and editing approaches extensively according to their model types. After that, we describe benchmark datasets and evaluation metrics as well as corresponding experimental results. Finally, we provide insights about the current research challenges and possible directions for future research.},
  keywords={Visualization;Image synthesis;Training;Surveys;Task analysis;Computational modeling;Transformers;Multimodality;image synthesis and editing;GANs;diffusion models;autoregressive models;NeRF},
  doi={10.1109/TPAMI.2023.3305243},
  ISSN={1939-3539},
  month={Dec},}@INPROCEEDINGS{9240734,
  author={Liu, Pei and Wang, Xuemin and Xiang, Chao and Meng, Weiye},
  booktitle={2020 International Conference on Computer Communication and Network Security (CCNS)}, 
  title={A Survey of Text Data Augmentation}, 
  year={2020},
  volume={},
  number={},
  pages={191-195},
  abstract={When natural language processing (NLP) technology is applied to industrial internet, problems such as lack of data and imbalance of data are often encountered. In order to improve the accuracy and robustness of the model, text data augmentation was proposed to expand data. Data augmentation is widely used in computer vision. For example, the semantics of the image will not be changed if the image is rotated several degrees or converted to gray level. However, augmentation of text data in NLP is pretty rare. Data augmentation is a low-cost means to expand the amount of data and improve the effect of the model, which has a wide range of applications.},
  keywords={Computational modeling;Semantics;Manuals;Data collection;Natural language processing;Data models;Robustness;text data augmentation;AI technology;natural language processing},
  doi={10.1109/CCNS50731.2020.00049},
  ISSN={},
  month={Aug},}@ARTICLE{11040082,
  author={Ren, Weibo and Wang, Zhijian and Chen, Zhongxin and Zhao, Shun and Dong, Lei and Li, Yanfeng and Fan, Xin},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Multifault Feature Wasserstein Generative Adversarial Networks for Fault Diagnosis in Unbalanced Data}, 
  year={2025},
  volume={74},
  number={},
  pages={1-9},
  abstract={Due to the limitation of industrial conditions in production, raw sensor data are always shown as an unbalanced dataset, characterized by abundant normal operational data and scarce fault instances. This unbalance can degrade the performance of conventional fault diagnosis methods, leading to reduced accuracy and unstable model training. To address this challenge in bearing fault diagnosis, this article proposes a multifault feature Wasserstein generative adversarial network (MFF-WGAN) to enhance diagnostic precision. First, the framework employs a multiencoder denoising autoencoder (DAE) architecture to mitigate noise interference in raw sensor data. Subsequently, the proposed MFF-WGAN integrates label information into its adversarial loss function to enable simultaneous generation of diverse fault categories, while incorporating interclass feature discrepancies to refine sample quality. Finally, the developed multifault feature Wasserstein generation adversarial network is tested on the Case Western Reserve University bearing dataset and the laboratory bearing dataset. Computational results show that the proposed method can generate high-quality bearing samples with multiple faults effectively, which can obtain a higher diagnosis accuracy of 99.01% and 97.71% compared with the existing methods.},
  keywords={Training;Data mining;Artificial intelligence;Bearing-unbalanced data;fault diagnosis;multiencoder denoising autoencoder (DAE);Wasserstein generative adversarial networks (WGANs)},
  doi={10.1109/TIM.2025.3580880},
  ISSN={1557-9662},
  month={},}@ARTICLE{10440507,
  author={Luo, Yixin and Yang, Zhouwang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DynGAN: Solving Mode Collapse in GANs With Dynamic Clustering}, 
  year={2024},
  volume={46},
  number={8},
  pages={5493-5503},
  abstract={Generative Adversarial Networks (GANs) are widely-used generative models for synthesizing complex and realistic data. However, mode collapse, where the diversity of generated samples is significantly lower than that of real samples, poses a major challenge for further applications. Our theoretical analysis demonstrates that the generator loss function is non-convex with respect to its parameters when there are multiple modes in real data. In particular, parameters that result in generated distributions with perfect partial mode coverage of the real distribution are the local minima of the generator loss function. To address mode collapse, we propose a unified framework called Dynamic GAN. This method detects collapsed samples in the generator by thresholding on observable discriminator outputs, divides the training set based on these collapsed samples, and trains a dynamic conditional model on the partitions. The theoretical outcome ensures progressive mode coverage and experiments on synthetic and real-world data sets demonstrate that our method surpasses several GAN variants. In conclusion, we examine the root cause of mode collapse and offer a novel approach to quantitatively detect and resolve it in GANs.},
  keywords={Generators;Training;Data models;Computational modeling;Unsupervised learning;Manifolds;Generative adversarial networks;Adversarial learning;conditional model;deep generative models;mode collapse;unsupervised learning},
  doi={10.1109/TPAMI.2024.3367532},
  ISSN={1939-3539},
  month={Aug},}@ARTICLE{9761924,
  author={Roy, Debashri and Mukherjee, Tathagata and Riden, Alec and Paquet, Jared and Pasiliao, Eduardo and Blasch, Erik},
  journal={IEEE Access}, 
  title={GANSAT: A GAN and SATellite Constellation Fingerprint-Based Framework for GPS Spoof-Detection and Location Estimation in GPS Deprived Environment}, 
  year={2022},
  volume={10},
  number={},
  pages={45485-45507},
  abstract={This paper presents a robust system for mitigating adversarial and natural GPS disruptions by presenting: (1) a software-based defense mechanism against spoofing attacks using generative adversarial networks (GANs), The system detects unauthorized or spoofed GPS signals from a hardware based spoofer, and (2) deep neural network models to infer positioning information in GPS-degraded /denied environments using the novel idea of GPS satellite constellation fingerprint. As the GAN and Satellite constellation fingerprinting are used together in a unified framework, we call it the “GANSAT positioning system.” Intuitively, the GANSAT neural networks implicitly learn a representation of the aggregation of the hardware fingerprints of the satellite’s in the GPS constellation at a given location and time. To demonstrate the approach, raw GPS signals were collected from the satellite transmitters using a software defined radio (SDR) at five different locations in the Florida panhandle area of the United States. Additionally, a GPS spoofer is implemented using a SDR and an open source software and used in an uncontrolled laboratory environment for spoofing the GPS signals at the aforementioned locations. In our experiments, the GANSAT framework yields ~99.5% accuracy for the task of identifying and filtering the spoofed GPS signals from real ones. It also achieves ~100% accuracy for the task of location estimation.},
  keywords={Global Positioning System;Receivers;Generative adversarial networks;Satellites;Satellite broadcasting;Generators;Training;GPS;GNSS;positioning;machine learning;generative adversarial nets;deep neural network},
  doi={10.1109/ACCESS.2022.3169420},
  ISSN={2169-3536},
  month={},}@ARTICLE{10379806,
  author={Huang, Kaiqiang and Mckeever, Susan and Miralles-Pechuán, Luis},
  journal={IEEE Access}, 
  title={Generalized Zero-Shot Learning for Action Recognition Fusing Text and Image GANs}, 
  year={2024},
  volume={12},
  number={},
  pages={5188-5202},
  abstract={Generalized Zero-Shot Action Recognition (GZSAR) is geared towards recognizing classes that the model has not been trained on, while still maintaining robust performance on the familiar, trained classes. This approach mitigates the need for an extensive amount of labeled training data and enhances the efficient utilization of available datasets. The main contribution of this paper is a novel approach for GZSAR that combines the power of two Generative Adversarial Networks (GANs). One GAN is responsible for generating embeddings from visual representations, while the other GAN focuses on generating embeddings from textual representations. These generated embeddings are fused, with the selection of the maximum value from each array that represents the embeddings, and this fused data is then utilized to train a GZSAR classifier in a supervised manner. This framework also incorporates a feature refinement component and an out-of-distribution detector to mitigate the domain shift problem between seen and unseen classes. In our experiments, notable improvements were observed. On the UCF101 benchmark dataset, we achieved a 7.43% increase in performance, rising from 50.93% (utilizing images and Word2Vec alone) to 54.71% with the implementation of two GANs. Additionally, on the HMDB51 dataset, we saw a 7.06% improvement, advancing from 36.11% using Text and Word2Vec to 38.66% with the dual-GAN approach. These results underscore the efficacy of our dual-GAN framework in enhancing GZSAR performance. The rest of the paper shows the main contributions to the field of GZSAR and highlights the potential and future lines of research in this exciting area.},
  keywords={Visualization;Feature extraction;Semantics;Training;Zero-shot learning;Videos;Text recognition;Generative adversarial networks;Human activity recognition;Generalized zero-shot action recognition;generalised zero-shot learning;generative adversarial networks;human action recognition},
  doi={10.1109/ACCESS.2024.3349510},
  ISSN={2169-3536},
  month={},}@ARTICLE{10546982,
  author={Zhou, Xiao and Balachandra, Akshara R. and Romano, Michael F. and Peter Chin, Sang and Au, Rhoda and Kolachalama, Vijaya B.},
  journal={IEEE Access}, 
  title={Adversarial Learning for MRI Reconstruction and Classification of Cognitively Impaired Individuals}, 
  year={2024},
  volume={12},
  number={},
  pages={83169-83182},
  abstract={Game theory-inspired deep learning using a generative adversarial network provides an environment to competitively interact and accomplish a goal. In the context of medical imaging, most work has focused on achieving single tasks such as improving image resolution, segmenting images, and correcting motion artifacts. We developed a dual-objective adversarial learning framework that simultaneously 1) reconstructs higher quality brain magnetic resonance images (MRIs) that 2) retain disease-specific imaging features critical for predicting progression from mild cognitive impairment (MCI) to Alzheimer’s disease (AD). We obtained 3-Tesla, T1-weighted brain MRIs of participants from the Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=342) and the National Alzheimer’s Coordinating Center (NACC, N =190) datasets. We simulated MRIs with missing data by removing 50% of sagittal slices from the original scans (i.e., diced scans). The generator was trained to reconstruct brain MRIs using the diced scans as input. We introduced a classifier into the GAN architecture to discriminate between stable (i.e., sMCI) and progressive MCI (i.e., pMCI) based on the generated images to facilitate encoding of disease-related information during reconstruction. The framework was trained using ADNI data and externally validated on NACC data. In the NACC cohort, generated images had better image quality than the diced scans (Structural similarity (SSIM) index:  $0.553 \pm 0.116$  versus  $0.348 \pm 0.108$ ). Furthermore, a classifier utilizing the generated images distinguished pMCI from sMCI more accurately than with the diced scans (F1-score:  $0.634 \pm 0.019$  versus  $0.573 \pm 0.028$ ). Competitive deep learning has potential to facilitate disease-oriented image reconstruction in those at risk of developing Alzheimer’s disease.},
  keywords={Magnetic resonance imaging;Image reconstruction;Biomedical imaging;Generative adversarial networks;Alzheimer's disease;Generators;Adversarial machine learning;Image reconstruction;generative adversarial network;magnetic resonance imaging;Alzheimer’s disease},
  doi={10.1109/ACCESS.2024.3408840},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9664669,
  author={Hevapathige, Asela},
  booktitle={2021 5th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)}, 
  title={Evaluation of Deep Learning Approaches for Anomaly Detection}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep learning is a machine learning technique which is inspired by basic human instincts and functionality of the brain. It can be leveraged to tackle anomaly detection problems due to their ability in performing complex learning and prediction. However, this has been challenging due to the diversity of anomalies, class imbalance and curse of dimensionality. This research study focused on analyzing the performance of deep learning models for anomaly detection in various domains. Multi-Layer Perceptron, Deep Neural Network, Recurrent Neural Network and Auto Encoder algorithms were tested on 7 numerical datasets ranging from small scale to large scale in terms of both data size and features. The experimental design used one class classification to train the models from non-anomalous data to identify new instances as either anomalous or non-anomalous. The experimental results indicate that deep learning algorithms improve performance with the increase of data size. This study also identified certain limitations of deep learning models on anomaly detection.},
  keywords={Deep learning;Recurrent neural networks;Prediction algorithms;Brain modeling;Distance measurement;Data models;Numerical models;Anomaly Detection;Deep Neural Networks;Multi-Layer Perceptron;Recurrent Neural Networks;Auto Encoders;One Class Classification},
  doi={10.1109/SLAAI-ICAI54477.2021.9664669},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9887460,
  author={Zunin, V.V. and Romanova, I.I.},
  booktitle={2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Parameterized Computing Module Generator Based on a Systolic Array}, 
  year={2022},
  volume={},
  number={},
  pages={217-220},
  abstract={In this paper, the use of systolic arrays for data processing in the training or executing neural networks is explored. Two types of systolic arrays were developed, and a comparison on spending resources (ALM) and result calculation time was made. The comparison was conducted with two variable parameters of the input matrices: the number of rows of the first matrix and the number of columns of the second matrix. It is shown that (depending on the available resources) one of the methods for calculating the result can be used to synthesize the systolic array module: 1) to generate a systolic array of a given size and multiply matrices in which the first of them does not exceed the array size; 2) to synthesize a systolic array of a limited size and perform the multiplication of two matrices using the “Divide-and-Conquer” algorithm.},
  keywords={Training;Neural networks;Data processing;Systolic arrays;Generators;Communications technology;Fourth Industrial Revolution;systolic array;FPGA;neural networks},
  doi={10.1109/IAICT55358.2022.9887460},
  ISSN={},
  month={July},}@INPROCEEDINGS{9065271,
  author={Gang, Sumyung and Fabrice, Ndayishimiye and Lee, JoonJae},
  booktitle={2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Coresets for PCB Character Recognition based on Deep Learning}, 
  year={2020},
  volume={},
  number={},
  pages={637-642},
  abstract={The PCB (Printed Circuit Board) character image is acquired in a limited environment. Characters in the form of printed text are considered to be an easier task than handwriting. However, because the different factories use different parts, the shape of the font used on the parts may also change. In addition, characters may be damaged due to the heat treatment occurring during the character printing process. In this study, we analyze the data collected at the production plant and build coresets to create a flexible deep learning model that can be applied to multiple plant sites. Also, This experiment reveals which factors are important for coresets configuration. At this time, the generated coresets are used to analyze performance through various ResNet models.},
  keywords={Machine learning;Character recognition;Production;Data models;Optical character recognition software;Data visualization;Shape;Deep learning;PCB Inspection;Coreset;OCR (Optical Character Recognition)},
  doi={10.1109/ICAIIC48513.2020.9065271},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10058222,
  author={Xie, Wenzhen and Han, Te and Shao, Haidong},
  booktitle={2022 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Unsupervised domain adaptation for bearing fault diagnosis using nonlinear impact dynamics model under limited supervision}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Rolling bearing is one of the crucial rotating parts of mechanical systems, which is usually exposed to high-load working conditions. The diagnosis of rolling bearing faults is significant for the health monitoring of the whole mechanical system. The deep learning method has been proven to be effective in many fault diagnosis occasions. However, sufficient labeled fault samples are unavailable in some practical industrial diagnosis tasks, which will lead to the serious performance degradation of traditional deep learning methods. Therefore, a rolling bearing dynamics model is established for generating sufficient simulation data for assisting the training process. Furthermore, to overcome the diagnostic performance degradation problem caused by the inconsistent feature distribution of simulation data and experimental data, adversarial learning is conducted to realize domain adaptation, thus capturing the generalized feature representation. The analysis results of an experimental rolling bearing dataset demonstrate the effectiveness of the proposed model, showing a potential industrial application value.},
  keywords={Fault diagnosis;Training;Employee welfare;Degradation;Adaptation models;Rolling bearings;Feature extraction;Rolling bearing;Fault diagnosis;Nonlinear impact dynamics model;Unsupervised domain adaptation},
  doi={10.1109/ICSMD57530.2022.10058222},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10581478,
  author={Han, Yuexing and Wan, Guanxin and Liu, Yi and Wang, Bing},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Conductivity and Hardness Prediction of Cu-Cr-Zr via Feature Augmentation on Geodesic Curve}, 
  year={2024},
  volume={},
  number={},
  pages={1937-1941},
  abstract={Cu-Cr-Zr alloy has garnered significant attention for its outstanding performance in high-temperature and high-radiation environments, such as nuclear fusion reactors. However, accurately predicting its key properties, such as conductivity and hardness, remains a challenging task. To address this issue, a pioneering approach is presented, integrating machine learning and shape space theory. We propose a performance prediction method for Cu-Cr-Zr based on feature augmentation on Geodesic curves, referred to as F AGC. The methodology initiates with the extraction of image features from Cu-Cr-Zr using a neural network model. Subsequently, multiple image features are projected into a Pre-shape space, where Geodesic curves are constructed to seamlessly fit and generate features. Experimental findings reveal that, in predicting the conductivity and hardness of Cu-Cr-Zr alloy, both attain an R2 value of 0.916 and 0.923, respectively.},
  keywords={Seminars;Shape;Metals;Automatic generation control;Prediction methods;Machine learning;Conductivity;Machine learning;feature augmentation;Cu-Cr-Zr alloy;the shape space;the pre-shape shape;Geodesic curve},
  doi={10.1109/AINIT61980.2024.10581478},
  ISSN={},
  month={March},}@INPROCEEDINGS{9497838,
  author={Alahassa, Nonvikan Karl-Augustt},
  booktitle={2021 International Conference on Artificial Intelligence and Computer Science Technology (ICAICST)}, 
  title={State-of-the-art Multivariate Regression with a General Nk Hidden Multi-Layer Feed Forward Neural Network Model}, 
  year={2021},
  volume={},
  number={},
  pages={31-36},
  abstract={We have introduced a novel multivariate regression model, called General Nk Hidden Multi-Layer Feed Forward Neural Network Model. The model prediction power comes essentially from a combination of Universal Approximation theorem, the Stochastic Gradient Descent convergence, and finally the use of ADAM optimizer which is also (at least) locally convergent. This Nk-architecture is proposed to fit any multivariate regression task, as well as any classification task when a softmax activation gate function is applied to the output layer nodes. The model can easily be augmented to thousands of possible layers without loss of predictive power, and has the potential to overcome our difficulties simultaneously in building a model that has a good fit on the test data, and don't overfit. Its hyper-parameters, the learning rate, the batch size, the number of training times (epochs), the size of each layer, the number of hidden layers, all can be chosen experimentally with cross-validation methods. We have run some experiments with the Mulan Project Datasets [29] to illustrate the performance of the model against Random Forest with a number of estimators from 5 to 10, and a maximum depth from 10 to 30. Not only has the model surpasses the Random Forest model in all tested configurations, but, we have also found this General Nk Hidden Multi-layer Feed Forward Neural Network to be so effective as it reaches state-of-the-art performance for multivariate regression in terms of Mean Squared Error.},
  keywords={Training;Computational modeling;Neural networks;Stochastic processes;Predictive models;Multivariate regression;Data models;Multivariate Regression;Multi-layer architecture;Feedforward Neural Networks;Random Forest;Stochastic Gradient Descent;State-of-the-art models},
  doi={10.1109/ICAICST53116.2021.9497838},
  ISSN={},
  month={June},}@INPROCEEDINGS{10692588,
  author={Chi, Shaoning and Bai, Haibin and Yuan, Baofeng and Zhang, Juncheng and Liang, Ruihong},
  booktitle={2024 5th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)}, 
  title={Research on Mobile Approval for Power Grid Production Operations Based on Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={654-657},
  abstract={With the increasing complexity and scale of power systems, managing production operations in the power grid faces unprecedented challenges. Traditional approval processes rely on manual operations, which are inefficient and prone to errors. To address this issue, this paper proposes a mobile approval system for power grid production operations based on deep learning. The system utilizes Convolutional Neural Network (CNN) deep learning technology to perform natural language processing on approval documents and operation plans, and provides decision support through predictive analysis. Experimental results show that the proposed mobile approval system performs excellently in improving approval efficiency, reducing error rates, and enhancing security.},
  keywords={Deep learning;Error analysis;Reviews;Production;Power grids;Natural language processing;Safety;Convolutional neural networks;Security;Predictive analytics;Deep Learning;Power Grid Production Operations;Mobile Approval;Natural Language Processing},
  doi={10.1109/AIEA62095.2024.10692588},
  ISSN={},
  month={June},}@ARTICLE{9272277,
  author={Li, Chaobo and Shen, Xulin and Li, Hongjun},
  journal={IEEE Access}, 
  title={S3GRN: Structural Similar Stepwise Generative Recognizable Network for Human Action Recognition With Limited Training Data}, 
  year={2020},
  volume={8},
  number={},
  pages={216219-216230},
  abstract={Human action recognition is a hot topic and it has been applied to various fields. Deep learning is one of the techniques in human action recognition which has achieved good results. However, the task is still challenging due to the less collected samples. In order to address this challenge and improve the recognition accuracy, the stepwise generative recognizable network is proposed based on the generative adversarial network, which can be used to expand limited training samples and then recognize. Firstly, the stepwise generative recognizable network is designed to combine the function of images generation and recognition for human action. Secondly, the structural similar constraint is introduced to stepwise generative recognizable network, called structural similar stepwise generative recognizable network, which can compare the similarity of generated images with real data to improve quality and diversity of generated images. Finally, the performance of proposed networks is verified by common databases and the self-build database which is collected in daily life. We achieved 97.14%, 94.88% and 99.69% recognition accuracy on MNIST, Weizmann and self-build dataset, respectively. The experimental results show that the combination of generation and recognition can improve the recognition accuracy without abundant training data, and the structural similar constraint not only can improve the quality and diversity of generated images but also perform better in convergence. The structural similar stepwise generative recognizable network reduces the workload of manual collection and solves the problem of lower recognition accuracy for limited training samples, which achieves the characteristics of natural expanded samples.},
  keywords={Image recognition;Generative adversarial networks;Generators;Training;Convolution;Image synthesis;Convergence;Generative network;recognizable network;combination;structural similar constraint},
  doi={10.1109/ACCESS.2020.3040758},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10137976,
  author={Hao, Qiushi and Ren, Jia},
  booktitle={2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Determination of the Mechanical Origination of Wheel-Rail Rolling Noise Based on Spectrum Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Acoustic emission technology has a great advantage over existing nondestructive technologies for real-time inspection, which will significantly improve the efficiency of wheel/rail defect detection. However, wheel-rail rolling noise impedes the application of acoustic emission technology in on-line operation, especially in high-speed or heavy-load condition. The key problem lies in that current researches haven’t developed adequate knowledge of the noise, making it difficult to gain the defect signal under the strong noise. To study mechanical originations of the noise and reveal its intrinsic properties, a spectral analysis method is proposed based on a fractal description of rough surfaces. Power spectra of the surface and those of the noise, as well as the relation of their fractal dimensions, are investigated. Then, under the instruction of spectral distributions of microscopic mechanical behaviors, the noise originations and influence of the vehicle speed are determined. It is found that the noise is generated based on the surface topography, while sliding friction, particle behavior, and abrasive wear are the main mechanical sources. The sliding friction dominates among the three behaviors. The speed promotes all the behaviors and then enhances the power level, while its effects on the sliding friction is relatively severer. The work offers a theoretical basis and mechanical explanation for the noise, which provides further guidance for the real-time detection of defect signals.},
  keywords={Friction;Acoustic emission;Surface roughness;Fractals;Real-time systems;Surface topography;Rough surfaces;acoustic emission;wheel-rail rolling noise;spectral analysis;mechanical motion},
  doi={10.1109/ACAIT56212.2022.10137976},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10085438,
  author={Dixit, Ramjee and Pal, Manoj Kumar},
  booktitle={2023 International Conference on Artificial Intelligence and Smart Communication (AISC)}, 
  title={A Review of Current Approaches to Automated Detection of Malignant Tumors Based on MR Images}, 
  year={2023},
  volume={},
  number={},
  pages={279-283},
  abstract={The brain tumor is a crucial organ of every human body and consists of neurons. A brain tumor is a serious disease and a major cause of death worldwide and is increasing daily. Brain tumors are categorized as benign and malignant. With staying in their prime location, benign tumors are less harmful. No abnormal cell proliferation is observed but should be removed as soon as possible. Malignant tumors can grow in other parts of the body and cause serious damage. Early detection of brain tumors is essential and fruitful in improving the patient’s life. Using MRI images of the brain, radiologists detect the tumor. Because MRI contains many slices, manual detection is time- consuming and many cells may not be visible using necked- eyes. Therefore, automatic methods are required. Since MRI is a medical image for detecting brain tumors, the steps of biomedical image processing are followed. Images are first acquired and then segmented to extract regions of interest (ROI). A feature selection and feature reduction process is then performed to find the best features. Tumors are then identified and classified. This paper describes recent work in this regard.},
  keywords={Image segmentation;Recurrent neural networks;Sensitivity;Magnetic resonance imaging;Malignant tumors;Feature extraction;Brain modeling;Tumor;Segmentation;Classification;MRI},
  doi={10.1109/AISC56616.2023.10085438},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10842834,
  author={Bordia, Anav and Chawla, Ayaan and Gao, Karen and McMahan, Larry},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={A Multimodal Approach for Skin Lesion Diagnosis}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Inspection by dermatologists remains the standard for skin cancer diagnosis, but it can be subjective and prone to error. We explore the potential of machine learning for dermoscopic image classification using 10,000 pigmented skin lesions. Although convolutional neural networks are powerful tools for skin lesion classification by themselves, by incorporating patient-specific textual data, we aim to improve the accuracy and accessibility of diagnosis. We propose multiple multimodal models: transfer learning and multimodal transformers. These multimodal approaches aim to utilise both dermoscopic images and corresponding patient-specific data, such as lesion location and patient demographics. Our multimodal model with the best overall evaluation is the multimodal transformer model which combines a vision transformer and a text transformer. It achieved an accuracy of 93.21% and an AUROC of 0.9860.},
  keywords={Industries;Accuracy;Transfer learning;Inspection;Transformers;Skin;Lesions;Medical diagnosis;Standards;Skin cancer;BERT;Transformers;ResNetV2;Skin Cancer},
  doi={10.1109/IDICAIEI61867.2024.10842834},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10169687,
  author={Vinora, A and Lloyds, E and Nancy Deborah, R and Anandha Surya, M.S. and Krithik Deivarajan, V and MuthuVignesh, M.},
  booktitle={2023 International Conference on Artificial Intelligence and Applications (ICAIA) Alliance Technology Conference (ATCON-1)}, 
  title={Heart Disease Prediction using Ensemble Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Heart Disease is one of the prominent fatal diseases that have caused a colossal amount of deaths over decades. Machine learning an effective domain has been a key factor to solve various problems over a wide spread of areas. If the presence or the indication of such a fatal disease can be predicted in advance, it will be effortless for doctors to diagnose them. The ensemble stacked model which offers a way to combine Support Vector Machine (SVM) and Decision Tree(DT) models is part of the Machine learning domain that has been applied in our model to develop an intelligent system to predict the accuracy of the disease. The ensemble model of SVM and DT has achieved a higher percentage of efficiency among the various methods used for prediction. The proposed system presents a machine-learning approach for predicting heart disease, using a dataset of significant health factors such as age, sex, cholesterol, blood pressure, and sugar, from patients. The proposed system enables precise prediction of heart disease that enhances medical care and reduces the cost incurred for prediction. The dataset has been obtained from Kaggle.},
  keywords={Heart;Support vector machines;Costs;Machine learning;Medical services;Predictive models;Decision trees;Machine learning;Heart Disease Prediction;SVM;DT;Ensemble Stacked Model},
  doi={10.1109/ICAIA57370.2023.10169687},
  ISSN={},
  month={April},}@ARTICLE{10925378,
  author={Sathya, K. and Balakrishnan, Arunkumar and Baskaran, P. and Kumar Ramamoorthy, Arun},
  journal={IEEE Access}, 
  title={Enhancing Plant Disease Detection Using Attention-Augmented Residual Networks and Faster Region–Convolutional Networks}, 
  year={2025},
  volume={13},
  number={},
  pages={48625-48642},
  abstract={Rapid and accurate detection of plant diseases is crucial for agricultural productivity and food security. Traditional methods are labor-intensive and often unreliable. To overcome these limitations, this research introduces an innovative approach that integrates attention mechanisms into residual networks (ResNets) and utilizes Generative Adversarial Networks (GANs) for data augmentation. The method incorporates Attention-Augmented Residual Networks (AARN), which enhance feature extraction and classification by focusing on critical image regions. A Conditional GAN (cGAN) generates synthetic images of diseased and healthy plants, increasing dataset diversity. By combining AARN with Faster Region-Convolutional Neural Network (Faster-RCNN), detection capabilities are further enhanced. Training the AARN model on this augmented dataset improves generalization, achieving an impressive 98.78% accuracy in plant disease classification. The attention-augmented residuals boost the Faster-RCNN’s effectiveness by 23.84%, improving feature relevance and reducing overfitting. Comparative analysis shows that this method outperforms existing techniques in accuracy, precision, recall, and F1-score, offering a robust solution for plant disease detection. This integration of advanced deep learning techniques significantly improves automated plant disease identification, benefiting agricultural management practices.},
  keywords={Diseases;Plant diseases;Accuracy;Deep learning;Generative adversarial networks;Real-time systems;Crops;Attention mechanisms;Training;Residual neural networks;Attention-augmented residual networks (AARN);data augmentation;faster region-convolutional neural network (Faster-RCNN);generative adversarial networks (GANs);plant disease detection},
  doi={10.1109/ACCESS.2025.3551242},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10087617,
  author={Lan, Chuanhao and Zhu, Yongzhi and Zhang, Qian and Cui, Yikai and Mao, Qinghang},
  booktitle={2022 2nd International Conference on Algorithms, High Performance Computing and Artificial Intelligence (AHPCAI)}, 
  title={Joint Extraction model of Chinese Recipe Entities and Relations Based on Adversarial Training and Cascaded Binary Tagging}, 
  year={2022},
  volume={},
  number={},
  pages={606-609},
  abstract={The process of creating a knowledge base or knowledge graph relies heavily on the extraction of relational triples from unstructured text. However, the existing methods rarely address the Chinese multi-triple and overlapping triples. We adopt a cascade binary Tagging framework combined with adversarial training (BertAdvCasLSTM) to solve this problem. When the subject information and sentence features are fused, we use a Bi-LSTM network to further feature extraction of the fused sentence features, which improves the performance of the model. We also personally crafted the Recipe_Chinese dataset to ensure the model's extraction capabilities. On both the Recipe_Chinese and DuIE1.0 Chinese datasets, our suggested BertAdvCasLSTM model outperforms the baselines. In-depth experimental analysis is carried out on different cases where a piece of data contains multiple triples, and the performance is improved in most cases. The impact of adversarial training on the overall model performance is also examined.},
  keywords={Training;Measurement;Annotations;Computational modeling;Knowledge based systems;Knowledge graphs;Tagging;Adversarial Training;Entity Relation Extraction;Cascade Binary Tagging;Chinese Recipes},
  doi={10.1109/AHPCAI57455.2022.10087617},
  ISSN={},
  month={Oct},}@ARTICLE{9239928,
  author={Lee, Tae Bok and Jung, Soo Hyun and Heo, Yong Seok},
  journal={IEEE Access}, 
  title={Progressive Semantic Face Deblurring}, 
  year={2020},
  volume={8},
  number={},
  pages={223548-223561},
  abstract={Previous face deblurring methods have utilized semantic segmentation maps as prior knowledge. Most of these methods generated the segmentation map from a blurred facial image, and restore it using the map in a sequential manner. However, the accuracy of the segmentation affects the restoration performance. Generally, it is difficult to obtain an accurate segmentation map from a blurred image. Instead of sequential methods, we propose an efficient method that learns the flows of facial component restoration without performing segmentation. To this end, we propose a multi-semantic progressive learning (MSPL) framework that progressively restores the entire face image starting from the facial components such as the skin, followed by the hair, and the inner parts (eyes, nose, and mouth). Furthermore, we propose a discriminator that observes the reconstruction-flow of the generator. In addition, we present new test datasets to facilitate the comparison of face deblurring methods. Various experiments demonstrate that the proposed MSPL framework achieves higher performance in facial image deblurring compared to the existing methods, both qualitatively and quantitatively. Our code, trained model and data are available at https://github.com/dolphin0104/MSPL-GAN.},
  keywords={Faces;Image restoration;Image segmentation;Semantics;Generators;Image reconstruction;Hair;Facial image deblurring;semantic mask;progressive learning;generative adversarial network;deep learning},
  doi={10.1109/ACCESS.2020.3033890},
  ISSN={2169-3536},
  month={},}@ARTICLE{10485642,
  author={Al-Maliki, Shawqi and Qayyum, Adnan and Ali, Hassan and Abdallah, Mohamed and Qadir, Junaid and Hoang, Dinh Thai and Niyato, Dusit and Al-Fuqaha, Ala},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally}, 
  year={2024},
  volume={5},
  number={9},
  pages={4322-4343},
  abstract={Deep neural networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples—input samples that have been perturbed to force DNN-based models to make errors. As a result, adversarial machine learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in antisocial AI applications. The emergence of new AI technologies that leverage large language models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing antisocial applications at scale. AdvML for social good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent prosocial applications. Regulators, practitioners, and researchers should collaborate to encourage the development of prosocial applications and hinder the development of antisocial ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating prosocial applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.},
  keywords={Robustness;Taxonomy;Computational modeling;Adversarial machine learning;Human computer interaction;Social implications of technology;Artificial neural networks;Adversarial machine learning (AdvML);AI for good;human-centered computing;ML for social good;socially good applications},
  doi={10.1109/TAI.2024.3383407},
  ISSN={2691-4581},
  month={Sep.},}@ARTICLE{11048922,
  author={Wang, Xiang and Li, Mingxing and Yao, Yu and Zhu, Xiaoyang},
  journal={IEEE Access}, 
  title={A Study of Factors Influencing Chinese Design Students’ Adoption of AIGC Tools}, 
  year={2025},
  volume={13},
  number={},
  pages={117753-117770},
  abstract={As a critical factor in determining a product’s success, users’ willingness to adopt artificial intelligence-generated content (AIGC) tools is a key driver for their sustainable development. As a new productivity tool, AIGC tool adoption by design students is influenced by various factors. However, there is a lack of systematic research on this topic in the academic community, which hinders the sustainable development of AIGC tools. Based on this, the study combines the Unified Theory of Acceptance and Use of Technology (UTAUT) and Task-Technology Fit (TTF) models to construct a dual-perspective model—integrating “technology (external)” and “individual (internal)” dimensions—to investigate the key factors influencing design students’ willingness to adopt AIGC tools in their professional learning. A questionnaire was distributed to Chinese design students, resulting in 517 valid responses. The structural equation model (SEM) analysis yielded the following findings: Characteristics of AIGC Technology (CoAT) and students’ task characteristics positively influence task-technology fit. CoAT positively affect performance expectations (PE) but have no significant impact on effort expectations (EE). Task-technology fit positively influences use behaviors and performance expectations. Additionally, performance expectations, effort expectations, and social influences (SI) significantly enhance willingness to use, which in turn drives use behaviors (UB). Conversely, facilitating conditions (FC) do not affect use behaviors. Based on the empirical findings, this study discusses strategies for enhancing design students’ adoption of AIGC tools and provides recommendations for promoting their adoption and sustainability in design education.eing rejected by search engines.},
  keywords={Education;Artificial intelligence;Art;Sustainable development;Mathematical models;Technological innovation;Systematics;Information technology;Industries;Fake news;Artificial intelligence;generative models;human computer interaction;educational technology;user interfaces},
  doi={10.1109/ACCESS.2025.3582599},
  ISSN={2169-3536},
  month={},}@ARTICLE{11048765,
  author={Onsu, Murat Arda and Lohan, Poonam and Kantarci, Burak},
  journal={IEEE Internet of Things Magazine}, 
  title={Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles}, 
  year={2025},
  volume={8},
  number={4},
  pages={148-155},
  abstract={The evolution of Artificial Intelligence (AI) and its subset Deep Learning (DL), has profoundly impacted numerous domains, including autonomous driving. The integration of autonomous driving in military settings reduces human casualties and enables precise and safe execution of missions in hazardous environments while allowing for reliable logistics support without the risks associated with fatigue-related errors. However, relying on autonomous driving solely requires an advanced decision-making model that is adaptable and optimum in any situation. Considering the presence of numerous interconnected autonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency Communication (URLLC) is vital for ensuring seamless coordination, real-time data exchange, and instantaneous response to dynamic driving environments. The advent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV) concept within the realm of Internet of Military Defense Things (IoMDT) by enabling robust connectivity, crucial for real-time data exchange, advanced navigation, and enhanced safety features through IoADV interactions. On the other hand, a critical advancement in this space is using pre-trained Generative Large Language Models (LLMs) for decision-making and communication optimization for autonomous driving. Hence, this work presents opportunities and challenges with a vision of realizing the full potential of these technologies in critical defense applications, especially through the advancement of IoADV and its role in enhancing autonomous military operations.},
  keywords={6G mobile communication;Large language models;Decision making;Ultra reliable low latency communication;Real-time systems;Safety;Reliability;Vehicle dynamics;Autonomous vehicles;Optimization;Defense industry},
  doi={10.1109/IOTM.001.2400162},
  ISSN={2576-3199},
  month={July},}@ARTICLE{10737883,
  author={Moser, Brian B. and Shanbhag, Arundhati S. and Raue, Federico and Frolov, Stanislav and Palacio, Sebastian and Dengel, Andreas},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Diffusion Models, Image Super-Resolution, and Everything: A Survey}, 
  year={2025},
  volume={36},
  number={7},
  pages={11793-11813},
  abstract={Diffusion models (DMs) have disrupted the image super-resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This article articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, conditioning techniques, guidance mechanisms, corruption spaces, and zero-shot learning approaches. By offering a detailed examination of the evolution and current trends in image SR through the lens of DMs, this article sheds light on the existing challenges and charts potential future directions, aiming to inspire further innovation in this rapidly advancing area.},
  keywords={Image quality;Superresolution;Diffusion models;Surveys;Computational modeling;Faces;Degradation;Training;Standards;Reviews;Diffusion models (DMs);super-resolution (SR);survey},
  doi={10.1109/TNNLS.2024.3476671},
  ISSN={2162-2388},
  month={July},}@INPROCEEDINGS{9311067,
  author={Ki Chan, Christopher Chun and Kumar, Vimal and Delaney, Steven and Gochoo, Munkhjargal},
  booktitle={2020 IEEE / ITU International Conference on Artificial Intelligence for Good (AI4G)}, 
  title={Combating Deepfakes: Multi-LSTM and Blockchain as Proof of Authenticity for Digital Media}, 
  year={2020},
  volume={},
  number={},
  pages={55-62},
  abstract={Malicious use of deep learning algorithms has allowed the proliferation of high realism fake digital content such as text, images, and videos, to exist on the internet as readily available and accessible consumable content. False information provided through algorithmically modified footage, images, audios, and videos (known as deepfakes), coupled with the virality of social networks, may cause major social unrest. The emergence of misinformation from fabricated digital content suggests the necessity for anti-disinformation methods such as deepfake detection algorithms or immutable metadata in order to verify the validity of digital content. Permissioned blockchain, notably Hyperledger Fabric 2.0, coupled with LSTMs for audio/video/descriptive captioning is a step towards providing a feasible tool for combating deepfake media. Original content would require the original artist attestation of untampered data. The smart contract combines a varied multiple LSTM networks into a process that allows for the tracing and tracking of a digital content's historical provenance. The result is a theoretical framework that enables proof of authenticity (PoA) for digital media using a decentralized blockchain using multiple LSTMs as a deep encoder for creating unique discriminative features; which is then compressed and hashed into a transaction. Our work assumes we trust the video at the point of reception. Our contribution is a decentralized blockchain framework of deep discriminative digital media to combat deepfakes.},
  keywords={Videos;Information integrity;Faces;Tools;Gallium nitride;Media;Decoding;artificial intelligence;blockchain;computer vision;deepfake;smart contracts},
  doi={10.1109/AI4G50087.2020.9311067},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9157676,
  author={Huang, Lei and Liu, Li and Zhu, Fan and Wan, Diwen and Yuan, Zehuan and Li, Bo and Shao, Ling},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Controllable Orthogonalization in Training DNNs}, 
  year={2020},
  volume={},
  number={},
  pages={6428-6437},
  abstract={Orthogonality is widely used for training deep neural networks (DNNs) due to its ability to maintain all singular values of the Jacobian close to 1 and reduce redundancy in representation. This paper proposes a computationally efficient and numerically stable orthogonalization method using Newton's iteration (ONI), to learn a layer-wise orthogonal weight matrix in DNNs. ONI works by iteratively stretching the singular values of a weight matrix towards 1. This property enables it to control the orthogonality of a weight matrix by its number of iterations. We show that our method improves the performance of image classification networks by effectively controlling the orthogonality to provide an optimal tradeoff between optimization benefits and representational capacity reduction. We also show that ONI stabilizes the training of generative adversarial networks (GANs) by maintaining the Lipschitz continuity of a network, similar to spectral normalization (SN), and further outperforms SN by providing controllable orthogonality.},
  keywords={Training;Matrix decomposition;Convergence;Neural networks;Eigenvalues and eigenfunctions;Covariance matrices;Jacobian matrices},
  doi={10.1109/CVPR42600.2020.00646},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10205149,
  author={Li, Yixuan and Ma, Chao and Yan, Yichao and Zhu, Wenhan and Yang, Xiaokang},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={3D-Aware Face Swapping}, 
  year={2023},
  volume={},
  number={},
  pages={12705-12714},
  abstract={Face swapping is an important research topic in computer vision with wide applications in entertainment and privacy protection. Existing methods directly learn to swap 2D facial images, taking no account of the geometric information of human faces. In the presence of large pose variance between the source and the target faces, there always exist undesirable artifacts on the swapped face. In this paper, we present a novel 3D-aware face swapping method that generates high-fidelity and multi-view-consistent swapped faces from single-view source and target images. To achieve this, we take advantage of the strong geometry and texture prior of 3D human faces, where the 2D faces are projected into the latent space of a 3D generative model. By disentangling the identity and attribute features in the latent space, we succeed in swapping faces in a 3D-aware manner, being robust to pose variations while transferring fine-grained facial details. Extensive experiments demonstrate the superiority of our 3D-aware face swapping framework in terms of visual quality, identity similarity, and multi-view consistency. Code is available at https://lyx0208.github.io/3dSwap.},
  keywords={Geometry;Solid modeling;Computer vision;Visualization;Three-dimensional displays;Codes;Face recognition;Humans: Face;body;pose;gesture;movement},
  doi={10.1109/CVPR52729.2023.01222},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10235900,
  author={Gao, Yuan and Tang, Hui and Ge, Rongjun and Liu, Jin and Chen, Xin and Xi, Yan and Ji, Xu and Shu, Huazhong and Zhu, Jian and Coatrieux, Gouenou and Coatrieux, Jean-Louis and Chen, Yang},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={3DSRNet: 3-D Spine Reconstruction Network Using 2-D Orthogonal X-Ray Images Based on Deep Learning}, 
  year={2023},
  volume={72},
  number={},
  pages={1-14},
  abstract={Orthopedic spine disease is one of the most common diseases in the clinic. The diagnosis of spinal orthopedic injury is an important basis for the treatment of spinal orthopedic diseases. Due to the complexity of the spine structure, doctors usually need to rely on orthopedic computed tomography (CT) image data for accurate diagnosis. In some cases, such as poor areas or in emergency situations, it is difficult for doctors to make accurate diagnoses using only 2-D x-ray images due to lack of 3-D imaging equipment or time crunch. Therefore, an approach based on 2-D x-ray images is needed to solve this problem. In this article, a novel 3-D spine reconstruction technique based on 2-D orthogonal x-ray images (3DSRNet) is designed. 3DSRNet uses a generative adversarial network (GAN) architecture and novel modules to make 3-D spine reconstruction more accurate and efficient. Spine reconstruction convolutional neural network (CNN)-transformer framework (SRCT) is employed to effectively integrate local bone surface information and long-range relation spinal structure information. Spine reconstruction texture framework (SRTE) is used to extract spine texture features to enhance the effect of pixel-level reconstruction. Experiments show that 3DSRNet achieves excellent 3-D spine reconstruction results on multiple metrics including peak signal-to-noise ratio (PSNR) (45.4666 dB), structural similarity index (SSIM) (0.8850), cosine similarity (CS) (0.7662), mean absolute error (MAE) (23.6696), mean squared error (MSE) (9016.1044), and learned perceptual image patch similarity (LPIPS) (0.0768).},
  keywords={Three-dimensional displays;Image reconstruction;X-ray imaging;Computed tomography;Cameras;Imaging;Feature extraction;3-D reconstruction;computed tomography (CT);deep learning;spine;x-ray},
  doi={10.1109/TIM.2023.3296838},
  ISSN={1557-9662},
  month={},}@INPROCEEDINGS{10377173,
  author={Zhang, Feng and Xu, Bin and Li, Zhiqiang and Liu, Xinran and Lu, Qingbo and Gao, Changxin and Sang, Nong},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Towards General Low-Light Raw Noise Synthesis and Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={10786-10796},
  abstract={Modeling and synthesizing low-light raw noise is a fundamental problem for computational photography and image processing applications. Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics-and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.},
  keywords={Training;Photography;Computational modeling;ISO;Noise reduction;Benchmark testing;Sensor phenomena and characterization},
  doi={10.1109/ICCV51070.2023.00993},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{9186008,
  author={Wei, Jianze and Wang, Yunlong and Wu, Xiang and He, Zhaofeng and He, Ran and Sun, Zhenan},
  booktitle={2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)}, 
  title={Cross-sensor iris recognition using adversarial strategy and sensor-specific information}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={Due to the growing demand of iris biometrics, lots of new sensors are being developed for high-quality image acquisition. However, upgrading the sensor and re-enrolling for users is expensive and time-consuming. This leads to a dilemma where enrolling on one type of sensor but recognizing on the others. For this cross-sensor matching, the large gap between distributions of enrolling and recognizing images usually results in degradation in recognition performance. To alleviate this degradation, we propose Cross-sensor iris network (CSIN) by applying the adversarial strategy and weakening interference of sensor-specific information. Specifically, there are three valuable efforts towards learning discriminative iris features. Firstly, the proposed CSIN adds extra feature extractors to generate residual components containing sensor-specific information and then utilizes these components to narrow the distribution gap. Secondly, an adversarial strategy is borrowed from Generative Adversarial Networks to align feature distributions and further reduce the discrepancy of images caused by sensors. Finally, we extend triplet loss and propose instance-anchor loss to pull the instances of the same class together and push away from others. It is worth mentioning that the proposed method doesn't need pair-same data or triplet, which reduced the cost of data preparation. Experiments on two real-world datasets validate the effectiveness of the proposed method in cross-sensor iris recognition.},
  keywords={Iris recognition;Feature extraction;Image sensors;Sensor phenomena and characterization;Degradation;Measurement},
  doi={10.1109/BTAS46853.2019.9186008},
  ISSN={2474-9699},
  month={Sep.},}@INPROCEEDINGS{8851721,
  author={Yu, Xintong and Guo, Tszhang and Fu, Kun and Li, Lei and Zhang, Changshui and Zhang, Jianwei},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Image Captioning with Partially Rewarded Imitation Learning}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={Current state-of-the-art image captioning algorithms have achieved great progress via reinforcement learning or generative adversarial nets, with hand-craft metrics such as CIDEr as the reward for the former and signals from adversarial discriminative networks for the latter. Despite the high scores on metrics or improvement in diversity gained from the application of these methods, they suffer from distinction with human-written sentences and drop of ratings on metrics respectively.In this paper, we propose a novel training objective for image captioning that consists of two parts representing explicit and implicit knowledge respectively. Optimizing the new reward partially with imitation learning, we devise an algorithm in which the caption generator is trained to maximize the combination of CIDEr and predictions from adversarial discriminator. Experiments on MSCOCO dataset demonstrate that the proposed method can integrate the strengths of state-of-the-arts, producing more human-like captions while maintaining comparable performance on traditional metrics.},
  keywords={Training;Measurement;Reinforcement learning;Task analysis;Generators;Neural networks;Cost function;image caption;reinforcement learning;adversarial learning;imitation learning},
  doi={10.1109/IJCNN.2019.8851721},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9933808,
  author={Lu, Jianfeng and Shi, Mengtao and Lu, Yuhang and Chang, Ching-Chun and Li, Li and Bai, Rui},
  journal={IEEE Access}, 
  title={Multi-Stage Generation of Tile Images Based on Generative Adversarial Network}, 
  year={2022},
  volume={10},
  number={},
  pages={127502-127513},
  abstract={Deep learning techniques have been recently widely used in the field of texture image generation. There are still two major problems when applying them to tile image design work. On the one hand, there is still lack of enough diverse ceramic tile images for the training process. On the other hand, the output image is difficult to control and adjust, and cannot meet the designer’s requirements of interactivity. Therefore, we propose a multi-stage generation algorithm of tile images based on generative adversarial network(GAN). First, the multi-scale attention GAN is applied to generate controllable texture image. Then, the SWAG texture synthesis GAN is also applied to obtain controllable and diverse image style. And finally, through the style iteration mechanism and the multiple step magnification method based on image super-resolution reconstruction network, the final tile images can be automatically generated with larger-size and higher-precision. The relevant experiments demonstrate that our method can not only generate high-quality tile images in a relatively short period of time, but also consider human interaction to a certain extent, and maintain a certain degree of control over the main texture and style of the final generated tile images. It has good and wide application value.},
  keywords={Generative adversarial networks;Generators;Superresolution;Image reconstruction;Training;Feature extraction;Tile images;generative adversarial networks;style transfer;image super-resolution magnification},
  doi={10.1109/ACCESS.2022.3218636},
  ISSN={2169-3536},
  month={},}@INBOOK{10016457,
  author={Joshi, Dhiraj and Desai, Nirmit and Prosad Chowdhury, Shyama and Lee, Wei&#x2010;Han and Bathen, Luis and Wang, Shiqiang and Verma, Dinesh},
  booktitle={IoT for Defense and National Security}, 
  title={AI at the Edge: Challenges, Applications, and Directions}, 
  year={2023},
  volume={},
  number={},
  pages={133-160},
  abstract={Robotic devices have several applications in commercial and defense IoT establishments. However, robots may not always have the capacity to run complex AI based applications, and high speed connectivity to exploit applications in cloud or data centers may not always be present. This situation arises in both defense and commercial contexts, with defense environments lacking sufficient network stability, and commercial environments concerned about data privacy and communications costs. The exploitation of AI capabilities at the edge can enable many use&#x2010;cases by bypassing issues with network connectivity. At IBM Research, we have been developing Distributed AI technology and working on several IoT use cases using a robotic dog to enable applications such as visual and thermal inspections to identify anomalous conditions in industrial assets. In this chapter, we will lay out those use&#x2010;cases, and discuss key Distributed AI components that enable such IoT applications, and how a robotic environment allows for new capabilities such as re&#x2010;positioning a robotic sensor for optimal sensing. We will also discuss relevance of the use&#x2010;cases to a military environment.},
  keywords={Inspection;Visualization;Artificial intelligence;Monitoring;Cameras;Temperature sensors;Temperature measurement},
  doi={10.1002/9781119892199.ch9},
  ISSN={},
  publisher={IEEE},
  isbn={9781119892182},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/10016457},}@ARTICLE{10600471,
  author={Nag, Anindya and Mondal, Hirak and Mehedi Hassan, Md and Al-Shehari, Taher and Kadrie, Mohammed and Al-Razgan, Muna and Alfakih, Taha and Biswas, Sujit and Kumar Bairagi, Anupam},
  journal={IEEE Access}, 
  title={TumorGANet: A Transfer Learning and Generative Adversarial Network- Based Data Augmentation Model for Brain Tumor Classification}, 
  year={2024},
  volume={12},
  number={},
  pages={103060-103081},
  abstract={Diagnosing brain tumors using magnetic resonance imaging (MRI) presents significant challenges due to the complexities of segmentation and the variability in tumor characteristics. To address the limitations inherent in traditional methods, this research employs an advanced deep learning approach, integrating ResNet50 for feature extraction and Generative Adversarial Networks (GANs) for data augmentation. A comprehensive evaluation of ten transfer learning algorithms, including GoogLeNet and VGG-16, was conducted for the classification of brain tumors. Model performance was assessed using precision, recall, and F1-score metrics, complemented by additional metrics such as Hamming loss and the Matthews correlation coefficient to provide a more comprehensive insight. To ensure transparency in image predictions, Explainable AI techniques, specifically Local Interpretable Model-Agnostic Explanations (LIME), were utilized. The study involved the analysis of 7023 MRI images, with TumorGANet being trained on a dataset encompassing gliomas, meningiomas, non-tumorous cases, and pituitary tumors. The results demonstrate the exceptional performance of proposed model named TumorGANet, achieving an accuracy of 99.53%, precision and recall rates of 100%, F1 scores of 99%, and a Hamming loss of 0.2%.},
  keywords={Tumors;Magnetic resonance imaging;Brain modeling;Data models;Feature extraction;Testing;Data augmentation;Brain imaging;brain tumor;transfer learning;generative adversarial network;explainable AI},
  doi={10.1109/ACCESS.2024.3429633},
  ISSN={2169-3536},
  month={},}@ARTICLE{10663680,
  author={Deng, Yunlong and Peng, Tao and Wang, Bangchao and Wu, Gan},
  journal={IEEE Transactions on Network and Service Management}, 
  title={ANDE: Detect the Anonymity Web Traffic With Comprehensive Model}, 
  year={2024},
  volume={21},
  number={6},
  pages={6924-6936},
  abstract={The escalating growth of network technology and users poses critical challenges to network security. This paper introduces ANDE, a novel framework designed to enhance the classification accuracy of anonymity networks. ANDE incorporates both raw data features and statistical features extracted from network traffic. Raw data features are transformed into images, enabling recognition and classification using robust image domain models. ANDE combines an enhanced Squeeze-and-Excitation (SE) ResNet with Multilayer Perceptrons (MLP), facilitating concurrent learning and classification of both feature types. Extensive experiments on two publicly available datasets demonstrate the superior performance of ANDE compared to traditional machine learning and deep learning methods. The comprehensive evaluation underscores ANDE’s effectiveness in accurately classifying network traffic within anonymity networks. Additionally, this study empirically validates the efficacy of the SE block in augmenting the classification capabilities of the proposed framework, establishing ANDE as a promising solution for network traffic classification in the realm of network security.},
  keywords={Peer-to-peer computing;Feature extraction;Telecommunication traffic;Deep learning;Network security;Data models;Servers;Anonymity Web;traffic classification;deep learning;squeeze-and-excitation networks},
  doi={10.1109/TNSM.2024.3453917},
  ISSN={1932-4537},
  month={Dec},}@INPROCEEDINGS{9060298,
  author={Zhang, Qiuyun and Guo, Bin and Wang, Hao and Liang, Yunji and Hao, Shaoyang and Yu, Zhiwen},
  booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, 
  title={AI-Powered Text Generation for Harmonious Human-Machine Interaction: Current State and Future Directions}, 
  year={2019},
  volume={},
  number={},
  pages={859-864},
  abstract={In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation.},
  keywords={Gallium nitride;Task analysis;Learning (artificial intelligence);Machine learning;Recurrent neural networks;Generators;Data models;text generation;deep learning;dialog system},
  doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00176},
  ISSN={},
  month={Aug},}@ARTICLE{10475594,
  author={Wang, Bohua and Tian, Zhiqiang and Ye, Aixue and Wen, Feng and Du, Shaoyi and Gao, Yue},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Generative Variational-Contrastive Learning for Self-Supervised Point Cloud Representation}, 
  year={2024},
  volume={46},
  number={9},
  pages={6154-6166},
  abstract={Self-supervised representation learning for 3D point clouds has attracted increasing attention. However, existing methods in the field of 3D computer vision generally use fixed embeddings to represent the latent features, and impose hard constraints on the embeddings to make the latent feature values of the positive samples converge to consistency, which limits the ability of feature extractors to generalize over different data domains. To address this issue, we propose a Generative Variational-Contrastive Learning (GVC) model, where Gaussian distribution is used to construct a continuous, smoothed representation of the latent features. A distribution constraint and cross-supervision are constructed to improve the transfer ability of the feature extractor over synthetic and real-world data. Specifically, we design a variational contrastive module to constrain the feature distribution instead of feature values corresponding to each sample in the latent space. Moreover, a generative cross-supervision module is introduced to preserve the invariance features and promote the consistency of feature distribution among positive samples. Experimental results demonstrate that GVC achieves SOTA on different downstream tasks. In particular, with only pre-training on the synthetic dataset, GVC achieves a lead of 8.4% and 14.2% when transferring to the real-world dataset in the linear classification and few-shot classification.},
  keywords={Feature extraction;Self-supervised learning;Point cloud compression;Three-dimensional displays;Synthetic data;Task analysis;Solid modeling;Contrastive learning;generative learning;point cloud;self-supervised;variational inference},
  doi={10.1109/TPAMI.2024.3378708},
  ISSN={1939-3539},
  month={Sep.},}@INPROCEEDINGS{10474812,
  author={Doriya, Rohit and Lachure, Jaykumar and Doriya, Rajesh},
  booktitle={2024 Third International Conference on Power, Control and Computing Technologies (ICPC2T)}, 
  title={Robust and Explainable AI: Auto-Augment with Label Preservation and Saliency Parameters}, 
  year={2024},
  volume={},
  number={},
  pages={298-303},
  abstract={Artificial intelligence (AI) relies on models that are reliable and simple to comprehend. Generative AI is a subset of AI as it can generate something new and unique from random noise or existing data inputs regarding an image, text, and data. In auto-augmentation, a generative AI can help developers better plan and estimate work. This research covers two critical areas of AI research: auto-augmentation and explainability. Additionally, our approach makes machine learning models more accessible to read by maintaining the purity of labels and adding saliency factors for better performance. The data is changed using auto-augmentation methods to improve model adaptation during training. These changes could alter ground truth labels, impacting how well the model works. We offer a label preservation way to rectify issues, ensuring data enhancement processes maintain label consistency. Applying the suggested method's saliency parameters makes it easier to understand how the model's forecasts work, which increases their dependability and openness. Using intermediate layer models without knowing anything about the domain makes hard-positive cases that keep the original labels. It improves performance across disciplines. Seeking model explainability is done with parameter saliency maps. It makes it easier to understand how models behave by finding and studying the network factors that lead to bad decisions. Using a ResNet18 classifier, the suggested method is tested thoroughly on the CIFAR100 dataset.},
  keywords={Training;Adaptation models;Generative AI;Computational modeling;Predictive models;Data augmentation;Data models;Deep Neural Networks;data augmentation;adversarial auto-augment;parameter saliency maps;model ex-plainability},
  doi={10.1109/ICPC2T60072.2024.10474812},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10910541,
  author={Sudha, S and K, Janikaa and Rajkumar, Nithya and VK, Tamizhini},
  booktitle={2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)}, 
  title={Intent Dataset Creation for Indian Tourism Based Chatbot}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={In recent years, the development of intelligent chatbots and generative Artificial Intelligence (AI) models has revolutionized the way users interact with digital systems, particularly in the domain of tourism. There is, however, a severe lack of data available for Indian tourism to train chatbots. This study proposes a new dataset specifically designed for classifying requests related to Indian tourism based on their purpose. For generative AI, intent categorization is essential for correctly defining the intention of a query and giving an appropriate answer. It aims at offering precise, contextualized interactions while avoiding ambiguous responses in order to enhance customer contentment. The dataset addresses various user intents including inquiries for information, reservations, planning activities and recommendations among others. The intents are split into 19 distinct types. With this dataset, chatbot systems will be able to understand and respond more accurately to user inquiries, thereby improving user experience. The paper provides a detailed discussion about how this was achieved, how the dataset is structured and its potential applications in the development of tourismrelated chatbots.},
  keywords={Generative AI;Digital systems;Chatbots;User experience;Planning;intent;datasets;chatbot;tourism},
  doi={10.1109/ICDDS62937.2024.10910541},
  ISSN={},
  month={Dec},}@ARTICLE{10599804,
  author={Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Class-Incremental Learning: A Survey}, 
  year={2024},
  volume={46},
  number={12},
  pages={9851-9873},
  abstract={Deep models, e.g., CNNs and Vision Transformers, have achieved impressive achievements in many vision tasks in the closed world. However, novel classes emerge from time to time in our ever-changing world, requiring a learning system to acquire new knowledge continually. Class-Incremental Learning (CIL) enables the learner to incorporate the knowledge of new classes incrementally and build a universal classifier among all seen classes. Correspondingly, when directly training the model with new class instances, a fatal problem occurs — the model tends to catastrophically forget the characteristics of former ones, and its performance drastically degrades. There have been numerous efforts to tackle catastrophic forgetting in the machine learning community. In this paper, we survey comprehensively recent advances in class-incremental learning and summarize these methods from several aspects. We also provide a rigorous and unified evaluation of 17 methods in benchmark image classification tasks to find out the characteristics of different algorithms empirically. Furthermore, we notice that the current comparison protocol ignores the influence of memory budget in model storage, which may result in unfair comparison and biased results. Hence, we advocate fair comparison by aligning the memory budget in evaluation, as well as several memory-agnostic performance measures.},
  keywords={Task analysis;Training;Surveys;Data models;Birds;Electronics packaging;Dogs;Catastrophic forgetting;class-incremental learning;continual learning;lifelong learning},
  doi={10.1109/TPAMI.2024.3429383},
  ISSN={1939-3539},
  month={Dec},}@ARTICLE{8491271,
  author={Jiang, Congmei and Mao, Yongfang and Chai, Yi and Yu, Mingbiao and Tao, Songbing},
  journal={IEEE Access}, 
  title={Scenario Generation for Wind Power Using Improved Generative Adversarial Networks}, 
  year={2018},
  volume={6},
  number={},
  pages={62193-62203},
  abstract={Wind power scenarios have a significant impact on stochastic optimization problems for power systems in which wind power is a significant component. Generative adversarial networks (GANs) are a powerful class of generative models, and can generate realistic scenarios for renewable power sources without the need for any modeling assumptions. However, the performance of GANs in generating scenarios can further be improved by modifying the way in which a Lipschitz constraint on discriminator network is imposed. Another critical problem of applying deep neural networks is overfitting, a phenomenon especially prone to appear on small training sets. In this paper, we propose an improved GAN for the generation of wind power scenarios. To improve the training speed, we use a gradient penalty term to enforce the Lipschitz constraint based on the output and input of the discriminator network. To improve the scenario quality, we further use a consistency term in the training procedure. Besides, the overfitting problem can be effectively alleviated by the enforced Lipschitz continuity. The proposed method is applied to actual time series data from the NREL wind integration data set. The experimental results demonstrate that our method outperforms the existing methods.},
  keywords={Wind power generation;Gallium nitride;Training;Production;Uncertainty;Wind forecasting;Autoregressive processes;Deep learning;generative adversarial networks;scenario generation;wind power},
  doi={10.1109/ACCESS.2018.2875936},
  ISSN={2169-3536},
  month={},}@ARTICLE{10094019,
  author={Huang, Tao and Yuan, Xin and Dong, Weisheng and Wu, Jinjian and Shi, Guangming},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deep Gaussian Scale Mixture Prior for Image Reconstruction}, 
  year={2023},
  volume={45},
  number={9},
  pages={10778-10794},
  abstract={Image reconstruction from partial observations has attracted increasing attention. Conventional image reconstruction methods with hand-crafted priors often fail to recover fine image details due to the poor representation capability of the hand-crafted priors. Deep learning methods attack this problem by directly learning mapping functions between the observations and the targeted images can achieve much better results. However, most powerful deep networks lack transparency and are nontrivial to design heuristically. This paper proposes a novel image reconstruction method based on the Maximum a Posterior (MAP) estimation framework using learned Gaussian Scale Mixture (GSM) prior. Unlike existing unfolding methods that only estimate the image means (i.e., the denoising prior) but neglected the variances, we propose characterizing images by the GSM models with learned means and variances through a deep network. Furthermore, to learn the long-range dependencies of images, we develop an enhanced variant based on the Swin Transformer for learning GSM models. All parameters of the MAP estimator and the deep network are jointly optimized through end-to-end training. Extensive simulation and real data experimental results on spectral compressive imaging and image super-resolution demonstrate that the proposed method outperforms existing state-of-the-art methods.},
  keywords={Image reconstruction;Imaging;GSM;Transformers;Image coding;Reconstruction algorithms;Superresolution;Gaussian scale mixture prior;maximum a posterior;spectral compressive imaging;single image super-resolution;swin transformer},
  doi={10.1109/TPAMI.2023.3265103},
  ISSN={1939-3539},
  month={Sep.},}@ARTICLE{9868157,
  author={Zhang, Rui and Zhang, Yunxing and Lu, Chengjun and Li, Xuelong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Unsupervised Graph Embedding via Adaptive Graph Learning}, 
  year={2023},
  volume={45},
  number={4},
  pages={5329-5336},
  abstract={Graph autoencoders (GAEs) are powerful tools in representation learning for graph embedding. However, the performance of GAEs is very dependent on the quality of the graph structure, i.e., of the adjacency matrix. In other words, GAEs would perform poorly when the adjacency matrix is incomplete or be disturbed. In this paper, two novel unsupervised graph embedding methods, unsupervised graph embedding via adaptive graph learning (BAGE) and unsupervised graph embedding via variational adaptive graph learning (VBAGE) are proposed. The proposed methods expand the application range of GAEs on graph embedding, i.e, on the general datasets without graph structure. Meanwhile, the adaptive learning mechanism can initialize the adjacency matrix without being affected by the parameter. Besides that, the latent representations are embedded with the Laplacian graph structure to preserve the topology structure of the graph in the vector space. Moreover, the adjacency matrix can be self-learned for better embedding performance when the original graph structure is incomplete. With adaptive learning, the proposed method is much more robust to the graph structure. Experimental studies on several datasets validate our design and demonstrate that our methods outperform baselines by a wide margin in node clustering, node classification, link prediction, and graph visualization tasks.},
  keywords={Laplace equations;Graph neural networks;Adaptation models;Adaptive learning;Task analysis;Decoding;Topology;Graph embedding;adaptive graph learning;graph autoencoder},
  doi={10.1109/TPAMI.2022.3202158},
  ISSN={1939-3539},
  month={April},}@ARTICLE{9146819,
  author={Li, Jinning and Zhou, Yichen and Ding, Jie and Chen, Cen and Yang, Xulei},
  journal={IEEE Access}, 
  title={ID Preserving Face Super-Resolution Generative Adversarial Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={138373-138381},
  abstract={We propose an ID Preserving Face Super-Resolution Generative Adversarial Networks (IP-FSRGAN) to reconstruct realistic super-resolution face images from low-resolution ones. Inspired by the success of generative adversarial networks (GAN), we introduce a novel ID preserving module to help the generator learn to infer the facial details and synthesize more realistic super-resolution faces. Our method produces satisfactory visual results and also quantitatively outperforms state-of-the-art super-resolution methods on the face datasets including CASIA-Webface, CelebA, and LFW datasets under the metrics of PSNR, SSIM, and cosine similarity. In addition, we propose a framework to apply IP-FSRGAN model to address the face verification task on low-resolution face images. The synthesized  $4\times $  super-resolution faces achieve a verification accuracy of 97.6%, improved from 92.8% of low resolution faces. We also prove by experiments that the proposed IP-FSRGAN model demonstrates excellent robustness under different downsample scaling factors and extensibility to various face verification models.},
  keywords={Face;Image resolution;Gallium nitride;Task analysis;Face recognition;Feature extraction;Image reconstruction;ID preserving;face super-resolution;generative adversarial networks;face verification},
  doi={10.1109/ACCESS.2020.3011699},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11004934,
  author={Almoqren, Nuha and Alrashoud, Mubarak},
  booktitle={2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)}, 
  title={A Smart Framework for Optimizing User Feedback Prioritization in Application Development}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In mobile app development, user reviews are a significant source of requirements. Users frequently report bugs, request new features, or suggest enhancements. Mobile app vendors aim to maximize user satisfaction by addressing these continuous comments and requests as early as possible. Typically, they prioritize delivering the most promising features, extracted from user reviews, in early releases while deferring fewer promising ones to later stages. However, due to the massive volume of reviews, redundancy, and conflicts among them, manually extracting requirements is inefficient and often challenging, making requirement prioritization even more difficult. Therefore, automating this process is essential. This paper presents a conceptual framework for the requirements prioritization process's automation, continuity, and scalability. The proposed framework follows a hybrid approach that integrates multiple advanced techniques: generative artificial intelligence, active learning, ontologies, and optimization algorithms. Generative artificial intelligence enables the identification of important patterns and the automatic extraction of requirements and their properties, which aids in assessing properties to determine requirement priorities. The generative artificial intelligence framework integrates with an active learning system to improve annotation efficiency. Ontologies help comprehend relationships, properties, and dependencies among requirements, aligning them with domain-specific knowledge. Optimization methods playa crucial role in the requirements prioritization process by computing the weights of various properties to identify the most effective combination of requirements and determine the optimal order for implementation. Consequently, this research presents a smart theoretical framework for enhancing user-driven maintenance and development of mobile applications. Researchers are tackling several critical challenges that remain unresolved in the field, with future directions focusing on evaluating its applicability and effectiveness in real-world scenarios.},
  keywords={Technological innovation;Reviews;Generative AI;Active learning;Ontologies;Feature extraction;Software;Mobile applications;Software measurement;Software development management;Software Requirements;App Reviews;Generative AI;Optimization;Mobile Applications;Smart Framework},
  doi={10.1109/ITIKD63574.2025.11004934},
  ISSN={},
  month={April},}@ARTICLE{11072464,
  author={Hsu, Gee-Sern Jison and Lin, Wei-Jun and Hsieh, Wei-Chun and Jian, Wei-Zhe and Chung, Sheng-Luen and Gavrilova, Marina L.},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Style-Preserving Generator for Synthetic License Plate Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={We propose the Style-Preserving Generator (SPG) to generate synthetic license plate data to train License Plate Recognition (LPR) models, and compare the performance with the same models trained on real-world data. The proposed SPG can edit the characters on real-world license plates while maintaining their original styles, allowing synthetic license plate data to be generated with user-specified characters. We can therefore synthesize license plates with desired characters to effectively alleviate the data attribute imbalance and privacy issues associated with real-world license plates. To the best of our knowledge, this work is the first study to present the making of synthetic LP data by proposing a novel text-editing approach tailor-made for LP data, that is the proposed SPG. The SPG consists of a transformer, a source encoder, a source style encoder, a character mask decoder, a target generator, and a target discriminator. Given a source license plate image and a specified text as input, these components collaborate to compute the self- and cross-attention embeddings, predict character masks, and generate a synthetic license plate in the source style but with source characters replaced by the specified characters. We adopt a two-phase training scheme. Phase 1 training uses synthetic data only, but Phase 2 training uses synthetic and real-life data. To showcase the effectiveness of the SPG, we introduce a new benchmark dataset, the LP-2025 (License Plate 2025), which alleviates the limitations of existing datasets and presents new challenges for license plate recognition and generative models. We validate SPG performance on the LP-2025 dataset and other benchmark datasets and compare it against state-of-the-art text-editing approaches.},
  keywords={License plate recognition;Training;Generators;Data models;Circuits and systems;Benchmark testing;Artificial intelligence;Synthetic data;Decoding;Codes;Generative AI;License Plate Recognition;Benchmark Database},
  doi={10.1109/TCSVT.2025.3586805},
  ISSN={1558-2205},
  month={},}@ARTICLE{11077663,
  author={Zeng, Xiaolu and Zhao, Han and Zhong, Shichao and Yin, Zixiang and Yang, Xiaopeng},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={LFHRNet: Less-Forgetting High-Resolution Network for Through-Wall Radar Imaging}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Through-wall radar (TWR) imaging is broadly applied in the detection of enclosed space, which plays an important role in security, rescue and military operations. However, the resolution of the existing TWR imaging method is not high enough to serve the practical application because of the physical constraints such as the limited antenna aperture. The deep learning method can improve the resolution due to its powerful reasoning capabilities, but it is challenging and costly to build comprehensive TWR datasets for training in practice. To address these challenges, this paper proposes a less-forgetting high-resolution network (LFHRNet) for through-wall radar imaging. Firstly, a cGAN network is trained on the source data as the pre-trained network to initialize the target network. The target network LFHRNet integrates two parallel branches by a router network. It forces a branch to focus on learning knowledge to solve target tasks, while another is frozen to alleviate the catastrophic forgetting. Finally, during the online phase, the low-resolution radar image in both source and target domains can be input into LFHRNet to get the high-resolution image. Simulation and practical experimental results show that LFHRNet can learn knowledge to reconstruct the shapes of the objects in target domain, while maintaining the source knowledge to reconstruct the shapes of the objects in source domain.},
  keywords={Radar imaging;Imaging;Radar;Shape;Reflection;Image resolution;Image reconstruction;Antenna arrays;Training;Delays;Through-wall radar imaging;generative adversarial networks;transfer learning;less forgetting network},
  doi={10.1109/TAES.2025.3587633},
  ISSN={1557-9603},
  month={},}@ARTICLE{10224552,
  author={Ghaleb, Fuad A. and Saeed, Faisal and Al-Sarem, Mohammed and Qasem, Sultan Noman and Al-Hadhrami, Tawfik},
  journal={IEEE Access}, 
  title={Ensemble Synthesized Minority Oversampling-Based Generative Adversarial Networks and Random Forest Algorithm for Credit Card Fraud Detection}, 
  year={2023},
  volume={11},
  number={},
  pages={89694-89710},
  abstract={The recent increase in credit card fraud is rapidly has caused huge monetary losses for individuals and financial institutions. Most credit card frauds are conducted online by illegally obtaining payment credentials through data breaches, phishing, or scamming. Many solutions have been suggested to address the credit card fraud problem for online transactions. However, the high-class imbalance is the major challenge that faces the existing solutions to construct an effective detection model. Most of the existing techniques used for class imbalance overestimate the distribution of the minority class, resulting in highly overlapped or noisy and unrepresentative features, which cause either overfitting or imprecise learning. In this study, a credit card fraud detection model (CCFDM) is proposed based on ensemble learning and a generative adversarial network (GAN) assisted by Ensemble Synthesized Minority Oversampling techniques (ESMOTE-GAN). Multiple subsets were extracted using under-sampling and SMOTE was applied to generate less skewed sets to prevent the GAN from modeling the noise. These subsets were used to train diverse sets of GAN models to generate the synthesized subsets. A set of Random Forest classifiers was then trained based on the proposed ESMOTE-GAN technique. The probabilistic outputs of the trained classifiers were combined using a weighted voting scheme for decision-making. The results show that the proposed model achieved 1.9%, and 3.2% improvements in overall performance and the detection rate, respectively, with a 0% false alarm rate. Due to the massive number of transactions, even a tiny false positive rate can overwhelm the analysis team. Thus, the proposed model has improved the detection performance and reduced the cost needed for manual analysis.},
  keywords={Fraud;Credit cards;Generative adversarial networks;Classification algorithms;Training;Noise measurement;Feature extraction;Class imbalance;credit card fraud detection;GAN;Random Forest;SMOTE},
  doi={10.1109/ACCESS.2023.3306621},
  ISSN={2169-3536},
  month={},}@ARTICLE{9439890,
  author={Randhawa, Rizwan Hamid and Aslam, Nauman and Alauthman, Mohammad and Rafiq, Husnain and Comeau, Frank},
  journal={IEEE Access}, 
  title={Security Hardening of Botnet Detectors Using Generative Adversarial Networks}, 
  year={2021},
  volume={9},
  number={},
  pages={78276-78292},
  abstract={Machine learning (ML) based botnet detectors are no exception to traditional ML models when it comes to adversarial evasion attacks. The datasets used to train these models have also scarcity and imbalance issues. We propose a new technique named Botshot, based on generative adversarial networks (GANs) for addressing these issues and proactively making botnet detectors aware of adversarial evasions. Botshot is cost-effective as compared to the network emulation for botnet traffic data generation rendering the dedicated hardware resources unnecessary. First, we use the extended set of network flow and time-based features for three publicly available botnet datasets. Second, we utilize two GANs (vanilla, conditional) for generating realistic botnet traffic. We evaluate the generator performance using classifier two-sample test (C2ST) with 10-fold 70-30 train-test split and propose the use of 'recall' in contrast to 'accuracy' for proactively learning adversarial evasions. We then augment the train set with the generated data and test using the unchanged test set. Last, we compare our results with benchmark oversampling methods with augmentation of additional botnet traffic data in terms of average accuracy, precision, recall and F1 score over six different ML classifiers. The empirical results demonstrate the effectiveness of the GAN-based oversampling for learning in advance the adversarial evasion attacks on botnet detectors.},
  keywords={Botnet;Generative adversarial networks;Detectors;Training;Generators;Biological system modeling;Feature extraction;Botnet detection;GANs;adversarial evasion attacks;unbalanced datasets},
  doi={10.1109/ACCESS.2021.3083421},
  ISSN={2169-3536},
  month={},}@ARTICLE{10466540,
  author={Kolekar, Maheshkumar H. and Bose, Samprit and Pai, Abhishek},
  journal={IEEE Access}, 
  title={SARain-GAN: Spatial Attention Residual UNet Based Conditional Generative Adversarial Network for Rain Streak Removal}, 
  year={2024},
  volume={12},
  number={},
  pages={43874-43888},
  abstract={Deraining of images plays a pivotal role in computer vision by addressing the challenges posed by rain, enhancing visibility, and refining image quality by eliminating rain streaks. Traditional methods often fall short of effectively handling intricate rain patterns, resulting in incomplete removal. In this paper, we propose an innovative deep learning-based deraining model leveraging a modified residual UNet and a multiscale attention-guided convolutional neural network module as a discriminator within a conditional generative adversarial network framework. The proposed approach introduces custom hyperparameters and a tailored loss function to facilitate the efficient removal of rain streaks from images. Evaluation on both synthetic and real-world datasets showcases superior performance, as indicated by improved image evaluation metrics such as PSNR, SSIM, and NIQE. The effectiveness of our model extends to improving both rainy and foggy images. We also conducted a comparative analysis of computational complexity in terms of running time, GFLOPs, and no. of parameters against other state-of-the-art methods to demonstrate our model’s superiority.},
  keywords={Rain;Generators;Generative adversarial networks;Adaptation models;Visualization;Computational modeling;Training;Image analysis;Image enhancement;Edge computing;Computer vision;Synthetic data;Image classification;Image deraining;deep learning;residual UNet;foggy image enhancement},
  doi={10.1109/ACCESS.2024.3375909},
  ISSN={2169-3536},
  month={},}@ARTICLE{10649559,
  author={Ali, Abid and Sharif, Muhammad and Muhammad Shahzad Faisal, Ch and Rizwan, Atif and Atteia, Ghada and Alabdulhafith, Maali},
  journal={IEEE Access}, 
  title={Brain Tumor Segmentation Using Generative Adversarial Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={183525-183541},
  abstract={Deep learning has played a vital role in advancing medical research, particularly in brain tumor segmentation. Despite using numerous deep learning algorithms for this purpose, accurately and reliably segmenting brain tumors remains a significant challenge. Segmentation of precise tumors is essential for the effective treatment of brain diseases. While deep learning offers a range of algorithms for segmentation, they still face limitations when analyzing medical images due to the variations in tumor shape, size, and location. This study proposes a deep learning approach combining a Generative Adversarial Network (GAN) with transfer learning and auto-encoder techniques to enhance brain tumor segmentation. The GAN incorporates a generator and discriminator to generate superior segmentation outcomes. In the generator, we applied downsampling and upsampling for tumor segmentation. In addition, an auto-encoder is applied in which the encoder retains as much information as possible and then the decoder with those encodings reconstructs the image. The transfer learning technique is applied at the bottleneck using the DenseNet model. Combining auto-encoder techniques with transfer learning methodologies in GANs feature learning is enhanced, training time is reduced, and stability is increased. In this work, we enhanced the accuracy of brain tumor segmentation and even achieved better results for tumors having small sizes. We train and evaluate our proposed model using the publicly available BraTS 2021 dataset. The experimental result shows a dice score of 0.94 for the whole tumor, 0.86 for the tumor core, and 0.82 for the enhancing tumor. It is also shown that we achieve 2% to 4% higher accuracy than other methods.},
  keywords={Tumors;Image segmentation;Brain modeling;Deep learning;Generative adversarial networks;Accuracy;Transfer learning;Autoencoders;Deep learning;GAN;auto-encoder;up sampling;down sampling;transfer learning;BraTS;DenseNet},
  doi={10.1109/ACCESS.2024.3450593},
  ISSN={2169-3536},
  month={},}@ARTICLE{10081368,
  author={Woo, Bing Hong and Tham, Mau-Luen and Chua, Sing Yee},
  journal={IEEE Access}, 
  title={Adaptive Coarse-to-Fine Single Pixel Imaging With Generative Adversarial Network Based Reconstruction}, 
  year={2023},
  volume={11},
  number={},
  pages={31024-31035},
  abstract={Single Pixel Imaging (SPI) that only uses one light intensity sensor has been researched extensively as an alternative imaging method. It is proven to work at low light conditions and unconventional wavelength bands where the classical pixel array sensors are limited. However, the major issues of SPI remain as the image quality and computational efficiency. Thus, this paper proposes an adaptive coarse-to-fine (C2F) sampling method to replace the typical uniform sampling method to achieve image reconstruction with better quality. This scalable sampling mechanism is adaptive to the target scene as it will progressively sample according to the image complexity and quality indicator. Subsequently, a deep Generative Adversarial Network (GAN) model is also proposed to improve the time efficiency of the multi-scale image reconstruction. The results show that C2F sampling consistently outperforms uniform sampling in terms of image quality (21% in SSIM, 8% in PSNR and 17% in RMSE). Besides, improvement in the efficiency is also achieved by the proposed GAN reconstruction, whereby the total time taken is only 0.025% of the time taken for the conventional L1 reconstruction method ( $\approx 4000$  times faster). In conclusion, the proposed adaptive C2F SPI using GAN reconstruction method can serve as an optimised solution to improve both the image quality and computational efficiency in SPI.},
  keywords={Image reconstruction;Image resolution;Deep learning;Image quality;Generative adversarial networks;Iterative methods;Coarse-to-fine;compressed sensing;deep learning;GAN;single pixel imaging},
  doi={10.1109/ACCESS.2023.3262165},
  ISSN={2169-3536},
  month={},}@ARTICLE{10778271,
  author={Ud Din, Ikram and Almogren, Ahmad and Han, Zhu and Guizani, Mohsen},
  journal={IEEE Internet of Things Journal}, 
  title={Building Reliable IoT Ecosystems: A Generative AI-Enabled Federated Learning-Based Trust Management Approach}, 
  year={2025},
  volume={12},
  number={10},
  pages={13353-13366},
  abstract={In the rapidly evolving domain of the Internet of Vehicles (IoV), ensuring robust trust management, privacy, and security presents significant challenges. This article proposes a novel approach integrating generative AI (GAI) and federated learning (FL) to address these challenges. FL allows distributed learning across vehicles without the need to share data, enhancing privacy compared to centralized methods. Our approach enhances trust management by raising the level of accuracy in detecting anomalies and preserving data privacy. As a result, the effectiveness of the proposed approach in practical real-world urban settings is illustrated by comprehensive evaluations using the CityPulse dataset. The results show a 20% improvement in trust scores under normal conditions, a 92% anomaly detection accuracy, and acceptable latency despite the added security measures. Additionally, 3-D visualizations illustrate the system’s robustness and scalability. This solution aligns with the objectives of 6G wireless communications, laying the groundwork for future intelligent, ultrareliable, and secure vehicular networks. Future research will focus on expanding the application of GAI and FL for real-time decision-making in large-scale IoV networks and optimizing cryptographic protocols.},
  keywords={Security;Trust management;Internet of Things;Data privacy;Privacy;Data models;Vehicle dynamics;Heuristic algorithms;Computational modeling;Training;Blockchain security;decentralized networks;generative artificial intelligence (GAI);Internet of Vehicles (IoV);trust management},
  doi={10.1109/JIOT.2024.3511634},
  ISSN={2327-4662},
  month={May},}@ARTICLE{10314505,
  author={Böhland, Moritz and Bruch, Roman and Bäuerle, Simon and Rettenberger, Luca and Reischl, Markus},
  journal={IEEE Access}, 
  title={Improving Generative Adversarial Networks for Patch-Based Unpaired Image-to-Image Translation}, 
  year={2023},
  volume={11},
  number={},
  pages={127895-127906},
  abstract={Deep learning models for image segmentation achieve high-quality results, but need large amounts of training data. Training data is primarily annotated manually, which is time-consuming and often not feasible for large-scale 2D and 3D images. Manual annotation can be reduced using synthetic training data generated by generative adversarial networks that perform unpaired image-to-image translation. As of now, large images need to be processed patch-wise during inference, resulting in local artifacts in border regions after merging the individual patches. To reduce these artifacts, we propose a new method that integrates overlapping patches into the training process. We incorporated our method into CycleGAN and tested it on our new 2D tiling strategy benchmark dataset. The results show that the artifacts are reduced by 85% compared to state-of-the-art weighted tiling. While our method increases training time, inference time decreases. Additionally, we demonstrate transferability to real-world 3D biological image data, receiving a high-quality synthetic dataset. Increasing the quality of synthetic training datasets can reduce manual annotation, increase the quality of model output, and can help develop and evaluate deep learning models.},
  keywords={Generative adversarial networks;Training;Training data;Three-dimensional displays;Benchmark testing;Image color analysis;Task analysis;Large scale integration;GAN;unpaired image-to-image translation;3D image synthesis;stitching;CycleGAN;tiling;large-scale},
  doi={10.1109/ACCESS.2023.3331819},
  ISSN={2169-3536},
  month={},}@ARTICLE{10689578,
  author={Noor, Jannatun and Mahmud, Abrar and Rahman, Moh. Absar and Sifar, Alimus and Mostafa, Fateen Yusuf and Tasnova, Lamia and Chellappan, Sriram},
  journal={IEEE Access}, 
  title={SAwareSSGI: Surrounding-Aware Screen-Space Global Illumination Using Generative Adversarial Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={139946-139961},
  abstract={Global Illumination (GI) is a technique that is employed in computer graphics to enhance realism. Various methods have been used to achieve this using computer-generated imagery. The most precise method involves conventional ray tracing, which yields highly realistic results but is computationally intensive and unsuitable for real-time applications. Alternatively, faster algorithms utilize post-processing on rasterization, making them more suitable for real-time scenarios. However, these algorithms are also resource-intensive and may produce inaccurate lighting due to limited information on screen-space features. our proposal involves utilizing a Generative Adversarial Network (GAN) approach to achieve real-time GI effects, following the methodology of conventional screen-space GI techniques. We take surrounding graphical information into account by going beyond screen-space and producing consistent GI effects that are comparatively closer to their physically correct ray-tracing counterpart. Moreover, our model provides a better quality of generated output than the other recent model which utilized a similar approach by scoring 0.90811 in SSIM, 0.00093 in MSE, and 30.30576 dB in PSNR on our developed dataset.},
  keywords={Lighting;Ray tracing;Real-time systems;Three-dimensional displays;Graphics;Rendering (computer graphics);Generative adversarial networks;Computer graphics;Neural networks;Computer graphics;global illumination;GAN;neural networks},
  doi={10.1109/ACCESS.2024.3467102},
  ISSN={2169-3536},
  month={},}@ARTICLE{9330536,
  author={Luo, Mandi and Cao, Jie and Ma, Xin and Zhang, Xiaoyu and He, Ran},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={FA-GAN: Face Augmentation GAN for Deformation-Invariant Face Recognition}, 
  year={2021},
  volume={16},
  number={},
  pages={2341-2355},
  abstract={Substantial improvements have been achieved in the field of face recognition due to the successful application of deep neural networks. However, existing methods are sensitive to both the quality and quantity of the training data. Despite the availability of large-scale datasets, the long tail data distribution induces strong biases in model learning. In this paper, we present a Face Augmentation Generative Adversarial Network (FA-GAN) to reduce the influence of imbalanced deformation attribute distributions. We propose to decouple these attributes from the identity representation with a novel hierarchical disentanglement module. Moreover, Graph Convolutional Networks (GCNs) are applied to recover geometric information by exploring the interrelations among local regions to guarantee the preservation of identities in face data augmentation. Extensive experiments on face reconstruction, face manipulation, and face recognition demonstrate the effectiveness and generalization ability of the proposed method.},
  keywords={Face recognition;Strain;Geometry;Frequency division multiplexing;Training;Task analysis;Semantics;Face augmentation;deformation-invariant face recognition;face disentanglement;graph convolutional networks},
  doi={10.1109/TIFS.2021.3053460},
  ISSN={1556-6021},
  month={},}@ARTICLE{9316964,
  author={Fu, Chaoyou and Hu, Yibo and Wu, Xiang and Wang, Guoli and Zhang, Qian and He, Ran},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={High-Fidelity Face Manipulation With Extreme Poses and Expressions}, 
  year={2021},
  volume={16},
  number={},
  pages={2218-2231},
  abstract={Face manipulation has shown remarkable advances with the flourish of Generative Adversarial Networks. However, due to the difficulties of controlling structures and textures, it is challenging to model poses and expressions simultaneously, especially for the extreme manipulation at high-resolution. In this article, we propose a novel framework that simplifies face manipulation into two correlated stages: a boundary prediction stage and a disentangled face synthesis stage. The first stage models poses and expressions jointly via boundary images. Specifically, a conditional encoder-decoder network is employed to predict the boundary image of the target face in a semi-supervised way. Pose and expression estimators are introduced to improve the prediction performance. In the second stage, the predicted boundary image and the input face image are encoded into the structure and the texture latent space by two encoder networks, respectively. A proxy network and a feature threshold loss are further imposed to disentangle the latent space. Furthermore, due to the lack of high-resolution face manipulation databases to verify the effectiveness of our method, we collect a new high-quality Multi-View Face (MVF-HQ) database. It contains 120,283 images at 6000 × 4000 resolution from 479 identities with diverse poses, expressions, and illuminations. MVF-HQ is much larger in scale and much higher in resolution than publicly available high-resolution face manipulation databases. We will release MVF-HQ soon to push forward the advance of face manipulation. Qualitative and quantitative experiments on four databases show that our method dramatically improves the synthesis quality.},
  keywords={Face recognition;Databases;Image resolution;Faces;Task analysis;Generators;Feature extraction;Face manipulation;extreme pose and expression;high-resolution;MVF-HQ},
  doi={10.1109/TIFS.2021.3050065},
  ISSN={1556-6021},
  month={},}@ARTICLE{10379553,
  author={Liu, Hongmin and Zhang, Qi and Hu, Yufan and Zeng, Hui and Fan, Bin},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Unsupervised Multi-Expert Learning Model for Underwater Image Enhancement}, 
  year={2024},
  volume={11},
  number={3},
  pages={708-722},
  abstract={Underwater image enhancement aims to restore a clean appearance and thus improves the quality of underwater degraded images. Current methods feed the whole image directly into the model for enhancement. However, they ignored that the R, G and B channels of underwater degraded images present varied degrees of degradation, due to the selective absorption for the light. To address this issue, we propose an unsupervised multi-expert learning model by considering the enhancement of each color channel. Specifically, an unsupervised architecture based on generative adversarial network is employed to alleviate the need for paired underwater images. Based on this, we design a generator, including a multi-expert encoder, a feature fusion module and a feature fusion-guided decoder, to generate the clear underwater image. Accordingly, a multi-expert discriminator is proposed to verify the authenticity of the R, G and B channels, respectively. In addition, content perceptual loss and edge loss are introduced into the loss function to further improve the content and details of the enhanced images. Extensive experiments on public datasets demon-strate that our method achieves more pleasing results in vision quality. Various metrics (PSNR, SSIM, UIQM and UCIQE) evaluated on our enhanced images have been improved obviously.},
  keywords={Image color analysis;Image enhancement;Imaging;Image edge detection;Degradation;Training;Task analysis;Multi-expert learning;underwater image enhancement;unsupervised learning},
  doi={10.1109/JAS.2023.123771},
  ISSN={2329-9274},
  month={March},}@ARTICLE{10367809,
  author={Zhang, Chi and Ma, Xiaoning and Xu, Liheng and Lu, Haoang and Wang, Le and Su, Yuanqi and Liu, Yuehu and Li, Li},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Worst Perception Scenario Search via Recurrent Neural Controller and K-Reciprocal Re-Ranking}, 
  year={2024},
  volume={25},
  number={6},
  pages={5612-5626},
  abstract={Achieving excellent generalization on perceiving real traffic scenarios with diversity is the long-term goal for building robust autonomous driving systems. A recent theoretical study shows that the generalization on the worst-group of test samples is far more difficult than others. Therefore, we propose to discover potential shortness of certain perception module by analyzing its worst-scenario performance. However, with the benchmark datasets growing huge and tremendous, exhaustive searching for the worst perception scenario (WPS) seems to be time consuming and unnecessary. To address this, we present an automatic searching scheme empowered by reinforcement learning. In this case, worst scenario mining is formulated as the discrete search on the Visual Operation Design Domain (ODD), namely scenario representation, by optimizing LSTM-RNN controller with the worst-performance reward. Moreover, a time-efficient K-reciprocal re-ranking technique is utilized to match the predicted scenario parameters with existing test data. The proposed method has been validated by finding the most challenging scenarios for various vehicle detectors on KITTI, BDD100k and our own benchmark set EVB. Furthermore, searching performances w.r.t different Visual ODDs are investigated and it is found that visual representations through generative adversarial network contribute to a better performance.},
  keywords={Testing;Visualization;Autonomous vehicles;Feature extraction;Visual perception;Machine learning algorithms;Reinforcement learning;Autonomous driving;Worst scenario search;visual operational design domain;reinforcement learning;K-reciprocal re-ranking},
  doi={10.1109/TITS.2023.3340257},
  ISSN={1558-0016},
  month={June},}@INPROCEEDINGS{10842712,
  author={Joseph, Jeethu and Judy, M.V.},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={Developing an Intelligent Integrated Software Agent for Streamlined Interview Automation and Enhanced Candidate Insights}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={As the field of Conversational AI is advancing rapidly, introducing automated intelligent software agents in human interactions can exhibit human-like communicative be-haviors facilitating in more natural and engaging interactions with humans. These advancements provide opportunities to automate the candidate interview process by replacing human interviewers with intelligent autonomous software agents. Using Conversational AI, agents can ask questions, understand and evaluate response and initiate dynamic conversations closely resembling human interviewers. This automation enhances the efficiency of the overall interview process and ensures consistent and unbiased evaluation leading to more effective and fair hiring practices. This research paper aims to present a comprehensive study on the development and implementation of an AI-driven interview system designed to conduct candidate evaluations in real-time. The system integrates multiple AI agents, each performing different roles such as selecting predefined questions, evaluating candidate response, analyzing speech to assess emotions and sentiments, and synthesizing these analysis to provide separate scores for answers and emotions in the performance assessment.},
  keywords={Emotion recognition;Conversational artificial intelligence;Automation;Speech enhancement;Transformers;Software agents;Real-time systems;Interviews;Recruitment;Testing;AI-Driven Interviews;Automated Candidate Assessment;Multimodal Emotion Detection;Natural Language Processing (NLP);Generative Pre-trained Transformers (GPT)},
  doi={10.1109/IDICAIEI61867.2024.10842712},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11094322,
  author={Zhou, Junjie and Wang, Shouju and Tang, Yuxia and Zhu, Qi and Zhang, Daoqiang and Shao, Wei},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={30886-30895},
  abstract={The prediction of nanoparticles (NPs) distribution is crucial for the diagnosis and treatment of tumors. Recent studies indicate that the heterogeneity of tumor microenvironment (TME) highly affects the distribution of NPs across tumors. Hence, it has become a research hotspot to generate the NPs distribution by the aid of multi-modal TME components. However, the distribution divergence among multi-modal TME components may cause side effects i.e., the best unimodal model may outperform the joint generative model. To address the above issues, we propose a Divergence-Aware Multi-Modal Diffusion model (i.e., DAMM-Diffusion) to adaptively generate the prediction results from uni-modal and multi-modal branches in a unified network. In detail, the uni-modal branch is composed of the U-Net architecture while the multi-modal branch extends it by introducing two novel fusion modules i.e., Multi-Modal Fusion Module (MMFM) and Uncertainty-Aware Fusion Module (UAFM). Specifically, the MMFM is proposed to fuse features from multiple modalities, while the UAFM module is introduced to learn the uncertainty map for cross-attention computation. Following the individual prediction results from each branch, the Divergence-Aware Multi-Modal Predictor (DAMMP) module is proposed to assess the consistency of multi-modal data with the uncertainty map, which determines whether the final prediction results come from multi-modal or uni-modal predictions. We predict the NPs distribution given the TME components of tumor vessels and cell nuclei, and the experimental results show that DAMM-Diffusion can generate the distribution of NPs with higher accuracy than the comparing methods. Additional results on the multi-modal brain image synthesis task further validate the effectiveness of the proposed method. The code is released†.},
  keywords={Nanoparticles;Adaptation models;Computer vision;Uncertainty;Image synthesis;Microprocessors;Computer architecture;Diffusion models;Pattern recognition;Tumors},
  doi={10.1109/CVPR52734.2025.02876},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9848468,
  author={Li, Lan and Chen, Mingju and Shi, Haode and Duan, Zhengxu and Xiong, Xingzhong},
  journal={IEEE Access}, 
  title={Multiscale Structure and Texture Feature Fusion for Image Inpainting}, 
  year={2022},
  volume={10},
  number={},
  pages={82668-82679},
  abstract={In order to achieve interaction between structure and texture information in generative adversarial image inpainting networks and improve the semantic veracity of the restored images, unlike the original two-stage inpainting ideas where texture and structure are restored separately, this paper constructs a multi-scale fusion approach to image generation, which embeds images into two collaborative subtasks, that is, structure generation and texture synthesis under structural constraints. We also introduce a self-attention mechanism into the partial convolution of the encoder to enhance the long range contextual information acquisition of the model in image inpainting, and design a multi-scale fusion network to fuse the generated structure and texture feature, so that the structure and texture information can be reused for reconstruction, perception and style loss compensation, thus enabling the fused images to achieve global consistency. In the training phase, feature matching loss are introduced to enhance the image in terms of structural generation plausibility. Finally, through comparison experiments with other inpainting networks on the CelebA, Paris StreetView and Places2 datasets, it is demonstrated that our method constructed in this paper has better objective evaluation metrics, more effective inpainting of structural and texture information of corrupted images and better image inpainting performance.},
  keywords={Image edge detection;Convolution;Generators;Deep learning;Kernel;Generative adversarial networks;Image reconstruction;Image inpainting;generative model;deep learning;generative adversarial network},
  doi={10.1109/ACCESS.2022.3196021},
  ISSN={2169-3536},
  month={},}@ARTICLE{9653809,
  author={Li, Xiao and Rosman, Guy and Gilitschenski, Igor and Araki, Brandon and Vasile, Cristian-Ioan and Karaman, Sertac and Rus, Daniela},
  journal={IEEE Robotics and Automation Letters}, 
  title={Learning an Explainable Trajectory Generator Using the Automaton Generative Network (AGN)}, 
  year={2022},
  volume={7},
  number={2},
  pages={984-991},
  abstract={Symbolic reasoning is a key component for enabling practical use of data-driven planners in autonomous driving. In that context, deterministic finite state automata (DFA) are often used to formalize the underlying high-level decision-making process. Manual design of an effective DFA can be tedious. In combination with deep learning pipelines, DFA can serve as an effective representation to learn and process complex behavioral patterns. The goal of this work is to leverage that potential. We propose the automaton generative network (AGN), a differentiable representation of DFAs. The resulting neural network module can be used standalone or as an embedded component within a larger architecture. In evaluations on deep learning based autonomous vehicle planning tasks, we demonstrate that incorporating AGN improves the explainability, sample efficiency, and generalizability of the model.},
  keywords={Learning automata;Trajectory;Task analysis;Automata;Neural networks;Robustness;Generators;Learning automata;robot learning;autonomous systems},
  doi={10.1109/LRA.2021.3135940},
  ISSN={2377-3766},
  month={April},}@ARTICLE{10914549,
  author={Xu, Zhenghua and Yang, Runhe and Xu, Zihang and Zhang, Shuo and Yang, Yuchen and Liu, Weipeng and Xu, Weichao and Chen, Junyang and Lukasiewicz, Thomas and Leung, Victor C. M.},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={PCA: Semi-Supervised Segmentation With Patch Confidence Adversarial Training}, 
  year={2025},
  volume={12},
  number={4},
  pages={2473-2486},
  abstract={Deep-learning-based semi-supervised learning (SSL) methods have achieved a strong performance in medical image segmentation, which can alleviate doctors' expensive annotation by utilizing a large amount of unlabeled data. Unlike most existing semi-supervised learning methods, adversarial training methods distinguish samples from different sources by learning the data distribution of the segmentation map, leading the segmenter to generate more accurate predictions. We argue that the current performance restrictions for such approaches are the problems of feature extraction and learning preferences. In this article, we propose a new semi-supervised adversarial method called Patch Confidence Adversarial Training (PCA) for medical image segmentation. The PCA method's discriminator penalizes patch-level structures, guiding the generator to optimize different patch areas, by leveraging pixel context, the generator is driven to focus on high-frequency features, making it harder to deceive the discriminator and easy to converge to an ideal state, which more effectively guides the segmenter to generate high-quality pseudo-labels. Furthermore, at the discriminator's input, we supplement image information constraints, making it simpler to fit the expected data distribution. Extensive experiments on the Automated Cardiac Diagnosis Challenge (ACDC) 2017 dataset and the Brain Tumor Segmentation (BraTS) 2019 challenge dataset show that our method outperforms the state-of-the-art semi-supervised methods, which demonstrates its effectiveness for medical image segmentation.},
  keywords={Image segmentation;Training;Biomedical imaging;Generators;Medical diagnostic imaging;Data models;Data mining;Annotations;Principal component analysis;Predictive models;Semi-supervised learning;adversarial learning;medical image segmentation},
  doi={10.1109/TNSE.2025.3548416},
  ISSN={2327-4697},
  month={July},}@ARTICLE{9789487,
  author={Huang, Huaibo and Luo, Mandi and He, Ran},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Memory Uncertainty Learning for Real-World Single Image Deraining}, 
  year={2023},
  volume={45},
  number={3},
  pages={3446-3460},
  abstract={Single image deraining has witnessed dramatic improvements by training deep neural networks on large-scale synthetic data. However, due to the discrepancy between authentic and synthetic rain images, it is challenging to directly extend existing methods to real-world scenes. To address this issue, we propose a memory-uncertainty guided semi-supervised method to learn rain properties simultaneously from synthetic and real data. The key aspect is developing a stochastic memory network that is equipped with memory modules to record prototypical rain patterns. The memory modules are updated in a self-supervised way, allowing the network to comprehensively capture rainy styles without the need for clean labels. The memory items are read stochastically according to their similarities with rain representations, leading to diverse predictions and efficient uncertainty estimation. Furthermore, we present an uncertainty-aware self-training mechanism to transfer knowledge from supervised deraining to unsupervised cases. An additional target network is adopted to produce pseudo-labels for unlabeled data, of which the incorrect ones are rectified by uncertainty estimates. Finally, we construct a new large-scale image deraining dataset of 10.2 k real rain images, significantly improving the diversity of real rain scenes. Experiments show that our method achieves more appealing results for real-world rain removal than recent state-of-the-art methods.},
  keywords={Rain;Uncertainty;Memory modules;Stochastic processes;Estimation;Predictive models;Visualization;Image deraining;semi-supervised learning;memory networks;uncertainty estimation},
  doi={10.1109/TPAMI.2022.3180560},
  ISSN={1939-3539},
  month={March},}@ARTICLE{10195886,
  author={Li, Yunan and Qi, Tianyu and Ma, Zhuoqi and Quan, Dou and Miao, Qiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Seeking a Hierarchical Prototype for Multimodal Gesture Recognition}, 
  year={2025},
  volume={36},
  number={1},
  pages={198-209},
  abstract={Gesture recognition has drawn considerable attention from many researchers owing to its wide range of applications. Although significant progress has been made in this field, previous works always focus on how to distinguish between different gesture classes, ignoring the influence of inner-class divergence caused by gesture-irrelevant factors. Meanwhile, for multimodal gesture recognition, feature or score fusion in the final stage is a general choice to combine the information of different modalities. Consequently, the gesture-relevant features in different modalities may be redundant, whereas the complementarity of modalities is not exploited sufficiently. To handle these problems, we propose a hierarchical gesture prototype framework to highlight gesture-relevant features such as poses and motions in this article. This framework consists of a sample-level prototype and a modal-level prototype. The sample-level gesture prototype is established with the structure of a memory bank, which avoids the distraction of gesture-irrelevant factors in each sample, such as the illumination, background, and the performers’ appearances. Then the modal-level prototype is obtained via a generative adversarial network (GAN)-based subnetwork, in which the modal-invariant features are extracted and pulled together. Meanwhile, the modal-specific attribute features are used to synthesize the feature of other modalities, and the circulation of modality information helps to leverage their complementarity. Extensive experiments on three widely used gesture datasets demonstrate that our method is effective to highlight gesture-relevant features and can outperform the state-of-the-art methods.},
  keywords={Prototypes;Gesture recognition;Feature extraction;Generative adversarial networks;Convolutional neural networks;Task analysis;Spatiotemporal phenomena;Generative adversarial network (GAN);gesture prototype;gesture recognition;memory bank;multimodal},
  doi={10.1109/TNNLS.2023.3295811},
  ISSN={2162-2388},
  month={Jan},}@ARTICLE{10856011,
  author={Wang, Zhan and Kasongo Dahouda, Mwamba and Hwang, Hyoseong and Joe, Inwhee},
  journal={IEEE Access}, 
  title={Explanatory LSTM-AE-Based Anomaly Detection for Time Series Data in Marine Transportation}, 
  year={2025},
  volume={13},
  number={},
  pages={23195-23208},
  abstract={Ensuring the normal operation of mechanical equipment is crucial in marine transportation, as data anomalies in these systems can lead to serious safety incidents, environmental pollution, and economic losses. To improve the accuracy and efficiency of anomaly detection in ship equipment data, a long-short-term memory auto-encoder model (LSTM-AE) has been developed, tailored for time-series anomaly detection. The normal behavior patterns of key metrics, such as temperature, pressure, and rotational speed, are learned and captured by this model, enabling abnormal states in these metrics to be accurately identified. The approach is based on the encoder in the long-short-term memory (LSTM) network, where input time-series data is converted into a lower-dimensional, implicit representation, and an attempt is made to reconstruct the original input data via a decoder. Trained exclusively on anomaly-free data, the model ensures a low reconstruction error on normal data. However, when input data that significantly deviates from the training set is encountered, a high reconstruction error is produced, thereby allowing potential anomalies to be flagged. To enhance the interpretability of the results, explainable artificial intelligence (XAI) techniques are incorporated, specifically shapley additive explanations (SHAP) and local interpretable model-agnostic explanations (LIME), to identify which features have the most impact on detected anomalies. The LSTM-AE model shows superior performance compared to other data generation models such as GAN and diffusion models, which have accuracy issues and require high computational cost. In addition, the integration of XAI methods has advantages in the interpretation of the results, solving the problem that these existing methods often lack transparency.},
  keywords={Data models;Anomaly detection;Predictive models;Time series analysis;Long short term memory;Marine vehicles;Diffusion models;Generative adversarial networks;Computational modeling;Generators;Long short-term memory auto-encoder;anomaly detection;time series data;interpretation},
  doi={10.1109/ACCESS.2025.3535695},
  ISSN={2169-3536},
  month={},}@ARTICLE{10589634,
  author={Gull, Muqaddas and Arif, Omar},
  journal={IEEE Access}, 
  title={Multi-Label Zero-Shot Learning With Adversarial and Variational Techniques}, 
  year={2024},
  volume={12},
  number={},
  pages={94990-95006},
  abstract={Multi-label zero-shot learning expands upon the traditional single-label zero-shot learning paradigm by addressing the challenge of accurately classifying images containing multiple unseen classes, which are not part of the training data. Current techniques rely on attention mechanisms to tackle the complexities of multi-label zero-shot learning (ZSL) and generalized zero-shot learning (GZSL). However, the generation of features, especially within the context of a generative approach, remains an unexplored area. In this paper, we propose a generative approach that leverages the capabilities of Conditional Variational Autoencoder (CVAE) and Conditional Generative Adversarial Network (CGAN) to enhance the quality of generative data for both multi-label ZSL and GZSL. Additionally, we introduce a novel “Regressor” as a supplementary tool to improve the reconstruction of visual features. This Regressor operates in conjunction with a “cycle-consistency loss” to ensure that the generated features preserve the key qualities of the original features even after undergoing transformations. To gauge the efficacy of our proposed approach, we conducted comprehensive experiments on two widely recognized benchmark datasets: NUS-WIDE and MS COCO. Our evaluation spanned both multi-label ZSL and GZSL scenarios. Notably, our approach yielded significant enhancements in mean Average Precision (mAP) for both datasets. Specifically, we observed a 0.2% increase in performance on the NUS-WIDE dataset and a notable 2.6% improvement on the MS COCO dataset in the context of Multi-label ZSL. The results clearly demonstrate that our generative approach outperforms existing methods on these widely-recognized datasets.},
  keywords={Semantics;Visualization;Generative adversarial networks;Task analysis;Data models;Zero-shot learning;Training;Encoders;Conditional variational autoencoder;conditional generative adversarial network;generalized zero-shot learning;regressor;zero-shot learning},
  doi={10.1109/ACCESS.2024.3425547},
  ISSN={2169-3536},
  month={},}@ARTICLE{10187125,
  author={Ahmad, Wan Siti Halimatul Munirah Wan and Ahmad Fauzi, Mohammad Faizal and Hasan, Md Jahid and Rehman, Zaka Ur and Lee, Jenny Tung Hiong and Khor, See Yee and Looi, Lai-Meng and Abas, Fazly Salleh and Adam, Afzan and Chan, Elaine Wan Ling and Kamata, Sei-Ichiro},
  journal={IEEE Access}, 
  title={Multi-Configuration Analysis of DenseNet Architecture for Whole Slide Image Scoring of ER-IHC}, 
  year={2023},
  volume={11},
  number={},
  pages={79911-79928},
  abstract={Nuclei classification is a mandatory process to obtain scoring information for whole slide images (WSIs). In immunohistochemistry (IHC) staining specifically for estrogen receptor (ER) biomarker, an Allred score based on the proportion and intensity of cancer nuclear staining is widely used in histopathology practice to predict response to hormonal treatment. This manually exhaustive process can be accelerated with the help of computational intelligence. In this article, we present a thorough analysis of 37 WSIs of breast cancer cases with over 2.8 million segmented nuclei. ER-stained nuclei were classified into negative, weak, moderate and strong intensities using DenseNet deep learning architecture, contributing to Allred scoring. Seven different models and configurations were exhaustively analysed in six tests to obtain the scoring reaching the best concordance of 56.8% and 81.1% with the pathologist’s manual score and suggested hormonal treatment. We also discussed in detail the causes that lead to the non-concordances. This study follows the pathologists’ workflow in obtaining the Allred score but is fully automated. It provides a basis for the development of more complex deep learning models, particularly for nuclei classification and achieving accurate scoring of ER-IHC stained WSIs.},
  keywords={Cancer;Task analysis;Image segmentation;Artificial intelligence;Computational modeling;Tumors;Image color analysis;Biomedical image processing;DenseNet;ER-IHC;nuclei classification;PyTorch;TensorFlow;whole slide image},
  doi={10.1109/ACCESS.2023.3296848},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10343974,
  author={Clarisó, Robert and Cabot, Jordi},
  booktitle={2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={Model-Driven Prompt Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={47-54},
  abstract={Generative artificial intelligence (AI) systems are capable of synthesizing complex content such as text, source code or images according to the instructions described in a natural language prompt. The quality of the output depends on crafting a suitable prompt. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems.Through experimentation, the creative and research communities have created guidelines and strategies for creating good prompts. However, even for the same task, these best practices vary depending on the particular system receiving the prompt. Moreover, some systems offer additional features using a custom platform-specific syntax, e.g., assigning a degree of relevance to specific concepts within the prompt.In this paper, we propose applying model-driven engineering to support the prompt engineering process. Using a domain-specific language (DSL), we define platform-independent prompts that can later be adapted to provide good quality outputs in a target AI system. The DSL also facilitates managing prompts by providing mechanisms for prompt versioning and prompt chaining. Tool support is available thanks to a Langium-based Visual Studio Code plugin.},
  keywords={Visualization;Source coding;Natural languages;Syntactics;Model driven engineering;DSL;Artificial intelligence;prompt engineering;model-driven engineering;domain-specific language;generative AI;large language models},
  doi={10.1109/MODELS58315.2023.00020},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8372029,
  author={Yin, Litian and Wang, Dong and Xin, Xin and Ding, Yue},
  booktitle={2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={SoGeM: Social Based Generative Model for Top-N Recommendation}, 
  year={2017},
  volume={},
  number={},
  pages={802-806},
  abstract={Social recommendation which incorporates social information has attracted wide attention across both academia and industry for its superior performance. However, most existing approaches interpret social information in a heuristic manner which is not effective to capture the strong interplay between social connections and behaviors of users. This paper proposes SoGeM (SOcial based GEnerative Model) which simulates user's behaviors in a generative way and models intrinsic preferences and social influences of users simultaneously. Different from the most approaches that preassign similarity weights between friends, SoGeM learns the social influences automatically and quantitatively. Thus, the learnt influence has a probabilistic interpretation, for it is produced along with the generative process. We use Gibbs Sampling to train SoGeM and conduct comprehensive experiments on three real datasets. The results show that SoGeM outperforms other state-of-the-art approaches.},
  keywords={Probabilistic logic;Recommender systems;Measurement;Analytical models;Computational modeling;Switches;Tools;Social Recommendation;Graphical Model;Topic Model},
  doi={10.1109/ICTAI.2017.00126},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{9861685,
  author={Zhou, Zhili and Li, Yujiang and Li, Jin and Yu, Keping and Kou, Guang and Wang, Meimin and Gupta, Brij Bhooshan},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={GAN-Siamese Network for Cross-Domain Vehicle Re-Identification in Intelligent Transport Systems}, 
  year={2023},
  volume={10},
  number={5},
  pages={2779-2790},
  abstract={The vehicle re-identification (Re-ID) has become one of most important techniques for tracking vehicles in intelligent transport system. Vehicle Re-ID aims at matching identical vehicle images captured by different surveillance cameras. Recent vehicle Re-ID approaches explored deep learning-based features or distance metric learning methods for vehicle matching. However, most of the existing approaches focus on the vehicle Re-ID in the same domain, but ignore the challenging cross-domain problem, i.e., identifying the identical vehicles in different domains including the day-time and night-time domain. To tackle this problem, we propose a GAN-Siamese network structure for vehicle Re-ID. In this network structure, a generative adversarial network (GAN)-based domain transformer is employed to transform the domains of two input vehicle images to another domains, and then a four-branch Siamese network is designed to learn two distance metrics between the images in the two domains, respectively. Finally, the two distances are fused to measure the final similarity between the two input images for vehicle Re-ID. Experimental results demonstrate the proposed GAN-Siamese network structure achieves the state-of-the-art performances on four large-scale vehicle datasets, i.e., VehicleID, VERI-Wild, VERI-Wild 2.0, and VeRi776.},
  keywords={Measurement;Task analysis;Feature extraction;Generative adversarial networks;Transformers;Visualization;Transforms;Cross-domain matching;Domain transformer;GAN-Siamese network;Distance metric learning;Vehicle Re-ID},
  doi={10.1109/TNSE.2022.3199919},
  ISSN={2327-4697},
  month={Sep.},}@INPROCEEDINGS{9156345,
  author={Zhang, Yang and Tsang, Ivor W. and Luo, Yawei and Hu, Chang-Hui and Lu, Xiaobo and Yu, Xin},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Copy and Paste GAN: Face Hallucination From Shaded Thumbnails}, 
  year={2020},
  volume={},
  number={},
  pages={7353-7362},
  abstract={Existing face hallucination methods based on convolutional neural networks (CNN) have achieved impressive performance on low-resolution (LR) faces in a normal illumination condition. However, their performance degrades dramatically when LR faces are captured in low or non-uniform illumination conditions. This paper proposes a Copy and Paste Generative Adversarial Network (CPGAN) to recover authentic high-resolution (HR) face images while compensating for low and non-uniform illumination. To this end, we develop two key components in our CPGAN: internal and external Copy and Paste nets (CPnets). Specifically, our internal CPnet exploits facial information residing in the input image to enhance facial details; while our external CPnet leverages an external HR face for illumination compensation. A new illumination compensation loss is thus developed to capture illumination from the external guided face image effectively. Furthermore, our method offsets illumination and upsamples facial details alternatively in a coarse-to-fine fashion, thus alleviating the correspondence ambiguity between LR inputs and external HR inputs. Extensive experiments demonstrate that our method manifests authentic HR face images in a uniform illumination condition and outperforms state-of-the-art methods qualitatively and quantitatively.},
  keywords={Face;Lighting;Gallium nitride;Image resolution;Feature extraction;Generative adversarial networks;Automation},
  doi={10.1109/CVPR42600.2020.00738},
  ISSN={2575-7075},
  month={June},}@ARTICLE{8895806,
  author={Yin, Yingjie and Xu, De and Wang, Xingang and Zhang, Lei},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Adversarial Feature Sampling Learning for Efficient Visual Tracking}, 
  year={2020},
  volume={17},
  number={2},
  pages={847-857},
  abstract={The tracking-by-detection tracking framework usually consists of two stages: drawing samples around the target object and classifying each sample as either the target object or background. Current popular trackers under this framework typically draw many samples from the raw image and feed them into the deep neural networks, resulting in high computational burden and low tracking speed. In this article, we propose an adversarial feature sampling learning (AFSL) method to address this problem. A convolutional neural network is designed, which takes only one cropped image around the target object as input, and samples are collected from the feature maps with spatial bilinear resampling. To enrich the appearance variations of positive samples in the feature space, which has limited spatial resolution, we fuse the high-level features and low-level features to better describe the target by using a generative adversarial network. Extensive experiments on benchmark data sets demonstrate that the proposed ASFL achieves leading tracking accuracy while significantly accelerating the speed of tracking-by-detection trackers.},
  keywords={Target tracking;Visualization;Generative adversarial networks;Spatial resolution;Semantics;Computational modeling;Adversarial learning;deep convolution neural network;feature sampling;visual tracking},
  doi={10.1109/TASE.2019.2948402},
  ISSN={1558-3783},
  month={April},}@INPROCEEDINGS{10365409,
  author={Favela, Luis H. and Amon, Mary Jean},
  booktitle={2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)}, 
  title={The ethics of human digital twins: Counterfeit people, personhood, and the right to privacy}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent years, generative artificial intelligence (AI) in the form of large language models (LLM) have sparked the interest of society at large. The perceived capabilities of such systems have reignited discussions concerning the actual or potential threats posed by AI. According to Daniel Dennett, these systems make possible the creation of counterfeit people, who can pass as real in digital environments like social media. Dennett claims that by undermining trust in relationships, counterfeit people pose a threat to democracy and human freedom. While the idea of counterfeit people is worrisome in the context of digital manipulation, we claim that human digital twins have the potential to facilitate human rights violations that may pose even greater challenges. High-fidelity human digital twins necessitate encroaching into features that constitute a human’s personhood, such as physical aspects and mental contents. In view of that, their creation raises pressing issues of consent and violations of privacy rights. As a result, because rights to privacy are rights of persons, such violations will simultaneously be human rights violations. Even with consent to use an individual’s data, human digital twins may still cause issues of personhood. The rapid adoption of technologies that facilitate counterfeit people and human digital twins demands that ethical issues not be treated as aside concerns, but at the forefront of technology development.},
  keywords={Privacy;Ethics;Social networking (online);Metaverse;Avatars;Pressing;Companies;digital twin;ethics;personhood;privacy;rights},
  doi={10.1109/DTPI59677.2023.10365409},
  ISSN={},
  month={Nov},}@ARTICLE{10556626,
  author={Liu, Yang and Li, Qingyong and Yao, Zhigang and Jiang, Jun and Qiu, Zhijun and Wang, Wen},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={CLDiff: Weakly Supervised Cloud Detection With Denoising Diffusion Probabilistic Models}, 
  year={2024},
  volume={62},
  number={},
  pages={1-18},
  abstract={Cloud detection is an essential step in remote sensing (RS) image processing, contributing to various applications. However, existing fully supervised cloud detection methods rely on massive pixel-wise annotations, which are expensive and time-consuming. To alleviate the annotation burden, weakly supervised cloud detection (WSCD) has received extensive attention recently. One standard approach performs cloud detection within a classification paradigm, which inevitably faces category ambiguity when detecting semitransparent clouds. To tackle this problem, we propose a novel WSCD framework based on the diffusion model, termed CLDiff. Specifically, a multiscale feature rectification (MFR) module is introduced to extract multiscale semantic features in the encoder, enabling a definite identification of clouds and mitigating interference from bright objects in the background. Considering that clouds exhibit varying optical thicknesses, a diffusion decoder is developed to model the intraclass variations of clouds in a generative strategy, improving thin cloud detection. Initially, it devises a Gaussian modulation function to recalibrate ambiguous cloud activations and emphasize semitransparent clouds. Subsequently, these modulated activations serve as semantic guidance to optimize the diffusion process. This approach enables CLDiff to activate cloud contours under definite semantic conditions and avoids the additional branches for semantic learning as found in previous methods. Experimental results demonstrate that CLDiff achieves state-of-the-art performance in WSCD. A public reference implementation of this work in PyTorch is available at https://github.com/YLiu-creator/CLDiff.},
  keywords={Clouds;Semantics;Feature extraction;Annotations;Decoding;Remote sensing;Generative adversarial networks;Diffusion model;remote sensing (RS) image;semantic segmentation;weakly supervised cloud detection (WSCD)},
  doi={10.1109/TGRS.2024.3413892},
  ISSN={1558-0644},
  month={},}@ARTICLE{10398515,
  author={Cui, Qi and Zhou, Zhili and Meng, Ruohan and Wang, Shaowei and Yan, Hongyang and Wu, Q. M. Jonathan},
  journal={IEEE Transactions on Multimedia}, 
  title={ARES: On Adversarial Robustness Enhancement for Image Steganographic Cost Learning}, 
  year={2024},
  volume={26},
  number={},
  pages={6542-6553},
  abstract={Taking the steganalytic discriminators as the adversaries, the existing Generative Adversarial Networks (GAN)-based steganographic approaches learn the implicit cost functions to measure the embedding distortion for steganography. However, the steganalytic discriminators in these approaches are trained by the stego-samples with insufficient diversity, and their network structures offer very limited representational capacity. As a result, these steganalytic discriminators will not exhibit robustness to various steganographic patterns, which causes learning suboptimal cost functions, thus compromising the anti-steganalysis capability. To address this issue, we propose a novel GAN-based steganographic approach, in which the Diversified Inverse-Adversarial Training (DIAT) strategy and the Steganalytic Feature Attention (SteFA) structure are designed to train a robust steganalytic discriminator. Specifically, the DIAT strategy provides the steganalytic discriminator with an expanded feature space by generating diversified adversarial stego-samples; the SteFA structure enables the steganalytic discriminator to capture more various steganalytic features by employing the channel-attention mechanism on higher-order statistics. Consequently, the steganalytic discriminator can build a more precise decision boundary to make it more robust, which facilitates learning a superior steganographic cost function. Extensive experiments demonstrate that the proposed steganographic approach achieves promising anti-steganalysis capability over the state-of-the-arts under the same embedding payloads.},
  keywords={Cost function;Steganography;Costs;Generators;Feature extraction;Training;Generative adversarial networks;Steganography;GAN;Adversarial training;Cost learning},
  doi={10.1109/TMM.2024.3353543},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10486027,
  author={Wu, Ling and Zhang, Jun},
  booktitle={2024 IEEE 3rd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={High-Fidelity StyleGAN Inversion and Attribute Editing for Low Light Images}, 
  year={2024},
  volume={},
  number={},
  pages={1559-1563},
  abstract={Recently, many studies have been explored the latent space of Generative Adversarial Networks (GANs), especially the W-space of StyleGAN. Real-world image manipulation has made great progress, and most of them follow the “invert first, edit later” methodology. These methods use well light images as input, but it is challenging for low light image captured in extremely weak illumination conditions. Therefore, applying the existing StyleGAN inversion techniques under such conditions leads to serious image distortion. In this study, an embedding module is proposed for image inversion and editing of dark faces in the real world. The module can be embedded in popular StyleGAN inversion techniques to enhance low-quality dark face images and provide high-quality input for subsequent inversion editing. It is called the SE module. The structure of the SE module is designed by combining the U-N et structure with the CBAM attention module. This has two benefits. First, it facilitates capturing complete information from the images. Second, the incorporation of the attention mechanism enables the network model to focus more on the crucial features of the images. This generalized embedding module effectively alleviates the shortcomings of dark face image inversion, achieve high-quality inversion of dark images. The experiments demonstrate the superiority of the module.},
  keywords={Electrical engineering;Lighting;Big Data;Generative adversarial networks;Distortion;Space exploration;Task analysis;GAN inversion;deep learning;image manipulation},
  doi={10.1109/EEBDA60612.2024.10486027},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10191189,
  author={Chang, Ching-Yu and Ye, Chun-Ting and Wei, Tzer-Jen},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Image-to-Image Translation on Defined Highlighting Regions by Semi-Supervised Semantic Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Image-to-image translations made remarkable performance in Generative Adversarial Network (GAN). While recent advances are easily generated a high-quality synthesized images, it usually remains a problem to recognize complicated scenarios. We believe that a few human annotations can greatly reduce the problems. In this paper, we propose Highlight-IT, which generates synthesized images and its corresponding pixel-level semantic segmentation. In addition, segmentation can be viewed as a strong prior and guide our framework to focus on human-defined important regions. In evaluation, we experiment with various categories of unlabeled and labeled datasets. The results show that our method achieves the quality of images of the state-of-the-art framework and also the performance of the famous semantic segmentation framework. In the end, we demonstrate the qualitative results of our work and the approaches proposed by others.},
  keywords={Sensitivity;Image recognition;Convolution;Annotations;Semantic segmentation;Neural networks;Generative adversarial networks},
  doi={10.1109/IJCNN54540.2023.10191189},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10845360,
  author={Zhang, Chunhui and Liang, Yanyue and Gao, Zhiwei and Zhao, Li},
  booktitle={2024 12th International Conference on Information Systems and Computing Technology (ISCTech)}, 
  title={Electromagnetic Signal Augmentation of Pantograph Arcs Using an Improved DCGAN}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In response to the insufficient sample issue in the electromagnetic signal data of high-speed railway pantograph arcs, this study proposes an improved Deep Convolutional Generative Adversarial Network (DCGAN) to augment image datasets. By optimizing the network structure, the proposed method significantly improves the quality of generated images and introduces a lightweight design that reduces the computational demands of the model, making it more suitable for resource-constrained environments. After 5,000 iterations, the proposed method achieves a Fréchet Inception Distance (FID) score of 36.33, compared to 45.54 for the original DCGAN and 50.21 for the WGAN. Moreover, the Inception Score (IS) of the proposed method is 1.91 ± 0.11, outperforming the original DCGAN (1.81 ± 0.23) and WGAN (1.77 ± 0.04), further demonstrating its effectiveness. This method provides a new technical reference for research on electromagnetic signals in high-speed railways.},
  keywords={Convolution;Computational modeling;Noise;Generative adversarial networks;Rail transportation;Robustness;Real-time systems;Electromagnetics;Signal analysis;Optimization;Electromagnetic signal;DCGAN;image augmentation;lightweight network},
  doi={10.1109/ISCTech63666.2024.10845360},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10424949,
  author={Cai, Qing and Zhang, Xiangyu and Ding, Haolun and Tao, Ran},
  booktitle={2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)}, 
  title={Efficient Information Recognition for Machine-printed Invoices}, 
  year={2023},
  volume={},
  number={},
  pages={913-918},
  abstract={Automatic recognition of information in machine-printed invoice images is significant to enterprises in saving labor. However, most invoice information identification methods rely on high-quality invoice images, and collecting these images requires the purchase of professional and expensive image acquisition equipment. In this paper, we propose a novel invoice recognition framework capable of identifying lower-quality invoice images captured by devices such as mobile phones. we present an image preprocessing algorithm to correct distorted invoice images based on perspective projection. Then, accurate content Key-Value (KV) structures are extracted by U-net and template matching combined with the coordinate fine-tuning method, and the Robust text recognizer with Automatic Rectification (RARE) is adapted to recognize characters. To further improve the proposed model in small data scenarios, we adapt the Generative Adversarial Networks (GAN) to extend training samples and improve the RARE by active learning method. Extensive experiments are conducted on both machine-printed invoice images collected from actual scenarios and the open-source ICDAR dataset. The comparison results demonstrate the superiority of the proposed method on both sufficient and small data scenarios.},
  keywords={Learning systems;Training;Image recognition;Text recognition;Training data;Generative adversarial networks;Character recognition;invoice recognition;text recognition;Form detection and recognition;active learning},
  doi={10.1109/ICICML60161.2023.10424949},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10731622,
  author={Zuo, Xiaoying and Du, Yue and Zhao, Xin},
  booktitle={2024 8th International Conference on Biomedical Engineering and Applications (ICBEA)}, 
  title={Category-level and Instance-level Synergistic Alignment Applied in Unsupervised Domain Adaptation for Pancreatic Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={6-11},
  abstract={In medical image segmentation, significant feature differences exist between medical images with different acquisition parameters or modes, leading to decreased performance of deep neural networks in clinical scenarios. Specifically, in pancreas organ segmentation, the organ’s intricate morphology, the high cost of manual annotation by medical professionals, and challenges posed by multimodal imaging data contribute to the difficulty of pancreas organ segmentation. Therefore, this paper proposes an unsupervised domain adaptation (UDA) approach aimed at leveraging publicly available source domain data with rich label information for pancreas organ segmentation, thereby improving segmentation performance in the target domain. The proposed method is based on Generative Adversarial Networks (GANs) and achieves source-to-target domain image translation through image-to-image transformation and feature alignment. This leads to an end-to-end unsupervised domain adaptation framework for pancreas organ segmentation. The method utilizes a feature fusion module to explore domain-invariant features and employs two orthogonal classifiers for category alignment. Through adversarial learning and deep supervision mechanisms, it integrates feature alignment and category alignment to enhance segmentation performance. In the experimental section, we utilized two publicly available datasets as the source and target domains and validated the effectiveness of the proposed method through performance comparison experiments. The experimental results demonstrate a significant improvement in segmentation performance on unlabeled target images. Specifically, the unsupervised semantic segmentation achieved a Dice coefficient of 62.2% on the publicly available datasets. This provides valuable reference and guidance for research and application in the field of medical image segmentation.},
  keywords={Image segmentation;Costs;Semantic segmentation;Morphology;Biological systems;Manuals;Generative adversarial networks;Pancreas;Biomedical imaging;Biomedical engineering;pancreas segmentation;unsupervised domain adaptation;neural network;medical image},
  doi={10.1109/ICBEA62825.2024.00011},
  ISSN={},
  month={March},}@INPROCEEDINGS{9635939,
  author={Yan, Fujian and Wang, Dali and He, Hongsheng},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Comprehension of Spatial Constraints by Neural Logic Learning from a Single RGB-D Scan}, 
  year={2021},
  volume={},
  number={},
  pages={9008-9013},
  abstract={Autonomous industrial assembly relies on the precise measurement of spatial constraints as designed by computer-aided design (CAD) software such as SolidWorks. This paper proposes a framework for an intelligent industrial robot to understand the spatial constraints for model assembly. An extended generative adversary network (GAN) with a 3D long short-term memory (LSTM) network was designed to composite 3D point clouds from a single RGB-D scan. The spatial constraints of the segmented point clouds are identified by a neural-logic network that incorporates general knowledge of spatial constraints in terms of first-order logic. The model was designed to comprehend a complete set of spatial constraints that are consistent with industrial CAD software, including left, right, above, below, front, behind, parallel, perpendicular, concentric, and coincident relations. The accuracy of 3D model composition and spatial constraint identification was evaluated by the RGB-D scans and 3D models in the ABC dataset. The proposed model achieved 57.23% intersection over union (IoU) in 3D model composition, and over 99% in comprehending all spatial constraints.},
  keywords={Point cloud compression;Knowledge engineering;Solid modeling;Three-dimensional displays;Design automation;Service robots;Generative adversarial networks;spatial constraints;neural-logic learning;logic rules},
  doi={10.1109/IROS51168.2021.9635939},
  ISSN={2153-0866},
  month={Sep.},}@INPROCEEDINGS{10887885,
  author={Li, Guohao and Yang, Hongyu and Men, Yifang and Huang, Di and Li, Weixin and Yang, Ruijie and Wang, Yunhong},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Generating Editable Head Avatars with 3D Gaussian GANs}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photo-realistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.},
  keywords={Training;Solid modeling;Three-dimensional displays;Head;Accuracy;Avatars;Superresolution;Animation;Neural radiance field;Generative adversarial networks;3D-aware GAN;Image and video synthesis},
  doi={10.1109/ICASSP49660.2025.10887885},
  ISSN={2379-190X},
  month={April},}
